Loading openmpi/cuda/64/3.1.4
  Loading requirement: hpcx/2.4.0
Loading pytorch-py36-cuda10.1-gcc/1.5.0
  Loading requirement: python36 ml-pythondeps-py36-cuda10.1-gcc/3.3.0
    openblas/dynamic/0.2.20 cudnn7.6-cuda10.1/7.6.5.32 hdf5_18/1.8.20
    nccl2-cuda10.1-gcc/2.7.8
Run:  0
 # ------------------ Running pipeline on bts_startsame color run_0 -------------------- #
cuda:0
 ------ Pipeline with following parameters ------
training_root_path :  /u/erdos/students/xcui32/cnslab/datasets/VOC2012/VOC2012_filtered/train
val_root_path :  /u/erdos/students/xcui32/cnslab/datasets/VOC2012/VOC2012_filtered/val
test_root_path :  /u/erdos/students/xcui32/cnslab/datasets/VOC2012/VOC2012_filtered/test
dataset_name :  VOC
target_distances :  [0.2, 0.4, 0.6, 0.8, 1]
training_mode :  bts_startsame
training_size :  None
background :  color
size :  (150, 150)
cls_to_use :  ['aeroplane', 'bicycle', 'bird', 'boat', 'car', 'cat', 'train', 'tvmonitor']
batch_size :  128
epochs :  150
resize_method :  long
n_folds :  5
num_workers :  16
model_name :  alexnet
device :  cuda:0
random_seed :  40
result_dirpath :  /u/erdos/students/xcui32/cnslab/results/VOC8AlexnetBlackCUR
save_checkpoints :  False
save_progress_checkpoints :  False
verbose :  0
 ---  Loading datasets ---
 ---  Running  ---
Parameters: --------------------
{'scheduler_kwargs': {'mode': 'min', 'factor': 0.1, 'patience': 5}, 'optim_kwargs': {'lr': 0.001, 'momentum': 0.9}, 'max_norm': None, 'val_target': 'current', 'patience': 30, 'early_stopping': True, 'scheduler_object': None, 'optimizer_object': <class 'torch.optim.sgd.SGD'>, 'criterion_object': <class 'torch.nn.modules.loss.CrossEntropyLoss'>, 'self': <pipelineCV2.RunModel object at 0x2aac786f5dd8>}
--------------------
Fold: 0
----- Training alexnet with sequence: [1] -----
Current group: 1
Epoch [1/150], Training Loss: 41.5417, Validation Loss Current: 10.3717, Validation Loss AVG: 10.3704, lr: 0.001
Epoch [2/150], Training Loss: 41.4215, Validation Loss Current: 10.3410, Validation Loss AVG: 10.3383, lr: 0.001
Epoch [3/150], Training Loss: 41.2359, Validation Loss Current: 10.3093, Validation Loss AVG: 10.3042, lr: 0.001
Epoch [4/150], Training Loss: 41.1575, Validation Loss Current: 10.2718, Validation Loss AVG: 10.2661, lr: 0.001
Epoch [5/150], Training Loss: 40.9298, Validation Loss Current: 10.2382, Validation Loss AVG: 10.2313, lr: 0.001
Epoch [6/150], Training Loss: 41.0454, Validation Loss Current: 10.1933, Validation Loss AVG: 10.1817, lr: 0.001
Epoch [7/150], Training Loss: 40.5001, Validation Loss Current: 10.1470, Validation Loss AVG: 10.1351, lr: 0.001
Epoch [8/150], Training Loss: 40.0015, Validation Loss Current: 10.0750, Validation Loss AVG: 10.0498, lr: 0.001
Epoch [9/150], Training Loss: 40.7307, Validation Loss Current: 10.0243, Validation Loss AVG: 10.0246, lr: 0.001
Epoch [10/150], Training Loss: 40.1426, Validation Loss Current: 10.0278, Validation Loss AVG: 10.0174, lr: 0.001
Epoch [11/150], Training Loss: 39.8706, Validation Loss Current: 9.9762, Validation Loss AVG: 9.9834, lr: 0.001
Epoch [12/150], Training Loss: 40.1082, Validation Loss Current: 9.9510, Validation Loss AVG: 9.9880, lr: 0.001
Epoch [13/150], Training Loss: 39.9910, Validation Loss Current: 9.9317, Validation Loss AVG: 9.9885, lr: 0.001
Epoch [14/150], Training Loss: 39.6243, Validation Loss Current: 9.9307, Validation Loss AVG: 9.9918, lr: 0.001
Epoch [15/150], Training Loss: 39.7345, Validation Loss Current: 9.9052, Validation Loss AVG: 9.9999, lr: 0.001
Epoch [16/150], Training Loss: 39.7980, Validation Loss Current: 9.8922, Validation Loss AVG: 9.9918, lr: 0.001
Epoch [17/150], Training Loss: 39.7184, Validation Loss Current: 9.8934, Validation Loss AVG: 10.0143, lr: 0.001
Epoch [18/150], Training Loss: 39.6933, Validation Loss Current: 9.8579, Validation Loss AVG: 9.9990, lr: 0.001
Epoch [19/150], Training Loss: 39.1769, Validation Loss Current: 9.8486, Validation Loss AVG: 10.0320, lr: 0.001
Epoch [20/150], Training Loss: 39.8594, Validation Loss Current: 9.8322, Validation Loss AVG: 10.0607, lr: 0.001
Epoch [21/150], Training Loss: 38.7500, Validation Loss Current: 9.8380, Validation Loss AVG: 10.0262, lr: 0.001
Epoch [22/150], Training Loss: 39.3268, Validation Loss Current: 9.8008, Validation Loss AVG: 10.0080, lr: 0.001
Epoch [23/150], Training Loss: 39.3537, Validation Loss Current: 9.7353, Validation Loss AVG: 10.0516, lr: 0.001
Epoch [24/150], Training Loss: 39.5549, Validation Loss Current: 9.7370, Validation Loss AVG: 10.0028, lr: 0.001
Epoch [25/150], Training Loss: 39.2243, Validation Loss Current: 9.7218, Validation Loss AVG: 10.0093, lr: 0.001
Epoch [26/150], Training Loss: 38.5994, Validation Loss Current: 9.6741, Validation Loss AVG: 9.9986, lr: 0.001
Epoch [27/150], Training Loss: 38.6637, Validation Loss Current: 9.6554, Validation Loss AVG: 10.0772, lr: 0.001
Epoch [28/150], Training Loss: 38.4772, Validation Loss Current: 9.6134, Validation Loss AVG: 10.0856, lr: 0.001
Epoch [29/150], Training Loss: 38.8508, Validation Loss Current: 9.5863, Validation Loss AVG: 9.9276, lr: 0.001
Epoch [30/150], Training Loss: 38.3079, Validation Loss Current: 9.5920, Validation Loss AVG: 10.3604, lr: 0.001
Epoch [31/150], Training Loss: 37.6189, Validation Loss Current: 9.5053, Validation Loss AVG: 10.2595, lr: 0.001
Epoch [32/150], Training Loss: 37.8570, Validation Loss Current: 9.4549, Validation Loss AVG: 9.9610, lr: 0.001
Epoch [33/150], Training Loss: 37.3623, Validation Loss Current: 9.3483, Validation Loss AVG: 9.9255, lr: 0.001
Epoch [34/150], Training Loss: 37.7034, Validation Loss Current: 9.2719, Validation Loss AVG: 9.8560, lr: 0.001
Epoch [35/150], Training Loss: 36.1511, Validation Loss Current: 9.3755, Validation Loss AVG: 10.2658, lr: 0.001
Epoch [36/150], Training Loss: 36.8054, Validation Loss Current: 9.2808, Validation Loss AVG: 10.3695, lr: 0.001
Epoch [37/150], Training Loss: 37.0682, Validation Loss Current: 9.5662, Validation Loss AVG: 10.1354, lr: 0.001
Epoch [38/150], Training Loss: 40.7826, Validation Loss Current: 10.2571, Validation Loss AVG: 10.3172, lr: 0.001
Epoch [39/150], Training Loss: 39.2302, Validation Loss Current: 9.5160, Validation Loss AVG: 9.6270, lr: 0.001
Epoch [40/150], Training Loss: 36.7097, Validation Loss Current: 9.0749, Validation Loss AVG: 9.6783, lr: 0.001
Epoch [41/150], Training Loss: 36.0224, Validation Loss Current: 8.9733, Validation Loss AVG: 9.4483, lr: 0.001
Epoch [42/150], Training Loss: 35.7977, Validation Loss Current: 8.7903, Validation Loss AVG: 9.3711, lr: 0.001
Epoch [43/150], Training Loss: 34.0954, Validation Loss Current: 8.7737, Validation Loss AVG: 9.3035, lr: 0.001
Epoch [44/150], Training Loss: 32.8556, Validation Loss Current: 8.9887, Validation Loss AVG: 9.7513, lr: 0.001
Epoch [45/150], Training Loss: 34.1333, Validation Loss Current: 9.0521, Validation Loss AVG: 10.6643, lr: 0.001
Epoch [46/150], Training Loss: 35.4065, Validation Loss Current: 8.8016, Validation Loss AVG: 9.6146, lr: 0.001
Epoch [47/150], Training Loss: 33.6558, Validation Loss Current: 8.7537, Validation Loss AVG: 9.4228, lr: 0.001
Epoch [48/150], Training Loss: 32.7110, Validation Loss Current: 8.5483, Validation Loss AVG: 9.2590, lr: 0.001
Epoch [49/150], Training Loss: 33.1942, Validation Loss Current: 8.5586, Validation Loss AVG: 9.4090, lr: 0.001
Epoch [50/150], Training Loss: 31.9456, Validation Loss Current: 8.3828, Validation Loss AVG: 9.0679, lr: 0.001
Epoch [51/150], Training Loss: 31.5442, Validation Loss Current: 8.3823, Validation Loss AVG: 9.1726, lr: 0.001
Epoch [52/150], Training Loss: 30.8739, Validation Loss Current: 8.4550, Validation Loss AVG: 9.2198, lr: 0.001
Epoch [53/150], Training Loss: 31.1622, Validation Loss Current: 9.1475, Validation Loss AVG: 10.5112, lr: 0.001
Epoch [54/150], Training Loss: 33.3329, Validation Loss Current: 8.0144, Validation Loss AVG: 9.1192, lr: 0.001
Epoch [55/150], Training Loss: 32.6170, Validation Loss Current: 8.1888, Validation Loss AVG: 9.0043, lr: 0.001
Epoch [56/150], Training Loss: 30.3018, Validation Loss Current: 8.1248, Validation Loss AVG: 9.0578, lr: 0.001
Epoch [57/150], Training Loss: 30.3565, Validation Loss Current: 8.2207, Validation Loss AVG: 9.3854, lr: 0.001
Epoch [58/150], Training Loss: 30.4644, Validation Loss Current: 7.8568, Validation Loss AVG: 9.3223, lr: 0.001
Epoch [59/150], Training Loss: 32.2987, Validation Loss Current: 7.6768, Validation Loss AVG: 8.8319, lr: 0.001
Epoch [60/150], Training Loss: 29.6803, Validation Loss Current: 7.6705, Validation Loss AVG: 8.9378, lr: 0.001
Epoch [61/150], Training Loss: 29.1609, Validation Loss Current: 7.8780, Validation Loss AVG: 8.8040, lr: 0.001
Epoch [62/150], Training Loss: 30.1190, Validation Loss Current: 8.9604, Validation Loss AVG: 10.8519, lr: 0.001
Epoch [63/150], Training Loss: 31.7338, Validation Loss Current: 7.6918, Validation Loss AVG: 8.8765, lr: 0.001
Epoch [64/150], Training Loss: 28.9309, Validation Loss Current: 7.3745, Validation Loss AVG: 8.7190, lr: 0.001
Epoch [65/150], Training Loss: 27.2085, Validation Loss Current: 7.4665, Validation Loss AVG: 9.2133, lr: 0.001
Epoch [66/150], Training Loss: 27.7347, Validation Loss Current: 7.5901, Validation Loss AVG: 10.2895, lr: 0.001
Epoch [67/150], Training Loss: 29.5811, Validation Loss Current: 8.2495, Validation Loss AVG: 11.3621, lr: 0.001
Epoch [68/150], Training Loss: 30.8168, Validation Loss Current: 7.4154, Validation Loss AVG: 8.7272, lr: 0.001
Epoch [69/150], Training Loss: 29.3869, Validation Loss Current: 7.2871, Validation Loss AVG: 9.3183, lr: 0.001
Epoch [70/150], Training Loss: 28.9739, Validation Loss Current: 7.6943, Validation Loss AVG: 8.7470, lr: 0.001
Epoch [71/150], Training Loss: 28.0872, Validation Loss Current: 7.1186, Validation Loss AVG: 8.9948, lr: 0.001
Epoch [72/150], Training Loss: 27.8673, Validation Loss Current: 7.2652, Validation Loss AVG: 9.3531, lr: 0.001
Epoch [73/150], Training Loss: 26.7751, Validation Loss Current: 7.5975, Validation Loss AVG: 9.0978, lr: 0.001
Epoch [74/150], Training Loss: 27.0362, Validation Loss Current: 6.9718, Validation Loss AVG: 9.1437, lr: 0.001
Epoch [75/150], Training Loss: 25.4934, Validation Loss Current: 7.7248, Validation Loss AVG: 9.7031, lr: 0.001
Epoch [76/150], Training Loss: 25.5570, Validation Loss Current: 7.8971, Validation Loss AVG: 9.3271, lr: 0.001
Epoch [77/150], Training Loss: 25.6081, Validation Loss Current: 7.3115, Validation Loss AVG: 8.9128, lr: 0.001
Epoch [78/150], Training Loss: 27.7789, Validation Loss Current: 7.3491, Validation Loss AVG: 11.0677, lr: 0.001
Epoch [79/150], Training Loss: 30.3000, Validation Loss Current: 8.2698, Validation Loss AVG: 9.8429, lr: 0.001
Epoch [80/150], Training Loss: 31.3146, Validation Loss Current: 7.4152, Validation Loss AVG: 9.5426, lr: 0.001
Epoch [81/150], Training Loss: 26.6358, Validation Loss Current: 7.2319, Validation Loss AVG: 8.8441, lr: 0.001
Epoch [82/150], Training Loss: 26.4043, Validation Loss Current: 6.8145, Validation Loss AVG: 8.7020, lr: 0.001
Epoch [83/150], Training Loss: 25.2485, Validation Loss Current: 6.8427, Validation Loss AVG: 9.2093, lr: 0.001
Epoch [84/150], Training Loss: 24.4550, Validation Loss Current: 6.7710, Validation Loss AVG: 8.7446, lr: 0.001
Epoch [85/150], Training Loss: 25.7610, Validation Loss Current: 7.8632, Validation Loss AVG: 12.2342, lr: 0.001
Epoch [86/150], Training Loss: 28.1322, Validation Loss Current: 6.7765, Validation Loss AVG: 8.5430, lr: 0.001
Epoch [87/150], Training Loss: 26.4514, Validation Loss Current: 6.7660, Validation Loss AVG: 8.3880, lr: 0.001
Epoch [88/150], Training Loss: 24.1608, Validation Loss Current: 6.9312, Validation Loss AVG: 9.4394, lr: 0.001
Epoch [89/150], Training Loss: 25.2272, Validation Loss Current: 7.6563, Validation Loss AVG: 9.3893, lr: 0.001
Epoch [90/150], Training Loss: 25.5439, Validation Loss Current: 6.8238, Validation Loss AVG: 10.0646, lr: 0.001
Epoch [91/150], Training Loss: 23.2612, Validation Loss Current: 6.7437, Validation Loss AVG: 8.5435, lr: 0.001
Epoch [92/150], Training Loss: 23.3970, Validation Loss Current: 6.5650, Validation Loss AVG: 9.1958, lr: 0.001
Epoch [93/150], Training Loss: 22.3136, Validation Loss Current: 6.5301, Validation Loss AVG: 9.9702, lr: 0.001
Epoch [94/150], Training Loss: 23.1277, Validation Loss Current: 6.6960, Validation Loss AVG: 9.7947, lr: 0.001
Epoch [95/150], Training Loss: 24.7775, Validation Loss Current: 6.5236, Validation Loss AVG: 8.9822, lr: 0.001
Epoch [96/150], Training Loss: 22.0844, Validation Loss Current: 6.6242, Validation Loss AVG: 8.8272, lr: 0.001
Epoch [97/150], Training Loss: 24.9980, Validation Loss Current: 6.8824, Validation Loss AVG: 9.3010, lr: 0.001
Epoch [98/150], Training Loss: 24.9490, Validation Loss Current: 6.5107, Validation Loss AVG: 8.7879, lr: 0.001
Epoch [99/150], Training Loss: 23.3982, Validation Loss Current: 6.6909, Validation Loss AVG: 9.1512, lr: 0.001
Epoch [100/150], Training Loss: 24.6844, Validation Loss Current: 6.7708, Validation Loss AVG: 8.4859, lr: 0.001
Epoch [101/150], Training Loss: 22.9426, Validation Loss Current: 6.5159, Validation Loss AVG: 8.9996, lr: 0.001
Epoch [102/150], Training Loss: 21.3802, Validation Loss Current: 6.3961, Validation Loss AVG: 9.5147, lr: 0.001
Epoch [103/150], Training Loss: 22.3048, Validation Loss Current: 7.1713, Validation Loss AVG: 11.4030, lr: 0.001
Epoch [104/150], Training Loss: 23.2905, Validation Loss Current: 6.6038, Validation Loss AVG: 8.8899, lr: 0.001
Epoch [105/150], Training Loss: 22.7343, Validation Loss Current: 7.0740, Validation Loss AVG: 11.1217, lr: 0.001
Epoch [106/150], Training Loss: 23.9021, Validation Loss Current: 6.5558, Validation Loss AVG: 8.5524, lr: 0.001
Epoch [107/150], Training Loss: 23.1919, Validation Loss Current: 6.5977, Validation Loss AVG: 8.8368, lr: 0.001
Epoch [108/150], Training Loss: 22.3097, Validation Loss Current: 7.1902, Validation Loss AVG: 10.7689, lr: 0.001
Epoch [109/150], Training Loss: 22.0388, Validation Loss Current: 6.5198, Validation Loss AVG: 10.2133, lr: 0.001
Epoch [110/150], Training Loss: 20.2582, Validation Loss Current: 6.3978, Validation Loss AVG: 9.0094, lr: 0.001
Epoch [111/150], Training Loss: 19.9658, Validation Loss Current: 6.4174, Validation Loss AVG: 10.0950, lr: 0.001
Epoch [112/150], Training Loss: 21.6993, Validation Loss Current: 7.0535, Validation Loss AVG: 10.0294, lr: 0.001
Epoch [113/150], Training Loss: 24.6464, Validation Loss Current: 7.0952, Validation Loss AVG: 8.8363, lr: 0.001
Epoch [114/150], Training Loss: 22.7675, Validation Loss Current: 6.3269, Validation Loss AVG: 8.6938, lr: 0.001
Epoch [115/150], Training Loss: 22.6887, Validation Loss Current: 6.4416, Validation Loss AVG: 9.5683, lr: 0.001
Epoch [116/150], Training Loss: 22.0744, Validation Loss Current: 8.0094, Validation Loss AVG: 12.1968, lr: 0.001
Epoch [117/150], Training Loss: 22.3709, Validation Loss Current: 6.6666, Validation Loss AVG: 9.2169, lr: 0.001
Epoch [118/150], Training Loss: 21.5293, Validation Loss Current: 6.3352, Validation Loss AVG: 9.0378, lr: 0.001
Epoch [119/150], Training Loss: 21.0089, Validation Loss Current: 6.5490, Validation Loss AVG: 10.4281, lr: 0.001
Epoch [120/150], Training Loss: 19.9741, Validation Loss Current: 7.1964, Validation Loss AVG: 9.2523, lr: 0.001
Epoch [121/150], Training Loss: 21.2115, Validation Loss Current: 6.9033, Validation Loss AVG: 9.1932, lr: 0.001
Epoch [122/150], Training Loss: 21.1363, Validation Loss Current: 6.4174, Validation Loss AVG: 9.0412, lr: 0.001
Epoch [123/150], Training Loss: 19.4899, Validation Loss Current: 6.2709, Validation Loss AVG: 9.8026, lr: 0.001
Epoch [124/150], Training Loss: 20.0228, Validation Loss Current: 6.3537, Validation Loss AVG: 10.2722, lr: 0.001
Epoch [125/150], Training Loss: 19.7961, Validation Loss Current: 6.6416, Validation Loss AVG: 9.6013, lr: 0.001
Epoch [126/150], Training Loss: 18.9089, Validation Loss Current: 6.8735, Validation Loss AVG: 11.7722, lr: 0.001
Epoch [127/150], Training Loss: 18.1016, Validation Loss Current: 7.2096, Validation Loss AVG: 11.7879, lr: 0.001
Epoch [128/150], Training Loss: 21.8122, Validation Loss Current: 6.6281, Validation Loss AVG: 10.6734, lr: 0.001
Epoch [129/150], Training Loss: 18.7391, Validation Loss Current: 6.4047, Validation Loss AVG: 10.2625, lr: 0.001
Epoch [130/150], Training Loss: 17.6030, Validation Loss Current: 6.6252, Validation Loss AVG: 10.8986, lr: 0.001
Epoch [131/150], Training Loss: 17.8283, Validation Loss Current: 6.4449, Validation Loss AVG: 11.5251, lr: 0.001
Epoch [132/150], Training Loss: 17.2816, Validation Loss Current: 6.9494, Validation Loss AVG: 9.8424, lr: 0.001
Epoch [133/150], Training Loss: 19.0231, Validation Loss Current: 6.5015, Validation Loss AVG: 11.1191, lr: 0.001
Epoch [134/150], Training Loss: 16.4909, Validation Loss Current: 6.2948, Validation Loss AVG: 11.1116, lr: 0.001
Epoch [135/150], Training Loss: 16.9746, Validation Loss Current: 6.6942, Validation Loss AVG: 11.9311, lr: 0.001
Epoch [136/150], Training Loss: 21.7492, Validation Loss Current: 6.1209, Validation Loss AVG: 9.7900, lr: 0.001
Epoch [137/150], Training Loss: 18.0567, Validation Loss Current: 6.0825, Validation Loss AVG: 10.5100, lr: 0.001
Epoch [138/150], Training Loss: 17.7808, Validation Loss Current: 7.3124, Validation Loss AVG: 13.2239, lr: 0.001
Epoch [139/150], Training Loss: 19.0370, Validation Loss Current: 7.0465, Validation Loss AVG: 9.8479, lr: 0.001
Epoch [140/150], Training Loss: 18.4560, Validation Loss Current: 6.5100, Validation Loss AVG: 10.5688, lr: 0.001
Epoch [141/150], Training Loss: 15.5098, Validation Loss Current: 6.3987, Validation Loss AVG: 11.2921, lr: 0.001
Epoch [142/150], Training Loss: 15.4323, Validation Loss Current: 6.1089, Validation Loss AVG: 10.4784, lr: 0.001
Epoch [143/150], Training Loss: 15.2697, Validation Loss Current: 8.1074, Validation Loss AVG: 11.2663, lr: 0.001
Epoch [144/150], Training Loss: 18.1913, Validation Loss Current: 7.2897, Validation Loss AVG: 11.3101, lr: 0.001
Epoch [145/150], Training Loss: 15.4149, Validation Loss Current: 6.5086, Validation Loss AVG: 11.6686, lr: 0.001
Epoch [146/150], Training Loss: 15.7323, Validation Loss Current: 7.7026, Validation Loss AVG: 14.3315, lr: 0.001
Epoch [147/150], Training Loss: 17.0223, Validation Loss Current: 6.5148, Validation Loss AVG: 11.0719, lr: 0.001
Epoch [148/150], Training Loss: 14.1450, Validation Loss Current: 6.5498, Validation Loss AVG: 11.5835, lr: 0.001
Epoch [149/150], Training Loss: 14.3633, Validation Loss Current: 6.8995, Validation Loss AVG: 12.6359, lr: 0.001
Epoch [150/150], Training Loss: 14.8307, Validation Loss Current: 7.7546, Validation Loss AVG: 12.9648, lr: 0.001
Patch distance: 1 finished training. Best epoch: 137 Best val accuracy: [0.22532894736842105, 0.22532894736842105, 0.22532894736842105, 0.22532894736842105, 0.22532894736842105, 0.22532894736842105, 0.22532894736842105, 0.22532894736842105, 0.22532894736842105, 0.22532894736842105, 0.22532894736842105, 0.22532894736842105, 0.22532894736842105, 0.22532894736842105, 0.22532894736842105, 0.22532894736842105, 0.22532894736842105, 0.22532894736842105, 0.22532894736842105, 0.22532894736842105, 0.22532894736842105, 0.22532894736842105, 0.22532894736842105, 0.22532894736842105, 0.22532894736842105, 0.22532894736842105, 0.22532894736842105, 0.23026315789473684, 0.2565789473684211, 0.2582236842105263, 0.28289473684210525, 0.30098684210526316, 0.30756578947368424, 0.31743421052631576, 0.3059210526315789, 0.31085526315789475, 0.2746710526315789, 0.18914473684210525, 0.2993421052631579, 0.34210526315789475, 0.3404605263157895, 0.36348684210526316, 0.34539473684210525, 0.35526315789473684, 0.3125, 0.3782894736842105, 0.35855263157894735, 0.3881578947368421, 0.3815789473684211, 0.39473684210526316, 0.4029605263157895, 0.4144736842105263, 0.37335526315789475, 0.41776315789473684, 0.39473684210526316, 0.4243421052631579, 0.3980263157894737, 0.41776315789473684, 0.46381578947368424, 0.45723684210526316, 0.43585526315789475, 0.40460526315789475, 0.4194078947368421, 0.48848684210526316, 0.48026315789473684, 0.43585526315789475, 0.38980263157894735, 0.46381578947368424, 0.4786184210526316, 0.43914473684210525, 0.4917763157894737, 0.48848684210526316, 0.46710526315789475, 0.5115131578947368, 0.4720394736842105, 0.4720394736842105, 0.49506578947368424, 0.48519736842105265, 0.40625, 0.4588815789473684, 0.5180921052631579, 0.5115131578947368, 0.4967105263157895, 0.5427631578947368, 0.48519736842105265, 0.524671052631579, 0.53125, 0.5148026315789473, 0.4917763157894737, 0.5263157894736842, 0.53125, 0.5476973684210527, 0.5427631578947368, 0.5476973684210527, 0.5444078947368421, 0.5460526315789473, 0.5197368421052632, 0.5608552631578947, 0.5427631578947368, 0.5263157894736842, 0.5444078947368421, 0.5707236842105263, 0.5082236842105263, 0.5180921052631579, 0.5213815789473685, 0.5164473684210527, 0.5509868421052632, 0.4967105263157895, 0.5460526315789473, 0.5608552631578947, 0.5657894736842105, 0.5345394736842105, 0.5032894736842105, 0.5657894736842105, 0.555921052631579, 0.4605263157894737, 0.5509868421052632, 0.5707236842105263, 0.569078947368421, 0.53125, 0.5509868421052632, 0.5493421052631579, 0.5723684210526315, 0.5707236842105263, 0.5756578947368421, 0.5427631578947368, 0.5345394736842105, 0.5674342105263158, 0.5773026315789473, 0.5822368421052632, 0.5822368421052632, 0.5723684210526315, 0.5838815789473685, 0.59375, 0.5740131578947368, 0.5805921052631579, 0.6085526315789473, 0.5592105263157895, 0.5789473684210527, 0.5888157894736842, 0.6134868421052632, 0.6217105263157895, 0.5509868421052632, 0.5805921052631579, 0.6052631578947368, 0.5197368421052632, 0.59375, 0.6134868421052632, 0.5953947368421053, 0.5575657894736842] Best val loss: 6.082541108131409


----- Training alexnet with sequence: [1, 0.8] -----
Sequence [1] already in state dictionary, jumped
Loaded best state dict for [1]
Current group: 0.8
Epoch [1/75], Training Loss: 21.0844, Validation Loss Current: 9.2352, Validation Loss AVG: 9.2352, lr: 0.001
Epoch [2/75], Training Loss: 18.4902, Validation Loss Current: 8.6057, Validation Loss AVG: 8.6057, lr: 0.001
Epoch [3/75], Training Loss: 17.6187, Validation Loss Current: 9.0196, Validation Loss AVG: 9.0196, lr: 0.001
Epoch [4/75], Training Loss: 18.6524, Validation Loss Current: 8.5988, Validation Loss AVG: 8.5988, lr: 0.001
Epoch [5/75], Training Loss: 25.1691, Validation Loss Current: 8.2853, Validation Loss AVG: 8.2853, lr: 0.001
Epoch [6/75], Training Loss: 20.0201, Validation Loss Current: 9.4634, Validation Loss AVG: 9.4634, lr: 0.001
Epoch [7/75], Training Loss: 17.9309, Validation Loss Current: 8.8137, Validation Loss AVG: 8.8137, lr: 0.001
Epoch [8/75], Training Loss: 17.0496, Validation Loss Current: 9.1622, Validation Loss AVG: 9.1622, lr: 0.001
Epoch [9/75], Training Loss: 15.9745, Validation Loss Current: 9.3195, Validation Loss AVG: 9.3195, lr: 0.001
Epoch [10/75], Training Loss: 15.2733, Validation Loss Current: 9.2908, Validation Loss AVG: 9.2908, lr: 0.001
Epoch [11/75], Training Loss: 16.5063, Validation Loss Current: 11.2813, Validation Loss AVG: 11.2813, lr: 0.001
Epoch [12/75], Training Loss: 19.1989, Validation Loss Current: 11.0587, Validation Loss AVG: 11.0587, lr: 0.001
Epoch [13/75], Training Loss: 21.2231, Validation Loss Current: 8.5984, Validation Loss AVG: 8.5984, lr: 0.001
Epoch [14/75], Training Loss: 16.5314, Validation Loss Current: 9.3628, Validation Loss AVG: 9.3628, lr: 0.001
Epoch [15/75], Training Loss: 14.5902, Validation Loss Current: 9.2830, Validation Loss AVG: 9.2830, lr: 0.001
Epoch [16/75], Training Loss: 13.1615, Validation Loss Current: 9.6563, Validation Loss AVG: 9.6563, lr: 0.001
Epoch [17/75], Training Loss: 12.9723, Validation Loss Current: 12.2028, Validation Loss AVG: 12.2028, lr: 0.001
Epoch [18/75], Training Loss: 18.0767, Validation Loss Current: 9.5050, Validation Loss AVG: 9.5050, lr: 0.001
Epoch [19/75], Training Loss: 18.9857, Validation Loss Current: 10.7912, Validation Loss AVG: 10.7912, lr: 0.001
Epoch [20/75], Training Loss: 15.7203, Validation Loss Current: 9.3460, Validation Loss AVG: 9.3460, lr: 0.001
Epoch [21/75], Training Loss: 15.4277, Validation Loss Current: 10.2451, Validation Loss AVG: 10.2451, lr: 0.001
Epoch [22/75], Training Loss: 13.7496, Validation Loss Current: 8.9455, Validation Loss AVG: 8.9455, lr: 0.001
Epoch [23/75], Training Loss: 13.6453, Validation Loss Current: 11.7099, Validation Loss AVG: 11.7099, lr: 0.001
Epoch [24/75], Training Loss: 15.8636, Validation Loss Current: 9.7763, Validation Loss AVG: 9.7763, lr: 0.001
Epoch [25/75], Training Loss: 12.5521, Validation Loss Current: 10.9277, Validation Loss AVG: 10.9277, lr: 0.001
Epoch [26/75], Training Loss: 11.0673, Validation Loss Current: 9.8959, Validation Loss AVG: 9.8959, lr: 0.001
Epoch [27/75], Training Loss: 10.7825, Validation Loss Current: 12.0902, Validation Loss AVG: 12.0902, lr: 0.001
Epoch [28/75], Training Loss: 12.6575, Validation Loss Current: 11.2171, Validation Loss AVG: 11.2171, lr: 0.001
Epoch [29/75], Training Loss: 12.8174, Validation Loss Current: 13.2959, Validation Loss AVG: 13.2959, lr: 0.001
Epoch [30/75], Training Loss: 15.7984, Validation Loss Current: 9.4189, Validation Loss AVG: 9.4189, lr: 0.001
Epoch [31/75], Training Loss: 10.8873, Validation Loss Current: 12.0213, Validation Loss AVG: 12.0213, lr: 0.001
Epoch [32/75], Training Loss: 10.4557, Validation Loss Current: 10.9586, Validation Loss AVG: 10.9586, lr: 0.001
Epoch [33/75], Training Loss: 9.3204, Validation Loss Current: 14.5618, Validation Loss AVG: 14.5618, lr: 0.001
Epoch [34/75], Training Loss: 17.3794, Validation Loss Current: 9.7295, Validation Loss AVG: 9.7295, lr: 0.001
Epoch [35/75], Training Loss: 13.9018, Validation Loss Current: 12.0071, Validation Loss AVG: 12.0071, lr: 0.001
 --- Early Stopped ---
Patch distance: 0.8 finished training. Best epoch: 5 Best val accuracy: [0.44506578947368425, 0.4585526315789473, 0.4480263157894736, 0.4743421052631579, 0.43782894736842104, 0.42269736842105265, 0.4496710526315789, 0.45953947368421044, 0.4394736842105263, 0.46282894736842106, 0.3855263157894737, 0.41776315789473684, 0.4309210526315789, 0.4302631578947368, 0.47006578947368416, 0.4582236842105263, 0.4171052631578947, 0.44703947368421043, 0.44539473684210523, 0.4828947368421052, 0.4546052631578947, 0.47631578947368425, 0.43947368421052635, 0.42631578947368426, 0.4608552631578947, 0.48190789473684215, 0.4375, 0.44703947368421054, 0.4230263157894737, 0.4782894736842106, 0.43980263157894744, 0.45723684210526316, 0.36348684210526316, 0.4546052631578947, 0.4240131578947368] Best val loss: 8.285340309143066


----- Training alexnet with sequence: [1, 0.8, 0.6] -----
Sequence [1] already in state dictionary, jumped
Sequence [1, 0.8] already in state dictionary, jumped
Loaded best state dict for [1, 0.8]
Current group: 0.6
Epoch [1/50], Training Loss: 21.3629, Validation Loss Current: 7.8933, Validation Loss AVG: 7.8933, lr: 0.001
Epoch [2/50], Training Loss: 20.2793, Validation Loss Current: 9.0526, Validation Loss AVG: 9.0526, lr: 0.001
Epoch [3/50], Training Loss: 32.1033, Validation Loss Current: 9.5409, Validation Loss AVG: 9.5409, lr: 0.001
Epoch [4/50], Training Loss: 26.7559, Validation Loss Current: 9.0348, Validation Loss AVG: 9.0348, lr: 0.001
Epoch [5/50], Training Loss: 22.3931, Validation Loss Current: 8.3238, Validation Loss AVG: 8.3238, lr: 0.001
Epoch [6/50], Training Loss: 18.8097, Validation Loss Current: 10.7342, Validation Loss AVG: 10.7342, lr: 0.001
Epoch [7/50], Training Loss: 21.1836, Validation Loss Current: 7.6660, Validation Loss AVG: 7.6660, lr: 0.001
Epoch [8/50], Training Loss: 16.7487, Validation Loss Current: 7.9155, Validation Loss AVG: 7.9155, lr: 0.001
Epoch [9/50], Training Loss: 15.4194, Validation Loss Current: 8.2670, Validation Loss AVG: 8.2670, lr: 0.001
Epoch [10/50], Training Loss: 15.0668, Validation Loss Current: 8.8121, Validation Loss AVG: 8.8121, lr: 0.001
Epoch [11/50], Training Loss: 14.0656, Validation Loss Current: 8.8044, Validation Loss AVG: 8.8044, lr: 0.001
Epoch [12/50], Training Loss: 13.6757, Validation Loss Current: 8.9442, Validation Loss AVG: 8.9442, lr: 0.001
Epoch [13/50], Training Loss: 12.8964, Validation Loss Current: 9.4385, Validation Loss AVG: 9.4385, lr: 0.001
Epoch [14/50], Training Loss: 12.8482, Validation Loss Current: 9.8594, Validation Loss AVG: 9.8594, lr: 0.001
Epoch [15/50], Training Loss: 16.9940, Validation Loss Current: 8.0130, Validation Loss AVG: 8.0130, lr: 0.001
Epoch [16/50], Training Loss: 12.6578, Validation Loss Current: 9.5531, Validation Loss AVG: 9.5531, lr: 0.001
Epoch [17/50], Training Loss: 11.7835, Validation Loss Current: 9.5384, Validation Loss AVG: 9.5384, lr: 0.001
Epoch [18/50], Training Loss: 11.6917, Validation Loss Current: 9.6843, Validation Loss AVG: 9.6843, lr: 0.001
Epoch [19/50], Training Loss: 13.2140, Validation Loss Current: 8.2807, Validation Loss AVG: 8.2807, lr: 0.001
Epoch [20/50], Training Loss: 9.6375, Validation Loss Current: 11.6608, Validation Loss AVG: 11.6608, lr: 0.001
Epoch [21/50], Training Loss: 10.2305, Validation Loss Current: 9.9979, Validation Loss AVG: 9.9979, lr: 0.001
Epoch [22/50], Training Loss: 9.2417, Validation Loss Current: 20.0499, Validation Loss AVG: 20.0499, lr: 0.001
Epoch [23/50], Training Loss: 18.3694, Validation Loss Current: 9.5221, Validation Loss AVG: 9.5221, lr: 0.001
Epoch [24/50], Training Loss: 11.1843, Validation Loss Current: 9.8356, Validation Loss AVG: 9.8356, lr: 0.001
Epoch [25/50], Training Loss: 9.4268, Validation Loss Current: 11.2120, Validation Loss AVG: 11.2120, lr: 0.001
Epoch [26/50], Training Loss: 12.8200, Validation Loss Current: 9.8674, Validation Loss AVG: 9.8674, lr: 0.001
Epoch [27/50], Training Loss: 10.7664, Validation Loss Current: 10.2148, Validation Loss AVG: 10.2148, lr: 0.001
Epoch [28/50], Training Loss: 7.7016, Validation Loss Current: 11.9673, Validation Loss AVG: 11.9673, lr: 0.001
Epoch [29/50], Training Loss: 6.8561, Validation Loss Current: 12.0902, Validation Loss AVG: 12.0902, lr: 0.001
Epoch [30/50], Training Loss: 6.3723, Validation Loss Current: 11.7405, Validation Loss AVG: 11.7405, lr: 0.001
Epoch [31/50], Training Loss: 5.8889, Validation Loss Current: 12.5114, Validation Loss AVG: 12.5114, lr: 0.001
Epoch [32/50], Training Loss: 4.8164, Validation Loss Current: 13.2942, Validation Loss AVG: 13.2942, lr: 0.001
Epoch [33/50], Training Loss: 6.9668, Validation Loss Current: 11.1045, Validation Loss AVG: 11.1045, lr: 0.001
Epoch [34/50], Training Loss: 4.8713, Validation Loss Current: 13.1664, Validation Loss AVG: 13.1664, lr: 0.001
Epoch [35/50], Training Loss: 4.5874, Validation Loss Current: 13.4998, Validation Loss AVG: 13.4998, lr: 0.001
Epoch [36/50], Training Loss: 3.7472, Validation Loss Current: 14.5783, Validation Loss AVG: 14.5783, lr: 0.001
Epoch [37/50], Training Loss: 3.8679, Validation Loss Current: 15.9245, Validation Loss AVG: 15.9245, lr: 0.001
 --- Early Stopped ---
Patch distance: 0.6 finished training. Best epoch: 7 Best val accuracy: [0.4773026315789474, 0.4286184210526316, 0.3401315789473684, 0.4394736842105263, 0.4805921052631579, 0.44703947368421054, 0.5078947368421053, 0.5144736842105264, 0.49703947368421053, 0.4930921052631579, 0.5055921052631579, 0.49375, 0.5059210526315789, 0.48190789473684215, 0.5042763157894736, 0.4865131578947368, 0.4963815789473684, 0.4697368421052632, 0.511842105263158, 0.4654605263157895, 0.5154605263157894, 0.3796052631578948, 0.49375, 0.5036184210526315, 0.4792763157894737, 0.4671052631578947, 0.4963815789473685, 0.49375, 0.49605263157894736, 0.46381578947368424, 0.5075657894736841, 0.5006578947368421, 0.5092105263157893, 0.5052631578947369, 0.49572368421052626, 0.4973684210526315, 0.47796052631578945] Best val loss: 7.6659959077835085


----- Training alexnet with sequence: [1, 0.8, 0.6, 0.4] -----
Sequence [1] already in state dictionary, jumped
Sequence [1, 0.8] already in state dictionary, jumped
Sequence [1, 0.8, 0.6] already in state dictionary, jumped
Loaded best state dict for [1, 0.8, 0.6]
Current group: 0.4
Epoch [1/38], Training Loss: 27.6655, Validation Loss Current: 7.4332, Validation Loss AVG: 7.4332, lr: 0.001
Epoch [2/38], Training Loss: 25.8335, Validation Loss Current: 7.7488, Validation Loss AVG: 7.7488, lr: 0.001
Epoch [3/38], Training Loss: 24.3350, Validation Loss Current: 8.2204, Validation Loss AVG: 8.2204, lr: 0.001
Epoch [4/38], Training Loss: 25.9969, Validation Loss Current: 8.1934, Validation Loss AVG: 8.1934, lr: 0.001
Epoch [5/38], Training Loss: 24.5582, Validation Loss Current: 7.8364, Validation Loss AVG: 7.8364, lr: 0.001
Epoch [6/38], Training Loss: 20.2283, Validation Loss Current: 9.4348, Validation Loss AVG: 9.4348, lr: 0.001
Epoch [7/38], Training Loss: 21.5913, Validation Loss Current: 8.0364, Validation Loss AVG: 8.0364, lr: 0.001
Epoch [8/38], Training Loss: 18.6178, Validation Loss Current: 10.2583, Validation Loss AVG: 10.2583, lr: 0.001
Epoch [9/38], Training Loss: 27.0735, Validation Loss Current: 7.5669, Validation Loss AVG: 7.5669, lr: 0.001
Epoch [10/38], Training Loss: 20.8795, Validation Loss Current: 8.2963, Validation Loss AVG: 8.2963, lr: 0.001
Epoch [11/38], Training Loss: 18.9727, Validation Loss Current: 7.8869, Validation Loss AVG: 7.8869, lr: 0.001
Epoch [12/38], Training Loss: 16.5712, Validation Loss Current: 8.0964, Validation Loss AVG: 8.0964, lr: 0.001
Epoch [13/38], Training Loss: 16.7800, Validation Loss Current: 8.6951, Validation Loss AVG: 8.6951, lr: 0.001
Epoch [14/38], Training Loss: 16.4561, Validation Loss Current: 8.8326, Validation Loss AVG: 8.8326, lr: 0.001
Epoch [15/38], Training Loss: 14.2588, Validation Loss Current: 9.6431, Validation Loss AVG: 9.6431, lr: 0.001
Epoch [16/38], Training Loss: 18.3091, Validation Loss Current: 9.3014, Validation Loss AVG: 9.3014, lr: 0.001
Epoch [17/38], Training Loss: 24.9034, Validation Loss Current: 8.0742, Validation Loss AVG: 8.0742, lr: 0.001
Epoch [18/38], Training Loss: 21.8799, Validation Loss Current: 7.8706, Validation Loss AVG: 7.8706, lr: 0.001
Epoch [19/38], Training Loss: 18.0290, Validation Loss Current: 8.2310, Validation Loss AVG: 8.2310, lr: 0.001
Epoch [20/38], Training Loss: 19.3173, Validation Loss Current: 8.2402, Validation Loss AVG: 8.2402, lr: 0.001
Epoch [21/38], Training Loss: 18.9384, Validation Loss Current: 8.4125, Validation Loss AVG: 8.4125, lr: 0.001
Epoch [22/38], Training Loss: 13.3954, Validation Loss Current: 9.1411, Validation Loss AVG: 9.1411, lr: 0.001
Epoch [23/38], Training Loss: 11.7872, Validation Loss Current: 9.7250, Validation Loss AVG: 9.7250, lr: 0.001
Epoch [24/38], Training Loss: 13.0490, Validation Loss Current: 9.7740, Validation Loss AVG: 9.7740, lr: 0.001
Epoch [25/38], Training Loss: 12.2533, Validation Loss Current: 9.6062, Validation Loss AVG: 9.6062, lr: 0.001
Epoch [26/38], Training Loss: 14.3809, Validation Loss Current: 9.5880, Validation Loss AVG: 9.5880, lr: 0.001
Epoch [27/38], Training Loss: 9.9465, Validation Loss Current: 10.7803, Validation Loss AVG: 10.7803, lr: 0.001
Epoch [28/38], Training Loss: 8.3959, Validation Loss Current: 10.3759, Validation Loss AVG: 10.3759, lr: 0.001
Epoch [29/38], Training Loss: 7.8221, Validation Loss Current: 11.5178, Validation Loss AVG: 11.5178, lr: 0.001
Epoch [30/38], Training Loss: 9.3425, Validation Loss Current: 10.6855, Validation Loss AVG: 10.6855, lr: 0.001
Epoch [31/38], Training Loss: 10.1274, Validation Loss Current: 11.1794, Validation Loss AVG: 11.1794, lr: 0.001
 --- Early Stopped ---
Patch distance: 0.4 finished training. Best epoch: 1 Best val accuracy: [0.4917763157894736, 0.4878289473684211, 0.4911184210526316, 0.45953947368421055, 0.4917763157894738, 0.48552631578947364, 0.5023026315789474, 0.43322368421052637, 0.49210526315789477, 0.48881578947368426, 0.49967105263157896, 0.4950657894736842, 0.49703947368421064, 0.5092105263157894, 0.45625, 0.44539473684210523, 0.4605263157894736, 0.4983552631578948, 0.47565789473684206, 0.4822368421052632, 0.5016447368421052, 0.4855263157894737, 0.48749999999999993, 0.47335526315789467, 0.5085526315789474, 0.48026315789473684, 0.4858552631578948, 0.47993421052631574, 0.4898026315789473, 0.4720394736842105, 0.49572368421052637] Best val loss: 7.433209490776062


----- Training alexnet with sequence: [1, 0.8, 0.6, 0.4, 0.2] -----
Sequence [1] already in state dictionary, jumped
Sequence [1, 0.8] already in state dictionary, jumped
Sequence [1, 0.8, 0.6] already in state dictionary, jumped
Sequence [1, 0.8, 0.6, 0.4] already in state dictionary, jumped
Loaded best state dict for [1, 0.8, 0.6, 0.4]
Current group: 0.2
Epoch [1/30], Training Loss: 35.5575, Validation Loss Current: 9.9600, Validation Loss AVG: 9.9600, lr: 0.001
Epoch [2/30], Training Loss: 37.0800, Validation Loss Current: 8.3273, Validation Loss AVG: 8.3273, lr: 0.001
Epoch [3/30], Training Loss: 33.1951, Validation Loss Current: 7.9879, Validation Loss AVG: 7.9879, lr: 0.001
Epoch [4/30], Training Loss: 30.6913, Validation Loss Current: 8.6232, Validation Loss AVG: 8.6232, lr: 0.001
Epoch [5/30], Training Loss: 28.2386, Validation Loss Current: 9.4880, Validation Loss AVG: 9.4880, lr: 0.001
Epoch [6/30], Training Loss: 27.2591, Validation Loss Current: 9.6855, Validation Loss AVG: 9.6855, lr: 0.001
Epoch [7/30], Training Loss: 25.1458, Validation Loss Current: 9.8711, Validation Loss AVG: 9.8711, lr: 0.001
Epoch [8/30], Training Loss: 24.6709, Validation Loss Current: 10.0220, Validation Loss AVG: 10.0220, lr: 0.001
Epoch [9/30], Training Loss: 24.8538, Validation Loss Current: 9.9661, Validation Loss AVG: 9.9661, lr: 0.001
Epoch [10/30], Training Loss: 24.0286, Validation Loss Current: 10.2394, Validation Loss AVG: 10.2394, lr: 0.001
Epoch [11/30], Training Loss: 23.5347, Validation Loss Current: 10.6503, Validation Loss AVG: 10.6503, lr: 0.001
Epoch [12/30], Training Loss: 23.1427, Validation Loss Current: 11.0240, Validation Loss AVG: 11.0240, lr: 0.001
Epoch [13/30], Training Loss: 21.7961, Validation Loss Current: 11.5742, Validation Loss AVG: 11.5742, lr: 0.001
Epoch [14/30], Training Loss: 20.0795, Validation Loss Current: 10.9758, Validation Loss AVG: 10.9758, lr: 0.001
Epoch [15/30], Training Loss: 20.2568, Validation Loss Current: 11.4667, Validation Loss AVG: 11.4667, lr: 0.001
Epoch [16/30], Training Loss: 22.8763, Validation Loss Current: 10.9152, Validation Loss AVG: 10.9152, lr: 0.001
Epoch [17/30], Training Loss: 20.2359, Validation Loss Current: 13.6207, Validation Loss AVG: 13.6207, lr: 0.001
Epoch [18/30], Training Loss: 22.3833, Validation Loss Current: 10.7437, Validation Loss AVG: 10.7437, lr: 0.001
Epoch [19/30], Training Loss: 20.7879, Validation Loss Current: 11.4006, Validation Loss AVG: 11.4006, lr: 0.001
Epoch [20/30], Training Loss: 21.1882, Validation Loss Current: 13.2941, Validation Loss AVG: 13.2941, lr: 0.001
Epoch [21/30], Training Loss: 30.0625, Validation Loss Current: 11.6176, Validation Loss AVG: 11.6176, lr: 0.001
Epoch [22/30], Training Loss: 26.2436, Validation Loss Current: 11.7197, Validation Loss AVG: 11.7197, lr: 0.001
Epoch [23/30], Training Loss: 21.8034, Validation Loss Current: 11.2526, Validation Loss AVG: 11.2526, lr: 0.001
Epoch [24/30], Training Loss: 18.7025, Validation Loss Current: 12.6894, Validation Loss AVG: 12.6894, lr: 0.001
Epoch [25/30], Training Loss: 18.9831, Validation Loss Current: 12.0998, Validation Loss AVG: 12.0998, lr: 0.001
Epoch [26/30], Training Loss: 18.4054, Validation Loss Current: 12.7128, Validation Loss AVG: 12.7128, lr: 0.001
Epoch [27/30], Training Loss: 24.2481, Validation Loss Current: 12.3911, Validation Loss AVG: 12.3911, lr: 0.001
Epoch [28/30], Training Loss: 18.4046, Validation Loss Current: 12.5293, Validation Loss AVG: 12.5293, lr: 0.001
Epoch [29/30], Training Loss: 21.8629, Validation Loss Current: 11.0028, Validation Loss AVG: 11.0028, lr: 0.001
Epoch [30/30], Training Loss: 19.5780, Validation Loss Current: 11.8740, Validation Loss AVG: 11.8740, lr: 0.001
Epoch [31/30], Training Loss: 16.6610, Validation Loss Current: 12.6747, Validation Loss AVG: 12.6747, lr: 0.001
Epoch [32/30], Training Loss: 16.2549, Validation Loss Current: 12.7391, Validation Loss AVG: 12.7391, lr: 0.001
Epoch [33/30], Training Loss: 18.3447, Validation Loss Current: 14.0891, Validation Loss AVG: 14.0891, lr: 0.001
 --- Early Stopped ---
Patch distance: 0.2 finished training. Best epoch: 3 Best val accuracy: [0.3289473684210526, 0.4180921052631579, 0.4513157894736842, 0.4273026315789473, 0.3832236842105263, 0.3950657894736842, 0.3891447368421053, 0.37828947368421056, 0.3986842105263158, 0.40230263157894736, 0.3588815789473684, 0.37269736842105267, 0.37467105263157896, 0.39868421052631586, 0.3671052631578947, 0.3703947368421053, 0.3026315789473685, 0.37730263157894733, 0.39342105263157895, 0.3595394736842105, 0.3069078947368421, 0.33026315789473687, 0.36874999999999997, 0.34901315789473686, 0.36842105263157887, 0.3506578947368421, 0.34210526315789475, 0.3628289473684211, 0.36480263157894743, 0.3631578947368421, 0.3667763157894736, 0.3680921052631579, 0.35164473684210523] Best val loss: 7.987930750846862


Fold: 1
----- Training alexnet with sequence: [1] -----
Current group: 1
Epoch [1/150], Training Loss: 41.5915, Validation Loss Current: 10.3784, Validation Loss AVG: 10.3781, lr: 0.001
Epoch [2/150], Training Loss: 41.4645, Validation Loss Current: 10.3437, Validation Loss AVG: 10.3436, lr: 0.001
Epoch [3/150], Training Loss: 41.3066, Validation Loss Current: 10.3103, Validation Loss AVG: 10.3104, lr: 0.001
Epoch [4/150], Training Loss: 41.2488, Validation Loss Current: 10.2755, Validation Loss AVG: 10.2702, lr: 0.001
Epoch [5/150], Training Loss: 41.0240, Validation Loss Current: 10.2364, Validation Loss AVG: 10.2321, lr: 0.001
Epoch [6/150], Training Loss: 40.9674, Validation Loss Current: 10.1872, Validation Loss AVG: 10.1816, lr: 0.001
Epoch [7/150], Training Loss: 40.8336, Validation Loss Current: 10.1393, Validation Loss AVG: 10.1281, lr: 0.001
Epoch [8/150], Training Loss: 40.7541, Validation Loss Current: 10.0751, Validation Loss AVG: 10.0599, lr: 0.001
Epoch [9/150], Training Loss: 40.4500, Validation Loss Current: 10.0207, Validation Loss AVG: 10.0094, lr: 0.001
Epoch [10/150], Training Loss: 39.9440, Validation Loss Current: 9.9783, Validation Loss AVG: 9.9773, lr: 0.001
Epoch [11/150], Training Loss: 40.4062, Validation Loss Current: 9.9576, Validation Loss AVG: 9.9549, lr: 0.001
Epoch [12/150], Training Loss: 40.0576, Validation Loss Current: 9.9412, Validation Loss AVG: 9.9478, lr: 0.001
Epoch [13/150], Training Loss: 39.9953, Validation Loss Current: 9.9288, Validation Loss AVG: 9.9443, lr: 0.001
Epoch [14/150], Training Loss: 39.7000, Validation Loss Current: 9.9210, Validation Loss AVG: 9.9421, lr: 0.001
Epoch [15/150], Training Loss: 39.5457, Validation Loss Current: 9.8812, Validation Loss AVG: 9.9521, lr: 0.001
Epoch [16/150], Training Loss: 39.8672, Validation Loss Current: 9.8810, Validation Loss AVG: 9.9347, lr: 0.001
Epoch [17/150], Training Loss: 39.9267, Validation Loss Current: 9.8626, Validation Loss AVG: 9.9616, lr: 0.001
Epoch [18/150], Training Loss: 39.5877, Validation Loss Current: 9.8579, Validation Loss AVG: 9.9614, lr: 0.001
Epoch [19/150], Training Loss: 39.9554, Validation Loss Current: 9.8297, Validation Loss AVG: 9.9947, lr: 0.001
Epoch [20/150], Training Loss: 39.9063, Validation Loss Current: 9.8742, Validation Loss AVG: 9.9979, lr: 0.001
Epoch [21/150], Training Loss: 39.6167, Validation Loss Current: 9.8150, Validation Loss AVG: 10.0061, lr: 0.001
Epoch [22/150], Training Loss: 39.5937, Validation Loss Current: 9.8335, Validation Loss AVG: 10.0231, lr: 0.001
Epoch [23/150], Training Loss: 39.2415, Validation Loss Current: 9.7912, Validation Loss AVG: 10.0280, lr: 0.001
Epoch [24/150], Training Loss: 38.9447, Validation Loss Current: 9.7596, Validation Loss AVG: 10.0479, lr: 0.001
Epoch [25/150], Training Loss: 39.1112, Validation Loss Current: 9.7030, Validation Loss AVG: 10.0534, lr: 0.001
Epoch [26/150], Training Loss: 39.0767, Validation Loss Current: 9.6928, Validation Loss AVG: 9.9836, lr: 0.001
Epoch [27/150], Training Loss: 38.6537, Validation Loss Current: 9.6400, Validation Loss AVG: 10.0137, lr: 0.001
Epoch [28/150], Training Loss: 39.0836, Validation Loss Current: 9.6523, Validation Loss AVG: 9.9967, lr: 0.001
Epoch [29/150], Training Loss: 38.4390, Validation Loss Current: 9.5991, Validation Loss AVG: 10.0890, lr: 0.001
Epoch [30/150], Training Loss: 39.1238, Validation Loss Current: 9.5900, Validation Loss AVG: 9.8728, lr: 0.001
Epoch [31/150], Training Loss: 38.4388, Validation Loss Current: 9.5870, Validation Loss AVG: 10.0834, lr: 0.001
Epoch [32/150], Training Loss: 39.3827, Validation Loss Current: 9.5867, Validation Loss AVG: 9.8663, lr: 0.001
Epoch [33/150], Training Loss: 38.9364, Validation Loss Current: 9.4928, Validation Loss AVG: 9.9340, lr: 0.001
Epoch [34/150], Training Loss: 38.8484, Validation Loss Current: 9.4602, Validation Loss AVG: 9.8803, lr: 0.001
Epoch [35/150], Training Loss: 37.3688, Validation Loss Current: 9.4372, Validation Loss AVG: 10.2747, lr: 0.001
Epoch [36/150], Training Loss: 37.9236, Validation Loss Current: 9.3354, Validation Loss AVG: 9.9100, lr: 0.001
Epoch [37/150], Training Loss: 37.9032, Validation Loss Current: 9.3080, Validation Loss AVG: 9.8376, lr: 0.001
Epoch [38/150], Training Loss: 38.1556, Validation Loss Current: 9.3243, Validation Loss AVG: 9.7626, lr: 0.001
Epoch [39/150], Training Loss: 37.6498, Validation Loss Current: 9.2546, Validation Loss AVG: 10.1866, lr: 0.001
Epoch [40/150], Training Loss: 37.5823, Validation Loss Current: 9.1590, Validation Loss AVG: 9.8521, lr: 0.001
Epoch [41/150], Training Loss: 36.9355, Validation Loss Current: 9.0428, Validation Loss AVG: 9.8220, lr: 0.001
Epoch [42/150], Training Loss: 35.8644, Validation Loss Current: 8.9878, Validation Loss AVG: 9.5938, lr: 0.001
Epoch [43/150], Training Loss: 36.7139, Validation Loss Current: 8.9014, Validation Loss AVG: 9.7862, lr: 0.001
Epoch [44/150], Training Loss: 36.0610, Validation Loss Current: 8.7702, Validation Loss AVG: 9.4092, lr: 0.001
Epoch [45/150], Training Loss: 35.3349, Validation Loss Current: 8.6425, Validation Loss AVG: 9.2852, lr: 0.001
Epoch [46/150], Training Loss: 35.7866, Validation Loss Current: 8.9711, Validation Loss AVG: 10.7661, lr: 0.001
Epoch [47/150], Training Loss: 34.9464, Validation Loss Current: 8.4240, Validation Loss AVG: 9.1275, lr: 0.001
Epoch [48/150], Training Loss: 34.0304, Validation Loss Current: 8.7027, Validation Loss AVG: 9.6951, lr: 0.001
Epoch [49/150], Training Loss: 34.1564, Validation Loss Current: 8.7042, Validation Loss AVG: 10.0836, lr: 0.001
Epoch [50/150], Training Loss: 35.1399, Validation Loss Current: 8.1696, Validation Loss AVG: 9.2240, lr: 0.001
Epoch [51/150], Training Loss: 32.9226, Validation Loss Current: 8.5680, Validation Loss AVG: 9.2163, lr: 0.001
Epoch [52/150], Training Loss: 34.7988, Validation Loss Current: 8.1507, Validation Loss AVG: 9.2190, lr: 0.001
Epoch [53/150], Training Loss: 32.8832, Validation Loss Current: 7.9533, Validation Loss AVG: 9.2234, lr: 0.001
Epoch [54/150], Training Loss: 33.1605, Validation Loss Current: 8.1682, Validation Loss AVG: 9.1067, lr: 0.001
Epoch [55/150], Training Loss: 32.5410, Validation Loss Current: 8.2019, Validation Loss AVG: 9.3169, lr: 0.001
Epoch [56/150], Training Loss: 32.1044, Validation Loss Current: 7.7373, Validation Loss AVG: 8.8250, lr: 0.001
Epoch [57/150], Training Loss: 30.8998, Validation Loss Current: 7.9440, Validation Loss AVG: 9.6425, lr: 0.001
Epoch [58/150], Training Loss: 32.1178, Validation Loss Current: 7.6436, Validation Loss AVG: 8.7470, lr: 0.001
Epoch [59/150], Training Loss: 31.9413, Validation Loss Current: 7.9975, Validation Loss AVG: 9.7126, lr: 0.001
Epoch [60/150], Training Loss: 31.8445, Validation Loss Current: 7.6732, Validation Loss AVG: 8.7447, lr: 0.001
Epoch [61/150], Training Loss: 32.3508, Validation Loss Current: 7.8420, Validation Loss AVG: 9.2344, lr: 0.001
Epoch [62/150], Training Loss: 30.5299, Validation Loss Current: 7.5808, Validation Loss AVG: 8.5305, lr: 0.001
Epoch [63/150], Training Loss: 29.5612, Validation Loss Current: 7.3541, Validation Loss AVG: 8.5306, lr: 0.001
Epoch [64/150], Training Loss: 30.1318, Validation Loss Current: 7.3352, Validation Loss AVG: 8.5124, lr: 0.001
Epoch [65/150], Training Loss: 29.2128, Validation Loss Current: 7.3545, Validation Loss AVG: 8.5220, lr: 0.001
Epoch [66/150], Training Loss: 28.9552, Validation Loss Current: 7.3175, Validation Loss AVG: 8.2820, lr: 0.001
Epoch [67/150], Training Loss: 28.6443, Validation Loss Current: 7.4676, Validation Loss AVG: 8.9662, lr: 0.001
Epoch [68/150], Training Loss: 28.5344, Validation Loss Current: 8.0813, Validation Loss AVG: 10.8158, lr: 0.001
Epoch [69/150], Training Loss: 32.8084, Validation Loss Current: 7.1914, Validation Loss AVG: 8.2413, lr: 0.001
Epoch [70/150], Training Loss: 28.8825, Validation Loss Current: 7.8499, Validation Loss AVG: 9.6513, lr: 0.001
Epoch [71/150], Training Loss: 29.3210, Validation Loss Current: 6.9609, Validation Loss AVG: 8.0554, lr: 0.001
Epoch [72/150], Training Loss: 28.1646, Validation Loss Current: 6.8236, Validation Loss AVG: 8.4313, lr: 0.001
Epoch [73/150], Training Loss: 27.1170, Validation Loss Current: 6.9747, Validation Loss AVG: 9.8813, lr: 0.001
Epoch [74/150], Training Loss: 26.9221, Validation Loss Current: 8.8797, Validation Loss AVG: 9.5610, lr: 0.001
Epoch [75/150], Training Loss: 32.1220, Validation Loss Current: 7.7755, Validation Loss AVG: 9.8777, lr: 0.001
Epoch [76/150], Training Loss: 28.7866, Validation Loss Current: 6.8908, Validation Loss AVG: 8.9464, lr: 0.001
Epoch [77/150], Training Loss: 29.4293, Validation Loss Current: 7.3140, Validation Loss AVG: 10.1754, lr: 0.001
Epoch [78/150], Training Loss: 30.3633, Validation Loss Current: 7.0388, Validation Loss AVG: 9.1560, lr: 0.001
Epoch [79/150], Training Loss: 28.8193, Validation Loss Current: 6.7102, Validation Loss AVG: 9.4988, lr: 0.001
Epoch [80/150], Training Loss: 26.6719, Validation Loss Current: 6.8119, Validation Loss AVG: 8.8667, lr: 0.001
Epoch [81/150], Training Loss: 26.5508, Validation Loss Current: 8.1604, Validation Loss AVG: 12.3272, lr: 0.001
Epoch [82/150], Training Loss: 31.8053, Validation Loss Current: 7.3223, Validation Loss AVG: 8.3516, lr: 0.001
Epoch [83/150], Training Loss: 28.8207, Validation Loss Current: 6.7535, Validation Loss AVG: 8.0735, lr: 0.001
Epoch [84/150], Training Loss: 25.5029, Validation Loss Current: 6.7835, Validation Loss AVG: 8.5821, lr: 0.001
Epoch [85/150], Training Loss: 25.4461, Validation Loss Current: 6.4671, Validation Loss AVG: 8.9794, lr: 0.001
Epoch [86/150], Training Loss: 25.4654, Validation Loss Current: 7.3642, Validation Loss AVG: 10.7253, lr: 0.001
Epoch [87/150], Training Loss: 29.6630, Validation Loss Current: 6.6606, Validation Loss AVG: 8.1664, lr: 0.001
Epoch [88/150], Training Loss: 27.3129, Validation Loss Current: 7.1748, Validation Loss AVG: 9.1899, lr: 0.001
Epoch [89/150], Training Loss: 28.3160, Validation Loss Current: 7.0095, Validation Loss AVG: 8.9162, lr: 0.001
Epoch [90/150], Training Loss: 25.9614, Validation Loss Current: 6.7051, Validation Loss AVG: 9.8331, lr: 0.001
Epoch [91/150], Training Loss: 25.8041, Validation Loss Current: 6.8412, Validation Loss AVG: 9.3042, lr: 0.001
Epoch [92/150], Training Loss: 25.4877, Validation Loss Current: 6.6753, Validation Loss AVG: 9.3333, lr: 0.001
Epoch [93/150], Training Loss: 27.3174, Validation Loss Current: 6.7684, Validation Loss AVG: 8.8280, lr: 0.001
Epoch [94/150], Training Loss: 25.2684, Validation Loss Current: 6.7074, Validation Loss AVG: 8.1558, lr: 0.001
Epoch [95/150], Training Loss: 25.7114, Validation Loss Current: 6.4004, Validation Loss AVG: 8.6527, lr: 0.001
Epoch [96/150], Training Loss: 23.6151, Validation Loss Current: 6.3833, Validation Loss AVG: 8.7361, lr: 0.001
Epoch [97/150], Training Loss: 27.0960, Validation Loss Current: 6.6385, Validation Loss AVG: 8.2689, lr: 0.001
Epoch [98/150], Training Loss: 24.9071, Validation Loss Current: 6.3126, Validation Loss AVG: 8.1391, lr: 0.001
Epoch [99/150], Training Loss: 23.9376, Validation Loss Current: 6.1983, Validation Loss AVG: 8.2579, lr: 0.001
Epoch [100/150], Training Loss: 22.5545, Validation Loss Current: 6.3959, Validation Loss AVG: 8.2523, lr: 0.001
Epoch [101/150], Training Loss: 25.1431, Validation Loss Current: 7.1455, Validation Loss AVG: 10.1785, lr: 0.001
Epoch [102/150], Training Loss: 29.0074, Validation Loss Current: 7.3353, Validation Loss AVG: 8.8007, lr: 0.001
Epoch [103/150], Training Loss: 26.8902, Validation Loss Current: 6.4365, Validation Loss AVG: 9.8243, lr: 0.001
Epoch [104/150], Training Loss: 24.6403, Validation Loss Current: 6.4132, Validation Loss AVG: 8.6497, lr: 0.001
Epoch [105/150], Training Loss: 25.1991, Validation Loss Current: 6.5922, Validation Loss AVG: 10.2293, lr: 0.001
Epoch [106/150], Training Loss: 24.7878, Validation Loss Current: 6.2038, Validation Loss AVG: 9.5347, lr: 0.001
Epoch [107/150], Training Loss: 22.7806, Validation Loss Current: 6.1463, Validation Loss AVG: 8.7880, lr: 0.001
Epoch [108/150], Training Loss: 22.5818, Validation Loss Current: 6.0575, Validation Loss AVG: 9.2270, lr: 0.001
Epoch [109/150], Training Loss: 22.7667, Validation Loss Current: 6.3639, Validation Loss AVG: 10.2807, lr: 0.001
Epoch [110/150], Training Loss: 23.2572, Validation Loss Current: 6.0786, Validation Loss AVG: 9.6032, lr: 0.001
Epoch [111/150], Training Loss: 21.4581, Validation Loss Current: 6.3843, Validation Loss AVG: 10.4099, lr: 0.001
Epoch [112/150], Training Loss: 21.7167, Validation Loss Current: 6.1926, Validation Loss AVG: 9.3134, lr: 0.001
Epoch [113/150], Training Loss: 22.3189, Validation Loss Current: 9.6340, Validation Loss AVG: 15.1430, lr: 0.001
Epoch [114/150], Training Loss: 32.5136, Validation Loss Current: 6.8400, Validation Loss AVG: 8.5787, lr: 0.001
Epoch [115/150], Training Loss: 26.2217, Validation Loss Current: 6.6165, Validation Loss AVG: 9.0445, lr: 0.001
Epoch [116/150], Training Loss: 26.0823, Validation Loss Current: 6.6444, Validation Loss AVG: 8.8799, lr: 0.001
Epoch [117/150], Training Loss: 25.2578, Validation Loss Current: 6.2663, Validation Loss AVG: 8.2868, lr: 0.001
Epoch [118/150], Training Loss: 24.1777, Validation Loss Current: 5.9925, Validation Loss AVG: 8.6055, lr: 0.001
Epoch [119/150], Training Loss: 22.1832, Validation Loss Current: 6.0461, Validation Loss AVG: 8.8821, lr: 0.001
Epoch [120/150], Training Loss: 21.1457, Validation Loss Current: 6.1582, Validation Loss AVG: 10.1246, lr: 0.001
Epoch [121/150], Training Loss: 21.4210, Validation Loss Current: 6.3888, Validation Loss AVG: 8.6950, lr: 0.001
Epoch [122/150], Training Loss: 21.9195, Validation Loss Current: 6.2739, Validation Loss AVG: 9.4659, lr: 0.001
Epoch [123/150], Training Loss: 21.8319, Validation Loss Current: 6.2462, Validation Loss AVG: 9.8329, lr: 0.001
Epoch [124/150], Training Loss: 22.0278, Validation Loss Current: 7.0861, Validation Loss AVG: 9.8229, lr: 0.001
Epoch [125/150], Training Loss: 26.8405, Validation Loss Current: 6.1321, Validation Loss AVG: 8.2430, lr: 0.001
Epoch [126/150], Training Loss: 22.0419, Validation Loss Current: 6.0231, Validation Loss AVG: 9.6182, lr: 0.001
Epoch [127/150], Training Loss: 20.3745, Validation Loss Current: 6.1153, Validation Loss AVG: 9.5368, lr: 0.001
Epoch [128/150], Training Loss: 21.8132, Validation Loss Current: 6.3592, Validation Loss AVG: 10.8422, lr: 0.001
Epoch [129/150], Training Loss: 21.8111, Validation Loss Current: 6.2433, Validation Loss AVG: 9.0443, lr: 0.001
Epoch [130/150], Training Loss: 21.0842, Validation Loss Current: 6.2908, Validation Loss AVG: 9.0020, lr: 0.001
Epoch [131/150], Training Loss: 20.3905, Validation Loss Current: 5.8073, Validation Loss AVG: 8.9061, lr: 0.001
Epoch [132/150], Training Loss: 20.9787, Validation Loss Current: 6.6798, Validation Loss AVG: 11.3264, lr: 0.001
Epoch [133/150], Training Loss: 21.0933, Validation Loss Current: 5.8933, Validation Loss AVG: 9.5420, lr: 0.001
Epoch [134/150], Training Loss: 19.3207, Validation Loss Current: 6.1899, Validation Loss AVG: 9.4978, lr: 0.001
Epoch [135/150], Training Loss: 18.9055, Validation Loss Current: 5.8054, Validation Loss AVG: 9.9618, lr: 0.001
Epoch [136/150], Training Loss: 19.0178, Validation Loss Current: 6.3034, Validation Loss AVG: 11.0141, lr: 0.001
Epoch [137/150], Training Loss: 19.2954, Validation Loss Current: 5.7758, Validation Loss AVG: 9.3179, lr: 0.001
Epoch [138/150], Training Loss: 19.0013, Validation Loss Current: 6.7545, Validation Loss AVG: 11.6091, lr: 0.001
Epoch [139/150], Training Loss: 22.7736, Validation Loss Current: 5.6916, Validation Loss AVG: 8.8711, lr: 0.001
Epoch [140/150], Training Loss: 20.1729, Validation Loss Current: 5.7511, Validation Loss AVG: 9.4499, lr: 0.001
Epoch [141/150], Training Loss: 18.8402, Validation Loss Current: 5.7397, Validation Loss AVG: 9.2558, lr: 0.001
Epoch [142/150], Training Loss: 19.3206, Validation Loss Current: 6.3271, Validation Loss AVG: 10.9100, lr: 0.001
Epoch [143/150], Training Loss: 17.5538, Validation Loss Current: 6.2644, Validation Loss AVG: 10.8696, lr: 0.001
Epoch [144/150], Training Loss: 18.0883, Validation Loss Current: 6.6427, Validation Loss AVG: 9.8529, lr: 0.001
Epoch [145/150], Training Loss: 20.1555, Validation Loss Current: 7.8448, Validation Loss AVG: 10.7476, lr: 0.001
Epoch [146/150], Training Loss: 19.3419, Validation Loss Current: 6.4079, Validation Loss AVG: 11.1622, lr: 0.001
Epoch [147/150], Training Loss: 19.7594, Validation Loss Current: 8.3879, Validation Loss AVG: 11.0959, lr: 0.001
Epoch [148/150], Training Loss: 30.7174, Validation Loss Current: 6.8557, Validation Loss AVG: 9.1675, lr: 0.001
Epoch [149/150], Training Loss: 22.7391, Validation Loss Current: 6.2322, Validation Loss AVG: 8.9885, lr: 0.001
Epoch [150/150], Training Loss: 19.8195, Validation Loss Current: 6.1693, Validation Loss AVG: 9.4555, lr: 0.001
Patch distance: 1 finished training. Best epoch: 139 Best val accuracy: [0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.27960526315789475, 0.2779605263157895, 0.3059210526315789, 0.3026315789473684, 0.3223684210526316, 0.3338815789473684, 0.34539473684210525, 0.3305921052631579, 0.34375, 0.34868421052631576, 0.35526315789473684, 0.35526315789473684, 0.3667763157894737, 0.37335526315789475, 0.37006578947368424, 0.39144736842105265, 0.37664473684210525, 0.35526315789473684, 0.39473684210526316, 0.3930921052631579, 0.3980263157894737, 0.40789473684210525, 0.41118421052631576, 0.4095394736842105, 0.43585526315789475, 0.4144736842105263, 0.4506578947368421, 0.4144736842105263, 0.4588815789473684, 0.44901315789473684, 0.4720394736842105, 0.48519736842105265, 0.47039473684210525, 0.5032894736842105, 0.48026315789473684, 0.49506578947368424, 0.43256578947368424, 0.5049342105263158, 0.46875, 0.5131578947368421, 0.524671052631579, 0.5131578947368421, 0.40131578947368424, 0.45723684210526316, 0.5032894736842105, 0.4588815789473684, 0.5148026315789473, 0.5180921052631579, 0.5296052631578947, 0.42598684210526316, 0.48355263157894735, 0.5526315789473685, 0.555921052631579, 0.5608552631578947, 0.5, 0.5411184210526315, 0.48355263157894735, 0.5213815789473685, 0.5279605263157895, 0.4934210526315789, 0.53125, 0.5460526315789473, 0.537828947368421, 0.5427631578947368, 0.5657894736842105, 0.5213815789473685, 0.5805921052631579, 0.5756578947368421, 0.5657894736842105, 0.555921052631579, 0.49506578947368424, 0.5476973684210527, 0.5279605263157895, 0.5230263157894737, 0.569078947368421, 0.5608552631578947, 0.5674342105263158, 0.5575657894736842, 0.5723684210526315, 0.5411184210526315, 0.5822368421052632, 0.39473684210526316, 0.524671052631579, 0.5148026315789473, 0.5279605263157895, 0.5427631578947368, 0.5674342105263158, 0.5740131578947368, 0.5707236842105263, 0.5641447368421053, 0.5575657894736842, 0.5542763157894737, 0.555921052631579, 0.569078947368421, 0.5773026315789473, 0.5756578947368421, 0.5608552631578947, 0.5740131578947368, 0.587171052631579, 0.6069078947368421, 0.5444078947368421, 0.5855263157894737, 0.5888157894736842, 0.6167763157894737, 0.5608552631578947, 0.6052631578947368, 0.5542763157894737, 0.6036184210526315, 0.6200657894736842, 0.6085526315789473, 0.569078947368421, 0.5970394736842105, 0.6134868421052632, 0.5805921052631579, 0.5657894736842105, 0.5460526315789473, 0.5049342105263158, 0.5838815789473685, 0.5855263157894737] Best val loss: 5.691620707511902


----- Training alexnet with sequence: [1, 0.8] -----
Sequence [1] already in state dictionary, jumped
Loaded best state dict for [1]
Current group: 0.8
Epoch [1/75], Training Loss: 21.3687, Validation Loss Current: 8.2382, Validation Loss AVG: 8.2382, lr: 0.001
Epoch [2/75], Training Loss: 21.1575, Validation Loss Current: 8.6753, Validation Loss AVG: 8.6753, lr: 0.001
Epoch [3/75], Training Loss: 25.0322, Validation Loss Current: 10.2106, Validation Loss AVG: 10.2106, lr: 0.001
Epoch [4/75], Training Loss: 33.9743, Validation Loss Current: 8.4915, Validation Loss AVG: 8.4915, lr: 0.001
Epoch [5/75], Training Loss: 27.0565, Validation Loss Current: 8.2036, Validation Loss AVG: 8.2036, lr: 0.001
Epoch [6/75], Training Loss: 23.8734, Validation Loss Current: 8.0950, Validation Loss AVG: 8.0950, lr: 0.001
Epoch [7/75], Training Loss: 23.5332, Validation Loss Current: 9.5436, Validation Loss AVG: 9.5436, lr: 0.001
Epoch [8/75], Training Loss: 23.9377, Validation Loss Current: 7.8403, Validation Loss AVG: 7.8403, lr: 0.001
Epoch [9/75], Training Loss: 19.6389, Validation Loss Current: 8.0138, Validation Loss AVG: 8.0138, lr: 0.001
Epoch [10/75], Training Loss: 20.2500, Validation Loss Current: 7.6734, Validation Loss AVG: 7.6734, lr: 0.001
Epoch [11/75], Training Loss: 19.3892, Validation Loss Current: 8.8461, Validation Loss AVG: 8.8461, lr: 0.001
Epoch [12/75], Training Loss: 19.5285, Validation Loss Current: 7.8591, Validation Loss AVG: 7.8591, lr: 0.001
Epoch [13/75], Training Loss: 18.0329, Validation Loss Current: 8.3490, Validation Loss AVG: 8.3490, lr: 0.001
Epoch [14/75], Training Loss: 17.1043, Validation Loss Current: 8.3855, Validation Loss AVG: 8.3855, lr: 0.001
Epoch [15/75], Training Loss: 18.7203, Validation Loss Current: 8.9021, Validation Loss AVG: 8.9021, lr: 0.001
Epoch [16/75], Training Loss: 20.3304, Validation Loss Current: 8.2017, Validation Loss AVG: 8.2017, lr: 0.001
Epoch [17/75], Training Loss: 21.4129, Validation Loss Current: 11.3738, Validation Loss AVG: 11.3738, lr: 0.001
Epoch [18/75], Training Loss: 27.1478, Validation Loss Current: 9.0265, Validation Loss AVG: 9.0265, lr: 0.001
Epoch [19/75], Training Loss: 21.7984, Validation Loss Current: 7.6709, Validation Loss AVG: 7.6709, lr: 0.001
Epoch [20/75], Training Loss: 20.9824, Validation Loss Current: 8.2731, Validation Loss AVG: 8.2731, lr: 0.001
Epoch [21/75], Training Loss: 20.6193, Validation Loss Current: 8.5505, Validation Loss AVG: 8.5505, lr: 0.001
Epoch [22/75], Training Loss: 18.0342, Validation Loss Current: 8.2177, Validation Loss AVG: 8.2177, lr: 0.001
Epoch [23/75], Training Loss: 16.0603, Validation Loss Current: 9.0321, Validation Loss AVG: 9.0321, lr: 0.001
Epoch [24/75], Training Loss: 15.4638, Validation Loss Current: 9.1351, Validation Loss AVG: 9.1351, lr: 0.001
Epoch [25/75], Training Loss: 16.4405, Validation Loss Current: 8.4110, Validation Loss AVG: 8.4110, lr: 0.001
Epoch [26/75], Training Loss: 17.6605, Validation Loss Current: 9.3325, Validation Loss AVG: 9.3325, lr: 0.001
Epoch [27/75], Training Loss: 19.8027, Validation Loss Current: 8.1453, Validation Loss AVG: 8.1453, lr: 0.001
Epoch [28/75], Training Loss: 17.7837, Validation Loss Current: 8.7461, Validation Loss AVG: 8.7461, lr: 0.001
Epoch [29/75], Training Loss: 14.3906, Validation Loss Current: 8.8894, Validation Loss AVG: 8.8894, lr: 0.001
Epoch [30/75], Training Loss: 13.6147, Validation Loss Current: 8.9252, Validation Loss AVG: 8.9252, lr: 0.001
Epoch [31/75], Training Loss: 12.8204, Validation Loss Current: 9.6662, Validation Loss AVG: 9.6662, lr: 0.001
Epoch [32/75], Training Loss: 13.0559, Validation Loss Current: 9.4626, Validation Loss AVG: 9.4626, lr: 0.001
Epoch [33/75], Training Loss: 14.7147, Validation Loss Current: 11.0273, Validation Loss AVG: 11.0273, lr: 0.001
Epoch [34/75], Training Loss: 15.4281, Validation Loss Current: 8.8636, Validation Loss AVG: 8.8636, lr: 0.001
Epoch [35/75], Training Loss: 14.8184, Validation Loss Current: 9.0592, Validation Loss AVG: 9.0592, lr: 0.001
Epoch [36/75], Training Loss: 14.3488, Validation Loss Current: 9.3624, Validation Loss AVG: 9.3624, lr: 0.001
Epoch [37/75], Training Loss: 14.5544, Validation Loss Current: 9.5698, Validation Loss AVG: 9.5698, lr: 0.001
Epoch [38/75], Training Loss: 14.4647, Validation Loss Current: 13.9144, Validation Loss AVG: 13.9144, lr: 0.001
Epoch [39/75], Training Loss: 24.8072, Validation Loss Current: 7.8537, Validation Loss AVG: 7.8537, lr: 0.001
Epoch [40/75], Training Loss: 17.8347, Validation Loss Current: 11.5695, Validation Loss AVG: 11.5695, lr: 0.001
Epoch [41/75], Training Loss: 25.6580, Validation Loss Current: 8.6499, Validation Loss AVG: 8.6499, lr: 0.001
Epoch [42/75], Training Loss: 17.7498, Validation Loss Current: 8.6962, Validation Loss AVG: 8.6962, lr: 0.001
Epoch [43/75], Training Loss: 14.8007, Validation Loss Current: 8.6470, Validation Loss AVG: 8.6470, lr: 0.001
Epoch [44/75], Training Loss: 16.5707, Validation Loss Current: 8.4580, Validation Loss AVG: 8.4580, lr: 0.001
Epoch [45/75], Training Loss: 13.8099, Validation Loss Current: 9.1237, Validation Loss AVG: 9.1237, lr: 0.001
Epoch [46/75], Training Loss: 12.6835, Validation Loss Current: 11.0532, Validation Loss AVG: 11.0532, lr: 0.001
Epoch [47/75], Training Loss: 13.0524, Validation Loss Current: 9.3094, Validation Loss AVG: 9.3094, lr: 0.001
Epoch [48/75], Training Loss: 11.0868, Validation Loss Current: 11.7056, Validation Loss AVG: 11.7056, lr: 0.001
Epoch [49/75], Training Loss: 19.9997, Validation Loss Current: 10.1970, Validation Loss AVG: 10.1970, lr: 0.001
 --- Early Stopped ---
Patch distance: 0.8 finished training. Best epoch: 19 Best val accuracy: [0.4496710526315789, 0.4496710526315789, 0.34407894736842104, 0.38651315789473684, 0.4036184210526315, 0.42598684210526316, 0.3740131578947368, 0.45953947368421055, 0.47269736842105264, 0.47993421052631574, 0.4184210526315789, 0.47368421052631576, 0.4565789473684211, 0.45000000000000007, 0.4302631578947368, 0.4796052631578947, 0.34013157894736845, 0.41085526315789467, 0.4713815789473685, 0.44703947368421054, 0.43256578947368424, 0.46118421052631586, 0.4463815789473684, 0.44703947368421043, 0.47302631578947363, 0.44868421052631574, 0.4615131578947368, 0.4546052631578947, 0.4546052631578948, 0.4697368421052632, 0.4490131578947369, 0.47236842105263166, 0.3990131578947368, 0.4634868421052631, 0.44901315789473684, 0.4743421052631579, 0.4707236842105263, 0.3194078947368421, 0.4648026315789474, 0.3539473684210527, 0.41578947368421043, 0.45559210526315796, 0.4756578947368421, 0.4720394736842105, 0.4769736842105264, 0.43256578947368424, 0.468092105263158, 0.44703947368421054, 0.42894736842105263] Best val loss: 7.670911014080048


----- Training alexnet with sequence: [1, 0.8, 0.6] -----
Sequence [1] already in state dictionary, jumped
Sequence [1, 0.8] already in state dictionary, jumped
Loaded best state dict for [1, 0.8]
Current group: 0.6
Epoch [1/50], Training Loss: 21.2285, Validation Loss Current: 7.3624, Validation Loss AVG: 7.3624, lr: 0.001
Epoch [2/50], Training Loss: 21.9991, Validation Loss Current: 7.6628, Validation Loss AVG: 7.6628, lr: 0.001
Epoch [3/50], Training Loss: 21.5354, Validation Loss Current: 7.9595, Validation Loss AVG: 7.9595, lr: 0.001
Epoch [4/50], Training Loss: 20.6448, Validation Loss Current: 7.9203, Validation Loss AVG: 7.9203, lr: 0.001
Epoch [5/50], Training Loss: 18.4380, Validation Loss Current: 7.8314, Validation Loss AVG: 7.8314, lr: 0.001
Epoch [6/50], Training Loss: 17.7107, Validation Loss Current: 7.4360, Validation Loss AVG: 7.4360, lr: 0.001
Epoch [7/50], Training Loss: 16.1721, Validation Loss Current: 8.0419, Validation Loss AVG: 8.0419, lr: 0.001
Epoch [8/50], Training Loss: 16.1766, Validation Loss Current: 8.9488, Validation Loss AVG: 8.9488, lr: 0.001
Epoch [9/50], Training Loss: 20.7798, Validation Loss Current: 7.6736, Validation Loss AVG: 7.6736, lr: 0.001
Epoch [10/50], Training Loss: 19.4813, Validation Loss Current: 9.0132, Validation Loss AVG: 9.0132, lr: 0.001
Epoch [11/50], Training Loss: 24.5659, Validation Loss Current: 7.7075, Validation Loss AVG: 7.7075, lr: 0.001
Epoch [12/50], Training Loss: 21.7813, Validation Loss Current: 7.4935, Validation Loss AVG: 7.4935, lr: 0.001
Epoch [13/50], Training Loss: 18.0968, Validation Loss Current: 7.9153, Validation Loss AVG: 7.9153, lr: 0.001
Epoch [14/50], Training Loss: 18.0647, Validation Loss Current: 8.1484, Validation Loss AVG: 8.1484, lr: 0.001
Epoch [15/50], Training Loss: 17.7893, Validation Loss Current: 7.4371, Validation Loss AVG: 7.4371, lr: 0.001
Epoch [16/50], Training Loss: 14.1013, Validation Loss Current: 8.1223, Validation Loss AVG: 8.1223, lr: 0.001
Epoch [17/50], Training Loss: 13.3200, Validation Loss Current: 8.8054, Validation Loss AVG: 8.8054, lr: 0.001
Epoch [18/50], Training Loss: 14.5040, Validation Loss Current: 8.7819, Validation Loss AVG: 8.7819, lr: 0.001
Epoch [19/50], Training Loss: 14.4311, Validation Loss Current: 13.7478, Validation Loss AVG: 13.7478, lr: 0.001
Epoch [20/50], Training Loss: 30.7044, Validation Loss Current: 8.4488, Validation Loss AVG: 8.4488, lr: 0.001
Epoch [21/50], Training Loss: 24.3617, Validation Loss Current: 8.5897, Validation Loss AVG: 8.5897, lr: 0.001
Epoch [22/50], Training Loss: 20.1171, Validation Loss Current: 7.8110, Validation Loss AVG: 7.8110, lr: 0.001
Epoch [23/50], Training Loss: 17.4467, Validation Loss Current: 7.8047, Validation Loss AVG: 7.8047, lr: 0.001
Epoch [24/50], Training Loss: 14.7718, Validation Loss Current: 7.6985, Validation Loss AVG: 7.6985, lr: 0.001
Epoch [25/50], Training Loss: 13.2899, Validation Loss Current: 8.8705, Validation Loss AVG: 8.8705, lr: 0.001
Epoch [26/50], Training Loss: 16.7761, Validation Loss Current: 8.1490, Validation Loss AVG: 8.1490, lr: 0.001
Epoch [27/50], Training Loss: 13.6947, Validation Loss Current: 8.1749, Validation Loss AVG: 8.1749, lr: 0.001
Epoch [28/50], Training Loss: 12.2585, Validation Loss Current: 8.4927, Validation Loss AVG: 8.4927, lr: 0.001
Epoch [29/50], Training Loss: 10.8045, Validation Loss Current: 9.0431, Validation Loss AVG: 9.0431, lr: 0.001
Epoch [30/50], Training Loss: 11.4542, Validation Loss Current: 9.8611, Validation Loss AVG: 9.8611, lr: 0.001
Epoch [31/50], Training Loss: 14.7515, Validation Loss Current: 10.0761, Validation Loss AVG: 10.0761, lr: 0.001
 --- Early Stopped ---
Patch distance: 0.6 finished training. Best epoch: 1 Best val accuracy: [0.4865131578947368, 0.5134868421052631, 0.4855263157894737, 0.48125, 0.49243421052631575, 0.5069078947368422, 0.4855263157894737, 0.46875, 0.5019736842105262, 0.4542763157894737, 0.47171052631578947, 0.5082236842105263, 0.5072368421052631, 0.4921052631578947, 0.5009868421052632, 0.5023026315789474, 0.4970394736842104, 0.49868421052631584, 0.3832236842105263, 0.40361842105263157, 0.44671052631578945, 0.48388157894736833, 0.49407894736842106, 0.5006578947368421, 0.4921052631578947, 0.4894736842105263, 0.5098684210526315, 0.5059210526315789, 0.5082236842105263, 0.47664473684210523, 0.4855263157894737] Best val loss: 7.362366771697998


----- Training alexnet with sequence: [1, 0.8, 0.6, 0.4] -----
Sequence [1] already in state dictionary, jumped
Sequence [1, 0.8] already in state dictionary, jumped
Sequence [1, 0.8, 0.6] already in state dictionary, jumped
Loaded best state dict for [1, 0.8, 0.6]
Current group: 0.4
Epoch [1/38], Training Loss: 25.9680, Validation Loss Current: 7.4031, Validation Loss AVG: 7.4031, lr: 0.001
Epoch [2/38], Training Loss: 23.6252, Validation Loss Current: 8.1049, Validation Loss AVG: 8.1049, lr: 0.001
Epoch [3/38], Training Loss: 24.0677, Validation Loss Current: 7.3253, Validation Loss AVG: 7.3253, lr: 0.001
Epoch [4/38], Training Loss: 20.4670, Validation Loss Current: 7.8503, Validation Loss AVG: 7.8503, lr: 0.001
Epoch [5/38], Training Loss: 20.6247, Validation Loss Current: 7.9156, Validation Loss AVG: 7.9156, lr: 0.001
Epoch [6/38], Training Loss: 19.4212, Validation Loss Current: 7.7389, Validation Loss AVG: 7.7389, lr: 0.001
Epoch [7/38], Training Loss: 18.8328, Validation Loss Current: 7.9940, Validation Loss AVG: 7.9940, lr: 0.001
Epoch [8/38], Training Loss: 18.5364, Validation Loss Current: 7.7677, Validation Loss AVG: 7.7677, lr: 0.001
Epoch [9/38], Training Loss: 19.4798, Validation Loss Current: 8.5790, Validation Loss AVG: 8.5790, lr: 0.001
Epoch [10/38], Training Loss: 24.6002, Validation Loss Current: 7.8608, Validation Loss AVG: 7.8608, lr: 0.001
Epoch [11/38], Training Loss: 20.0773, Validation Loss Current: 7.9063, Validation Loss AVG: 7.9063, lr: 0.001
Epoch [12/38], Training Loss: 19.7141, Validation Loss Current: 7.7033, Validation Loss AVG: 7.7033, lr: 0.001
Epoch [13/38], Training Loss: 18.7580, Validation Loss Current: 7.9451, Validation Loss AVG: 7.9451, lr: 0.001
Epoch [14/38], Training Loss: 20.8273, Validation Loss Current: 7.7989, Validation Loss AVG: 7.7989, lr: 0.001
Epoch [15/38], Training Loss: 18.7351, Validation Loss Current: 7.6388, Validation Loss AVG: 7.6388, lr: 0.001
Epoch [16/38], Training Loss: 16.3113, Validation Loss Current: 7.7573, Validation Loss AVG: 7.7573, lr: 0.001
Epoch [17/38], Training Loss: 16.2199, Validation Loss Current: 8.4359, Validation Loss AVG: 8.4359, lr: 0.001
Epoch [18/38], Training Loss: 14.7007, Validation Loss Current: 8.7389, Validation Loss AVG: 8.7389, lr: 0.001
Epoch [19/38], Training Loss: 13.4361, Validation Loss Current: 9.0669, Validation Loss AVG: 9.0669, lr: 0.001
Epoch [20/38], Training Loss: 13.9773, Validation Loss Current: 8.3948, Validation Loss AVG: 8.3948, lr: 0.001
Epoch [21/38], Training Loss: 13.4869, Validation Loss Current: 8.2976, Validation Loss AVG: 8.2976, lr: 0.001
Epoch [22/38], Training Loss: 11.6454, Validation Loss Current: 9.7394, Validation Loss AVG: 9.7394, lr: 0.001
Epoch [23/38], Training Loss: 14.4478, Validation Loss Current: 8.5972, Validation Loss AVG: 8.5972, lr: 0.001
Epoch [24/38], Training Loss: 12.1547, Validation Loss Current: 9.0806, Validation Loss AVG: 9.0806, lr: 0.001
Epoch [25/38], Training Loss: 11.7184, Validation Loss Current: 8.9422, Validation Loss AVG: 8.9422, lr: 0.001
Epoch [26/38], Training Loss: 9.6522, Validation Loss Current: 9.1945, Validation Loss AVG: 9.1945, lr: 0.001
Epoch [27/38], Training Loss: 10.6215, Validation Loss Current: 9.4864, Validation Loss AVG: 9.4864, lr: 0.001
Epoch [28/38], Training Loss: 11.0170, Validation Loss Current: 9.6411, Validation Loss AVG: 9.6411, lr: 0.001
Epoch [29/38], Training Loss: 11.0382, Validation Loss Current: 11.5368, Validation Loss AVG: 11.5368, lr: 0.001
Epoch [30/38], Training Loss: 22.7996, Validation Loss Current: 8.3563, Validation Loss AVG: 8.3563, lr: 0.001
Epoch [31/38], Training Loss: 14.5847, Validation Loss Current: 8.9277, Validation Loss AVG: 8.9277, lr: 0.001
Epoch [32/38], Training Loss: 12.1805, Validation Loss Current: 8.8722, Validation Loss AVG: 8.8722, lr: 0.001
Epoch [33/38], Training Loss: 10.9852, Validation Loss Current: 9.2889, Validation Loss AVG: 9.2889, lr: 0.001
 --- Early Stopped ---
Patch distance: 0.4 finished training. Best epoch: 3 Best val accuracy: [0.4983552631578947, 0.4786184210526316, 0.5164473684210525, 0.49375, 0.5023026315789474, 0.5082236842105263, 0.5032894736842105, 0.48881578947368415, 0.4953947368421052, 0.49375, 0.4921052631578947, 0.4898026315789473, 0.48914473684210524, 0.4953947368421052, 0.5217105263157894, 0.5023026315789474, 0.48684210526315785, 0.4901315789473684, 0.5105263157894737, 0.50625, 0.5128289473684211, 0.4875, 0.49671052631578955, 0.4776315789473685, 0.4967105263157895, 0.5138157894736842, 0.5016447368421053, 0.5052631578947369, 0.39375, 0.4480263157894737, 0.47697368421052627, 0.49671052631578955, 0.47828947368421054] Best val loss: 7.325318086147308


----- Training alexnet with sequence: [1, 0.8, 0.6, 0.4, 0.2] -----
Sequence [1] already in state dictionary, jumped
Sequence [1, 0.8] already in state dictionary, jumped
Sequence [1, 0.8, 0.6] already in state dictionary, jumped
Sequence [1, 0.8, 0.6, 0.4] already in state dictionary, jumped
Loaded best state dict for [1, 0.8, 0.6, 0.4]
Current group: 0.2
Epoch [1/30], Training Loss: 33.0719, Validation Loss Current: 8.7266, Validation Loss AVG: 8.7266, lr: 0.001
Epoch [2/30], Training Loss: 29.2446, Validation Loss Current: 11.3122, Validation Loss AVG: 11.3122, lr: 0.001
Epoch [3/30], Training Loss: 32.5250, Validation Loss Current: 8.8208, Validation Loss AVG: 8.8208, lr: 0.001
Epoch [4/30], Training Loss: 28.1431, Validation Loss Current: 12.2752, Validation Loss AVG: 12.2752, lr: 0.001
Epoch [5/30], Training Loss: 30.5247, Validation Loss Current: 9.2539, Validation Loss AVG: 9.2539, lr: 0.001
Epoch [6/30], Training Loss: 28.6930, Validation Loss Current: 10.3346, Validation Loss AVG: 10.3346, lr: 0.001
Epoch [7/30], Training Loss: 27.3612, Validation Loss Current: 10.3900, Validation Loss AVG: 10.3900, lr: 0.001
Epoch [8/30], Training Loss: 26.0110, Validation Loss Current: 11.0616, Validation Loss AVG: 11.0616, lr: 0.001
Epoch [9/30], Training Loss: 28.1049, Validation Loss Current: 9.0024, Validation Loss AVG: 9.0024, lr: 0.001
Epoch [10/30], Training Loss: 25.3217, Validation Loss Current: 10.9647, Validation Loss AVG: 10.9647, lr: 0.001
Epoch [11/30], Training Loss: 23.7825, Validation Loss Current: 10.9247, Validation Loss AVG: 10.9247, lr: 0.001
Epoch [12/30], Training Loss: 26.3685, Validation Loss Current: 9.9864, Validation Loss AVG: 9.9864, lr: 0.001
Epoch [13/30], Training Loss: 22.6615, Validation Loss Current: 10.9210, Validation Loss AVG: 10.9210, lr: 0.001
Epoch [14/30], Training Loss: 21.5563, Validation Loss Current: 10.4396, Validation Loss AVG: 10.4396, lr: 0.001
Epoch [15/30], Training Loss: 21.5336, Validation Loss Current: 10.7901, Validation Loss AVG: 10.7901, lr: 0.001
Epoch [16/30], Training Loss: 21.0024, Validation Loss Current: 10.3019, Validation Loss AVG: 10.3019, lr: 0.001
Epoch [17/30], Training Loss: 21.2315, Validation Loss Current: 11.6853, Validation Loss AVG: 11.6853, lr: 0.001
Epoch [18/30], Training Loss: 20.6475, Validation Loss Current: 11.8282, Validation Loss AVG: 11.8282, lr: 0.001
Epoch [19/30], Training Loss: 22.1172, Validation Loss Current: 11.3543, Validation Loss AVG: 11.3543, lr: 0.001
Epoch [20/30], Training Loss: 21.2950, Validation Loss Current: 12.1549, Validation Loss AVG: 12.1549, lr: 0.001
Epoch [21/30], Training Loss: 22.7374, Validation Loss Current: 11.0296, Validation Loss AVG: 11.0296, lr: 0.001
Epoch [22/30], Training Loss: 20.4987, Validation Loss Current: 11.9906, Validation Loss AVG: 11.9906, lr: 0.001
Epoch [23/30], Training Loss: 19.3882, Validation Loss Current: 11.7823, Validation Loss AVG: 11.7823, lr: 0.001
Epoch [24/30], Training Loss: 20.7134, Validation Loss Current: 12.4239, Validation Loss AVG: 12.4239, lr: 0.001
Epoch [25/30], Training Loss: 20.8710, Validation Loss Current: 11.6059, Validation Loss AVG: 11.6059, lr: 0.001
Epoch [26/30], Training Loss: 19.9847, Validation Loss Current: 12.9775, Validation Loss AVG: 12.9775, lr: 0.001
Epoch [27/30], Training Loss: 17.1974, Validation Loss Current: 12.8282, Validation Loss AVG: 12.8282, lr: 0.001
Epoch [28/30], Training Loss: 15.7509, Validation Loss Current: 14.1393, Validation Loss AVG: 14.1393, lr: 0.001
Epoch [29/30], Training Loss: 16.6419, Validation Loss Current: 12.9127, Validation Loss AVG: 12.9127, lr: 0.001
Epoch [30/30], Training Loss: 16.5364, Validation Loss Current: 13.5152, Validation Loss AVG: 13.5152, lr: 0.001
Epoch [31/30], Training Loss: 17.3750, Validation Loss Current: 13.7619, Validation Loss AVG: 13.7619, lr: 0.001
 --- Early Stopped ---
Patch distance: 0.2 finished training. Best epoch: 1 Best val accuracy: [0.44342105263157894, 0.33125, 0.3990131578947368, 0.29210526315789476, 0.3917763157894737, 0.34769736842105264, 0.3447368421052631, 0.3559210526315789, 0.40592105263157896, 0.3141447368421053, 0.37763157894736843, 0.37532894736842104, 0.3796052631578947, 0.3680921052631579, 0.37203947368421053, 0.41875, 0.37302631578947365, 0.38355263157894737, 0.3815789473684211, 0.3503289473684211, 0.37203947368421053, 0.3605263157894737, 0.3733552631578947, 0.3345394736842105, 0.3759868421052632, 0.34210526315789475, 0.34736842105263155, 0.35559210526315793, 0.36578947368421055, 0.38815789473684215, 0.35460526315789476] Best val loss: 8.726552438735961


Fold: 2
----- Training alexnet with sequence: [1] -----
Current group: 1
Epoch [1/150], Training Loss: 41.5817, Validation Loss Current: 10.3798, Validation Loss AVG: 10.3788, lr: 0.001
Epoch [2/150], Training Loss: 41.4236, Validation Loss Current: 10.3449, Validation Loss AVG: 10.3425, lr: 0.001
Epoch [3/150], Training Loss: 41.2526, Validation Loss Current: 10.3075, Validation Loss AVG: 10.3074, lr: 0.001
Epoch [4/150], Training Loss: 41.1777, Validation Loss Current: 10.2686, Validation Loss AVG: 10.2669, lr: 0.001
Epoch [5/150], Training Loss: 40.8403, Validation Loss Current: 10.2342, Validation Loss AVG: 10.2246, lr: 0.001
Epoch [6/150], Training Loss: 40.5379, Validation Loss Current: 10.1664, Validation Loss AVG: 10.1536, lr: 0.001
Epoch [7/150], Training Loss: 40.3133, Validation Loss Current: 10.1076, Validation Loss AVG: 10.0867, lr: 0.001
Epoch [8/150], Training Loss: 39.9226, Validation Loss Current: 10.0967, Validation Loss AVG: 10.0927, lr: 0.001
Epoch [9/150], Training Loss: 39.7942, Validation Loss Current: 10.1044, Validation Loss AVG: 10.0907, lr: 0.001
Epoch [10/150], Training Loss: 40.0599, Validation Loss Current: 10.0696, Validation Loss AVG: 10.0708, lr: 0.001
Epoch [11/150], Training Loss: 40.1326, Validation Loss Current: 10.0567, Validation Loss AVG: 10.0673, lr: 0.001
Epoch [12/150], Training Loss: 40.3168, Validation Loss Current: 10.0554, Validation Loss AVG: 10.0725, lr: 0.001
Epoch [13/150], Training Loss: 39.4921, Validation Loss Current: 10.0074, Validation Loss AVG: 10.0664, lr: 0.001
Epoch [14/150], Training Loss: 39.4143, Validation Loss Current: 10.0145, Validation Loss AVG: 10.1298, lr: 0.001
Epoch [15/150], Training Loss: 39.7096, Validation Loss Current: 9.9869, Validation Loss AVG: 10.1078, lr: 0.001
Epoch [16/150], Training Loss: 39.7705, Validation Loss Current: 9.9980, Validation Loss AVG: 10.0980, lr: 0.001
Epoch [17/150], Training Loss: 39.0425, Validation Loss Current: 9.9647, Validation Loss AVG: 10.1560, lr: 0.001
Epoch [18/150], Training Loss: 39.6168, Validation Loss Current: 9.9318, Validation Loss AVG: 10.1540, lr: 0.001
Epoch [19/150], Training Loss: 39.3928, Validation Loss Current: 9.8894, Validation Loss AVG: 10.1278, lr: 0.001
Epoch [20/150], Training Loss: 38.8974, Validation Loss Current: 9.8568, Validation Loss AVG: 10.1566, lr: 0.001
Epoch [21/150], Training Loss: 39.4874, Validation Loss Current: 9.8395, Validation Loss AVG: 10.0523, lr: 0.001
Epoch [22/150], Training Loss: 39.3188, Validation Loss Current: 9.7965, Validation Loss AVG: 10.1701, lr: 0.001
Epoch [23/150], Training Loss: 38.5372, Validation Loss Current: 9.7730, Validation Loss AVG: 10.0737, lr: 0.001
Epoch [24/150], Training Loss: 38.8265, Validation Loss Current: 9.7308, Validation Loss AVG: 9.9937, lr: 0.001
Epoch [25/150], Training Loss: 38.7466, Validation Loss Current: 9.6890, Validation Loss AVG: 10.0509, lr: 0.001
Epoch [26/150], Training Loss: 38.9286, Validation Loss Current: 9.6726, Validation Loss AVG: 9.9178, lr: 0.001
Epoch [27/150], Training Loss: 38.2089, Validation Loss Current: 9.7842, Validation Loss AVG: 10.8722, lr: 0.001
Epoch [28/150], Training Loss: 38.2039, Validation Loss Current: 9.6288, Validation Loss AVG: 10.1132, lr: 0.001
Epoch [29/150], Training Loss: 37.9947, Validation Loss Current: 9.5890, Validation Loss AVG: 10.1485, lr: 0.001
Epoch [30/150], Training Loss: 38.0818, Validation Loss Current: 9.5128, Validation Loss AVG: 9.9345, lr: 0.001
Epoch [31/150], Training Loss: 37.2741, Validation Loss Current: 9.4568, Validation Loss AVG: 10.1504, lr: 0.001
Epoch [32/150], Training Loss: 37.1086, Validation Loss Current: 9.3359, Validation Loss AVG: 9.8213, lr: 0.001
Epoch [33/150], Training Loss: 35.9265, Validation Loss Current: 9.5075, Validation Loss AVG: 11.1249, lr: 0.001
Epoch [34/150], Training Loss: 37.0929, Validation Loss Current: 9.2044, Validation Loss AVG: 9.7258, lr: 0.001
Epoch [35/150], Training Loss: 36.7620, Validation Loss Current: 9.2164, Validation Loss AVG: 10.7369, lr: 0.001
Epoch [36/150], Training Loss: 37.6988, Validation Loss Current: 9.1571, Validation Loss AVG: 9.5176, lr: 0.001
Epoch [37/150], Training Loss: 37.9063, Validation Loss Current: 8.9947, Validation Loss AVG: 9.5413, lr: 0.001
Epoch [38/150], Training Loss: 35.0301, Validation Loss Current: 8.9866, Validation Loss AVG: 10.0108, lr: 0.001
Epoch [39/150], Training Loss: 35.3901, Validation Loss Current: 8.6569, Validation Loss AVG: 9.4191, lr: 0.001
Epoch [40/150], Training Loss: 34.2781, Validation Loss Current: 8.5710, Validation Loss AVG: 9.2408, lr: 0.001
Epoch [41/150], Training Loss: 34.3189, Validation Loss Current: 9.1990, Validation Loss AVG: 9.9379, lr: 0.001
Epoch [42/150], Training Loss: 35.2123, Validation Loss Current: 8.3032, Validation Loss AVG: 9.0534, lr: 0.001
Epoch [43/150], Training Loss: 33.1673, Validation Loss Current: 8.3182, Validation Loss AVG: 9.0526, lr: 0.001
Epoch [44/150], Training Loss: 32.2635, Validation Loss Current: 9.6690, Validation Loss AVG: 11.4876, lr: 0.001
Epoch [45/150], Training Loss: 34.7076, Validation Loss Current: 8.4369, Validation Loss AVG: 9.8782, lr: 0.001
Epoch [46/150], Training Loss: 32.6659, Validation Loss Current: 8.3294, Validation Loss AVG: 9.1724, lr: 0.001
Epoch [47/150], Training Loss: 32.2342, Validation Loss Current: 8.0961, Validation Loss AVG: 9.0130, lr: 0.001
Epoch [48/150], Training Loss: 31.6973, Validation Loss Current: 7.9881, Validation Loss AVG: 9.2012, lr: 0.001
Epoch [49/150], Training Loss: 32.3521, Validation Loss Current: 8.6058, Validation Loss AVG: 9.6887, lr: 0.001
Epoch [50/150], Training Loss: 32.6203, Validation Loss Current: 8.3622, Validation Loss AVG: 10.1923, lr: 0.001
Epoch [51/150], Training Loss: 35.6158, Validation Loss Current: 8.7295, Validation Loss AVG: 9.6387, lr: 0.001
Epoch [52/150], Training Loss: 33.6209, Validation Loss Current: 8.0028, Validation Loss AVG: 8.7684, lr: 0.001
Epoch [53/150], Training Loss: 32.3175, Validation Loss Current: 7.8226, Validation Loss AVG: 8.9496, lr: 0.001
Epoch [54/150], Training Loss: 31.6606, Validation Loss Current: 7.6648, Validation Loss AVG: 8.9124, lr: 0.001
Epoch [55/150], Training Loss: 30.9457, Validation Loss Current: 8.1508, Validation Loss AVG: 9.7660, lr: 0.001
Epoch [56/150], Training Loss: 31.7329, Validation Loss Current: 7.6668, Validation Loss AVG: 8.6830, lr: 0.001
Epoch [57/150], Training Loss: 30.8241, Validation Loss Current: 7.6019, Validation Loss AVG: 8.6265, lr: 0.001
Epoch [58/150], Training Loss: 31.1161, Validation Loss Current: 7.7807, Validation Loss AVG: 8.5798, lr: 0.001
Epoch [59/150], Training Loss: 29.6963, Validation Loss Current: 7.4892, Validation Loss AVG: 8.6068, lr: 0.001
Epoch [60/150], Training Loss: 29.4674, Validation Loss Current: 7.5996, Validation Loss AVG: 8.5541, lr: 0.001
Epoch [61/150], Training Loss: 29.3645, Validation Loss Current: 7.4289, Validation Loss AVG: 9.1538, lr: 0.001
Epoch [62/150], Training Loss: 29.1946, Validation Loss Current: 7.4624, Validation Loss AVG: 9.1250, lr: 0.001
Epoch [63/150], Training Loss: 28.7807, Validation Loss Current: 7.5757, Validation Loss AVG: 9.7713, lr: 0.001
Epoch [64/150], Training Loss: 29.1202, Validation Loss Current: 7.6000, Validation Loss AVG: 8.8001, lr: 0.001
Epoch [65/150], Training Loss: 28.8730, Validation Loss Current: 7.2707, Validation Loss AVG: 8.3912, lr: 0.001
Epoch [66/150], Training Loss: 28.6554, Validation Loss Current: 7.9000, Validation Loss AVG: 9.9798, lr: 0.001
Epoch [67/150], Training Loss: 29.2676, Validation Loss Current: 7.5303, Validation Loss AVG: 9.8457, lr: 0.001
Epoch [68/150], Training Loss: 29.5655, Validation Loss Current: 7.1103, Validation Loss AVG: 8.9644, lr: 0.001
Epoch [69/150], Training Loss: 28.8555, Validation Loss Current: 7.3720, Validation Loss AVG: 8.5983, lr: 0.001
Epoch [70/150], Training Loss: 28.2391, Validation Loss Current: 7.1765, Validation Loss AVG: 8.2650, lr: 0.001
Epoch [71/150], Training Loss: 28.6902, Validation Loss Current: 7.3549, Validation Loss AVG: 9.2182, lr: 0.001
Epoch [72/150], Training Loss: 27.1048, Validation Loss Current: 6.9010, Validation Loss AVG: 9.0245, lr: 0.001
Epoch [73/150], Training Loss: 28.2895, Validation Loss Current: 7.6253, Validation Loss AVG: 11.3064, lr: 0.001
Epoch [74/150], Training Loss: 31.3457, Validation Loss Current: 7.1581, Validation Loss AVG: 8.2691, lr: 0.001
Epoch [75/150], Training Loss: 28.7844, Validation Loss Current: 7.5614, Validation Loss AVG: 10.3807, lr: 0.001
Epoch [76/150], Training Loss: 28.6017, Validation Loss Current: 7.5300, Validation Loss AVG: 8.4049, lr: 0.001
Epoch [77/150], Training Loss: 27.8634, Validation Loss Current: 7.0882, Validation Loss AVG: 8.5304, lr: 0.001
Epoch [78/150], Training Loss: 26.6607, Validation Loss Current: 6.9859, Validation Loss AVG: 8.8627, lr: 0.001
Epoch [79/150], Training Loss: 26.6914, Validation Loss Current: 7.2404, Validation Loss AVG: 10.3350, lr: 0.001
Epoch [80/150], Training Loss: 26.4750, Validation Loss Current: 7.0786, Validation Loss AVG: 10.4522, lr: 0.001
Epoch [81/150], Training Loss: 25.0017, Validation Loss Current: 6.8440, Validation Loss AVG: 8.9715, lr: 0.001
Epoch [82/150], Training Loss: 24.9822, Validation Loss Current: 6.6497, Validation Loss AVG: 9.4155, lr: 0.001
Epoch [83/150], Training Loss: 25.5570, Validation Loss Current: 7.8519, Validation Loss AVG: 11.9741, lr: 0.001
Epoch [84/150], Training Loss: 30.4993, Validation Loss Current: 7.3956, Validation Loss AVG: 8.3616, lr: 0.001
Epoch [85/150], Training Loss: 27.3581, Validation Loss Current: 7.7593, Validation Loss AVG: 10.3067, lr: 0.001
Epoch [86/150], Training Loss: 27.3131, Validation Loss Current: 6.8872, Validation Loss AVG: 8.8839, lr: 0.001
Epoch [87/150], Training Loss: 24.7634, Validation Loss Current: 6.7977, Validation Loss AVG: 8.2478, lr: 0.001
Epoch [88/150], Training Loss: 23.7988, Validation Loss Current: 6.7689, Validation Loss AVG: 9.1194, lr: 0.001
Epoch [89/150], Training Loss: 24.1911, Validation Loss Current: 8.4766, Validation Loss AVG: 10.1938, lr: 0.001
Epoch [90/150], Training Loss: 25.6871, Validation Loss Current: 6.7718, Validation Loss AVG: 9.0942, lr: 0.001
Epoch [91/150], Training Loss: 24.0280, Validation Loss Current: 6.7228, Validation Loss AVG: 9.3576, lr: 0.001
Epoch [92/150], Training Loss: 23.3524, Validation Loss Current: 6.9346, Validation Loss AVG: 8.7535, lr: 0.001
Epoch [93/150], Training Loss: 23.9263, Validation Loss Current: 7.0819, Validation Loss AVG: 8.5171, lr: 0.001
Epoch [94/150], Training Loss: 23.6273, Validation Loss Current: 6.8017, Validation Loss AVG: 9.0360, lr: 0.001
Epoch [95/150], Training Loss: 23.8087, Validation Loss Current: 6.5257, Validation Loss AVG: 9.3599, lr: 0.001
Epoch [96/150], Training Loss: 23.6468, Validation Loss Current: 6.5330, Validation Loss AVG: 9.5675, lr: 0.001
Epoch [97/150], Training Loss: 23.2443, Validation Loss Current: 6.6737, Validation Loss AVG: 9.2368, lr: 0.001
Epoch [98/150], Training Loss: 22.2916, Validation Loss Current: 6.4676, Validation Loss AVG: 9.6098, lr: 0.001
Epoch [99/150], Training Loss: 21.5325, Validation Loss Current: 6.4321, Validation Loss AVG: 8.9171, lr: 0.001
Epoch [100/150], Training Loss: 21.7463, Validation Loss Current: 6.6165, Validation Loss AVG: 10.2488, lr: 0.001
Epoch [101/150], Training Loss: 22.2654, Validation Loss Current: 6.4101, Validation Loss AVG: 9.5053, lr: 0.001
Epoch [102/150], Training Loss: 22.0023, Validation Loss Current: 6.6788, Validation Loss AVG: 9.7758, lr: 0.001
Epoch [103/150], Training Loss: 21.9787, Validation Loss Current: 6.4957, Validation Loss AVG: 8.8904, lr: 0.001
Epoch [104/150], Training Loss: 21.5041, Validation Loss Current: 6.1878, Validation Loss AVG: 9.0509, lr: 0.001
Epoch [105/150], Training Loss: 20.5678, Validation Loss Current: 7.2819, Validation Loss AVG: 9.8996, lr: 0.001
Epoch [106/150], Training Loss: 21.2889, Validation Loss Current: 6.2676, Validation Loss AVG: 10.0918, lr: 0.001
Epoch [107/150], Training Loss: 20.2942, Validation Loss Current: 6.6101, Validation Loss AVG: 10.5612, lr: 0.001
Epoch [108/150], Training Loss: 21.0297, Validation Loss Current: 6.4215, Validation Loss AVG: 8.9041, lr: 0.001
Epoch [109/150], Training Loss: 21.2530, Validation Loss Current: 6.7873, Validation Loss AVG: 10.3802, lr: 0.001
Epoch [110/150], Training Loss: 19.9555, Validation Loss Current: 6.3081, Validation Loss AVG: 10.2569, lr: 0.001
Epoch [111/150], Training Loss: 20.9739, Validation Loss Current: 9.5191, Validation Loss AVG: 15.4181, lr: 0.001
Epoch [112/150], Training Loss: 26.6772, Validation Loss Current: 6.3365, Validation Loss AVG: 9.3014, lr: 0.001
Epoch [113/150], Training Loss: 21.8136, Validation Loss Current: 6.4073, Validation Loss AVG: 9.9355, lr: 0.001
Epoch [114/150], Training Loss: 21.1027, Validation Loss Current: 6.2505, Validation Loss AVG: 8.7196, lr: 0.001
Epoch [115/150], Training Loss: 22.5401, Validation Loss Current: 7.3848, Validation Loss AVG: 11.8669, lr: 0.001
Epoch [116/150], Training Loss: 22.3517, Validation Loss Current: 6.4276, Validation Loss AVG: 9.1163, lr: 0.001
Epoch [117/150], Training Loss: 23.1998, Validation Loss Current: 7.3433, Validation Loss AVG: 11.8091, lr: 0.001
Epoch [118/150], Training Loss: 25.4940, Validation Loss Current: 6.3842, Validation Loss AVG: 8.6118, lr: 0.001
Epoch [119/150], Training Loss: 22.8658, Validation Loss Current: 6.4653, Validation Loss AVG: 9.7564, lr: 0.001
Epoch [120/150], Training Loss: 20.4392, Validation Loss Current: 6.2576, Validation Loss AVG: 9.1861, lr: 0.001
Epoch [121/150], Training Loss: 19.2179, Validation Loss Current: 6.2829, Validation Loss AVG: 10.0590, lr: 0.001
Epoch [122/150], Training Loss: 19.5819, Validation Loss Current: 6.7802, Validation Loss AVG: 11.0378, lr: 0.001
Epoch [123/150], Training Loss: 19.2962, Validation Loss Current: 6.0696, Validation Loss AVG: 9.5103, lr: 0.001
Epoch [124/150], Training Loss: 18.6056, Validation Loss Current: 7.4524, Validation Loss AVG: 12.9494, lr: 0.001
Epoch [125/150], Training Loss: 21.1994, Validation Loss Current: 6.2999, Validation Loss AVG: 9.3660, lr: 0.001
Epoch [126/150], Training Loss: 20.0156, Validation Loss Current: 6.9294, Validation Loss AVG: 9.5548, lr: 0.001
Epoch [127/150], Training Loss: 21.1806, Validation Loss Current: 6.0587, Validation Loss AVG: 9.6520, lr: 0.001
Epoch [128/150], Training Loss: 18.8332, Validation Loss Current: 7.3397, Validation Loss AVG: 9.5007, lr: 0.001
Epoch [129/150], Training Loss: 21.7791, Validation Loss Current: 6.1086, Validation Loss AVG: 9.0391, lr: 0.001
Epoch [130/150], Training Loss: 20.7934, Validation Loss Current: 8.4086, Validation Loss AVG: 11.6041, lr: 0.001
Epoch [131/150], Training Loss: 28.2716, Validation Loss Current: 6.9212, Validation Loss AVG: 8.4100, lr: 0.001
Epoch [132/150], Training Loss: 21.1434, Validation Loss Current: 7.1760, Validation Loss AVG: 9.0354, lr: 0.001
Epoch [133/150], Training Loss: 19.7115, Validation Loss Current: 6.5940, Validation Loss AVG: 8.9851, lr: 0.001
Epoch [134/150], Training Loss: 19.0930, Validation Loss Current: 11.8915, Validation Loss AVG: 15.2678, lr: 0.001
Epoch [135/150], Training Loss: 27.6924, Validation Loss Current: 6.8923, Validation Loss AVG: 10.2737, lr: 0.001
Epoch [136/150], Training Loss: 20.4856, Validation Loss Current: 6.8285, Validation Loss AVG: 8.7229, lr: 0.001
Epoch [137/150], Training Loss: 19.7947, Validation Loss Current: 6.1601, Validation Loss AVG: 9.5798, lr: 0.001
Epoch [138/150], Training Loss: 19.0005, Validation Loss Current: 6.2328, Validation Loss AVG: 9.4657, lr: 0.001
Epoch [139/150], Training Loss: 17.0655, Validation Loss Current: 6.4450, Validation Loss AVG: 10.9020, lr: 0.001
Epoch [140/150], Training Loss: 17.2962, Validation Loss Current: 6.3094, Validation Loss AVG: 10.8276, lr: 0.001
Epoch [141/150], Training Loss: 16.6603, Validation Loss Current: 6.8418, Validation Loss AVG: 10.5017, lr: 0.001
Epoch [142/150], Training Loss: 16.8590, Validation Loss Current: 6.6021, Validation Loss AVG: 11.6696, lr: 0.001
Epoch [143/150], Training Loss: 15.7460, Validation Loss Current: 6.5256, Validation Loss AVG: 10.7671, lr: 0.001
Epoch [144/150], Training Loss: 15.4245, Validation Loss Current: 6.3270, Validation Loss AVG: 11.0754, lr: 0.001
Epoch [145/150], Training Loss: 16.0283, Validation Loss Current: 7.7488, Validation Loss AVG: 15.9934, lr: 0.001
Epoch [146/150], Training Loss: 17.9515, Validation Loss Current: 7.3780, Validation Loss AVG: 12.6033, lr: 0.001
Epoch [147/150], Training Loss: 22.2590, Validation Loss Current: 7.0102, Validation Loss AVG: 11.3549, lr: 0.001
Epoch [148/150], Training Loss: 18.0259, Validation Loss Current: 7.1178, Validation Loss AVG: 12.6343, lr: 0.001
Epoch [149/150], Training Loss: 16.6817, Validation Loss Current: 6.4584, Validation Loss AVG: 11.8134, lr: 0.001
Epoch [150/150], Training Loss: 17.2885, Validation Loss Current: 6.6784, Validation Loss AVG: 10.4457, lr: 0.001
Patch distance: 1 finished training. Best epoch: 127 Best val accuracy: [0.23026315789473684, 0.23026315789473684, 0.23026315789473684, 0.23026315789473684, 0.23026315789473684, 0.23026315789473684, 0.23026315789473684, 0.23026315789473684, 0.23026315789473684, 0.23026315789473684, 0.23026315789473684, 0.23026315789473684, 0.23026315789473684, 0.23026315789473684, 0.23026315789473684, 0.23026315789473684, 0.23026315789473684, 0.23026315789473684, 0.23026315789473684, 0.23026315789473684, 0.23026315789473684, 0.23026315789473684, 0.23026315789473684, 0.23026315789473684, 0.23026315789473684, 0.23026315789473684, 0.23026315789473684, 0.24342105263157895, 0.26973684210526316, 0.29605263157894735, 0.2993421052631579, 0.33881578947368424, 0.29769736842105265, 0.34868421052631576, 0.3569078947368421, 0.3569078947368421, 0.35526315789473684, 0.33881578947368424, 0.3404605263157895, 0.3717105263157895, 0.30756578947368424, 0.3717105263157895, 0.37664473684210525, 0.35855263157894735, 0.375, 0.3881578947368421, 0.39473684210526316, 0.39144736842105265, 0.3996710526315789, 0.3717105263157895, 0.35526315789473684, 0.42269736842105265, 0.4292763157894737, 0.4292763157894737, 0.3881578947368421, 0.4473684210526316, 0.4375, 0.4440789473684211, 0.45230263157894735, 0.4440789473684211, 0.45394736842105265, 0.4506578947368421, 0.4769736842105263, 0.4621710526315789, 0.4868421052631579, 0.4407894736842105, 0.4654605263157895, 0.4868421052631579, 0.4901315789473684, 0.48848684210526316, 0.4654605263157895, 0.5049342105263158, 0.44243421052631576, 0.48848684210526316, 0.47039473684210525, 0.48026315789473684, 0.5032894736842105, 0.5049342105263158, 0.5032894736842105, 0.5131578947368421, 0.5164473684210527, 0.5328947368421053, 0.45723684210526316, 0.4786184210526316, 0.4786184210526316, 0.5180921052631579, 0.5098684210526315, 0.5164473684210527, 0.46381578947368424, 0.5131578947368421, 0.5361842105263158, 0.53125, 0.5082236842105263, 0.53125, 0.5476973684210527, 0.5394736842105263, 0.5411184210526315, 0.5526315789473685, 0.5361842105263158, 0.537828947368421, 0.5592105263157895, 0.5509868421052632, 0.5444078947368421, 0.5509868421052632, 0.5213815789473685, 0.5493421052631579, 0.537828947368421, 0.5592105263157895, 0.537828947368421, 0.5657894736842105, 0.42598684210526316, 0.5427631578947368, 0.5509868421052632, 0.5625, 0.5131578947368421, 0.5444078947368421, 0.48026315789473684, 0.5608552631578947, 0.569078947368421, 0.5707236842105263, 0.5592105263157895, 0.5493421052631579, 0.5805921052631579, 0.5082236842105263, 0.5707236842105263, 0.5493421052631579, 0.5805921052631579, 0.5460526315789473, 0.5575657894736842, 0.4967105263157895, 0.5016447368421053, 0.5279605263157895, 0.5657894736842105, 0.4342105263157895, 0.5328947368421053, 0.5279605263157895, 0.5921052631578947, 0.5740131578947368, 0.5674342105263158, 0.5773026315789473, 0.5493421052631579, 0.5805921052631579, 0.59375, 0.59375, 0.5526315789473685, 0.5493421052631579, 0.5296052631578947, 0.5723684210526315, 0.5723684210526315, 0.5723684210526315] Best val loss: 6.058739185333252


----- Training alexnet with sequence: [1, 0.8] -----
Sequence [1] already in state dictionary, jumped
Loaded best state dict for [1]
Current group: 0.8
Epoch [1/75], Training Loss: 20.3160, Validation Loss Current: 9.4925, Validation Loss AVG: 9.4925, lr: 0.001
Epoch [2/75], Training Loss: 21.4717, Validation Loss Current: 9.2391, Validation Loss AVG: 9.2391, lr: 0.001
Epoch [3/75], Training Loss: 20.9874, Validation Loss Current: 10.7866, Validation Loss AVG: 10.7866, lr: 0.001
Epoch [4/75], Training Loss: 31.6087, Validation Loss Current: 8.6770, Validation Loss AVG: 8.6770, lr: 0.001
Epoch [5/75], Training Loss: 23.9153, Validation Loss Current: 8.9958, Validation Loss AVG: 8.9958, lr: 0.001
Epoch [6/75], Training Loss: 21.0591, Validation Loss Current: 9.3741, Validation Loss AVG: 9.3741, lr: 0.001
Epoch [7/75], Training Loss: 22.0624, Validation Loss Current: 8.1738, Validation Loss AVG: 8.1738, lr: 0.001
Epoch [8/75], Training Loss: 19.8616, Validation Loss Current: 8.6439, Validation Loss AVG: 8.6439, lr: 0.001
Epoch [9/75], Training Loss: 19.8969, Validation Loss Current: 7.9788, Validation Loss AVG: 7.9788, lr: 0.001
Epoch [10/75], Training Loss: 17.7860, Validation Loss Current: 8.6756, Validation Loss AVG: 8.6756, lr: 0.001
Epoch [11/75], Training Loss: 18.7629, Validation Loss Current: 9.6436, Validation Loss AVG: 9.6436, lr: 0.001
Epoch [12/75], Training Loss: 23.7863, Validation Loss Current: 9.2526, Validation Loss AVG: 9.2526, lr: 0.001
Epoch [13/75], Training Loss: 18.7065, Validation Loss Current: 8.4218, Validation Loss AVG: 8.4218, lr: 0.001
Epoch [14/75], Training Loss: 17.7725, Validation Loss Current: 9.6596, Validation Loss AVG: 9.6596, lr: 0.001
Epoch [15/75], Training Loss: 18.9710, Validation Loss Current: 8.8206, Validation Loss AVG: 8.8206, lr: 0.001
Epoch [16/75], Training Loss: 17.4586, Validation Loss Current: 9.7344, Validation Loss AVG: 9.7344, lr: 0.001
Epoch [17/75], Training Loss: 18.2905, Validation Loss Current: 8.1244, Validation Loss AVG: 8.1244, lr: 0.001
Epoch [18/75], Training Loss: 16.4608, Validation Loss Current: 9.0276, Validation Loss AVG: 9.0276, lr: 0.001
Epoch [19/75], Training Loss: 18.9475, Validation Loss Current: 7.9808, Validation Loss AVG: 7.9808, lr: 0.001
Epoch [20/75], Training Loss: 16.4157, Validation Loss Current: 9.0559, Validation Loss AVG: 9.0559, lr: 0.001
Epoch [21/75], Training Loss: 17.5162, Validation Loss Current: 10.0698, Validation Loss AVG: 10.0698, lr: 0.001
Epoch [22/75], Training Loss: 18.6448, Validation Loss Current: 8.3806, Validation Loss AVG: 8.3806, lr: 0.001
Epoch [23/75], Training Loss: 16.7998, Validation Loss Current: 8.2977, Validation Loss AVG: 8.2977, lr: 0.001
Epoch [24/75], Training Loss: 16.7633, Validation Loss Current: 10.1100, Validation Loss AVG: 10.1100, lr: 0.001
Epoch [25/75], Training Loss: 22.8972, Validation Loss Current: 7.7525, Validation Loss AVG: 7.7525, lr: 0.001
Epoch [26/75], Training Loss: 18.1952, Validation Loss Current: 8.6180, Validation Loss AVG: 8.6180, lr: 0.001
Epoch [27/75], Training Loss: 19.1613, Validation Loss Current: 8.2049, Validation Loss AVG: 8.2049, lr: 0.001
Epoch [28/75], Training Loss: 17.4053, Validation Loss Current: 8.2691, Validation Loss AVG: 8.2691, lr: 0.001
Epoch [29/75], Training Loss: 15.1872, Validation Loss Current: 9.0828, Validation Loss AVG: 9.0828, lr: 0.001
Epoch [30/75], Training Loss: 14.0997, Validation Loss Current: 9.6453, Validation Loss AVG: 9.6453, lr: 0.001
Epoch [31/75], Training Loss: 13.5529, Validation Loss Current: 8.4144, Validation Loss AVG: 8.4144, lr: 0.001
Epoch [32/75], Training Loss: 12.4379, Validation Loss Current: 9.3964, Validation Loss AVG: 9.3964, lr: 0.001
Epoch [33/75], Training Loss: 13.2814, Validation Loss Current: 19.8331, Validation Loss AVG: 19.8331, lr: 0.001
Epoch [34/75], Training Loss: 34.2039, Validation Loss Current: 9.9672, Validation Loss AVG: 9.9672, lr: 0.001
Epoch [35/75], Training Loss: 27.9433, Validation Loss Current: 8.1163, Validation Loss AVG: 8.1163, lr: 0.001
Epoch [36/75], Training Loss: 21.8481, Validation Loss Current: 7.9994, Validation Loss AVG: 7.9994, lr: 0.001
Epoch [37/75], Training Loss: 18.0150, Validation Loss Current: 8.5230, Validation Loss AVG: 8.5230, lr: 0.001
Epoch [38/75], Training Loss: 16.1563, Validation Loss Current: 8.3828, Validation Loss AVG: 8.3828, lr: 0.001
Epoch [39/75], Training Loss: 16.5776, Validation Loss Current: 9.3641, Validation Loss AVG: 9.3641, lr: 0.001
Epoch [40/75], Training Loss: 18.4808, Validation Loss Current: 8.3281, Validation Loss AVG: 8.3281, lr: 0.001
Epoch [41/75], Training Loss: 15.1915, Validation Loss Current: 9.0417, Validation Loss AVG: 9.0417, lr: 0.001
Epoch [42/75], Training Loss: 15.0802, Validation Loss Current: 8.6367, Validation Loss AVG: 8.6367, lr: 0.001
Epoch [43/75], Training Loss: 13.8362, Validation Loss Current: 10.5636, Validation Loss AVG: 10.5636, lr: 0.001
Epoch [44/75], Training Loss: 21.3764, Validation Loss Current: 8.2354, Validation Loss AVG: 8.2354, lr: 0.001
Epoch [45/75], Training Loss: 15.6818, Validation Loss Current: 9.0594, Validation Loss AVG: 9.0594, lr: 0.001
Epoch [46/75], Training Loss: 13.0245, Validation Loss Current: 8.4686, Validation Loss AVG: 8.4686, lr: 0.001
Epoch [47/75], Training Loss: 12.4264, Validation Loss Current: 14.1289, Validation Loss AVG: 14.1289, lr: 0.001
Epoch [48/75], Training Loss: 20.3633, Validation Loss Current: 7.9435, Validation Loss AVG: 7.9435, lr: 0.001
Epoch [49/75], Training Loss: 14.6739, Validation Loss Current: 8.9681, Validation Loss AVG: 8.9681, lr: 0.001
Epoch [50/75], Training Loss: 11.9290, Validation Loss Current: 11.4202, Validation Loss AVG: 11.4202, lr: 0.001
Epoch [51/75], Training Loss: 22.6177, Validation Loss Current: 8.5670, Validation Loss AVG: 8.5670, lr: 0.001
Epoch [52/75], Training Loss: 15.2920, Validation Loss Current: 8.7014, Validation Loss AVG: 8.7014, lr: 0.001
Epoch [53/75], Training Loss: 16.8462, Validation Loss Current: 9.0923, Validation Loss AVG: 9.0923, lr: 0.001
Epoch [54/75], Training Loss: 15.1845, Validation Loss Current: 8.5655, Validation Loss AVG: 8.5655, lr: 0.001
Epoch [55/75], Training Loss: 12.6349, Validation Loss Current: 9.2241, Validation Loss AVG: 9.2241, lr: 0.001
 --- Early Stopped ---
Patch distance: 0.8 finished training. Best epoch: 25 Best val accuracy: [0.42269736842105265, 0.3973684210526316, 0.37467105263157896, 0.3832236842105264, 0.4220394736842105, 0.4128289473684211, 0.4526315789473685, 0.43980263157894733, 0.4703947368421053, 0.46118421052631575, 0.42828947368421044, 0.41907894736842105, 0.44868421052631585, 0.44013157894736843, 0.45953947368421055, 0.4394736842105263, 0.48914473684210524, 0.46809210526315786, 0.4865131578947369, 0.45855263157894743, 0.4358552631578948, 0.4677631578947368, 0.4832236842105263, 0.4167763157894737, 0.4608552631578947, 0.4585526315789473, 0.47006578947368427, 0.4802631578947369, 0.46677631578947365, 0.469078947368421, 0.4934210526315789, 0.48355263157894735, 0.29276315789473684, 0.3694078947368421, 0.41940789473684215, 0.46743421052631573, 0.4671052631578948, 0.47269736842105264, 0.4266447368421053, 0.4845394736842105, 0.4726973684210526, 0.48717105263157895, 0.43618421052631573, 0.4480263157894736, 0.4720394736842105, 0.5055921052631579, 0.39769736842105263, 0.47335526315789467, 0.49144736842105263, 0.4565789473684211, 0.4440789473684211, 0.4759868421052632, 0.44835526315789476, 0.4875, 0.4930921052631579] Best val loss: 7.752462577819824


----- Training alexnet with sequence: [1, 0.8, 0.6] -----
Sequence [1] already in state dictionary, jumped
Sequence [1, 0.8] already in state dictionary, jumped
Loaded best state dict for [1, 0.8]
Current group: 0.6
Epoch [1/50], Training Loss: 20.6780, Validation Loss Current: 7.8190, Validation Loss AVG: 7.8190, lr: 0.001
Epoch [2/50], Training Loss: 19.2762, Validation Loss Current: 7.5971, Validation Loss AVG: 7.5971, lr: 0.001
Epoch [3/50], Training Loss: 17.9886, Validation Loss Current: 9.7717, Validation Loss AVG: 9.7717, lr: 0.001
Epoch [4/50], Training Loss: 22.5063, Validation Loss Current: 7.3854, Validation Loss AVG: 7.3854, lr: 0.001
Epoch [5/50], Training Loss: 18.7138, Validation Loss Current: 9.1986, Validation Loss AVG: 9.1986, lr: 0.001
Epoch [6/50], Training Loss: 26.4856, Validation Loss Current: 7.4055, Validation Loss AVG: 7.4055, lr: 0.001
Epoch [7/50], Training Loss: 20.1772, Validation Loss Current: 7.4059, Validation Loss AVG: 7.4059, lr: 0.001
Epoch [8/50], Training Loss: 17.0875, Validation Loss Current: 7.2572, Validation Loss AVG: 7.2572, lr: 0.001
Epoch [9/50], Training Loss: 14.9588, Validation Loss Current: 7.3441, Validation Loss AVG: 7.3441, lr: 0.001
Epoch [10/50], Training Loss: 15.5945, Validation Loss Current: 7.9050, Validation Loss AVG: 7.9050, lr: 0.001
Epoch [11/50], Training Loss: 16.2440, Validation Loss Current: 7.7806, Validation Loss AVG: 7.7806, lr: 0.001
Epoch [12/50], Training Loss: 15.5295, Validation Loss Current: 8.2146, Validation Loss AVG: 8.2146, lr: 0.001
Epoch [13/50], Training Loss: 14.1539, Validation Loss Current: 8.6767, Validation Loss AVG: 8.6767, lr: 0.001
Epoch [14/50], Training Loss: 16.8861, Validation Loss Current: 8.0175, Validation Loss AVG: 8.0175, lr: 0.001
Epoch [15/50], Training Loss: 13.6727, Validation Loss Current: 8.4462, Validation Loss AVG: 8.4462, lr: 0.001
Epoch [16/50], Training Loss: 12.6163, Validation Loss Current: 8.1067, Validation Loss AVG: 8.1067, lr: 0.001
Epoch [17/50], Training Loss: 12.8390, Validation Loss Current: 8.7377, Validation Loss AVG: 8.7377, lr: 0.001
Epoch [18/50], Training Loss: 14.1039, Validation Loss Current: 8.4204, Validation Loss AVG: 8.4204, lr: 0.001
Epoch [19/50], Training Loss: 13.1003, Validation Loss Current: 8.7357, Validation Loss AVG: 8.7357, lr: 0.001
Epoch [20/50], Training Loss: 12.0393, Validation Loss Current: 8.7464, Validation Loss AVG: 8.7464, lr: 0.001
Epoch [21/50], Training Loss: 10.5408, Validation Loss Current: 8.5403, Validation Loss AVG: 8.5403, lr: 0.001
Epoch [22/50], Training Loss: 11.2346, Validation Loss Current: 10.4763, Validation Loss AVG: 10.4763, lr: 0.001
Epoch [23/50], Training Loss: 16.2515, Validation Loss Current: 8.0060, Validation Loss AVG: 8.0060, lr: 0.001
Epoch [24/50], Training Loss: 12.8068, Validation Loss Current: 8.3583, Validation Loss AVG: 8.3583, lr: 0.001
Epoch [25/50], Training Loss: 10.3365, Validation Loss Current: 10.0745, Validation Loss AVG: 10.0745, lr: 0.001
Epoch [26/50], Training Loss: 13.3431, Validation Loss Current: 8.7078, Validation Loss AVG: 8.7078, lr: 0.001
Epoch [27/50], Training Loss: 9.9863, Validation Loss Current: 9.3591, Validation Loss AVG: 9.3591, lr: 0.001
Epoch [28/50], Training Loss: 9.7242, Validation Loss Current: 9.1016, Validation Loss AVG: 9.1016, lr: 0.001
Epoch [29/50], Training Loss: 9.3654, Validation Loss Current: 9.0066, Validation Loss AVG: 9.0066, lr: 0.001
Epoch [30/50], Training Loss: 9.2750, Validation Loss Current: 9.1159, Validation Loss AVG: 9.1159, lr: 0.001
Epoch [31/50], Training Loss: 6.9682, Validation Loss Current: 9.5596, Validation Loss AVG: 9.5596, lr: 0.001
Epoch [32/50], Training Loss: 7.1327, Validation Loss Current: 12.1267, Validation Loss AVG: 12.1267, lr: 0.001
Epoch [33/50], Training Loss: 21.9036, Validation Loss Current: 8.3397, Validation Loss AVG: 8.3397, lr: 0.001
Epoch [34/50], Training Loss: 13.4779, Validation Loss Current: 12.9864, Validation Loss AVG: 12.9864, lr: 0.001
Epoch [35/50], Training Loss: 22.1644, Validation Loss Current: 7.6412, Validation Loss AVG: 7.6412, lr: 0.001
Epoch [36/50], Training Loss: 14.0316, Validation Loss Current: 8.4127, Validation Loss AVG: 8.4127, lr: 0.001
Epoch [37/50], Training Loss: 12.5507, Validation Loss Current: 9.2570, Validation Loss AVG: 9.2570, lr: 0.001
Epoch [38/50], Training Loss: 11.6600, Validation Loss Current: 8.9158, Validation Loss AVG: 8.9158, lr: 0.001
 --- Early Stopped ---
Patch distance: 0.6 finished training. Best epoch: 8 Best val accuracy: [0.49177631578947373, 0.5131578947368421, 0.4776315789473684, 0.5256578947368421, 0.47072368421052635, 0.4822368421052631, 0.5105263157894737, 0.5345394736842105, 0.5407894736842105, 0.5177631578947368, 0.5184210526315789, 0.5154605263157895, 0.48717105263157895, 0.4963815789473685, 0.5197368421052632, 0.5157894736842106, 0.5210526315789473, 0.5088815789473685, 0.5115131578947368, 0.5148026315789475, 0.5240131578947368, 0.43881578947368427, 0.49736842105263157, 0.5286184210526316, 0.4842105263157895, 0.5236842105263159, 0.5266447368421052, 0.5180921052631579, 0.5121710526315789, 0.5200657894736842, 0.525657894736842, 0.47269736842105264, 0.4786184210526316, 0.3845394736842105, 0.49078947368421055, 0.4967105263157895, 0.4789473684210527, 0.5059210526315789] Best val loss: 7.257154285907745


----- Training alexnet with sequence: [1, 0.8, 0.6, 0.4] -----
Sequence [1] already in state dictionary, jumped
Sequence [1, 0.8] already in state dictionary, jumped
Sequence [1, 0.8, 0.6] already in state dictionary, jumped
Loaded best state dict for [1, 0.8, 0.6]
Current group: 0.4
Epoch [1/38], Training Loss: 24.3752, Validation Loss Current: 7.2236, Validation Loss AVG: 7.2236, lr: 0.001
Epoch [2/38], Training Loss: 21.5917, Validation Loss Current: 7.7045, Validation Loss AVG: 7.7045, lr: 0.001
Epoch [3/38], Training Loss: 22.4536, Validation Loss Current: 7.7318, Validation Loss AVG: 7.7318, lr: 0.001
Epoch [4/38], Training Loss: 19.7903, Validation Loss Current: 7.7970, Validation Loss AVG: 7.7970, lr: 0.001
Epoch [5/38], Training Loss: 18.1411, Validation Loss Current: 8.8306, Validation Loss AVG: 8.8306, lr: 0.001
Epoch [6/38], Training Loss: 21.8390, Validation Loss Current: 8.0434, Validation Loss AVG: 8.0434, lr: 0.001
Epoch [7/38], Training Loss: 19.2860, Validation Loss Current: 8.0000, Validation Loss AVG: 8.0000, lr: 0.001
Epoch [8/38], Training Loss: 18.5879, Validation Loss Current: 8.0489, Validation Loss AVG: 8.0489, lr: 0.001
Epoch [9/38], Training Loss: 16.2244, Validation Loss Current: 7.7454, Validation Loss AVG: 7.7454, lr: 0.001
Epoch [10/38], Training Loss: 14.0405, Validation Loss Current: 7.9370, Validation Loss AVG: 7.9370, lr: 0.001
Epoch [11/38], Training Loss: 13.5142, Validation Loss Current: 9.4880, Validation Loss AVG: 9.4880, lr: 0.001
Epoch [12/38], Training Loss: 20.6607, Validation Loss Current: 7.3666, Validation Loss AVG: 7.3666, lr: 0.001
Epoch [13/38], Training Loss: 16.6588, Validation Loss Current: 8.2445, Validation Loss AVG: 8.2445, lr: 0.001
Epoch [14/38], Training Loss: 13.6417, Validation Loss Current: 8.3380, Validation Loss AVG: 8.3380, lr: 0.001
Epoch [15/38], Training Loss: 12.3244, Validation Loss Current: 8.8919, Validation Loss AVG: 8.8919, lr: 0.001
Epoch [16/38], Training Loss: 11.3245, Validation Loss Current: 9.0776, Validation Loss AVG: 9.0776, lr: 0.001
Epoch [17/38], Training Loss: 11.2421, Validation Loss Current: 9.4998, Validation Loss AVG: 9.4998, lr: 0.001
Epoch [18/38], Training Loss: 14.3117, Validation Loss Current: 8.3262, Validation Loss AVG: 8.3262, lr: 0.001
Epoch [19/38], Training Loss: 12.6553, Validation Loss Current: 8.6600, Validation Loss AVG: 8.6600, lr: 0.001
Epoch [20/38], Training Loss: 12.8392, Validation Loss Current: 8.2205, Validation Loss AVG: 8.2205, lr: 0.001
Epoch [21/38], Training Loss: 11.0180, Validation Loss Current: 9.2037, Validation Loss AVG: 9.2037, lr: 0.001
Epoch [22/38], Training Loss: 8.4038, Validation Loss Current: 9.9167, Validation Loss AVG: 9.9167, lr: 0.001
Epoch [23/38], Training Loss: 8.4021, Validation Loss Current: 12.8975, Validation Loss AVG: 12.8975, lr: 0.001
Epoch [24/38], Training Loss: 17.3146, Validation Loss Current: 8.9201, Validation Loss AVG: 8.9201, lr: 0.001
Epoch [25/38], Training Loss: 10.7485, Validation Loss Current: 9.5333, Validation Loss AVG: 9.5333, lr: 0.001
Epoch [26/38], Training Loss: 9.6691, Validation Loss Current: 10.0735, Validation Loss AVG: 10.0735, lr: 0.001
Epoch [27/38], Training Loss: 7.2646, Validation Loss Current: 10.2232, Validation Loss AVG: 10.2232, lr: 0.001
Epoch [28/38], Training Loss: 6.5013, Validation Loss Current: 11.7190, Validation Loss AVG: 11.7190, lr: 0.001
Epoch [29/38], Training Loss: 9.0227, Validation Loss Current: 11.0789, Validation Loss AVG: 11.0789, lr: 0.001
Epoch [30/38], Training Loss: 8.4442, Validation Loss Current: 11.4628, Validation Loss AVG: 11.4628, lr: 0.001
Epoch [31/38], Training Loss: 6.7775, Validation Loss Current: 10.7063, Validation Loss AVG: 10.7063, lr: 0.001
 --- Early Stopped ---
Patch distance: 0.4 finished training. Best epoch: 1 Best val accuracy: [0.5082236842105263, 0.49046052631578946, 0.49703947368421053, 0.4842105263157895, 0.475, 0.5016447368421053, 0.5105263157894737, 0.49572368421052637, 0.5253289473684211, 0.5253289473684211, 0.4736842105263158, 0.5259868421052631, 0.4983552631578947, 0.5263157894736842, 0.5105263157894738, 0.5148026315789475, 0.469078947368421, 0.5233552631578948, 0.488157894736842, 0.537171052631579, 0.5157894736842106, 0.5128289473684211, 0.4322368421052632, 0.49375, 0.49769736842105267, 0.5085526315789474, 0.5069078947368422, 0.47894736842105257, 0.49046052631578946, 0.47796052631578945, 0.5289473684210526] Best val loss: 7.223648953437805


----- Training alexnet with sequence: [1, 0.8, 0.6, 0.4, 0.2] -----
Sequence [1] already in state dictionary, jumped
Sequence [1, 0.8] already in state dictionary, jumped
Sequence [1, 0.8, 0.6] already in state dictionary, jumped
Sequence [1, 0.8, 0.6, 0.4] already in state dictionary, jumped
Loaded best state dict for [1, 0.8, 0.6, 0.4]
Current group: 0.2
Epoch [1/30], Training Loss: 32.9004, Validation Loss Current: 9.0683, Validation Loss AVG: 9.0683, lr: 0.001
Epoch [2/30], Training Loss: 28.7473, Validation Loss Current: 8.9092, Validation Loss AVG: 8.9092, lr: 0.001
Epoch [3/30], Training Loss: 27.3909, Validation Loss Current: 9.3890, Validation Loss AVG: 9.3890, lr: 0.001
Epoch [4/30], Training Loss: 27.9522, Validation Loss Current: 9.8241, Validation Loss AVG: 9.8241, lr: 0.001
Epoch [5/30], Training Loss: 26.7482, Validation Loss Current: 9.2905, Validation Loss AVG: 9.2905, lr: 0.001
Epoch [6/30], Training Loss: 26.0909, Validation Loss Current: 9.3716, Validation Loss AVG: 9.3716, lr: 0.001
Epoch [7/30], Training Loss: 24.4796, Validation Loss Current: 10.0923, Validation Loss AVG: 10.0923, lr: 0.001
Epoch [8/30], Training Loss: 23.1353, Validation Loss Current: 9.8969, Validation Loss AVG: 9.8969, lr: 0.001
Epoch [9/30], Training Loss: 20.3141, Validation Loss Current: 10.5221, Validation Loss AVG: 10.5221, lr: 0.001
Epoch [10/30], Training Loss: 20.5506, Validation Loss Current: 9.8539, Validation Loss AVG: 9.8539, lr: 0.001
Epoch [11/30], Training Loss: 21.5780, Validation Loss Current: 10.1436, Validation Loss AVG: 10.1436, lr: 0.001
Epoch [12/30], Training Loss: 20.6465, Validation Loss Current: 10.4054, Validation Loss AVG: 10.4054, lr: 0.001
Epoch [13/30], Training Loss: 19.5726, Validation Loss Current: 11.3277, Validation Loss AVG: 11.3277, lr: 0.001
Epoch [14/30], Training Loss: 26.7552, Validation Loss Current: 9.8275, Validation Loss AVG: 9.8275, lr: 0.001
Epoch [15/30], Training Loss: 22.8749, Validation Loss Current: 10.6158, Validation Loss AVG: 10.6158, lr: 0.001
Epoch [16/30], Training Loss: 20.6356, Validation Loss Current: 11.1272, Validation Loss AVG: 11.1272, lr: 0.001
Epoch [17/30], Training Loss: 22.4637, Validation Loss Current: 10.5412, Validation Loss AVG: 10.5412, lr: 0.001
Epoch [18/30], Training Loss: 20.0926, Validation Loss Current: 11.5508, Validation Loss AVG: 11.5508, lr: 0.001
Epoch [19/30], Training Loss: 18.1801, Validation Loss Current: 12.0844, Validation Loss AVG: 12.0844, lr: 0.001
Epoch [20/30], Training Loss: 18.8780, Validation Loss Current: 14.9122, Validation Loss AVG: 14.9122, lr: 0.001
Epoch [21/30], Training Loss: 27.3520, Validation Loss Current: 10.4687, Validation Loss AVG: 10.4687, lr: 0.001
Epoch [22/30], Training Loss: 22.6043, Validation Loss Current: 11.3905, Validation Loss AVG: 11.3905, lr: 0.001
Epoch [23/30], Training Loss: 23.2496, Validation Loss Current: 11.3306, Validation Loss AVG: 11.3306, lr: 0.001
Epoch [24/30], Training Loss: 20.0541, Validation Loss Current: 10.8862, Validation Loss AVG: 10.8862, lr: 0.001
Epoch [25/30], Training Loss: 26.1156, Validation Loss Current: 10.4246, Validation Loss AVG: 10.4246, lr: 0.001
Epoch [26/30], Training Loss: 20.7921, Validation Loss Current: 12.5698, Validation Loss AVG: 12.5698, lr: 0.001
Epoch [27/30], Training Loss: 20.2186, Validation Loss Current: 12.1820, Validation Loss AVG: 12.1820, lr: 0.001
Epoch [28/30], Training Loss: 16.6546, Validation Loss Current: 12.2825, Validation Loss AVG: 12.2825, lr: 0.001
Epoch [29/30], Training Loss: 21.9784, Validation Loss Current: 11.3710, Validation Loss AVG: 11.3710, lr: 0.001
Epoch [30/30], Training Loss: 19.8189, Validation Loss Current: 11.8593, Validation Loss AVG: 11.8593, lr: 0.001
Epoch [31/30], Training Loss: 15.7221, Validation Loss Current: 12.8566, Validation Loss AVG: 12.8566, lr: 0.001
Epoch [32/30], Training Loss: 18.8865, Validation Loss Current: 11.4142, Validation Loss AVG: 11.4142, lr: 0.001
 --- Early Stopped ---
Patch distance: 0.2 finished training. Best epoch: 2 Best val accuracy: [0.42828947368421055, 0.4046052631578947, 0.4230263157894737, 0.40032894736842106, 0.41052631578947363, 0.39144736842105265, 0.40164473684210533, 0.4197368421052632, 0.39671052631578946, 0.41743421052631574, 0.3927631578947368, 0.40855263157894733, 0.38289473684210523, 0.37763157894736843, 0.36447368421052634, 0.3875, 0.4042763157894737, 0.3740131578947369, 0.37138157894736845, 0.31250000000000006, 0.3180921052631579, 0.3325657894736842, 0.34342105263157896, 0.36546052631578946, 0.368421052631579, 0.32401315789473684, 0.37039473684210533, 0.38421052631578945, 0.34769736842105264, 0.3572368421052632, 0.3766447368421053, 0.3792763157894737] Best val loss: 8.90916666984558


Fold: 3
----- Training alexnet with sequence: [1] -----
Current group: 1
Epoch [1/150], Training Loss: 41.5731, Validation Loss Current: 10.3830, Validation Loss AVG: 10.3816, lr: 0.001
Epoch [2/150], Training Loss: 41.4638, Validation Loss Current: 10.3455, Validation Loss AVG: 10.3446, lr: 0.001
Epoch [3/150], Training Loss: 41.2838, Validation Loss Current: 10.3140, Validation Loss AVG: 10.3111, lr: 0.001
Epoch [4/150], Training Loss: 41.1126, Validation Loss Current: 10.2765, Validation Loss AVG: 10.2742, lr: 0.001
Epoch [5/150], Training Loss: 40.9090, Validation Loss Current: 10.2362, Validation Loss AVG: 10.2274, lr: 0.001
Epoch [6/150], Training Loss: 40.6409, Validation Loss Current: 10.1762, Validation Loss AVG: 10.1694, lr: 0.001
Epoch [7/150], Training Loss: 40.5457, Validation Loss Current: 10.1215, Validation Loss AVG: 10.0969, lr: 0.001
Epoch [8/150], Training Loss: 40.4941, Validation Loss Current: 10.0720, Validation Loss AVG: 10.0608, lr: 0.001
Epoch [9/150], Training Loss: 39.8556, Validation Loss Current: 10.0475, Validation Loss AVG: 10.0458, lr: 0.001
Epoch [10/150], Training Loss: 39.7695, Validation Loss Current: 10.0365, Validation Loss AVG: 10.0427, lr: 0.001
Epoch [11/150], Training Loss: 39.9133, Validation Loss Current: 10.0185, Validation Loss AVG: 10.0388, lr: 0.001
Epoch [12/150], Training Loss: 39.5330, Validation Loss Current: 10.0145, Validation Loss AVG: 10.0481, lr: 0.001
Epoch [13/150], Training Loss: 40.1448, Validation Loss Current: 9.9830, Validation Loss AVG: 10.0552, lr: 0.001
Epoch [14/150], Training Loss: 40.1013, Validation Loss Current: 9.9885, Validation Loss AVG: 10.0374, lr: 0.001
Epoch [15/150], Training Loss: 40.0537, Validation Loss Current: 10.0003, Validation Loss AVG: 10.0644, lr: 0.001
Epoch [16/150], Training Loss: 39.4461, Validation Loss Current: 9.9711, Validation Loss AVG: 10.0872, lr: 0.001
Epoch [17/150], Training Loss: 39.2535, Validation Loss Current: 9.9492, Validation Loss AVG: 10.1095, lr: 0.001
Epoch [18/150], Training Loss: 39.4092, Validation Loss Current: 9.9234, Validation Loss AVG: 10.0828, lr: 0.001
Epoch [19/150], Training Loss: 38.6536, Validation Loss Current: 9.8806, Validation Loss AVG: 10.0959, lr: 0.001
Epoch [20/150], Training Loss: 39.5713, Validation Loss Current: 9.8596, Validation Loss AVG: 10.0562, lr: 0.001
Epoch [21/150], Training Loss: 39.6184, Validation Loss Current: 9.8431, Validation Loss AVG: 10.0453, lr: 0.001
Epoch [22/150], Training Loss: 38.9491, Validation Loss Current: 9.8303, Validation Loss AVG: 10.1523, lr: 0.001
Epoch [23/150], Training Loss: 39.2828, Validation Loss Current: 9.8333, Validation Loss AVG: 10.1393, lr: 0.001
Epoch [24/150], Training Loss: 39.1712, Validation Loss Current: 9.7851, Validation Loss AVG: 10.1278, lr: 0.001
Epoch [25/150], Training Loss: 39.0942, Validation Loss Current: 9.7551, Validation Loss AVG: 10.0644, lr: 0.001
Epoch [26/150], Training Loss: 38.5115, Validation Loss Current: 9.7218, Validation Loss AVG: 10.0582, lr: 0.001
Epoch [27/150], Training Loss: 39.4520, Validation Loss Current: 9.6751, Validation Loss AVG: 9.9836, lr: 0.001
Epoch [28/150], Training Loss: 38.6505, Validation Loss Current: 9.6959, Validation Loss AVG: 10.4602, lr: 0.001
Epoch [29/150], Training Loss: 38.3287, Validation Loss Current: 9.6241, Validation Loss AVG: 9.9969, lr: 0.001
Epoch [30/150], Training Loss: 38.9959, Validation Loss Current: 9.6059, Validation Loss AVG: 9.8365, lr: 0.001
Epoch [31/150], Training Loss: 38.0189, Validation Loss Current: 9.6210, Validation Loss AVG: 10.3021, lr: 0.001
Epoch [32/150], Training Loss: 38.6660, Validation Loss Current: 9.5081, Validation Loss AVG: 10.0644, lr: 0.001
Epoch [33/150], Training Loss: 38.1837, Validation Loss Current: 9.5001, Validation Loss AVG: 10.1639, lr: 0.001
Epoch [34/150], Training Loss: 37.1372, Validation Loss Current: 9.5022, Validation Loss AVG: 10.0640, lr: 0.001
Epoch [35/150], Training Loss: 37.8783, Validation Loss Current: 9.4361, Validation Loss AVG: 10.3929, lr: 0.001
Epoch [36/150], Training Loss: 37.0936, Validation Loss Current: 9.4355, Validation Loss AVG: 10.4229, lr: 0.001
Epoch [37/150], Training Loss: 37.7217, Validation Loss Current: 9.3373, Validation Loss AVG: 10.1537, lr: 0.001
Epoch [38/150], Training Loss: 36.5981, Validation Loss Current: 9.2618, Validation Loss AVG: 10.3477, lr: 0.001
Epoch [39/150], Training Loss: 37.0100, Validation Loss Current: 9.1025, Validation Loss AVG: 9.6937, lr: 0.001
Epoch [40/150], Training Loss: 37.3816, Validation Loss Current: 9.0417, Validation Loss AVG: 9.5650, lr: 0.001
Epoch [41/150], Training Loss: 36.9587, Validation Loss Current: 9.1145, Validation Loss AVG: 9.8542, lr: 0.001
Epoch [42/150], Training Loss: 35.2739, Validation Loss Current: 9.2379, Validation Loss AVG: 10.4842, lr: 0.001
Epoch [43/150], Training Loss: 35.6990, Validation Loss Current: 8.8523, Validation Loss AVG: 9.3717, lr: 0.001
Epoch [44/150], Training Loss: 35.2510, Validation Loss Current: 8.8989, Validation Loss AVG: 9.3848, lr: 0.001
Epoch [45/150], Training Loss: 34.0173, Validation Loss Current: 8.4739, Validation Loss AVG: 9.3455, lr: 0.001
Epoch [46/150], Training Loss: 34.4254, Validation Loss Current: 9.2144, Validation Loss AVG: 10.3916, lr: 0.001
Epoch [47/150], Training Loss: 35.7506, Validation Loss Current: 9.0246, Validation Loss AVG: 10.6517, lr: 0.001
Epoch [48/150], Training Loss: 34.1207, Validation Loss Current: 8.4328, Validation Loss AVG: 9.2007, lr: 0.001
Epoch [49/150], Training Loss: 33.0038, Validation Loss Current: 8.5023, Validation Loss AVG: 9.4002, lr: 0.001
Epoch [50/150], Training Loss: 33.7665, Validation Loss Current: 8.4061, Validation Loss AVG: 9.2307, lr: 0.001
Epoch [51/150], Training Loss: 33.1331, Validation Loss Current: 8.4953, Validation Loss AVG: 9.5848, lr: 0.001
Epoch [52/150], Training Loss: 32.5475, Validation Loss Current: 8.3905, Validation Loss AVG: 9.2603, lr: 0.001
Epoch [53/150], Training Loss: 32.1865, Validation Loss Current: 8.7622, Validation Loss AVG: 10.6442, lr: 0.001
Epoch [54/150], Training Loss: 34.7984, Validation Loss Current: 8.3890, Validation Loss AVG: 9.1674, lr: 0.001
Epoch [55/150], Training Loss: 31.7559, Validation Loss Current: 7.9682, Validation Loss AVG: 9.0794, lr: 0.001
Epoch [56/150], Training Loss: 31.4448, Validation Loss Current: 7.9350, Validation Loss AVG: 9.4164, lr: 0.001
Epoch [57/150], Training Loss: 30.6855, Validation Loss Current: 8.4143, Validation Loss AVG: 9.4660, lr: 0.001
Epoch [58/150], Training Loss: 30.6831, Validation Loss Current: 7.9429, Validation Loss AVG: 8.7957, lr: 0.001
Epoch [59/150], Training Loss: 30.8084, Validation Loss Current: 8.1003, Validation Loss AVG: 10.3043, lr: 0.001
Epoch [60/150], Training Loss: 33.6584, Validation Loss Current: 8.1321, Validation Loss AVG: 9.0777, lr: 0.001
Epoch [61/150], Training Loss: 31.1143, Validation Loss Current: 8.0784, Validation Loss AVG: 9.0297, lr: 0.001
Epoch [62/150], Training Loss: 30.8232, Validation Loss Current: 8.0587, Validation Loss AVG: 9.3168, lr: 0.001
Epoch [63/150], Training Loss: 30.5887, Validation Loss Current: 9.4076, Validation Loss AVG: 11.2370, lr: 0.001
Epoch [64/150], Training Loss: 32.7848, Validation Loss Current: 8.7038, Validation Loss AVG: 9.1560, lr: 0.001
Epoch [65/150], Training Loss: 32.4353, Validation Loss Current: 7.6043, Validation Loss AVG: 8.6589, lr: 0.001
Epoch [66/150], Training Loss: 28.5145, Validation Loss Current: 7.9354, Validation Loss AVG: 8.6259, lr: 0.001
Epoch [67/150], Training Loss: 29.7180, Validation Loss Current: 7.4724, Validation Loss AVG: 9.0481, lr: 0.001
Epoch [68/150], Training Loss: 29.3682, Validation Loss Current: 7.3227, Validation Loss AVG: 8.2951, lr: 0.001
Epoch [69/150], Training Loss: 28.0207, Validation Loss Current: 7.4217, Validation Loss AVG: 8.4292, lr: 0.001
Epoch [70/150], Training Loss: 28.9027, Validation Loss Current: 7.3019, Validation Loss AVG: 9.2677, lr: 0.001
Epoch [71/150], Training Loss: 28.5532, Validation Loss Current: 7.1917, Validation Loss AVG: 9.3348, lr: 0.001
Epoch [72/150], Training Loss: 29.0494, Validation Loss Current: 7.1806, Validation Loss AVG: 9.5592, lr: 0.001
Epoch [73/150], Training Loss: 28.8845, Validation Loss Current: 7.3166, Validation Loss AVG: 8.3337, lr: 0.001
Epoch [74/150], Training Loss: 28.1847, Validation Loss Current: 7.4163, Validation Loss AVG: 9.8877, lr: 0.001
Epoch [75/150], Training Loss: 27.9890, Validation Loss Current: 7.9761, Validation Loss AVG: 9.1061, lr: 0.001
Epoch [76/150], Training Loss: 28.7192, Validation Loss Current: 7.3871, Validation Loss AVG: 8.3433, lr: 0.001
Epoch [77/150], Training Loss: 26.6194, Validation Loss Current: 7.1137, Validation Loss AVG: 8.6187, lr: 0.001
Epoch [78/150], Training Loss: 27.7560, Validation Loss Current: 6.8478, Validation Loss AVG: 8.9976, lr: 0.001
Epoch [79/150], Training Loss: 25.9533, Validation Loss Current: 6.8031, Validation Loss AVG: 8.8377, lr: 0.001
Epoch [80/150], Training Loss: 25.0122, Validation Loss Current: 7.1856, Validation Loss AVG: 8.4241, lr: 0.001
Epoch [81/150], Training Loss: 25.8888, Validation Loss Current: 7.2646, Validation Loss AVG: 8.6716, lr: 0.001
Epoch [82/150], Training Loss: 25.1445, Validation Loss Current: 6.8469, Validation Loss AVG: 8.9972, lr: 0.001
Epoch [83/150], Training Loss: 25.9887, Validation Loss Current: 6.7598, Validation Loss AVG: 8.3991, lr: 0.001
Epoch [84/150], Training Loss: 25.5711, Validation Loss Current: 7.1727, Validation Loss AVG: 8.8332, lr: 0.001
Epoch [85/150], Training Loss: 26.6051, Validation Loss Current: 7.3901, Validation Loss AVG: 8.6585, lr: 0.001
Epoch [86/150], Training Loss: 30.3090, Validation Loss Current: 7.1525, Validation Loss AVG: 8.6758, lr: 0.001
Epoch [87/150], Training Loss: 26.0302, Validation Loss Current: 6.7708, Validation Loss AVG: 8.8116, lr: 0.001
Epoch [88/150], Training Loss: 25.5605, Validation Loss Current: 6.6470, Validation Loss AVG: 9.2958, lr: 0.001
Epoch [89/150], Training Loss: 24.0488, Validation Loss Current: 6.6505, Validation Loss AVG: 9.3405, lr: 0.001
Epoch [90/150], Training Loss: 24.0114, Validation Loss Current: 6.6639, Validation Loss AVG: 8.8485, lr: 0.001
Epoch [91/150], Training Loss: 25.7681, Validation Loss Current: 6.5492, Validation Loss AVG: 9.8389, lr: 0.001
Epoch [92/150], Training Loss: 28.7967, Validation Loss Current: 6.7015, Validation Loss AVG: 8.2617, lr: 0.001
Epoch [93/150], Training Loss: 24.7033, Validation Loss Current: 6.2530, Validation Loss AVG: 8.3959, lr: 0.001
Epoch [94/150], Training Loss: 24.6515, Validation Loss Current: 6.8206, Validation Loss AVG: 10.3241, lr: 0.001
Epoch [95/150], Training Loss: 25.0186, Validation Loss Current: 6.7631, Validation Loss AVG: 8.7030, lr: 0.001
Epoch [96/150], Training Loss: 24.5606, Validation Loss Current: 6.5612, Validation Loss AVG: 9.6531, lr: 0.001
Epoch [97/150], Training Loss: 22.8554, Validation Loss Current: 6.2367, Validation Loss AVG: 9.5308, lr: 0.001
Epoch [98/150], Training Loss: 23.2629, Validation Loss Current: 6.2509, Validation Loss AVG: 9.8301, lr: 0.001
Epoch [99/150], Training Loss: 24.5096, Validation Loss Current: 6.8113, Validation Loss AVG: 11.0824, lr: 0.001
Epoch [100/150], Training Loss: 24.7881, Validation Loss Current: 6.3246, Validation Loss AVG: 9.2610, lr: 0.001
Epoch [101/150], Training Loss: 23.6537, Validation Loss Current: 6.4021, Validation Loss AVG: 9.2619, lr: 0.001
Epoch [102/150], Training Loss: 22.8524, Validation Loss Current: 6.4300, Validation Loss AVG: 8.8923, lr: 0.001
Epoch [103/150], Training Loss: 22.8559, Validation Loss Current: 6.5293, Validation Loss AVG: 8.5121, lr: 0.001
Epoch [104/150], Training Loss: 21.9071, Validation Loss Current: 6.0121, Validation Loss AVG: 9.1021, lr: 0.001
Epoch [105/150], Training Loss: 22.4197, Validation Loss Current: 6.9873, Validation Loss AVG: 11.2183, lr: 0.001
Epoch [106/150], Training Loss: 22.4518, Validation Loss Current: 6.2840, Validation Loss AVG: 8.4458, lr: 0.001
Epoch [107/150], Training Loss: 21.7960, Validation Loss Current: 7.1547, Validation Loss AVG: 9.1151, lr: 0.001
Epoch [108/150], Training Loss: 23.3536, Validation Loss Current: 6.6327, Validation Loss AVG: 8.6529, lr: 0.001
Epoch [109/150], Training Loss: 21.5129, Validation Loss Current: 6.4669, Validation Loss AVG: 8.6886, lr: 0.001
Epoch [110/150], Training Loss: 22.3380, Validation Loss Current: 5.9997, Validation Loss AVG: 8.5672, lr: 0.001
Epoch [111/150], Training Loss: 22.5588, Validation Loss Current: 7.4377, Validation Loss AVG: 10.1502, lr: 0.001
Epoch [112/150], Training Loss: 27.2110, Validation Loss Current: 6.3844, Validation Loss AVG: 9.2680, lr: 0.001
Epoch [113/150], Training Loss: 22.0384, Validation Loss Current: 6.0213, Validation Loss AVG: 8.2926, lr: 0.001
Epoch [114/150], Training Loss: 20.5404, Validation Loss Current: 6.3120, Validation Loss AVG: 9.8152, lr: 0.001
Epoch [115/150], Training Loss: 20.8534, Validation Loss Current: 5.8433, Validation Loss AVG: 8.9020, lr: 0.001
Epoch [116/150], Training Loss: 20.2716, Validation Loss Current: 6.2711, Validation Loss AVG: 10.3628, lr: 0.001
Epoch [117/150], Training Loss: 20.8116, Validation Loss Current: 6.9496, Validation Loss AVG: 9.6901, lr: 0.001
Epoch [118/150], Training Loss: 21.4007, Validation Loss Current: 5.9268, Validation Loss AVG: 9.3381, lr: 0.001
Epoch [119/150], Training Loss: 19.5882, Validation Loss Current: 6.1449, Validation Loss AVG: 9.2390, lr: 0.001
Epoch [120/150], Training Loss: 19.7555, Validation Loss Current: 6.3682, Validation Loss AVG: 9.8334, lr: 0.001
Epoch [121/150], Training Loss: 21.4974, Validation Loss Current: 6.2246, Validation Loss AVG: 9.1314, lr: 0.001
Epoch [122/150], Training Loss: 21.2529, Validation Loss Current: 7.8323, Validation Loss AVG: 11.5414, lr: 0.001
Epoch [123/150], Training Loss: 27.4429, Validation Loss Current: 7.1875, Validation Loss AVG: 9.5047, lr: 0.001
Epoch [124/150], Training Loss: 23.9596, Validation Loss Current: 6.1875, Validation Loss AVG: 9.2288, lr: 0.001
Epoch [125/150], Training Loss: 20.8480, Validation Loss Current: 6.1365, Validation Loss AVG: 8.7490, lr: 0.001
Epoch [126/150], Training Loss: 22.2145, Validation Loss Current: 6.0118, Validation Loss AVG: 9.6220, lr: 0.001
Epoch [127/150], Training Loss: 23.9168, Validation Loss Current: 6.1248, Validation Loss AVG: 9.3006, lr: 0.001
Epoch [128/150], Training Loss: 20.6837, Validation Loss Current: 5.9866, Validation Loss AVG: 9.7707, lr: 0.001
Epoch [129/150], Training Loss: 20.1841, Validation Loss Current: 6.4693, Validation Loss AVG: 8.3893, lr: 0.001
Epoch [130/150], Training Loss: 21.7249, Validation Loss Current: 5.9778, Validation Loss AVG: 7.7113, lr: 0.001
Epoch [131/150], Training Loss: 19.6564, Validation Loss Current: 6.6143, Validation Loss AVG: 8.8754, lr: 0.001
Epoch [132/150], Training Loss: 19.5016, Validation Loss Current: 6.2555, Validation Loss AVG: 8.7186, lr: 0.001
Epoch [133/150], Training Loss: 18.4228, Validation Loss Current: 6.1788, Validation Loss AVG: 9.1116, lr: 0.001
Epoch [134/150], Training Loss: 18.7408, Validation Loss Current: 5.6720, Validation Loss AVG: 9.8848, lr: 0.001
Epoch [135/150], Training Loss: 17.2461, Validation Loss Current: 5.7125, Validation Loss AVG: 9.5576, lr: 0.001
Epoch [136/150], Training Loss: 16.3755, Validation Loss Current: 6.1841, Validation Loss AVG: 10.6297, lr: 0.001
Epoch [137/150], Training Loss: 19.3663, Validation Loss Current: 9.2840, Validation Loss AVG: 14.6620, lr: 0.001
Epoch [138/150], Training Loss: 29.4154, Validation Loss Current: 6.8504, Validation Loss AVG: 8.6386, lr: 0.001
Epoch [139/150], Training Loss: 21.5768, Validation Loss Current: 6.5031, Validation Loss AVG: 8.5779, lr: 0.001
Epoch [140/150], Training Loss: 20.0619, Validation Loss Current: 6.6954, Validation Loss AVG: 11.0275, lr: 0.001
Epoch [141/150], Training Loss: 21.7786, Validation Loss Current: 8.5275, Validation Loss AVG: 13.4429, lr: 0.001
Epoch [142/150], Training Loss: 28.2254, Validation Loss Current: 6.9113, Validation Loss AVG: 8.6684, lr: 0.001
Epoch [143/150], Training Loss: 21.6592, Validation Loss Current: 6.1008, Validation Loss AVG: 8.5959, lr: 0.001
Epoch [144/150], Training Loss: 19.5774, Validation Loss Current: 5.9293, Validation Loss AVG: 9.4497, lr: 0.001
Epoch [145/150], Training Loss: 20.1770, Validation Loss Current: 5.7243, Validation Loss AVG: 8.5589, lr: 0.001
Epoch [146/150], Training Loss: 18.7415, Validation Loss Current: 5.6719, Validation Loss AVG: 9.2882, lr: 0.001
Epoch [147/150], Training Loss: 17.2151, Validation Loss Current: 6.0120, Validation Loss AVG: 9.1860, lr: 0.001
Epoch [148/150], Training Loss: 17.7908, Validation Loss Current: 8.4428, Validation Loss AVG: 14.0344, lr: 0.001
Epoch [149/150], Training Loss: 24.0359, Validation Loss Current: 6.5785, Validation Loss AVG: 10.5362, lr: 0.001
Epoch [150/150], Training Loss: 23.3545, Validation Loss Current: 6.6912, Validation Loss AVG: 9.2488, lr: 0.001
Patch distance: 1 finished training. Best epoch: 146 Best val accuracy: [0.23519736842105263, 0.23519736842105263, 0.23519736842105263, 0.23519736842105263, 0.23519736842105263, 0.23519736842105263, 0.23519736842105263, 0.23519736842105263, 0.23519736842105263, 0.23519736842105263, 0.23519736842105263, 0.23519736842105263, 0.23519736842105263, 0.23519736842105263, 0.23519736842105263, 0.23519736842105263, 0.23519736842105263, 0.23519736842105263, 0.23519736842105263, 0.23519736842105263, 0.23519736842105263, 0.23519736842105263, 0.23519736842105263, 0.23519736842105263, 0.23519736842105263, 0.23519736842105263, 0.23519736842105263, 0.23519736842105263, 0.23519736842105263, 0.23519736842105263, 0.23519736842105263, 0.26973684210526316, 0.2779605263157895, 0.28125, 0.3059210526315789, 0.3026315789473684, 0.33223684210526316, 0.34210526315789475, 0.3815789473684211, 0.3832236842105263, 0.375, 0.34375, 0.3651315789473684, 0.3404605263157895, 0.3963815789473684, 0.3338815789473684, 0.3717105263157895, 0.38980263157894735, 0.3996710526315789, 0.3930921052631579, 0.375, 0.4095394736842105, 0.3569078947368421, 0.40131578947368424, 0.41776315789473684, 0.4194078947368421, 0.42269736842105265, 0.4144736842105263, 0.3963815789473684, 0.3963815789473684, 0.43914473684210525, 0.40625, 0.32894736842105265, 0.3881578947368421, 0.4292763157894737, 0.4440789473684211, 0.45230263157894735, 0.4605263157894737, 0.47039473684210525, 0.4769736842105263, 0.46875, 0.46875, 0.4506578947368421, 0.4819078947368421, 0.4276315789473684, 0.4786184210526316, 0.5016447368421053, 0.5098684210526315, 0.5016447368421053, 0.5016447368421053, 0.49506578947368424, 0.5164473684210527, 0.5296052631578947, 0.5148026315789473, 0.5049342105263158, 0.48848684210526316, 0.5115131578947368, 0.5263157894736842, 0.53125, 0.5230263157894737, 0.5279605263157895, 0.5230263157894737, 0.537828947368421, 0.5098684210526315, 0.5296052631578947, 0.524671052631579, 0.5411184210526315, 0.5394736842105263, 0.5098684210526315, 0.5345394736842105, 0.5476973684210527, 0.5427631578947368, 0.5625, 0.5575657894736842, 0.5230263157894737, 0.5411184210526315, 0.5460526315789473, 0.5345394736842105, 0.5444078947368421, 0.5509868421052632, 0.5180921052631579, 0.5345394736842105, 0.5674342105263158, 0.5592105263157895, 0.5740131578947368, 0.5773026315789473, 0.5460526315789473, 0.5575657894736842, 0.569078947368421, 0.5740131578947368, 0.5641447368421053, 0.5016447368421053, 0.5180921052631579, 0.5509868421052632, 0.5789473684210527, 0.5805921052631579, 0.5493421052631579, 0.5625, 0.555921052631579, 0.5476973684210527, 0.5756578947368421, 0.5707236842105263, 0.5838815789473685, 0.5822368421052632, 0.6019736842105263, 0.5953947368421053, 0.43914473684210525, 0.48519736842105265, 0.5427631578947368, 0.5526315789473685, 0.47039473684210525, 0.5049342105263158, 0.5608552631578947, 0.5855263157894737, 0.5838815789473685, 0.5838815789473685, 0.6085526315789473, 0.47368421052631576, 0.53125, 0.5592105263157895] Best val loss: 5.671946883201599


----- Training alexnet with sequence: [1, 0.8] -----
Sequence [1] already in state dictionary, jumped
Loaded best state dict for [1]
Current group: 0.8
Epoch [1/75], Training Loss: 22.1631, Validation Loss Current: 8.1508, Validation Loss AVG: 8.1508, lr: 0.001
Epoch [2/75], Training Loss: 21.9577, Validation Loss Current: 8.4463, Validation Loss AVG: 8.4463, lr: 0.001
Epoch [3/75], Training Loss: 22.9716, Validation Loss Current: 7.7433, Validation Loss AVG: 7.7433, lr: 0.001
Epoch [4/75], Training Loss: 22.5115, Validation Loss Current: 8.4771, Validation Loss AVG: 8.4771, lr: 0.001
Epoch [5/75], Training Loss: 21.6897, Validation Loss Current: 7.5948, Validation Loss AVG: 7.5948, lr: 0.001
Epoch [6/75], Training Loss: 20.0109, Validation Loss Current: 8.6997, Validation Loss AVG: 8.6997, lr: 0.001
Epoch [7/75], Training Loss: 21.0356, Validation Loss Current: 8.2299, Validation Loss AVG: 8.2299, lr: 0.001
Epoch [8/75], Training Loss: 19.6911, Validation Loss Current: 9.4814, Validation Loss AVG: 9.4814, lr: 0.001
Epoch [9/75], Training Loss: 21.0760, Validation Loss Current: 8.5866, Validation Loss AVG: 8.5866, lr: 0.001
Epoch [10/75], Training Loss: 20.5396, Validation Loss Current: 8.1590, Validation Loss AVG: 8.1590, lr: 0.001
Epoch [11/75], Training Loss: 20.3219, Validation Loss Current: 7.8700, Validation Loss AVG: 7.8700, lr: 0.001
Epoch [12/75], Training Loss: 18.6965, Validation Loss Current: 8.1592, Validation Loss AVG: 8.1592, lr: 0.001
Epoch [13/75], Training Loss: 19.5181, Validation Loss Current: 8.4469, Validation Loss AVG: 8.4469, lr: 0.001
Epoch [14/75], Training Loss: 23.2784, Validation Loss Current: 7.8720, Validation Loss AVG: 7.8720, lr: 0.001
Epoch [15/75], Training Loss: 18.5894, Validation Loss Current: 8.8119, Validation Loss AVG: 8.8119, lr: 0.001
Epoch [16/75], Training Loss: 16.9116, Validation Loss Current: 8.0715, Validation Loss AVG: 8.0715, lr: 0.001
Epoch [17/75], Training Loss: 15.8502, Validation Loss Current: 7.9929, Validation Loss AVG: 7.9929, lr: 0.001
Epoch [18/75], Training Loss: 15.5948, Validation Loss Current: 8.2781, Validation Loss AVG: 8.2781, lr: 0.001
Epoch [19/75], Training Loss: 15.4808, Validation Loss Current: 8.7314, Validation Loss AVG: 8.7314, lr: 0.001
Epoch [20/75], Training Loss: 18.3147, Validation Loss Current: 8.0877, Validation Loss AVG: 8.0877, lr: 0.001
Epoch [21/75], Training Loss: 18.0900, Validation Loss Current: 8.0432, Validation Loss AVG: 8.0432, lr: 0.001
Epoch [22/75], Training Loss: 15.2868, Validation Loss Current: 8.4462, Validation Loss AVG: 8.4462, lr: 0.001
Epoch [23/75], Training Loss: 15.3694, Validation Loss Current: 9.0004, Validation Loss AVG: 9.0004, lr: 0.001
Epoch [24/75], Training Loss: 14.7249, Validation Loss Current: 8.6262, Validation Loss AVG: 8.6262, lr: 0.001
Epoch [25/75], Training Loss: 14.8677, Validation Loss Current: 10.6411, Validation Loss AVG: 10.6411, lr: 0.001
Epoch [26/75], Training Loss: 16.3878, Validation Loss Current: 8.2102, Validation Loss AVG: 8.2102, lr: 0.001
Epoch [27/75], Training Loss: 13.8091, Validation Loss Current: 8.2427, Validation Loss AVG: 8.2427, lr: 0.001
Epoch [28/75], Training Loss: 13.5692, Validation Loss Current: 8.3022, Validation Loss AVG: 8.3022, lr: 0.001
Epoch [29/75], Training Loss: 13.6527, Validation Loss Current: 10.5270, Validation Loss AVG: 10.5270, lr: 0.001
Epoch [30/75], Training Loss: 16.8113, Validation Loss Current: 7.8180, Validation Loss AVG: 7.8180, lr: 0.001
Epoch [31/75], Training Loss: 13.0101, Validation Loss Current: 9.7924, Validation Loss AVG: 9.7924, lr: 0.001
Epoch [32/75], Training Loss: 18.0585, Validation Loss Current: 8.7523, Validation Loss AVG: 8.7523, lr: 0.001
Epoch [33/75], Training Loss: 13.1298, Validation Loss Current: 9.4186, Validation Loss AVG: 9.4186, lr: 0.001
Epoch [34/75], Training Loss: 11.6057, Validation Loss Current: 10.2200, Validation Loss AVG: 10.2200, lr: 0.001
Epoch [35/75], Training Loss: 11.0739, Validation Loss Current: 9.5077, Validation Loss AVG: 9.5077, lr: 0.001
 --- Early Stopped ---
Patch distance: 0.8 finished training. Best epoch: 5 Best val accuracy: [0.44671052631578945, 0.4529605263157895, 0.4444078947368421, 0.4131578947368421, 0.48190789473684204, 0.42269736842105265, 0.44013157894736843, 0.4430921052631579, 0.43684210526315786, 0.4582236842105263, 0.4598684210526315, 0.4536184210526316, 0.4273026315789473, 0.4365131578947368, 0.4342105263157895, 0.47302631578947374, 0.47730263157894737, 0.47828947368421054, 0.4582236842105264, 0.47664473684210523, 0.48256578947368417, 0.47269736842105264, 0.44342105263157894, 0.46611842105263157, 0.4532894736842105, 0.46611842105263157, 0.48026315789473684, 0.4825657894736842, 0.41578947368421054, 0.4743421052631579, 0.44407894736842096, 0.4529605263157895, 0.4513157894736842, 0.43421052631578955, 0.47171052631578947] Best val loss: 7.594776463508606


----- Training alexnet with sequence: [1, 0.8, 0.6] -----
Sequence [1] already in state dictionary, jumped
Sequence [1, 0.8] already in state dictionary, jumped
Loaded best state dict for [1, 0.8]
Current group: 0.6
Epoch [1/50], Training Loss: 23.3711, Validation Loss Current: 7.3450, Validation Loss AVG: 7.3450, lr: 0.001
Epoch [2/50], Training Loss: 20.8372, Validation Loss Current: 7.7127, Validation Loss AVG: 7.7127, lr: 0.001
Epoch [3/50], Training Loss: 20.5476, Validation Loss Current: 7.3914, Validation Loss AVG: 7.3914, lr: 0.001
Epoch [4/50], Training Loss: 17.8695, Validation Loss Current: 7.4500, Validation Loss AVG: 7.4500, lr: 0.001
Epoch [5/50], Training Loss: 17.1927, Validation Loss Current: 7.0010, Validation Loss AVG: 7.0010, lr: 0.001
Epoch [6/50], Training Loss: 15.9221, Validation Loss Current: 7.5809, Validation Loss AVG: 7.5809, lr: 0.001
Epoch [7/50], Training Loss: 15.5207, Validation Loss Current: 7.8499, Validation Loss AVG: 7.8499, lr: 0.001
Epoch [8/50], Training Loss: 16.4737, Validation Loss Current: 7.4219, Validation Loss AVG: 7.4219, lr: 0.001
Epoch [9/50], Training Loss: 14.7955, Validation Loss Current: 7.8443, Validation Loss AVG: 7.8443, lr: 0.001
Epoch [10/50], Training Loss: 15.0223, Validation Loss Current: 9.0556, Validation Loss AVG: 9.0556, lr: 0.001
Epoch [11/50], Training Loss: 16.7410, Validation Loss Current: 7.6832, Validation Loss AVG: 7.6832, lr: 0.001
Epoch [12/50], Training Loss: 15.4637, Validation Loss Current: 8.9815, Validation Loss AVG: 8.9815, lr: 0.001
Epoch [13/50], Training Loss: 16.0915, Validation Loss Current: 7.4876, Validation Loss AVG: 7.4876, lr: 0.001
Epoch [14/50], Training Loss: 14.0880, Validation Loss Current: 8.0748, Validation Loss AVG: 8.0748, lr: 0.001
Epoch [15/50], Training Loss: 12.9070, Validation Loss Current: 8.1926, Validation Loss AVG: 8.1926, lr: 0.001
Epoch [16/50], Training Loss: 14.4728, Validation Loss Current: 7.9673, Validation Loss AVG: 7.9673, lr: 0.001
Epoch [17/50], Training Loss: 12.4803, Validation Loss Current: 8.5769, Validation Loss AVG: 8.5769, lr: 0.001
Epoch [18/50], Training Loss: 14.3976, Validation Loss Current: 7.8380, Validation Loss AVG: 7.8380, lr: 0.001
Epoch [19/50], Training Loss: 15.2440, Validation Loss Current: 9.3158, Validation Loss AVG: 9.3158, lr: 0.001
Epoch [20/50], Training Loss: 23.8226, Validation Loss Current: 8.6390, Validation Loss AVG: 8.6390, lr: 0.001
Epoch [21/50], Training Loss: 17.2062, Validation Loss Current: 7.3773, Validation Loss AVG: 7.3773, lr: 0.001
Epoch [22/50], Training Loss: 12.5860, Validation Loss Current: 7.8141, Validation Loss AVG: 7.8141, lr: 0.001
Epoch [23/50], Training Loss: 12.0470, Validation Loss Current: 8.9055, Validation Loss AVG: 8.9055, lr: 0.001
Epoch [24/50], Training Loss: 16.7158, Validation Loss Current: 8.3775, Validation Loss AVG: 8.3775, lr: 0.001
Epoch [25/50], Training Loss: 13.6583, Validation Loss Current: 8.2617, Validation Loss AVG: 8.2617, lr: 0.001
Epoch [26/50], Training Loss: 16.8478, Validation Loss Current: 8.4630, Validation Loss AVG: 8.4630, lr: 0.001
Epoch [27/50], Training Loss: 16.2018, Validation Loss Current: 7.8824, Validation Loss AVG: 7.8824, lr: 0.001
Epoch [28/50], Training Loss: 12.4827, Validation Loss Current: 8.7644, Validation Loss AVG: 8.7644, lr: 0.001
Epoch [29/50], Training Loss: 11.7506, Validation Loss Current: 8.4719, Validation Loss AVG: 8.4719, lr: 0.001
Epoch [30/50], Training Loss: 10.6516, Validation Loss Current: 9.4233, Validation Loss AVG: 9.4233, lr: 0.001
Epoch [31/50], Training Loss: 11.3296, Validation Loss Current: 8.5706, Validation Loss AVG: 8.5706, lr: 0.001
Epoch [32/50], Training Loss: 8.6803, Validation Loss Current: 8.8042, Validation Loss AVG: 8.8042, lr: 0.001
Epoch [33/50], Training Loss: 7.9534, Validation Loss Current: 8.9525, Validation Loss AVG: 8.9525, lr: 0.001
Epoch [34/50], Training Loss: 7.9586, Validation Loss Current: 9.5561, Validation Loss AVG: 9.5561, lr: 0.001
Epoch [35/50], Training Loss: 7.8072, Validation Loss Current: 12.8576, Validation Loss AVG: 12.8576, lr: 0.001
 --- Early Stopped ---
Patch distance: 0.6 finished training. Best epoch: 5 Best val accuracy: [0.48684210526315785, 0.4743421052631579, 0.4878289473684211, 0.5078947368421053, 0.5197368421052632, 0.5095394736842105, 0.5108552631578946, 0.5105263157894736, 0.5069078947368421, 0.44309210526315795, 0.4894736842105263, 0.46875, 0.5269736842105261, 0.5108552631578948, 0.5055921052631579, 0.5134868421052631, 0.5075657894736841, 0.5223684210526315, 0.4756578947368421, 0.43519736842105267, 0.5279605263157895, 0.5210526315789473, 0.5013157894736842, 0.47631578947368414, 0.5052631578947369, 0.48519736842105265, 0.5101973684210526, 0.5095394736842105, 0.5108552631578946, 0.4634868421052632, 0.5013157894736842, 0.5184210526315789, 0.5256578947368421, 0.5177631578947368, 0.44210526315789467] Best val loss: 7.000970447063446


----- Training alexnet with sequence: [1, 0.8, 0.6, 0.4] -----
Sequence [1] already in state dictionary, jumped
Sequence [1, 0.8] already in state dictionary, jumped
Sequence [1, 0.8, 0.6] already in state dictionary, jumped
Loaded best state dict for [1, 0.8, 0.6]
Current group: 0.4
Epoch [1/38], Training Loss: 28.7509, Validation Loss Current: 7.2162, Validation Loss AVG: 7.2162, lr: 0.001
Epoch [2/38], Training Loss: 20.9509, Validation Loss Current: 7.5106, Validation Loss AVG: 7.5106, lr: 0.001
Epoch [3/38], Training Loss: 18.9205, Validation Loss Current: 7.5097, Validation Loss AVG: 7.5097, lr: 0.001
Epoch [4/38], Training Loss: 17.6612, Validation Loss Current: 7.7378, Validation Loss AVG: 7.7378, lr: 0.001
Epoch [5/38], Training Loss: 18.7749, Validation Loss Current: 9.4761, Validation Loss AVG: 9.4761, lr: 0.001
Epoch [6/38], Training Loss: 25.1436, Validation Loss Current: 7.5075, Validation Loss AVG: 7.5075, lr: 0.001
Epoch [7/38], Training Loss: 19.2599, Validation Loss Current: 7.4966, Validation Loss AVG: 7.4966, lr: 0.001
Epoch [8/38], Training Loss: 17.8641, Validation Loss Current: 7.3499, Validation Loss AVG: 7.3499, lr: 0.001
Epoch [9/38], Training Loss: 17.4814, Validation Loss Current: 7.9351, Validation Loss AVG: 7.9351, lr: 0.001
Epoch [10/38], Training Loss: 16.7336, Validation Loss Current: 7.8922, Validation Loss AVG: 7.8922, lr: 0.001
Epoch [11/38], Training Loss: 18.2159, Validation Loss Current: 10.7519, Validation Loss AVG: 10.7519, lr: 0.001
Epoch [12/38], Training Loss: 28.9138, Validation Loss Current: 8.4556, Validation Loss AVG: 8.4556, lr: 0.001
Epoch [13/38], Training Loss: 22.4099, Validation Loss Current: 7.7713, Validation Loss AVG: 7.7713, lr: 0.001
Epoch [14/38], Training Loss: 17.6293, Validation Loss Current: 8.0404, Validation Loss AVG: 8.0404, lr: 0.001
Epoch [15/38], Training Loss: 15.9995, Validation Loss Current: 8.0135, Validation Loss AVG: 8.0135, lr: 0.001
Epoch [16/38], Training Loss: 14.9238, Validation Loss Current: 8.1995, Validation Loss AVG: 8.1995, lr: 0.001
Epoch [17/38], Training Loss: 15.8423, Validation Loss Current: 8.5557, Validation Loss AVG: 8.5557, lr: 0.001
Epoch [18/38], Training Loss: 15.0334, Validation Loss Current: 7.8483, Validation Loss AVG: 7.8483, lr: 0.001
Epoch [19/38], Training Loss: 12.3323, Validation Loss Current: 9.1654, Validation Loss AVG: 9.1654, lr: 0.001
Epoch [20/38], Training Loss: 11.4131, Validation Loss Current: 8.7049, Validation Loss AVG: 8.7049, lr: 0.001
Epoch [21/38], Training Loss: 12.2302, Validation Loss Current: 8.7945, Validation Loss AVG: 8.7945, lr: 0.001
Epoch [22/38], Training Loss: 11.0326, Validation Loss Current: 8.9144, Validation Loss AVG: 8.9144, lr: 0.001
Epoch [23/38], Training Loss: 12.0504, Validation Loss Current: 8.8709, Validation Loss AVG: 8.8709, lr: 0.001
Epoch [24/38], Training Loss: 10.8272, Validation Loss Current: 9.4925, Validation Loss AVG: 9.4925, lr: 0.001
Epoch [25/38], Training Loss: 9.4117, Validation Loss Current: 9.7405, Validation Loss AVG: 9.7405, lr: 0.001
Epoch [26/38], Training Loss: 9.4183, Validation Loss Current: 10.5874, Validation Loss AVG: 10.5874, lr: 0.001
Epoch [27/38], Training Loss: 10.6606, Validation Loss Current: 9.6821, Validation Loss AVG: 9.6821, lr: 0.001
Epoch [28/38], Training Loss: 9.3287, Validation Loss Current: 9.5571, Validation Loss AVG: 9.5571, lr: 0.001
Epoch [29/38], Training Loss: 7.1555, Validation Loss Current: 10.9934, Validation Loss AVG: 10.9934, lr: 0.001
Epoch [30/38], Training Loss: 5.7323, Validation Loss Current: 10.4483, Validation Loss AVG: 10.4483, lr: 0.001
Epoch [31/38], Training Loss: 5.3006, Validation Loss Current: 10.8079, Validation Loss AVG: 10.8079, lr: 0.001
 --- Early Stopped ---
Patch distance: 0.4 finished training. Best epoch: 1 Best val accuracy: [0.5059210526315789, 0.5105263157894736, 0.5292763157894738, 0.5180921052631579, 0.46842105263157896, 0.5200657894736842, 0.5184210526315789, 0.5299342105263158, 0.5361842105263158, 0.5210526315789474, 0.3648026315789474, 0.42171052631578954, 0.47335526315789467, 0.5029605263157895, 0.5049342105263158, 0.49078947368421055, 0.4726973684210526, 0.5121710526315789, 0.4894736842105264, 0.5177631578947368, 0.5164473684210527, 0.5125, 0.4858552631578947, 0.4973684210526315, 0.5046052631578948, 0.4845394736842105, 0.47434210526315795, 0.5200657894736842, 0.5016447368421052, 0.5210526315789473, 0.5101973684210526] Best val loss: 7.216153573989868


----- Training alexnet with sequence: [1, 0.8, 0.6, 0.4, 0.2] -----
Sequence [1] already in state dictionary, jumped
Sequence [1, 0.8] already in state dictionary, jumped
Sequence [1, 0.8, 0.6] already in state dictionary, jumped
Sequence [1, 0.8, 0.6, 0.4] already in state dictionary, jumped
Loaded best state dict for [1, 0.8, 0.6, 0.4]
Current group: 0.2
Epoch [1/30], Training Loss: 36.6830, Validation Loss Current: 9.0292, Validation Loss AVG: 9.0292, lr: 0.001
Epoch [2/30], Training Loss: 29.4282, Validation Loss Current: 9.6847, Validation Loss AVG: 9.6847, lr: 0.001
Epoch [3/30], Training Loss: 28.5599, Validation Loss Current: 8.9123, Validation Loss AVG: 8.9123, lr: 0.001
Epoch [4/30], Training Loss: 26.4618, Validation Loss Current: 9.5272, Validation Loss AVG: 9.5272, lr: 0.001
Epoch [5/30], Training Loss: 25.3493, Validation Loss Current: 9.9861, Validation Loss AVG: 9.9861, lr: 0.001
Epoch [6/30], Training Loss: 25.2524, Validation Loss Current: 10.7857, Validation Loss AVG: 10.7857, lr: 0.001
Epoch [7/30], Training Loss: 27.4538, Validation Loss Current: 10.9856, Validation Loss AVG: 10.9856, lr: 0.001
Epoch [8/30], Training Loss: 31.4421, Validation Loss Current: 10.6231, Validation Loss AVG: 10.6231, lr: 0.001
Epoch [9/30], Training Loss: 26.0590, Validation Loss Current: 11.1899, Validation Loss AVG: 11.1899, lr: 0.001
Epoch [10/30], Training Loss: 26.7053, Validation Loss Current: 9.7565, Validation Loss AVG: 9.7565, lr: 0.001
Epoch [11/30], Training Loss: 23.9933, Validation Loss Current: 10.2189, Validation Loss AVG: 10.2189, lr: 0.001
Epoch [12/30], Training Loss: 23.9709, Validation Loss Current: 10.4173, Validation Loss AVG: 10.4173, lr: 0.001
Epoch [13/30], Training Loss: 25.0099, Validation Loss Current: 10.2326, Validation Loss AVG: 10.2326, lr: 0.001
Epoch [14/30], Training Loss: 27.4670, Validation Loss Current: 9.7233, Validation Loss AVG: 9.7233, lr: 0.001
Epoch [15/30], Training Loss: 28.9634, Validation Loss Current: 9.5063, Validation Loss AVG: 9.5063, lr: 0.001
Epoch [16/30], Training Loss: 26.0673, Validation Loss Current: 9.5587, Validation Loss AVG: 9.5587, lr: 0.001
Epoch [17/30], Training Loss: 24.9599, Validation Loss Current: 10.0711, Validation Loss AVG: 10.0711, lr: 0.001
Epoch [18/30], Training Loss: 23.1600, Validation Loss Current: 11.3664, Validation Loss AVG: 11.3664, lr: 0.001
Epoch [19/30], Training Loss: 26.7237, Validation Loss Current: 9.4822, Validation Loss AVG: 9.4822, lr: 0.001
Epoch [20/30], Training Loss: 23.7561, Validation Loss Current: 10.2917, Validation Loss AVG: 10.2917, lr: 0.001
Epoch [21/30], Training Loss: 23.7871, Validation Loss Current: 9.9143, Validation Loss AVG: 9.9143, lr: 0.001
Epoch [22/30], Training Loss: 23.1785, Validation Loss Current: 10.4358, Validation Loss AVG: 10.4358, lr: 0.001
Epoch [23/30], Training Loss: 21.8268, Validation Loss Current: 10.5885, Validation Loss AVG: 10.5885, lr: 0.001
Epoch [24/30], Training Loss: 20.9458, Validation Loss Current: 10.6466, Validation Loss AVG: 10.6466, lr: 0.001
Epoch [25/30], Training Loss: 19.4129, Validation Loss Current: 12.0776, Validation Loss AVG: 12.0776, lr: 0.001
Epoch [26/30], Training Loss: 21.7882, Validation Loss Current: 11.0461, Validation Loss AVG: 11.0461, lr: 0.001
Epoch [27/30], Training Loss: 21.0455, Validation Loss Current: 11.7113, Validation Loss AVG: 11.7113, lr: 0.001
Epoch [28/30], Training Loss: 19.6473, Validation Loss Current: 11.4258, Validation Loss AVG: 11.4258, lr: 0.001
Epoch [29/30], Training Loss: 18.9159, Validation Loss Current: 11.8482, Validation Loss AVG: 11.8482, lr: 0.001
Epoch [30/30], Training Loss: 19.5197, Validation Loss Current: 11.6576, Validation Loss AVG: 11.6576, lr: 0.001
Epoch [31/30], Training Loss: 21.6884, Validation Loss Current: 10.6171, Validation Loss AVG: 10.6171, lr: 0.001
Epoch [32/30], Training Loss: 21.9138, Validation Loss Current: 11.5535, Validation Loss AVG: 11.5535, lr: 0.001
Epoch [33/30], Training Loss: 20.4374, Validation Loss Current: 11.1703, Validation Loss AVG: 11.1703, lr: 0.001
 --- Early Stopped ---
Patch distance: 0.2 finished training. Best epoch: 3 Best val accuracy: [0.43453947368421053, 0.40032894736842106, 0.4115131578947368, 0.4296052631578947, 0.4095394736842105, 0.38585526315789476, 0.362171052631579, 0.39309210526315785, 0.3575657894736842, 0.40032894736842106, 0.39342105263157895, 0.35559210526315793, 0.40592105263157896, 0.3957236842105263, 0.3536184210526315, 0.34407894736842104, 0.35131578947368425, 0.3391447368421053, 0.3417763157894737, 0.3401315789473684, 0.35526315789473684, 0.3391447368421053, 0.34605263157894733, 0.36217105263157895, 0.33124999999999993, 0.35427631578947366, 0.3631578947368421, 0.36743421052631586, 0.37105263157894736, 0.36085526315789473, 0.3302631578947368, 0.3526315789473684, 0.39671052631578946] Best val loss: 8.912346386909485


Fold: 4
----- Training alexnet with sequence: [1] -----
Current group: 1
Epoch [1/150], Training Loss: 41.5480, Validation Loss Current: 10.3712, Validation Loss AVG: 10.3703, lr: 0.001
Epoch [2/150], Training Loss: 41.3940, Validation Loss Current: 10.3278, Validation Loss AVG: 10.3259, lr: 0.001
Epoch [3/150], Training Loss: 41.2999, Validation Loss Current: 10.2852, Validation Loss AVG: 10.2815, lr: 0.001
Epoch [4/150], Training Loss: 41.1583, Validation Loss Current: 10.2457, Validation Loss AVG: 10.2384, lr: 0.001
Epoch [5/150], Training Loss: 40.8790, Validation Loss Current: 10.1923, Validation Loss AVG: 10.1867, lr: 0.001
Epoch [6/150], Training Loss: 40.5865, Validation Loss Current: 10.1206, Validation Loss AVG: 10.1022, lr: 0.001
Epoch [7/150], Training Loss: 39.8734, Validation Loss Current: 10.0491, Validation Loss AVG: 10.0302, lr: 0.001
Epoch [8/150], Training Loss: 40.2951, Validation Loss Current: 10.0062, Validation Loss AVG: 10.0019, lr: 0.001
Epoch [9/150], Training Loss: 40.2873, Validation Loss Current: 10.0038, Validation Loss AVG: 9.9996, lr: 0.001
Epoch [10/150], Training Loss: 40.1528, Validation Loss Current: 9.9729, Validation Loss AVG: 9.9818, lr: 0.001
Epoch [11/150], Training Loss: 40.0524, Validation Loss Current: 9.9408, Validation Loss AVG: 9.9571, lr: 0.001
Epoch [12/150], Training Loss: 39.7237, Validation Loss Current: 9.9097, Validation Loss AVG: 9.9330, lr: 0.001
Epoch [13/150], Training Loss: 39.5038, Validation Loss Current: 9.9002, Validation Loss AVG: 9.9435, lr: 0.001
Epoch [14/150], Training Loss: 39.7553, Validation Loss Current: 9.8836, Validation Loss AVG: 9.9501, lr: 0.001
Epoch [15/150], Training Loss: 40.2958, Validation Loss Current: 9.8757, Validation Loss AVG: 9.9272, lr: 0.001
Epoch [16/150], Training Loss: 39.8065, Validation Loss Current: 9.8678, Validation Loss AVG: 9.9193, lr: 0.001
Epoch [17/150], Training Loss: 40.1619, Validation Loss Current: 9.8364, Validation Loss AVG: 9.9565, lr: 0.001
Epoch [18/150], Training Loss: 39.7409, Validation Loss Current: 9.8476, Validation Loss AVG: 9.9193, lr: 0.001
Epoch [19/150], Training Loss: 39.5816, Validation Loss Current: 9.7847, Validation Loss AVG: 9.9600, lr: 0.001
Epoch [20/150], Training Loss: 38.9193, Validation Loss Current: 9.7593, Validation Loss AVG: 9.9451, lr: 0.001
Epoch [21/150], Training Loss: 39.6708, Validation Loss Current: 9.7398, Validation Loss AVG: 9.8916, lr: 0.001
Epoch [22/150], Training Loss: 39.4109, Validation Loss Current: 9.6819, Validation Loss AVG: 9.9724, lr: 0.001
Epoch [23/150], Training Loss: 39.2516, Validation Loss Current: 9.6699, Validation Loss AVG: 9.9330, lr: 0.001
Epoch [24/150], Training Loss: 39.2127, Validation Loss Current: 9.5874, Validation Loss AVG: 9.9591, lr: 0.001
Epoch [25/150], Training Loss: 38.7393, Validation Loss Current: 9.5436, Validation Loss AVG: 10.1129, lr: 0.001
Epoch [26/150], Training Loss: 38.8140, Validation Loss Current: 9.5392, Validation Loss AVG: 10.0607, lr: 0.001
Epoch [27/150], Training Loss: 38.6455, Validation Loss Current: 9.5360, Validation Loss AVG: 9.7786, lr: 0.001
Epoch [28/150], Training Loss: 37.8634, Validation Loss Current: 9.4444, Validation Loss AVG: 9.8553, lr: 0.001
Epoch [29/150], Training Loss: 38.0252, Validation Loss Current: 9.3688, Validation Loss AVG: 9.9542, lr: 0.001
Epoch [30/150], Training Loss: 38.0104, Validation Loss Current: 9.3590, Validation Loss AVG: 10.1911, lr: 0.001
Epoch [31/150], Training Loss: 38.2200, Validation Loss Current: 9.2777, Validation Loss AVG: 9.7947, lr: 0.001
Epoch [32/150], Training Loss: 37.6035, Validation Loss Current: 9.2587, Validation Loss AVG: 10.0295, lr: 0.001
Epoch [33/150], Training Loss: 37.5403, Validation Loss Current: 9.1219, Validation Loss AVG: 9.7333, lr: 0.001
Epoch [34/150], Training Loss: 36.6136, Validation Loss Current: 9.0965, Validation Loss AVG: 10.0104, lr: 0.001
Epoch [35/150], Training Loss: 36.8894, Validation Loss Current: 8.9784, Validation Loss AVG: 9.3396, lr: 0.001
Epoch [36/150], Training Loss: 36.5772, Validation Loss Current: 9.0548, Validation Loss AVG: 9.3620, lr: 0.001
Epoch [37/150], Training Loss: 35.7920, Validation Loss Current: 8.6102, Validation Loss AVG: 9.5019, lr: 0.001
Epoch [38/150], Training Loss: 36.2625, Validation Loss Current: 8.7550, Validation Loss AVG: 10.5359, lr: 0.001
Epoch [39/150], Training Loss: 33.8517, Validation Loss Current: 8.5085, Validation Loss AVG: 10.0125, lr: 0.001
Epoch [40/150], Training Loss: 34.0839, Validation Loss Current: 8.4199, Validation Loss AVG: 9.9737, lr: 0.001
Epoch [41/150], Training Loss: 33.7020, Validation Loss Current: 8.2246, Validation Loss AVG: 8.9924, lr: 0.001
Epoch [42/150], Training Loss: 32.7932, Validation Loss Current: 11.7617, Validation Loss AVG: 19.3592, lr: 0.001
Epoch [43/150], Training Loss: 38.1440, Validation Loss Current: 8.5806, Validation Loss AVG: 9.0841, lr: 0.001
Epoch [44/150], Training Loss: 34.1612, Validation Loss Current: 8.5950, Validation Loss AVG: 10.1003, lr: 0.001
Epoch [45/150], Training Loss: 34.5826, Validation Loss Current: 8.1649, Validation Loss AVG: 8.9014, lr: 0.001
Epoch [46/150], Training Loss: 33.7068, Validation Loss Current: 8.1159, Validation Loss AVG: 9.4598, lr: 0.001
Epoch [47/150], Training Loss: 32.2349, Validation Loss Current: 8.0542, Validation Loss AVG: 8.9298, lr: 0.001
Epoch [48/150], Training Loss: 32.1955, Validation Loss Current: 7.9020, Validation Loss AVG: 8.9863, lr: 0.001
Epoch [49/150], Training Loss: 31.9632, Validation Loss Current: 8.9063, Validation Loss AVG: 12.1147, lr: 0.001
Epoch [50/150], Training Loss: 33.2897, Validation Loss Current: 7.9075, Validation Loss AVG: 8.9468, lr: 0.001
Epoch [51/150], Training Loss: 32.8827, Validation Loss Current: 8.9352, Validation Loss AVG: 11.0156, lr: 0.001
Epoch [52/150], Training Loss: 34.8161, Validation Loss Current: 8.3224, Validation Loss AVG: 10.0319, lr: 0.001
Epoch [53/150], Training Loss: 33.5749, Validation Loss Current: 8.3375, Validation Loss AVG: 9.7481, lr: 0.001
Epoch [54/150], Training Loss: 34.2519, Validation Loss Current: 8.0067, Validation Loss AVG: 8.8642, lr: 0.001
Epoch [55/150], Training Loss: 33.7004, Validation Loss Current: 8.5469, Validation Loss AVG: 9.7941, lr: 0.001
Epoch [56/150], Training Loss: 37.7014, Validation Loss Current: 8.7422, Validation Loss AVG: 9.2836, lr: 0.001
Epoch [57/150], Training Loss: 33.6095, Validation Loss Current: 7.9117, Validation Loss AVG: 8.8017, lr: 0.001
Epoch [58/150], Training Loss: 31.1945, Validation Loss Current: 7.5431, Validation Loss AVG: 8.5363, lr: 0.001
Epoch [59/150], Training Loss: 31.1224, Validation Loss Current: 7.4889, Validation Loss AVG: 8.3922, lr: 0.001
Epoch [60/150], Training Loss: 28.8368, Validation Loss Current: 7.5034, Validation Loss AVG: 8.6192, lr: 0.001
Epoch [61/150], Training Loss: 29.4830, Validation Loss Current: 7.3911, Validation Loss AVG: 8.6276, lr: 0.001
Epoch [62/150], Training Loss: 29.3288, Validation Loss Current: 7.5392, Validation Loss AVG: 8.5839, lr: 0.001
Epoch [63/150], Training Loss: 31.5724, Validation Loss Current: 7.3113, Validation Loss AVG: 8.3270, lr: 0.001
Epoch [64/150], Training Loss: 30.4409, Validation Loss Current: 7.6070, Validation Loss AVG: 8.9431, lr: 0.001
Epoch [65/150], Training Loss: 29.5122, Validation Loss Current: 7.5261, Validation Loss AVG: 8.6169, lr: 0.001
Epoch [66/150], Training Loss: 30.1791, Validation Loss Current: 7.1004, Validation Loss AVG: 8.4343, lr: 0.001
Epoch [67/150], Training Loss: 28.6242, Validation Loss Current: 7.2791, Validation Loss AVG: 8.4019, lr: 0.001
Epoch [68/150], Training Loss: 28.8996, Validation Loss Current: 7.1225, Validation Loss AVG: 8.4820, lr: 0.001
Epoch [69/150], Training Loss: 29.2551, Validation Loss Current: 7.0126, Validation Loss AVG: 8.2081, lr: 0.001
Epoch [70/150], Training Loss: 28.5675, Validation Loss Current: 7.2478, Validation Loss AVG: 8.2496, lr: 0.001
Epoch [71/150], Training Loss: 29.7423, Validation Loss Current: 7.3415, Validation Loss AVG: 8.5195, lr: 0.001
Epoch [72/150], Training Loss: 31.3055, Validation Loss Current: 7.6138, Validation Loss AVG: 8.5569, lr: 0.001
Epoch [73/150], Training Loss: 32.6040, Validation Loss Current: 7.4260, Validation Loss AVG: 8.5790, lr: 0.001
Epoch [74/150], Training Loss: 30.3269, Validation Loss Current: 7.7523, Validation Loss AVG: 10.2470, lr: 0.001
Epoch [75/150], Training Loss: 30.3282, Validation Loss Current: 7.2188, Validation Loss AVG: 8.3235, lr: 0.001
Epoch [76/150], Training Loss: 27.8715, Validation Loss Current: 7.0259, Validation Loss AVG: 8.4038, lr: 0.001
Epoch [77/150], Training Loss: 27.9307, Validation Loss Current: 6.9470, Validation Loss AVG: 8.1773, lr: 0.001
Epoch [78/150], Training Loss: 27.5732, Validation Loss Current: 6.7368, Validation Loss AVG: 8.0776, lr: 0.001
Epoch [79/150], Training Loss: 27.0793, Validation Loss Current: 8.0170, Validation Loss AVG: 9.6996, lr: 0.001
Epoch [80/150], Training Loss: 28.1296, Validation Loss Current: 6.5553, Validation Loss AVG: 8.6132, lr: 0.001
Epoch [81/150], Training Loss: 26.6825, Validation Loss Current: 7.0488, Validation Loss AVG: 8.7527, lr: 0.001
Epoch [82/150], Training Loss: 26.1135, Validation Loss Current: 7.2846, Validation Loss AVG: 8.7721, lr: 0.001
Epoch [83/150], Training Loss: 28.8862, Validation Loss Current: 7.4625, Validation Loss AVG: 10.6933, lr: 0.001
Epoch [84/150], Training Loss: 32.5235, Validation Loss Current: 7.5400, Validation Loss AVG: 9.0462, lr: 0.001
Epoch [85/150], Training Loss: 28.6920, Validation Loss Current: 6.6298, Validation Loss AVG: 8.6471, lr: 0.001
Epoch [86/150], Training Loss: 26.7780, Validation Loss Current: 6.6422, Validation Loss AVG: 10.1525, lr: 0.001
Epoch [87/150], Training Loss: 25.9954, Validation Loss Current: 6.5169, Validation Loss AVG: 8.5061, lr: 0.001
Epoch [88/150], Training Loss: 26.0427, Validation Loss Current: 6.2775, Validation Loss AVG: 8.1480, lr: 0.001
Epoch [89/150], Training Loss: 24.9931, Validation Loss Current: 7.1846, Validation Loss AVG: 11.4535, lr: 0.001
Epoch [90/150], Training Loss: 28.0266, Validation Loss Current: 6.5730, Validation Loss AVG: 8.1478, lr: 0.001
Epoch [91/150], Training Loss: 24.2637, Validation Loss Current: 6.2402, Validation Loss AVG: 8.9693, lr: 0.001
Epoch [92/150], Training Loss: 25.7482, Validation Loss Current: 6.2083, Validation Loss AVG: 9.6190, lr: 0.001
Epoch [93/150], Training Loss: 23.8213, Validation Loss Current: 6.7470, Validation Loss AVG: 9.3666, lr: 0.001
Epoch [94/150], Training Loss: 24.4812, Validation Loss Current: 6.1989, Validation Loss AVG: 9.4327, lr: 0.001
Epoch [95/150], Training Loss: 23.1055, Validation Loss Current: 6.1753, Validation Loss AVG: 9.2830, lr: 0.001
Epoch [96/150], Training Loss: 24.7481, Validation Loss Current: 6.7299, Validation Loss AVG: 9.5955, lr: 0.001
Epoch [97/150], Training Loss: 25.4325, Validation Loss Current: 6.5105, Validation Loss AVG: 9.5307, lr: 0.001
Epoch [98/150], Training Loss: 23.9391, Validation Loss Current: 6.2660, Validation Loss AVG: 10.0340, lr: 0.001
Epoch [99/150], Training Loss: 23.6346, Validation Loss Current: 6.1987, Validation Loss AVG: 10.1435, lr: 0.001
Epoch [100/150], Training Loss: 23.7284, Validation Loss Current: 6.0790, Validation Loss AVG: 9.1664, lr: 0.001
Epoch [101/150], Training Loss: 22.4623, Validation Loss Current: 6.3849, Validation Loss AVG: 8.9427, lr: 0.001
Epoch [102/150], Training Loss: 24.7777, Validation Loss Current: 7.2228, Validation Loss AVG: 11.4821, lr: 0.001
Epoch [103/150], Training Loss: 28.3424, Validation Loss Current: 7.0990, Validation Loss AVG: 9.0246, lr: 0.001
Epoch [104/150], Training Loss: 26.4127, Validation Loss Current: 6.1197, Validation Loss AVG: 8.7240, lr: 0.001
Epoch [105/150], Training Loss: 24.4739, Validation Loss Current: 6.2118, Validation Loss AVG: 8.3514, lr: 0.001
Epoch [106/150], Training Loss: 25.1838, Validation Loss Current: 6.1772, Validation Loss AVG: 9.8362, lr: 0.001
Epoch [107/150], Training Loss: 24.2168, Validation Loss Current: 8.4672, Validation Loss AVG: 11.4983, lr: 0.001
Epoch [108/150], Training Loss: 29.2388, Validation Loss Current: 6.5607, Validation Loss AVG: 9.1185, lr: 0.001
Epoch [109/150], Training Loss: 24.9446, Validation Loss Current: 6.4508, Validation Loss AVG: 10.7777, lr: 0.001
Epoch [110/150], Training Loss: 26.0203, Validation Loss Current: 6.4605, Validation Loss AVG: 8.5172, lr: 0.001
Epoch [111/150], Training Loss: 24.4257, Validation Loss Current: 5.9216, Validation Loss AVG: 9.2307, lr: 0.001
Epoch [112/150], Training Loss: 23.3332, Validation Loss Current: 6.1758, Validation Loss AVG: 8.8476, lr: 0.001
Epoch [113/150], Training Loss: 22.8328, Validation Loss Current: 6.5656, Validation Loss AVG: 10.0645, lr: 0.001
Epoch [114/150], Training Loss: 27.6572, Validation Loss Current: 6.2329, Validation Loss AVG: 8.4373, lr: 0.001
Epoch [115/150], Training Loss: 23.8778, Validation Loss Current: 6.1922, Validation Loss AVG: 8.0003, lr: 0.001
Epoch [116/150], Training Loss: 25.8906, Validation Loss Current: 6.1754, Validation Loss AVG: 8.6883, lr: 0.001
Epoch [117/150], Training Loss: 23.0400, Validation Loss Current: 5.9292, Validation Loss AVG: 8.3856, lr: 0.001
Epoch [118/150], Training Loss: 22.7762, Validation Loss Current: 7.3530, Validation Loss AVG: 12.2496, lr: 0.001
Epoch [119/150], Training Loss: 26.3535, Validation Loss Current: 6.0978, Validation Loss AVG: 8.3520, lr: 0.001
Epoch [120/150], Training Loss: 23.4433, Validation Loss Current: 6.1125, Validation Loss AVG: 8.1187, lr: 0.001
Epoch [121/150], Training Loss: 21.2688, Validation Loss Current: 6.5228, Validation Loss AVG: 9.3274, lr: 0.001
Epoch [122/150], Training Loss: 23.4025, Validation Loss Current: 5.9052, Validation Loss AVG: 8.5460, lr: 0.001
Epoch [123/150], Training Loss: 20.4137, Validation Loss Current: 6.1115, Validation Loss AVG: 9.9412, lr: 0.001
Epoch [124/150], Training Loss: 19.8449, Validation Loss Current: 5.9556, Validation Loss AVG: 9.6874, lr: 0.001
Epoch [125/150], Training Loss: 19.9743, Validation Loss Current: 5.9559, Validation Loss AVG: 9.6256, lr: 0.001
Epoch [126/150], Training Loss: 19.5362, Validation Loss Current: 6.2755, Validation Loss AVG: 10.4726, lr: 0.001
Epoch [127/150], Training Loss: 19.8001, Validation Loss Current: 6.1929, Validation Loss AVG: 8.8927, lr: 0.001
Epoch [128/150], Training Loss: 20.2386, Validation Loss Current: 6.0777, Validation Loss AVG: 8.9486, lr: 0.001
Epoch [129/150], Training Loss: 19.9463, Validation Loss Current: 6.4392, Validation Loss AVG: 8.7146, lr: 0.001
Epoch [130/150], Training Loss: 22.1573, Validation Loss Current: 6.4273, Validation Loss AVG: 9.9684, lr: 0.001
Epoch [131/150], Training Loss: 20.0474, Validation Loss Current: 6.0399, Validation Loss AVG: 9.0541, lr: 0.001
Epoch [132/150], Training Loss: 19.2512, Validation Loss Current: 6.4566, Validation Loss AVG: 9.7232, lr: 0.001
Epoch [133/150], Training Loss: 18.7653, Validation Loss Current: 5.6892, Validation Loss AVG: 10.0721, lr: 0.001
Epoch [134/150], Training Loss: 18.8539, Validation Loss Current: 5.9729, Validation Loss AVG: 10.8341, lr: 0.001
Epoch [135/150], Training Loss: 20.0050, Validation Loss Current: 6.1956, Validation Loss AVG: 10.3783, lr: 0.001
Epoch [136/150], Training Loss: 20.1430, Validation Loss Current: 5.5815, Validation Loss AVG: 10.3087, lr: 0.001
Epoch [137/150], Training Loss: 18.3107, Validation Loss Current: 6.0246, Validation Loss AVG: 9.6378, lr: 0.001
Epoch [138/150], Training Loss: 18.4285, Validation Loss Current: 6.4292, Validation Loss AVG: 9.6907, lr: 0.001
Epoch [139/150], Training Loss: 17.6953, Validation Loss Current: 6.2694, Validation Loss AVG: 9.3370, lr: 0.001
Epoch [140/150], Training Loss: 18.3707, Validation Loss Current: 6.1149, Validation Loss AVG: 10.5070, lr: 0.001
Epoch [141/150], Training Loss: 20.9707, Validation Loss Current: 5.9022, Validation Loss AVG: 10.6519, lr: 0.001
Epoch [142/150], Training Loss: 19.4896, Validation Loss Current: 5.9399, Validation Loss AVG: 11.6263, lr: 0.001
Epoch [143/150], Training Loss: 18.5093, Validation Loss Current: 6.1612, Validation Loss AVG: 11.4229, lr: 0.001
Epoch [144/150], Training Loss: 17.1072, Validation Loss Current: 5.7374, Validation Loss AVG: 10.5758, lr: 0.001
Epoch [145/150], Training Loss: 16.8385, Validation Loss Current: 6.0770, Validation Loss AVG: 11.4695, lr: 0.001
Epoch [146/150], Training Loss: 16.9368, Validation Loss Current: 5.9002, Validation Loss AVG: 10.4339, lr: 0.001
Epoch [147/150], Training Loss: 17.0768, Validation Loss Current: 7.8508, Validation Loss AVG: 10.0938, lr: 0.001
Epoch [148/150], Training Loss: 21.3222, Validation Loss Current: 5.9345, Validation Loss AVG: 8.4953, lr: 0.001
Epoch [149/150], Training Loss: 17.1394, Validation Loss Current: 5.8192, Validation Loss AVG: 9.4966, lr: 0.001
Epoch [150/150], Training Loss: 16.0123, Validation Loss Current: 5.7784, Validation Loss AVG: 10.4097, lr: 0.001
Patch distance: 1 finished training. Best epoch: 136 Best val accuracy: [0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2993421052631579, 0.3026315789473684, 0.31085526315789475, 0.3042763157894737, 0.3338815789473684, 0.3355263157894737, 0.34539473684210525, 0.3651315789473684, 0.3815789473684211, 0.35526315789473684, 0.3684210526315789, 0.37006578947368424, 0.3815789473684211, 0.3832236842105263, 0.3980263157894737, 0.3503289473684211, 0.4029605263157895, 0.38651315789473684, 0.38651315789473684, 0.42269736842105265, 0.40460526315789475, 0.42598684210526316, 0.4029605263157895, 0.43585526315789475, 0.3305921052631579, 0.3930921052631579, 0.3618421052631579, 0.4161184210526316, 0.36348684210526316, 0.3569078947368421, 0.4473684210526316, 0.4555921052631579, 0.4440789473684211, 0.4769736842105263, 0.4769736842105263, 0.4769736842105263, 0.48848684210526316, 0.45723684210526316, 0.4769736842105263, 0.4819078947368421, 0.5049342105263158, 0.4917763157894737, 0.4934210526315789, 0.4588815789473684, 0.4901315789473684, 0.47368421052631576, 0.45230263157894735, 0.4342105263157895, 0.4917763157894737, 0.5082236842105263, 0.5049342105263158, 0.5164473684210527, 0.4819078947368421, 0.5197368421052632, 0.4819078947368421, 0.49835526315789475, 0.4753289473684211, 0.4819078947368421, 0.5098684210526315, 0.5296052631578947, 0.53125, 0.5460526315789473, 0.4934210526315789, 0.524671052631579, 0.5427631578947368, 0.5493421052631579, 0.5411184210526315, 0.5411184210526315, 0.5460526315789473, 0.53125, 0.5263157894736842, 0.5476973684210527, 0.5592105263157895, 0.5509868421052632, 0.5411184210526315, 0.48519736842105265, 0.4621710526315789, 0.5509868421052632, 0.5493421052631579, 0.5394736842105263, 0.40625, 0.5148026315789473, 0.5180921052631579, 0.5427631578947368, 0.5592105263157895, 0.5575657894736842, 0.537828947368421, 0.5674342105263158, 0.5641447368421053, 0.5509868421052632, 0.5740131578947368, 0.4786184210526316, 0.5592105263157895, 0.555921052631579, 0.5542763157894737, 0.5789473684210527, 0.5608552631578947, 0.587171052631579, 0.5855263157894737, 0.5526315789473685, 0.5822368421052632, 0.5822368421052632, 0.5657894736842105, 0.5427631578947368, 0.587171052631579, 0.5707236842105263, 0.5970394736842105, 0.5855263157894737, 0.5625, 0.5888157894736842, 0.5838815789473685, 0.5707236842105263, 0.5904605263157895, 0.5674342105263158, 0.5657894736842105, 0.5789473684210527, 0.5888157894736842, 0.6085526315789473, 0.6101973684210527, 0.6151315789473685, 0.506578947368421, 0.6052631578947368, 0.6118421052631579, 0.618421052631579] Best val loss: 5.581539452075958


----- Training alexnet with sequence: [1, 0.8] -----
Sequence [1] already in state dictionary, jumped
Loaded best state dict for [1]
Current group: 0.8
Epoch [1/75], Training Loss: 20.4696, Validation Loss Current: 8.8251, Validation Loss AVG: 8.8251, lr: 0.001
Epoch [2/75], Training Loss: 19.2683, Validation Loss Current: 8.7085, Validation Loss AVG: 8.7085, lr: 0.001
Epoch [3/75], Training Loss: 18.5439, Validation Loss Current: 9.7721, Validation Loss AVG: 9.7721, lr: 0.001
Epoch [4/75], Training Loss: 23.5743, Validation Loss Current: 13.0495, Validation Loss AVG: 13.0495, lr: 0.001
Epoch [5/75], Training Loss: 33.0948, Validation Loss Current: 8.1899, Validation Loss AVG: 8.1899, lr: 0.001
Epoch [6/75], Training Loss: 24.7178, Validation Loss Current: 8.1424, Validation Loss AVG: 8.1424, lr: 0.001
Epoch [7/75], Training Loss: 21.5372, Validation Loss Current: 8.1185, Validation Loss AVG: 8.1185, lr: 0.001
Epoch [8/75], Training Loss: 19.0717, Validation Loss Current: 9.0108, Validation Loss AVG: 9.0108, lr: 0.001
Epoch [9/75], Training Loss: 19.1717, Validation Loss Current: 8.6443, Validation Loss AVG: 8.6443, lr: 0.001
Epoch [10/75], Training Loss: 19.5833, Validation Loss Current: 8.3283, Validation Loss AVG: 8.3283, lr: 0.001
Epoch [11/75], Training Loss: 20.2506, Validation Loss Current: 8.4735, Validation Loss AVG: 8.4735, lr: 0.001
Epoch [12/75], Training Loss: 23.4076, Validation Loss Current: 8.0373, Validation Loss AVG: 8.0373, lr: 0.001
Epoch [13/75], Training Loss: 18.5933, Validation Loss Current: 9.0868, Validation Loss AVG: 9.0868, lr: 0.001
Epoch [14/75], Training Loss: 19.2525, Validation Loss Current: 11.3273, Validation Loss AVG: 11.3273, lr: 0.001
Epoch [15/75], Training Loss: 21.8229, Validation Loss Current: 7.8635, Validation Loss AVG: 7.8635, lr: 0.001
Epoch [16/75], Training Loss: 17.1411, Validation Loss Current: 7.7865, Validation Loss AVG: 7.7865, lr: 0.001
Epoch [17/75], Training Loss: 16.5367, Validation Loss Current: 9.3825, Validation Loss AVG: 9.3825, lr: 0.001
Epoch [18/75], Training Loss: 18.3534, Validation Loss Current: 8.1687, Validation Loss AVG: 8.1687, lr: 0.001
Epoch [19/75], Training Loss: 15.6149, Validation Loss Current: 8.5986, Validation Loss AVG: 8.5986, lr: 0.001
Epoch [20/75], Training Loss: 14.5872, Validation Loss Current: 8.8514, Validation Loss AVG: 8.8514, lr: 0.001
Epoch [21/75], Training Loss: 14.6931, Validation Loss Current: 10.0224, Validation Loss AVG: 10.0224, lr: 0.001
Epoch [22/75], Training Loss: 17.6509, Validation Loss Current: 8.2476, Validation Loss AVG: 8.2476, lr: 0.001
Epoch [23/75], Training Loss: 14.8971, Validation Loss Current: 10.0749, Validation Loss AVG: 10.0749, lr: 0.001
Epoch [24/75], Training Loss: 18.1145, Validation Loss Current: 9.8714, Validation Loss AVG: 9.8714, lr: 0.001
Epoch [25/75], Training Loss: 14.4199, Validation Loss Current: 9.2173, Validation Loss AVG: 9.2173, lr: 0.001
Epoch [26/75], Training Loss: 14.0814, Validation Loss Current: 10.8088, Validation Loss AVG: 10.8088, lr: 0.001
Epoch [27/75], Training Loss: 15.5505, Validation Loss Current: 14.7349, Validation Loss AVG: 14.7349, lr: 0.001
Epoch [28/75], Training Loss: 25.2649, Validation Loss Current: 8.0338, Validation Loss AVG: 8.0338, lr: 0.001
Epoch [29/75], Training Loss: 17.3898, Validation Loss Current: 8.9783, Validation Loss AVG: 8.9783, lr: 0.001
Epoch [30/75], Training Loss: 13.9992, Validation Loss Current: 8.9598, Validation Loss AVG: 8.9598, lr: 0.001
Epoch [31/75], Training Loss: 13.0558, Validation Loss Current: 9.9102, Validation Loss AVG: 9.9102, lr: 0.001
Epoch [32/75], Training Loss: 12.7483, Validation Loss Current: 11.0750, Validation Loss AVG: 11.0750, lr: 0.001
Epoch [33/75], Training Loss: 13.9924, Validation Loss Current: 9.7189, Validation Loss AVG: 9.7189, lr: 0.001
Epoch [34/75], Training Loss: 12.3565, Validation Loss Current: 10.2512, Validation Loss AVG: 10.2512, lr: 0.001
Epoch [35/75], Training Loss: 12.3328, Validation Loss Current: 10.0540, Validation Loss AVG: 10.0540, lr: 0.001
Epoch [36/75], Training Loss: 14.1084, Validation Loss Current: 11.6174, Validation Loss AVG: 11.6174, lr: 0.001
Epoch [37/75], Training Loss: 16.0201, Validation Loss Current: 9.1516, Validation Loss AVG: 9.1516, lr: 0.001
Epoch [38/75], Training Loss: 11.8007, Validation Loss Current: 10.1874, Validation Loss AVG: 10.1874, lr: 0.001
Epoch [39/75], Training Loss: 10.5926, Validation Loss Current: 12.3819, Validation Loss AVG: 12.3819, lr: 0.001
Epoch [40/75], Training Loss: 14.6483, Validation Loss Current: 8.6924, Validation Loss AVG: 8.6924, lr: 0.001
Epoch [41/75], Training Loss: 11.4249, Validation Loss Current: 10.4107, Validation Loss AVG: 10.4107, lr: 0.001
Epoch [42/75], Training Loss: 9.9911, Validation Loss Current: 12.4227, Validation Loss AVG: 12.4227, lr: 0.001
Epoch [43/75], Training Loss: 11.8383, Validation Loss Current: 10.2117, Validation Loss AVG: 10.2117, lr: 0.001
Epoch [44/75], Training Loss: 9.9120, Validation Loss Current: 10.7836, Validation Loss AVG: 10.7836, lr: 0.001
Epoch [45/75], Training Loss: 8.4806, Validation Loss Current: 13.7536, Validation Loss AVG: 13.7536, lr: 0.001
Epoch [46/75], Training Loss: 7.3806, Validation Loss Current: 12.1413, Validation Loss AVG: 12.1413, lr: 0.001
 --- Early Stopped ---
Patch distance: 0.8 finished training. Best epoch: 16 Best val accuracy: [0.4276315789473684, 0.4365131578947369, 0.4052631578947368, 0.368421052631579, 0.4236842105263158, 0.43256578947368424, 0.4381578947368422, 0.41381578947368425, 0.42796052631578946, 0.46546052631578955, 0.4407894736842105, 0.4615131578947368, 0.43618421052631573, 0.41480263157894737, 0.47138157894736843, 0.48980263157894743, 0.4375000000000001, 0.46875, 0.4759868421052631, 0.48190789473684204, 0.44342105263157894, 0.4825657894736842, 0.43256578947368424, 0.42105263157894735, 0.4615131578947368, 0.43947368421052635, 0.3279605263157894, 0.44342105263157905, 0.4473684210526316, 0.475, 0.4598684210526316, 0.4305921052631579, 0.4667763157894737, 0.46085526315789477, 0.4634868421052632, 0.45625, 0.47072368421052635, 0.4621710526315789, 0.4440789473684211, 0.47796052631578945, 0.47171052631578947, 0.46052631578947373, 0.4546052631578947, 0.4733552631578948, 0.438157894736842, 0.4598684210526316] Best val loss: 7.786465609073639


----- Training alexnet with sequence: [1, 0.8, 0.6] -----
Sequence [1] already in state dictionary, jumped
Sequence [1, 0.8] already in state dictionary, jumped
Loaded best state dict for [1, 0.8]
Current group: 0.6
Epoch [1/50], Training Loss: 19.9312, Validation Loss Current: 7.8663, Validation Loss AVG: 7.8663, lr: 0.001
Epoch [2/50], Training Loss: 19.8510, Validation Loss Current: 8.8331, Validation Loss AVG: 8.8331, lr: 0.001
Epoch [3/50], Training Loss: 18.8267, Validation Loss Current: 8.1284, Validation Loss AVG: 8.1284, lr: 0.001
Epoch [4/50], Training Loss: 19.5224, Validation Loss Current: 10.4892, Validation Loss AVG: 10.4892, lr: 0.001
Epoch [5/50], Training Loss: 26.2140, Validation Loss Current: 8.0670, Validation Loss AVG: 8.0670, lr: 0.001
Epoch [6/50], Training Loss: 19.5885, Validation Loss Current: 7.3838, Validation Loss AVG: 7.3838, lr: 0.001
Epoch [7/50], Training Loss: 16.2175, Validation Loss Current: 7.9897, Validation Loss AVG: 7.9897, lr: 0.001
Epoch [8/50], Training Loss: 15.8648, Validation Loss Current: 11.8044, Validation Loss AVG: 11.8044, lr: 0.001
Epoch [9/50], Training Loss: 30.1223, Validation Loss Current: 8.1930, Validation Loss AVG: 8.1930, lr: 0.001
Epoch [10/50], Training Loss: 22.1506, Validation Loss Current: 7.8457, Validation Loss AVG: 7.8457, lr: 0.001
Epoch [11/50], Training Loss: 21.0954, Validation Loss Current: 7.4519, Validation Loss AVG: 7.4519, lr: 0.001
Epoch [12/50], Training Loss: 18.3581, Validation Loss Current: 8.4062, Validation Loss AVG: 8.4062, lr: 0.001
Epoch [13/50], Training Loss: 19.2369, Validation Loss Current: 7.9373, Validation Loss AVG: 7.9373, lr: 0.001
Epoch [14/50], Training Loss: 18.2440, Validation Loss Current: 13.1518, Validation Loss AVG: 13.1518, lr: 0.001
Epoch [15/50], Training Loss: 39.4156, Validation Loss Current: 8.7997, Validation Loss AVG: 8.7997, lr: 0.001
Epoch [16/50], Training Loss: 28.3909, Validation Loss Current: 12.1733, Validation Loss AVG: 12.1733, lr: 0.001
Epoch [17/50], Training Loss: 31.3483, Validation Loss Current: 8.1225, Validation Loss AVG: 8.1225, lr: 0.001
Epoch [18/50], Training Loss: 22.3080, Validation Loss Current: 7.3501, Validation Loss AVG: 7.3501, lr: 0.001
Epoch [19/50], Training Loss: 18.2817, Validation Loss Current: 8.1110, Validation Loss AVG: 8.1110, lr: 0.001
Epoch [20/50], Training Loss: 17.4244, Validation Loss Current: 7.5739, Validation Loss AVG: 7.5739, lr: 0.001
Epoch [21/50], Training Loss: 17.2995, Validation Loss Current: 7.7950, Validation Loss AVG: 7.7950, lr: 0.001
Epoch [22/50], Training Loss: 15.4407, Validation Loss Current: 8.3807, Validation Loss AVG: 8.3807, lr: 0.001
Epoch [23/50], Training Loss: 14.4878, Validation Loss Current: 8.1082, Validation Loss AVG: 8.1082, lr: 0.001
Epoch [24/50], Training Loss: 13.4739, Validation Loss Current: 9.6680, Validation Loss AVG: 9.6680, lr: 0.001
Epoch [25/50], Training Loss: 19.3796, Validation Loss Current: 7.5484, Validation Loss AVG: 7.5484, lr: 0.001
Epoch [26/50], Training Loss: 14.5119, Validation Loss Current: 8.5291, Validation Loss AVG: 8.5291, lr: 0.001
Epoch [27/50], Training Loss: 17.2426, Validation Loss Current: 8.0820, Validation Loss AVG: 8.0820, lr: 0.001
Epoch [28/50], Training Loss: 12.5479, Validation Loss Current: 9.2322, Validation Loss AVG: 9.2322, lr: 0.001
Epoch [29/50], Training Loss: 10.4128, Validation Loss Current: 9.6055, Validation Loss AVG: 9.6055, lr: 0.001
Epoch [30/50], Training Loss: 10.7188, Validation Loss Current: 11.2010, Validation Loss AVG: 11.2010, lr: 0.001
Epoch [31/50], Training Loss: 18.6466, Validation Loss Current: 8.8523, Validation Loss AVG: 8.8523, lr: 0.001
Epoch [32/50], Training Loss: 12.8885, Validation Loss Current: 8.4960, Validation Loss AVG: 8.4960, lr: 0.001
Epoch [33/50], Training Loss: 11.1572, Validation Loss Current: 11.2641, Validation Loss AVG: 11.2641, lr: 0.001
Epoch [34/50], Training Loss: 16.8620, Validation Loss Current: 7.6748, Validation Loss AVG: 7.6748, lr: 0.001
Epoch [35/50], Training Loss: 12.7283, Validation Loss Current: 8.9070, Validation Loss AVG: 8.9070, lr: 0.001
Epoch [36/50], Training Loss: 11.1752, Validation Loss Current: 9.8202, Validation Loss AVG: 9.8202, lr: 0.001
Epoch [37/50], Training Loss: 10.5003, Validation Loss Current: 9.1324, Validation Loss AVG: 9.1324, lr: 0.001
Epoch [38/50], Training Loss: 8.1650, Validation Loss Current: 9.9673, Validation Loss AVG: 9.9673, lr: 0.001
Epoch [39/50], Training Loss: 6.7391, Validation Loss Current: 11.3101, Validation Loss AVG: 11.3101, lr: 0.001
Epoch [40/50], Training Loss: 6.2922, Validation Loss Current: 10.5725, Validation Loss AVG: 10.5725, lr: 0.001
Epoch [41/50], Training Loss: 5.5393, Validation Loss Current: 11.3705, Validation Loss AVG: 11.3705, lr: 0.001
Epoch [42/50], Training Loss: 5.2061, Validation Loss Current: 12.6387, Validation Loss AVG: 12.6387, lr: 0.001
Epoch [43/50], Training Loss: 4.6759, Validation Loss Current: 11.6909, Validation Loss AVG: 11.6909, lr: 0.001
Epoch [44/50], Training Loss: 4.3126, Validation Loss Current: 13.0128, Validation Loss AVG: 13.0128, lr: 0.001
Epoch [45/50], Training Loss: 4.1978, Validation Loss Current: 13.3138, Validation Loss AVG: 13.3138, lr: 0.001
Epoch [46/50], Training Loss: 4.8158, Validation Loss Current: 24.2815, Validation Loss AVG: 24.2815, lr: 0.001
Epoch [47/50], Training Loss: 34.5876, Validation Loss Current: 8.1248, Validation Loss AVG: 8.1248, lr: 0.001
Epoch [48/50], Training Loss: 22.3891, Validation Loss Current: 9.9386, Validation Loss AVG: 9.9386, lr: 0.001
 --- Early Stopped ---
Patch distance: 0.6 finished training. Best epoch: 18 Best val accuracy: [0.5016447368421053, 0.46940789473684214, 0.5052631578947369, 0.4148026315789474, 0.44572368421052627, 0.4930921052631579, 0.5082236842105263, 0.37467105263157896, 0.4167763157894736, 0.4391447368421053, 0.46809210526315786, 0.4713815789473683, 0.4769736842105264, 0.37730263157894733, 0.36019736842105265, 0.3203947368421053, 0.44243421052631576, 0.5092105263157896, 0.4911184210526316, 0.5046052631578948, 0.5016447368421053, 0.49046052631578946, 0.5095394736842105, 0.49046052631578946, 0.4858552631578948, 0.4740131578947368, 0.47631578947368425, 0.4753289473684211, 0.5088815789473685, 0.4513157894736842, 0.4671052631578948, 0.5026315789473684, 0.48421052631578954, 0.5151315789473685, 0.4858552631578948, 0.48157894736842105, 0.49144736842105274, 0.5023026315789474, 0.5055921052631579, 0.4963815789473684, 0.4944078947368421, 0.4881578947368421, 0.5006578947368421, 0.49243421052631586, 0.49736842105263157, 0.3388157894736842, 0.4411184210526316, 0.4042763157894737] Best val loss: 7.350078582763672


----- Training alexnet with sequence: [1, 0.8, 0.6, 0.4] -----
Sequence [1] already in state dictionary, jumped
Sequence [1, 0.8] already in state dictionary, jumped
Sequence [1, 0.8, 0.6] already in state dictionary, jumped
Loaded best state dict for [1, 0.8, 0.6]
Current group: 0.4
Epoch [1/38], Training Loss: 28.0141, Validation Loss Current: 7.6704, Validation Loss AVG: 7.6704, lr: 0.001
Epoch [2/38], Training Loss: 25.3924, Validation Loss Current: 7.7359, Validation Loss AVG: 7.7359, lr: 0.001
Epoch [3/38], Training Loss: 23.3690, Validation Loss Current: 7.5378, Validation Loss AVG: 7.5378, lr: 0.001
Epoch [4/38], Training Loss: 22.9931, Validation Loss Current: 7.2459, Validation Loss AVG: 7.2459, lr: 0.001
Epoch [5/38], Training Loss: 23.9859, Validation Loss Current: 7.0364, Validation Loss AVG: 7.0364, lr: 0.001
Epoch [6/38], Training Loss: 21.1710, Validation Loss Current: 7.5373, Validation Loss AVG: 7.5373, lr: 0.001
Epoch [7/38], Training Loss: 19.1369, Validation Loss Current: 6.9924, Validation Loss AVG: 6.9924, lr: 0.001
Epoch [8/38], Training Loss: 17.7756, Validation Loss Current: 7.4720, Validation Loss AVG: 7.4720, lr: 0.001
Epoch [9/38], Training Loss: 17.0910, Validation Loss Current: 7.4535, Validation Loss AVG: 7.4535, lr: 0.001
Epoch [10/38], Training Loss: 18.3070, Validation Loss Current: 7.8864, Validation Loss AVG: 7.8864, lr: 0.001
Epoch [11/38], Training Loss: 22.4332, Validation Loss Current: 8.3012, Validation Loss AVG: 8.3012, lr: 0.001
Epoch [12/38], Training Loss: 21.7699, Validation Loss Current: 7.5365, Validation Loss AVG: 7.5365, lr: 0.001
Epoch [13/38], Training Loss: 18.3772, Validation Loss Current: 7.7260, Validation Loss AVG: 7.7260, lr: 0.001
Epoch [14/38], Training Loss: 15.3327, Validation Loss Current: 7.6681, Validation Loss AVG: 7.6681, lr: 0.001
Epoch [15/38], Training Loss: 14.2842, Validation Loss Current: 8.4533, Validation Loss AVG: 8.4533, lr: 0.001
Epoch [16/38], Training Loss: 15.8304, Validation Loss Current: 7.8885, Validation Loss AVG: 7.8885, lr: 0.001
Epoch [17/38], Training Loss: 13.7372, Validation Loss Current: 7.8390, Validation Loss AVG: 7.8390, lr: 0.001
Epoch [18/38], Training Loss: 15.0245, Validation Loss Current: 8.9449, Validation Loss AVG: 8.9449, lr: 0.001
Epoch [19/38], Training Loss: 12.4714, Validation Loss Current: 8.4015, Validation Loss AVG: 8.4015, lr: 0.001
Epoch [20/38], Training Loss: 11.1092, Validation Loss Current: 8.3562, Validation Loss AVG: 8.3562, lr: 0.001
Epoch [21/38], Training Loss: 10.3301, Validation Loss Current: 9.0316, Validation Loss AVG: 9.0316, lr: 0.001
Epoch [22/38], Training Loss: 9.4871, Validation Loss Current: 9.2307, Validation Loss AVG: 9.2307, lr: 0.001
Epoch [23/38], Training Loss: 9.5423, Validation Loss Current: 9.3246, Validation Loss AVG: 9.3246, lr: 0.001
Epoch [24/38], Training Loss: 8.4077, Validation Loss Current: 9.9657, Validation Loss AVG: 9.9657, lr: 0.001
Epoch [25/38], Training Loss: 9.0683, Validation Loss Current: 15.7436, Validation Loss AVG: 15.7436, lr: 0.001
Epoch [26/38], Training Loss: 22.7889, Validation Loss Current: 8.9000, Validation Loss AVG: 8.9000, lr: 0.001
Epoch [27/38], Training Loss: 16.8455, Validation Loss Current: 8.3040, Validation Loss AVG: 8.3040, lr: 0.001
Epoch [28/38], Training Loss: 11.6698, Validation Loss Current: 9.2992, Validation Loss AVG: 9.2992, lr: 0.001
Epoch [29/38], Training Loss: 10.9565, Validation Loss Current: 12.8082, Validation Loss AVG: 12.8082, lr: 0.001
Epoch [30/38], Training Loss: 25.6142, Validation Loss Current: 7.7778, Validation Loss AVG: 7.7778, lr: 0.001
Epoch [31/38], Training Loss: 16.0167, Validation Loss Current: 8.7529, Validation Loss AVG: 8.7529, lr: 0.001
Epoch [32/38], Training Loss: 11.9673, Validation Loss Current: 9.4799, Validation Loss AVG: 9.4799, lr: 0.001
Epoch [33/38], Training Loss: 14.6395, Validation Loss Current: 9.0257, Validation Loss AVG: 9.0257, lr: 0.001
Epoch [34/38], Training Loss: 10.4847, Validation Loss Current: 9.5559, Validation Loss AVG: 9.5559, lr: 0.001
Epoch [35/38], Training Loss: 9.4170, Validation Loss Current: 10.4981, Validation Loss AVG: 10.4981, lr: 0.001
Epoch [36/38], Training Loss: 16.0109, Validation Loss Current: 9.6617, Validation Loss AVG: 9.6617, lr: 0.001
Epoch [37/38], Training Loss: 12.8017, Validation Loss Current: 9.5302, Validation Loss AVG: 9.5302, lr: 0.001
 --- Early Stopped ---
Patch distance: 0.4 finished training. Best epoch: 7 Best val accuracy: [0.47072368421052635, 0.46842105263157896, 0.49375, 0.47960526315789476, 0.5082236842105263, 0.49802631578947365, 0.5282894736842104, 0.5092105263157896, 0.5095394736842105, 0.4858552631578947, 0.4733552631578948, 0.4855263157894737, 0.5036184210526315, 0.5236842105263158, 0.5042763157894737, 0.5138157894736842, 0.5305921052631579, 0.4947368421052631, 0.5013157894736842, 0.5243421052631578, 0.5, 0.512828947368421, 0.5125, 0.5006578947368421, 0.4180921052631579, 0.4552631578947368, 0.48519736842105265, 0.4845394736842105, 0.35690789473684215, 0.5052631578947369, 0.4756578947368421, 0.49243421052631575, 0.48256578947368417, 0.4769736842105264, 0.47105263157894733, 0.4447368421052632, 0.4822368421052632] Best val loss: 6.992401897907257


----- Training alexnet with sequence: [1, 0.8, 0.6, 0.4, 0.2] -----
Sequence [1] already in state dictionary, jumped
Sequence [1, 0.8] already in state dictionary, jumped
Sequence [1, 0.8, 0.6] already in state dictionary, jumped
Sequence [1, 0.8, 0.6, 0.4] already in state dictionary, jumped
Loaded best state dict for [1, 0.8, 0.6, 0.4]
Current group: 0.2
Epoch [1/30], Training Loss: 34.7180, Validation Loss Current: 8.4586, Validation Loss AVG: 8.4586, lr: 0.001
Epoch [2/30], Training Loss: 31.1465, Validation Loss Current: 8.8648, Validation Loss AVG: 8.8648, lr: 0.001
Epoch [3/30], Training Loss: 29.5500, Validation Loss Current: 8.9490, Validation Loss AVG: 8.9490, lr: 0.001
Epoch [4/30], Training Loss: 32.7180, Validation Loss Current: 8.6752, Validation Loss AVG: 8.6752, lr: 0.001
Epoch [5/30], Training Loss: 27.8639, Validation Loss Current: 9.0836, Validation Loss AVG: 9.0836, lr: 0.001
Epoch [6/30], Training Loss: 28.9151, Validation Loss Current: 8.8271, Validation Loss AVG: 8.8271, lr: 0.001
Epoch [7/30], Training Loss: 28.9641, Validation Loss Current: 8.6212, Validation Loss AVG: 8.6212, lr: 0.001
Epoch [8/30], Training Loss: 25.4821, Validation Loss Current: 9.3924, Validation Loss AVG: 9.3924, lr: 0.001
Epoch [9/30], Training Loss: 25.1980, Validation Loss Current: 9.5140, Validation Loss AVG: 9.5140, lr: 0.001
Epoch [10/30], Training Loss: 24.2481, Validation Loss Current: 12.0549, Validation Loss AVG: 12.0549, lr: 0.001
Epoch [11/30], Training Loss: 25.1622, Validation Loss Current: 10.0223, Validation Loss AVG: 10.0223, lr: 0.001
Epoch [12/30], Training Loss: 26.3749, Validation Loss Current: 9.2658, Validation Loss AVG: 9.2658, lr: 0.001
Epoch [13/30], Training Loss: 24.4489, Validation Loss Current: 9.4996, Validation Loss AVG: 9.4996, lr: 0.001
Epoch [14/30], Training Loss: 23.7074, Validation Loss Current: 10.2955, Validation Loss AVG: 10.2955, lr: 0.001
Epoch [15/30], Training Loss: 23.0398, Validation Loss Current: 10.7164, Validation Loss AVG: 10.7164, lr: 0.001
Epoch [16/30], Training Loss: 21.1321, Validation Loss Current: 11.1213, Validation Loss AVG: 11.1213, lr: 0.001
Epoch [17/30], Training Loss: 20.3504, Validation Loss Current: 9.9874, Validation Loss AVG: 9.9874, lr: 0.001
Epoch [18/30], Training Loss: 23.0542, Validation Loss Current: 10.2502, Validation Loss AVG: 10.2502, lr: 0.001
Epoch [19/30], Training Loss: 21.0664, Validation Loss Current: 11.1458, Validation Loss AVG: 11.1458, lr: 0.001
Epoch [20/30], Training Loss: 24.7277, Validation Loss Current: 9.9457, Validation Loss AVG: 9.9457, lr: 0.001
Epoch [21/30], Training Loss: 20.8345, Validation Loss Current: 10.9735, Validation Loss AVG: 10.9735, lr: 0.001
Epoch [22/30], Training Loss: 20.4490, Validation Loss Current: 10.2544, Validation Loss AVG: 10.2544, lr: 0.001
Epoch [23/30], Training Loss: 19.3896, Validation Loss Current: 10.8089, Validation Loss AVG: 10.8089, lr: 0.001
Epoch [24/30], Training Loss: 17.2551, Validation Loss Current: 10.8189, Validation Loss AVG: 10.8189, lr: 0.001
Epoch [25/30], Training Loss: 18.1533, Validation Loss Current: 11.1591, Validation Loss AVG: 11.1591, lr: 0.001
Epoch [26/30], Training Loss: 18.2278, Validation Loss Current: 13.6091, Validation Loss AVG: 13.6091, lr: 0.001
Epoch [27/30], Training Loss: 29.6378, Validation Loss Current: 9.4477, Validation Loss AVG: 9.4477, lr: 0.001
Epoch [28/30], Training Loss: 23.6226, Validation Loss Current: 11.6371, Validation Loss AVG: 11.6371, lr: 0.001
Epoch [29/30], Training Loss: 19.8075, Validation Loss Current: 11.7429, Validation Loss AVG: 11.7429, lr: 0.001
Epoch [30/30], Training Loss: 18.4242, Validation Loss Current: 11.9224, Validation Loss AVG: 11.9224, lr: 0.001
Epoch [31/30], Training Loss: 17.1408, Validation Loss Current: 11.8116, Validation Loss AVG: 11.8116, lr: 0.001
 --- Early Stopped ---
Patch distance: 0.2 finished training. Best epoch: 1 Best val accuracy: [0.46118421052631586, 0.42434210526315785, 0.42828947368421055, 0.42796052631578946, 0.41447368421052627, 0.42302631578947364, 0.43980263157894733, 0.41381578947368425, 0.41217105263157894, 0.35888157894736844, 0.3924342105263158, 0.41217105263157894, 0.41414473684210523, 0.3736842105263158, 0.3914473684210526, 0.3944078947368421, 0.4213815789473684, 0.4009868421052632, 0.3582236842105263, 0.3779605263157895, 0.36546052631578946, 0.3875, 0.3848684210526316, 0.3996710526315789, 0.3805921052631579, 0.34177631578947365, 0.4128289473684211, 0.3292763157894737, 0.3707236842105263, 0.3921052631578947, 0.37730263157894733] Best val loss: 8.458643579483033


-------------------- All training done --------------------


 --- Evaluating ---
Fold: 0
---- Testing model trained on sequence: [1] ----
Test set distance: 0.2 Top 1 Accuracy: 0.18487394957983194
Test set distance: 0.4 Top 1 Accuracy: 0.25439266615737205
Test set distance: 0.6 Top 1 Accuracy: 0.40106951871657753
Test set distance: 0.8 Top 1 Accuracy: 0.4957983193277311
Test set distance: 1 Top 1 Accuracy: 0.5309396485867074
---- Testing model trained on sequence: [1, 0.8] ----
Test set distance: 0.2 Top 1 Accuracy: 0.20702826585179526
Test set distance: 0.4 Top 1 Accuracy: 0.3048128342245989
Test set distance: 0.6 Top 1 Accuracy: 0.453781512605042
Test set distance: 0.8 Top 1 Accuracy: 0.5271199388846448
Test set distance: 1 Top 1 Accuracy: 0.494270435446906
---- Testing model trained on sequence: [1, 0.8, 0.6] ----
Test set distance: 0.2 Top 1 Accuracy: 0.17647058823529413
Test set distance: 0.4 Top 1 Accuracy: 0.440794499618029
Test set distance: 0.6 Top 1 Accuracy: 0.6165011459129106
Test set distance: 0.8 Top 1 Accuracy: 0.5981665393430099
Test set distance: 1 Top 1 Accuracy: 0.5439266615737204
---- Testing model trained on sequence: [1, 0.8, 0.6, 0.4] ----
Test set distance: 0.2 Top 1 Accuracy: 0.3414820473644003
Test set distance: 0.4 Top 1 Accuracy: 0.5905271199388846
Test set distance: 0.6 Top 1 Accuracy: 0.5851795263559969
Test set distance: 0.8 Top 1 Accuracy: 0.5210084033613446
Test set distance: 1 Top 1 Accuracy: 0.4430863254392666
---- Testing model trained on sequence: [1, 0.8, 0.6, 0.4, 0.2] ----
Test set distance: 0.2 Top 1 Accuracy: 0.47288006111535524
Test set distance: 0.4 Top 1 Accuracy: 0.440794499618029
Test set distance: 0.6 Top 1 Accuracy: 0.34835752482811305
Test set distance: 0.8 Top 1 Accuracy: 0.24904507257448433
Test set distance: 1 Top 1 Accuracy: 0.2001527883880825
Fold: 1
---- Testing model trained on sequence: [1] ----
Test set distance: 0.2 Top 1 Accuracy: 0.19938884644766997
Test set distance: 0.4 Top 1 Accuracy: 0.24140565317035906
Test set distance: 0.6 Top 1 Accuracy: 0.4239877769289534
Test set distance: 0.8 Top 1 Accuracy: 0.5622612681436211
Test set distance: 1 Top 1 Accuracy: 0.5951107715813598
---- Testing model trained on sequence: [1, 0.8] ----
Test set distance: 0.2 Top 1 Accuracy: 0.1573720397249809
Test set distance: 0.4 Top 1 Accuracy: 0.2880061115355233
Test set distance: 0.6 Top 1 Accuracy: 0.5217723453017571
Test set distance: 0.8 Top 1 Accuracy: 0.5828877005347594
Test set distance: 1 Top 1 Accuracy: 0.573720397249809
---- Testing model trained on sequence: [1, 0.8, 0.6] ----
Test set distance: 0.2 Top 1 Accuracy: 0.19480519480519481
Test set distance: 0.4 Top 1 Accuracy: 0.4423223834988541
Test set distance: 0.6 Top 1 Accuracy: 0.6279602750190986
Test set distance: 0.8 Top 1 Accuracy: 0.6042780748663101
Test set distance: 1 Top 1 Accuracy: 0.5485103132161956
---- Testing model trained on sequence: [1, 0.8, 0.6, 0.4] ----
Test set distance: 0.2 Top 1 Accuracy: 0.3277310924369748
Test set distance: 0.4 Top 1 Accuracy: 0.6317799847211611
Test set distance: 0.6 Top 1 Accuracy: 0.5920550038197097
Test set distance: 0.8 Top 1 Accuracy: 0.4988540870893812
Test set distance: 1 Top 1 Accuracy: 0.400305576776165
---- Testing model trained on sequence: [1, 0.8, 0.6, 0.4, 0.2] ----
Test set distance: 0.2 Top 1 Accuracy: 0.5210084033613446
Test set distance: 0.4 Top 1 Accuracy: 0.439266615737204
Test set distance: 0.6 Top 1 Accuracy: 0.3460656990068755
Test set distance: 0.8 Top 1 Accuracy: 0.25744843391902217
Test set distance: 1 Top 1 Accuracy: 0.20626432391138275
Fold: 2
---- Testing model trained on sequence: [1] ----
Test set distance: 0.2 Top 1 Accuracy: 0.22001527883880825
Test set distance: 0.4 Top 1 Accuracy: 0.2857142857142857
Test set distance: 0.6 Top 1 Accuracy: 0.4530175706646295
Test set distance: 0.8 Top 1 Accuracy: 0.5546218487394958
Test set distance: 1 Top 1 Accuracy: 0.5989304812834224
---- Testing model trained on sequence: [1, 0.8] ----
Test set distance: 0.2 Top 1 Accuracy: 0.2055003819709702
Test set distance: 0.4 Top 1 Accuracy: 0.40106951871657753
Test set distance: 0.6 Top 1 Accuracy: 0.5668449197860963
Test set distance: 0.8 Top 1 Accuracy: 0.6333078686019863
Test set distance: 1 Top 1 Accuracy: 0.6310160427807486
---- Testing model trained on sequence: [1, 0.8, 0.6] ----
Test set distance: 0.2 Top 1 Accuracy: 0.2765469824293354
Test set distance: 0.4 Top 1 Accuracy: 0.5210084033613446
Test set distance: 0.6 Top 1 Accuracy: 0.6088617265087853
Test set distance: 0.8 Top 1 Accuracy: 0.5889992360580596
Test set distance: 1 Top 1 Accuracy: 0.5202444614209321
---- Testing model trained on sequence: [1, 0.8, 0.6, 0.4] ----
Test set distance: 0.2 Top 1 Accuracy: 0.37662337662337664
Test set distance: 0.4 Top 1 Accuracy: 0.6065699006875478
Test set distance: 0.6 Top 1 Accuracy: 0.5874713521772346
Test set distance: 0.8 Top 1 Accuracy: 0.5240641711229946
Test set distance: 1 Top 1 Accuracy: 0.45454545454545453
---- Testing model trained on sequence: [1, 0.8, 0.6, 0.4, 0.2] ----
Test set distance: 0.2 Top 1 Accuracy: 0.47288006111535524
Test set distance: 0.4 Top 1 Accuracy: 0.4766997708174179
Test set distance: 0.6 Top 1 Accuracy: 0.40183346065699005
Test set distance: 0.8 Top 1 Accuracy: 0.3155080213903743
Test set distance: 1 Top 1 Accuracy: 0.24293353705118412
Fold: 3
---- Testing model trained on sequence: [1] ----
Test set distance: 0.2 Top 1 Accuracy: 0.22307104660045837
Test set distance: 0.4 Top 1 Accuracy: 0.2849503437738732
Test set distance: 0.6 Top 1 Accuracy: 0.4140565317035905
Test set distance: 0.8 Top 1 Accuracy: 0.5210084033613446
Test set distance: 1 Top 1 Accuracy: 0.5584415584415584
---- Testing model trained on sequence: [1, 0.8] ----
Test set distance: 0.2 Top 1 Accuracy: 0.18181818181818182
Test set distance: 0.4 Top 1 Accuracy: 0.3437738731856379
Test set distance: 0.6 Top 1 Accuracy: 0.5485103132161956
Test set distance: 0.8 Top 1 Accuracy: 0.6340718105423988
Test set distance: 1 Top 1 Accuracy: 0.6149732620320856
---- Testing model trained on sequence: [1, 0.8, 0.6] ----
Test set distance: 0.2 Top 1 Accuracy: 0.20168067226890757
Test set distance: 0.4 Top 1 Accuracy: 0.3804430863254393
Test set distance: 0.6 Top 1 Accuracy: 0.5408708938120703
Test set distance: 0.8 Top 1 Accuracy: 0.5706646294881589
Test set distance: 1 Top 1 Accuracy: 0.5179526355996944
---- Testing model trained on sequence: [1, 0.8, 0.6, 0.4] ----
Test set distance: 0.2 Top 1 Accuracy: 0.29411764705882354
Test set distance: 0.4 Top 1 Accuracy: 0.5889992360580596
Test set distance: 0.6 Top 1 Accuracy: 0.5767761650114591
Test set distance: 0.8 Top 1 Accuracy: 0.5217723453017571
Test set distance: 1 Top 1 Accuracy: 0.440794499618029
---- Testing model trained on sequence: [1, 0.8, 0.6, 0.4, 0.2] ----
Test set distance: 0.2 Top 1 Accuracy: 0.5133689839572193
Test set distance: 0.4 Top 1 Accuracy: 0.49961802902979374
Test set distance: 0.6 Top 1 Accuracy: 0.39343009931245226
Test set distance: 0.8 Top 1 Accuracy: 0.3009931245225363
Test set distance: 1 Top 1 Accuracy: 0.23071046600458364
Fold: 4
---- Testing model trained on sequence: [1] ----
Test set distance: 0.2 Top 1 Accuracy: 0.1711229946524064
Test set distance: 0.4 Top 1 Accuracy: 0.2620320855614973
Test set distance: 0.6 Top 1 Accuracy: 0.43086325439266615
Test set distance: 0.8 Top 1 Accuracy: 0.5614973262032086
Test set distance: 1 Top 1 Accuracy: 0.5874713521772346
---- Testing model trained on sequence: [1, 0.8] ----
Test set distance: 0.2 Top 1 Accuracy: 0.18181818181818182
Test set distance: 0.4 Top 1 Accuracy: 0.3575248281130634
Test set distance: 0.6 Top 1 Accuracy: 0.5401069518716578
Test set distance: 0.8 Top 1 Accuracy: 0.6203208556149733
Test set distance: 1 Top 1 Accuracy: 0.5966386554621849
---- Testing model trained on sequence: [1, 0.8, 0.6] ----
Test set distance: 0.2 Top 1 Accuracy: 0.16806722689075632
Test set distance: 0.4 Top 1 Accuracy: 0.32620320855614976
Test set distance: 0.6 Top 1 Accuracy: 0.5118411000763942
Test set distance: 0.8 Top 1 Accuracy: 0.5301757066462949
Test set distance: 1 Top 1 Accuracy: 0.5110771581359816
---- Testing model trained on sequence: [1, 0.8, 0.6, 0.4] ----
Test set distance: 0.2 Top 1 Accuracy: 0.3330786860198625
Test set distance: 0.4 Top 1 Accuracy: 0.6019862490450726
Test set distance: 0.6 Top 1 Accuracy: 0.5851795263559969
Test set distance: 0.8 Top 1 Accuracy: 0.49732620320855614
Test set distance: 1 Top 1 Accuracy: 0.38961038961038963
---- Testing model trained on sequence: [1, 0.8, 0.6, 0.4, 0.2] ----
Test set distance: 0.2 Top 1 Accuracy: 0.5301757066462949
Test set distance: 0.4 Top 1 Accuracy: 0.48892284186401835
Test set distance: 0.6 Top 1 Accuracy: 0.36592818945760125
Test set distance: 0.8 Top 1 Accuracy: 0.2612681436210848
Test set distance: 1 Top 1 Accuracy: 0.19327731092436976
------------------------------ End ------------------------------


 # ------------------ Running pipeline on stb_endsame color run_0 -------------------- #
cuda:0
 ------ Pipeline with following parameters ------
training_root_path :  /u/erdos/students/xcui32/cnslab/datasets/VOC2012/VOC2012_filtered/train
val_root_path :  /u/erdos/students/xcui32/cnslab/datasets/VOC2012/VOC2012_filtered/val
test_root_path :  /u/erdos/students/xcui32/cnslab/datasets/VOC2012/VOC2012_filtered/test
dataset_name :  VOC
target_distances :  [0.2, 0.4, 0.6, 0.8, 1]
training_mode :  stb_endsame
training_size :  None
background :  color
size :  (150, 150)
cls_to_use :  ['aeroplane', 'bicycle', 'bird', 'boat', 'car', 'cat', 'train', 'tvmonitor']
batch_size :  128
epochs :  150
resize_method :  long
n_folds :  5
num_workers :  16
model_name :  alexnet
device :  cuda:0
random_seed :  40
result_dirpath :  /u/erdos/students/xcui32/cnslab/results/VOC8AlexnetBlackCUR
save_checkpoints :  False
save_progress_checkpoints :  False
verbose :  0
 ---  Loading datasets ---
 ---  Running  ---
Parameters: --------------------
{'scheduler_kwargs': {'mode': 'min', 'factor': 0.1, 'patience': 5}, 'optim_kwargs': {'lr': 0.001, 'momentum': 0.9}, 'max_norm': None, 'val_target': 'current', 'patience': 30, 'early_stopping': True, 'scheduler_object': None, 'optimizer_object': <class 'torch.optim.sgd.SGD'>, 'criterion_object': <class 'torch.nn.modules.loss.CrossEntropyLoss'>, 'self': <pipelineCV2.RunModel object at 0x2aac3465ce10>}
--------------------
Fold: 0
----- Training alexnet with sequence: [0.2, 0.4, 0.6, 0.8, 1] -----
Current group: 0.2
Epoch [1/30], Training Loss: 41.5295, Validation Loss Current: 10.3581, Validation Loss AVG: 10.3581, lr: 0.001
Epoch [2/30], Training Loss: 41.3428, Validation Loss Current: 10.3212, Validation Loss AVG: 10.3212, lr: 0.001
Epoch [3/30], Training Loss: 41.1464, Validation Loss Current: 10.2626, Validation Loss AVG: 10.2626, lr: 0.001
Epoch [4/30], Training Loss: 40.8171, Validation Loss Current: 10.1708, Validation Loss AVG: 10.1708, lr: 0.001
Epoch [5/30], Training Loss: 40.3025, Validation Loss Current: 10.0558, Validation Loss AVG: 10.0558, lr: 0.001
Epoch [6/30], Training Loss: 40.2830, Validation Loss Current: 10.0449, Validation Loss AVG: 10.0449, lr: 0.001
Epoch [7/30], Training Loss: 39.8756, Validation Loss Current: 10.0479, Validation Loss AVG: 10.0479, lr: 0.001
Epoch [8/30], Training Loss: 39.7244, Validation Loss Current: 10.0342, Validation Loss AVG: 10.0342, lr: 0.001
Epoch [9/30], Training Loss: 39.7631, Validation Loss Current: 10.0382, Validation Loss AVG: 10.0382, lr: 0.001
Epoch [10/30], Training Loss: 39.8212, Validation Loss Current: 10.0445, Validation Loss AVG: 10.0445, lr: 0.001
Epoch [11/30], Training Loss: 39.9065, Validation Loss Current: 10.0348, Validation Loss AVG: 10.0348, lr: 0.001
Epoch [12/30], Training Loss: 40.0462, Validation Loss Current: 10.0434, Validation Loss AVG: 10.0434, lr: 0.001
Epoch [13/30], Training Loss: 39.4066, Validation Loss Current: 10.0352, Validation Loss AVG: 10.0352, lr: 0.001
Epoch [14/30], Training Loss: 40.1699, Validation Loss Current: 10.0654, Validation Loss AVG: 10.0654, lr: 0.001
Epoch [15/30], Training Loss: 40.1073, Validation Loss Current: 10.0706, Validation Loss AVG: 10.0706, lr: 0.001
Epoch [16/30], Training Loss: 40.0228, Validation Loss Current: 10.0472, Validation Loss AVG: 10.0472, lr: 0.001
Epoch [17/30], Training Loss: 39.4447, Validation Loss Current: 9.9970, Validation Loss AVG: 9.9970, lr: 0.001
Epoch [18/30], Training Loss: 40.1465, Validation Loss Current: 10.0223, Validation Loss AVG: 10.0223, lr: 0.001
Epoch [19/30], Training Loss: 40.2262, Validation Loss Current: 10.0527, Validation Loss AVG: 10.0527, lr: 0.001
Epoch [20/30], Training Loss: 40.0044, Validation Loss Current: 10.0413, Validation Loss AVG: 10.0413, lr: 0.001
Epoch [21/30], Training Loss: 40.2435, Validation Loss Current: 10.0439, Validation Loss AVG: 10.0439, lr: 0.001
Epoch [22/30], Training Loss: 40.1074, Validation Loss Current: 10.0533, Validation Loss AVG: 10.0533, lr: 0.001
Epoch [23/30], Training Loss: 40.1867, Validation Loss Current: 10.0390, Validation Loss AVG: 10.0390, lr: 0.001
Epoch [24/30], Training Loss: 40.6051, Validation Loss Current: 10.0670, Validation Loss AVG: 10.0670, lr: 0.001
Epoch [25/30], Training Loss: 39.5813, Validation Loss Current: 10.0543, Validation Loss AVG: 10.0543, lr: 0.001
Epoch [26/30], Training Loss: 40.3562, Validation Loss Current: 10.0534, Validation Loss AVG: 10.0534, lr: 0.001
Epoch [27/30], Training Loss: 40.2476, Validation Loss Current: 10.0768, Validation Loss AVG: 10.0768, lr: 0.001
Epoch [28/30], Training Loss: 39.7496, Validation Loss Current: 10.0370, Validation Loss AVG: 10.0370, lr: 0.001
Epoch [29/30], Training Loss: 39.8314, Validation Loss Current: 10.0457, Validation Loss AVG: 10.0457, lr: 0.001
Epoch [30/30], Training Loss: 40.1487, Validation Loss Current: 10.0457, Validation Loss AVG: 10.0457, lr: 0.001
Epoch [31/30], Training Loss: 40.3526, Validation Loss Current: 10.0458, Validation Loss AVG: 10.0458, lr: 0.001
Epoch [32/30], Training Loss: 40.3120, Validation Loss Current: 10.0534, Validation Loss AVG: 10.0534, lr: 0.001
Epoch [33/30], Training Loss: 39.6806, Validation Loss Current: 10.0291, Validation Loss AVG: 10.0291, lr: 0.001
Epoch [34/30], Training Loss: 40.0554, Validation Loss Current: 10.0446, Validation Loss AVG: 10.0446, lr: 0.001
Epoch [35/30], Training Loss: 40.0347, Validation Loss Current: 10.0388, Validation Loss AVG: 10.0388, lr: 0.001
Epoch [36/30], Training Loss: 40.3470, Validation Loss Current: 10.0321, Validation Loss AVG: 10.0321, lr: 0.001
Epoch [37/30], Training Loss: 39.3450, Validation Loss Current: 10.0263, Validation Loss AVG: 10.0263, lr: 0.001
Epoch [38/30], Training Loss: 40.6419, Validation Loss Current: 10.0507, Validation Loss AVG: 10.0507, lr: 0.001
Epoch [39/30], Training Loss: 40.1126, Validation Loss Current: 10.0923, Validation Loss AVG: 10.0923, lr: 0.001
Epoch [40/30], Training Loss: 40.0661, Validation Loss Current: 10.0125, Validation Loss AVG: 10.0125, lr: 0.001
Epoch [41/30], Training Loss: 39.8464, Validation Loss Current: 10.0459, Validation Loss AVG: 10.0459, lr: 0.001
Epoch [42/30], Training Loss: 39.8644, Validation Loss Current: 10.0314, Validation Loss AVG: 10.0314, lr: 0.001
Epoch [43/30], Training Loss: 39.5860, Validation Loss Current: 10.0270, Validation Loss AVG: 10.0270, lr: 0.001
Epoch [44/30], Training Loss: 40.3546, Validation Loss Current: 10.0426, Validation Loss AVG: 10.0426, lr: 0.001
Epoch [45/30], Training Loss: 40.3130, Validation Loss Current: 10.0557, Validation Loss AVG: 10.0557, lr: 0.001
Epoch [46/30], Training Loss: 40.3932, Validation Loss Current: 10.0321, Validation Loss AVG: 10.0321, lr: 0.001
Epoch [47/30], Training Loss: 39.8729, Validation Loss Current: 10.0444, Validation Loss AVG: 10.0444, lr: 0.001
 --- Early Stopped ---
Patch distance: 0.2 finished training. Best epoch: 17 Best val accuracy: [0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107] Best val loss: 9.99703369140625


Loaded best state dict for [0.2]
Current group: 0.4
Epoch [1/30], Training Loss: 39.9231, Validation Loss Current: 9.9818, Validation Loss AVG: 9.9818, lr: 0.001
Epoch [2/30], Training Loss: 39.7896, Validation Loss Current: 9.9715, Validation Loss AVG: 9.9715, lr: 0.001
Epoch [3/30], Training Loss: 39.6941, Validation Loss Current: 9.9576, Validation Loss AVG: 9.9576, lr: 0.001
Epoch [4/30], Training Loss: 39.1474, Validation Loss Current: 9.9611, Validation Loss AVG: 9.9611, lr: 0.001
Epoch [5/30], Training Loss: 39.5801, Validation Loss Current: 9.9731, Validation Loss AVG: 9.9731, lr: 0.001
Epoch [6/30], Training Loss: 39.3668, Validation Loss Current: 9.9504, Validation Loss AVG: 9.9504, lr: 0.001
Epoch [7/30], Training Loss: 39.4379, Validation Loss Current: 9.9561, Validation Loss AVG: 9.9561, lr: 0.001
Epoch [8/30], Training Loss: 39.6644, Validation Loss Current: 9.9400, Validation Loss AVG: 9.9400, lr: 0.001
Epoch [9/30], Training Loss: 39.2517, Validation Loss Current: 9.9422, Validation Loss AVG: 9.9422, lr: 0.001
Epoch [10/30], Training Loss: 39.1433, Validation Loss Current: 9.9325, Validation Loss AVG: 9.9325, lr: 0.001
Epoch [11/30], Training Loss: 40.0019, Validation Loss Current: 9.9603, Validation Loss AVG: 9.9603, lr: 0.001
Epoch [12/30], Training Loss: 39.4123, Validation Loss Current: 9.9207, Validation Loss AVG: 9.9207, lr: 0.001
Epoch [13/30], Training Loss: 40.0179, Validation Loss Current: 9.9064, Validation Loss AVG: 9.9064, lr: 0.001
Epoch [14/30], Training Loss: 39.7586, Validation Loss Current: 9.9116, Validation Loss AVG: 9.9116, lr: 0.001
Epoch [15/30], Training Loss: 39.4495, Validation Loss Current: 9.8936, Validation Loss AVG: 9.8936, lr: 0.001
Epoch [16/30], Training Loss: 39.2915, Validation Loss Current: 9.8730, Validation Loss AVG: 9.8730, lr: 0.001
Epoch [17/30], Training Loss: 38.7087, Validation Loss Current: 9.8384, Validation Loss AVG: 9.8384, lr: 0.001
Epoch [18/30], Training Loss: 38.9477, Validation Loss Current: 9.8232, Validation Loss AVG: 9.8232, lr: 0.001
Epoch [19/30], Training Loss: 38.2288, Validation Loss Current: 9.8354, Validation Loss AVG: 9.8354, lr: 0.001
Epoch [20/30], Training Loss: 38.6708, Validation Loss Current: 9.8675, Validation Loss AVG: 9.8675, lr: 0.001
Epoch [21/30], Training Loss: 38.1072, Validation Loss Current: 9.7316, Validation Loss AVG: 9.7316, lr: 0.001
Epoch [22/30], Training Loss: 38.2724, Validation Loss Current: 9.7001, Validation Loss AVG: 9.7001, lr: 0.001
Epoch [23/30], Training Loss: 38.3914, Validation Loss Current: 9.7489, Validation Loss AVG: 9.7489, lr: 0.001
Epoch [24/30], Training Loss: 38.2597, Validation Loss Current: 9.6370, Validation Loss AVG: 9.6370, lr: 0.001
Epoch [25/30], Training Loss: 37.2439, Validation Loss Current: 9.6860, Validation Loss AVG: 9.6860, lr: 0.001
Epoch [26/30], Training Loss: 39.0653, Validation Loss Current: 9.8564, Validation Loss AVG: 9.8564, lr: 0.001
Epoch [27/30], Training Loss: 38.6931, Validation Loss Current: 9.6181, Validation Loss AVG: 9.6181, lr: 0.001
Epoch [28/30], Training Loss: 37.5687, Validation Loss Current: 9.6132, Validation Loss AVG: 9.6132, lr: 0.001
Epoch [29/30], Training Loss: 37.1763, Validation Loss Current: 9.7179, Validation Loss AVG: 9.7179, lr: 0.001
Epoch [30/30], Training Loss: 38.2427, Validation Loss Current: 9.5202, Validation Loss AVG: 9.5202, lr: 0.001
Epoch [31/30], Training Loss: 37.1884, Validation Loss Current: 9.5177, Validation Loss AVG: 9.5177, lr: 0.001
Epoch [32/30], Training Loss: 36.5422, Validation Loss Current: 9.5750, Validation Loss AVG: 9.5750, lr: 0.001
Epoch [33/30], Training Loss: 37.5060, Validation Loss Current: 9.4750, Validation Loss AVG: 9.4750, lr: 0.001
Epoch [34/30], Training Loss: 36.5502, Validation Loss Current: 9.4345, Validation Loss AVG: 9.4345, lr: 0.001
Epoch [35/30], Training Loss: 36.4304, Validation Loss Current: 9.3591, Validation Loss AVG: 9.3591, lr: 0.001
Epoch [36/30], Training Loss: 36.7765, Validation Loss Current: 9.4010, Validation Loss AVG: 9.4010, lr: 0.001
Epoch [37/30], Training Loss: 36.5998, Validation Loss Current: 9.3481, Validation Loss AVG: 9.3481, lr: 0.001
Epoch [38/30], Training Loss: 36.1552, Validation Loss Current: 9.3645, Validation Loss AVG: 9.3645, lr: 0.001
Epoch [39/30], Training Loss: 37.0158, Validation Loss Current: 9.4066, Validation Loss AVG: 9.4066, lr: 0.001
Epoch [40/30], Training Loss: 36.3910, Validation Loss Current: 9.2325, Validation Loss AVG: 9.2325, lr: 0.001
Epoch [41/30], Training Loss: 36.0979, Validation Loss Current: 9.2637, Validation Loss AVG: 9.2637, lr: 0.001
Epoch [42/30], Training Loss: 36.9246, Validation Loss Current: 10.0518, Validation Loss AVG: 10.0518, lr: 0.001
Epoch [43/30], Training Loss: 41.2913, Validation Loss Current: 10.3729, Validation Loss AVG: 10.3729, lr: 0.001
Epoch [44/30], Training Loss: 41.1027, Validation Loss Current: 10.2318, Validation Loss AVG: 10.2318, lr: 0.001
Epoch [45/30], Training Loss: 40.8837, Validation Loss Current: 10.1561, Validation Loss AVG: 10.1561, lr: 0.001
Epoch [46/30], Training Loss: 40.4881, Validation Loss Current: 10.1746, Validation Loss AVG: 10.1746, lr: 0.001
Epoch [47/30], Training Loss: 40.4342, Validation Loss Current: 10.1331, Validation Loss AVG: 10.1331, lr: 0.001
Epoch [48/30], Training Loss: 40.4305, Validation Loss Current: 10.1160, Validation Loss AVG: 10.1160, lr: 0.001
Epoch [49/30], Training Loss: 40.2918, Validation Loss Current: 10.0660, Validation Loss AVG: 10.0660, lr: 0.001
Epoch [50/30], Training Loss: 40.1111, Validation Loss Current: 10.0348, Validation Loss AVG: 10.0348, lr: 0.001
Epoch [51/30], Training Loss: 39.8655, Validation Loss Current: 10.0389, Validation Loss AVG: 10.0389, lr: 0.001
Epoch [52/30], Training Loss: 39.9473, Validation Loss Current: 10.0231, Validation Loss AVG: 10.0231, lr: 0.001
Epoch [53/30], Training Loss: 39.3456, Validation Loss Current: 9.9316, Validation Loss AVG: 9.9316, lr: 0.001
Epoch [54/30], Training Loss: 39.4320, Validation Loss Current: 9.9587, Validation Loss AVG: 9.9587, lr: 0.001
Epoch [55/30], Training Loss: 39.6523, Validation Loss Current: 9.9751, Validation Loss AVG: 9.9751, lr: 0.001
Epoch [56/30], Training Loss: 39.2450, Validation Loss Current: 9.8517, Validation Loss AVG: 9.8517, lr: 0.001
Epoch [57/30], Training Loss: 38.8376, Validation Loss Current: 9.7930, Validation Loss AVG: 9.7930, lr: 0.001
Epoch [58/30], Training Loss: 38.6558, Validation Loss Current: 9.7640, Validation Loss AVG: 9.7640, lr: 0.001
Epoch [59/30], Training Loss: 38.0147, Validation Loss Current: 9.7770, Validation Loss AVG: 9.7770, lr: 0.001
Epoch [60/30], Training Loss: 39.6595, Validation Loss Current: 9.6856, Validation Loss AVG: 9.6856, lr: 0.001
Epoch [61/30], Training Loss: 38.7045, Validation Loss Current: 9.6685, Validation Loss AVG: 9.6685, lr: 0.001
Epoch [62/30], Training Loss: 38.4669, Validation Loss Current: 9.6448, Validation Loss AVG: 9.6448, lr: 0.001
Epoch [63/30], Training Loss: 37.7334, Validation Loss Current: 9.5799, Validation Loss AVG: 9.5799, lr: 0.001
Epoch [64/30], Training Loss: 38.0396, Validation Loss Current: 9.5718, Validation Loss AVG: 9.5718, lr: 0.001
Epoch [65/30], Training Loss: 36.6171, Validation Loss Current: 9.3635, Validation Loss AVG: 9.3635, lr: 0.001
Epoch [66/30], Training Loss: 36.6885, Validation Loss Current: 9.3238, Validation Loss AVG: 9.3238, lr: 0.001
Epoch [67/30], Training Loss: 36.0463, Validation Loss Current: 9.1669, Validation Loss AVG: 9.1669, lr: 0.001
Epoch [68/30], Training Loss: 35.9081, Validation Loss Current: 9.1098, Validation Loss AVG: 9.1098, lr: 0.001
Epoch [69/30], Training Loss: 35.0185, Validation Loss Current: 9.0829, Validation Loss AVG: 9.0829, lr: 0.001
Epoch [70/30], Training Loss: 35.5682, Validation Loss Current: 9.1367, Validation Loss AVG: 9.1367, lr: 0.001
Epoch [71/30], Training Loss: 34.4240, Validation Loss Current: 9.0248, Validation Loss AVG: 9.0248, lr: 0.001
Epoch [72/30], Training Loss: 35.4270, Validation Loss Current: 9.0119, Validation Loss AVG: 9.0119, lr: 0.001
Epoch [73/30], Training Loss: 35.2767, Validation Loss Current: 9.0100, Validation Loss AVG: 9.0100, lr: 0.001
Epoch [74/30], Training Loss: 33.9411, Validation Loss Current: 9.3087, Validation Loss AVG: 9.3087, lr: 0.001
Epoch [75/30], Training Loss: 33.8895, Validation Loss Current: 8.8942, Validation Loss AVG: 8.8942, lr: 0.001
Epoch [76/30], Training Loss: 33.3242, Validation Loss Current: 8.9786, Validation Loss AVG: 8.9786, lr: 0.001
Epoch [77/30], Training Loss: 35.0955, Validation Loss Current: 9.2235, Validation Loss AVG: 9.2235, lr: 0.001
Epoch [78/30], Training Loss: 34.6325, Validation Loss Current: 8.8144, Validation Loss AVG: 8.8144, lr: 0.001
Epoch [79/30], Training Loss: 33.4808, Validation Loss Current: 8.7180, Validation Loss AVG: 8.7180, lr: 0.001
Epoch [80/30], Training Loss: 33.3265, Validation Loss Current: 8.7479, Validation Loss AVG: 8.7479, lr: 0.001
Epoch [81/30], Training Loss: 32.9174, Validation Loss Current: 8.6693, Validation Loss AVG: 8.6693, lr: 0.001
Epoch [82/30], Training Loss: 32.5354, Validation Loss Current: 9.4385, Validation Loss AVG: 9.4385, lr: 0.001
Epoch [83/30], Training Loss: 34.5056, Validation Loss Current: 9.5324, Validation Loss AVG: 9.5324, lr: 0.001
Epoch [84/30], Training Loss: 35.3823, Validation Loss Current: 8.7110, Validation Loss AVG: 8.7110, lr: 0.001
Epoch [85/30], Training Loss: 33.5304, Validation Loss Current: 8.7340, Validation Loss AVG: 8.7340, lr: 0.001
Epoch [86/30], Training Loss: 31.4639, Validation Loss Current: 8.5640, Validation Loss AVG: 8.5640, lr: 0.001
Epoch [87/30], Training Loss: 32.2876, Validation Loss Current: 8.6884, Validation Loss AVG: 8.6884, lr: 0.001
Epoch [88/30], Training Loss: 31.7350, Validation Loss Current: 8.6084, Validation Loss AVG: 8.6084, lr: 0.001
Epoch [89/30], Training Loss: 32.4359, Validation Loss Current: 8.9264, Validation Loss AVG: 8.9264, lr: 0.001
Epoch [90/30], Training Loss: 32.8941, Validation Loss Current: 9.3807, Validation Loss AVG: 9.3807, lr: 0.001
Epoch [91/30], Training Loss: 34.3363, Validation Loss Current: 8.5569, Validation Loss AVG: 8.5569, lr: 0.001
Epoch [92/30], Training Loss: 31.9489, Validation Loss Current: 8.5744, Validation Loss AVG: 8.5744, lr: 0.001
Epoch [93/30], Training Loss: 32.4246, Validation Loss Current: 8.7954, Validation Loss AVG: 8.7954, lr: 0.001
Epoch [94/30], Training Loss: 32.1241, Validation Loss Current: 8.5095, Validation Loss AVG: 8.5095, lr: 0.001
Epoch [95/30], Training Loss: 31.8404, Validation Loss Current: 8.4083, Validation Loss AVG: 8.4083, lr: 0.001
Epoch [96/30], Training Loss: 31.5486, Validation Loss Current: 8.5035, Validation Loss AVG: 8.5035, lr: 0.001
Epoch [97/30], Training Loss: 31.3724, Validation Loss Current: 8.4808, Validation Loss AVG: 8.4808, lr: 0.001
Epoch [98/30], Training Loss: 31.1210, Validation Loss Current: 8.4671, Validation Loss AVG: 8.4671, lr: 0.001
Epoch [99/30], Training Loss: 30.9910, Validation Loss Current: 8.6511, Validation Loss AVG: 8.6511, lr: 0.001
Epoch [100/30], Training Loss: 31.5667, Validation Loss Current: 8.4727, Validation Loss AVG: 8.4727, lr: 0.001
Epoch [101/30], Training Loss: 30.5125, Validation Loss Current: 8.3344, Validation Loss AVG: 8.3344, lr: 0.001
Epoch [102/30], Training Loss: 29.9926, Validation Loss Current: 8.3955, Validation Loss AVG: 8.3955, lr: 0.001
Epoch [103/30], Training Loss: 29.3917, Validation Loss Current: 8.5769, Validation Loss AVG: 8.5769, lr: 0.001
Epoch [104/30], Training Loss: 28.6534, Validation Loss Current: 8.3638, Validation Loss AVG: 8.3638, lr: 0.001
Epoch [105/30], Training Loss: 29.7984, Validation Loss Current: 8.3713, Validation Loss AVG: 8.3713, lr: 0.001
Epoch [106/30], Training Loss: 30.2441, Validation Loss Current: 9.2717, Validation Loss AVG: 9.2717, lr: 0.001
Epoch [107/30], Training Loss: 34.4708, Validation Loss Current: 8.4917, Validation Loss AVG: 8.4917, lr: 0.001
Epoch [108/30], Training Loss: 31.4453, Validation Loss Current: 8.4971, Validation Loss AVG: 8.4971, lr: 0.001
Epoch [109/30], Training Loss: 29.0852, Validation Loss Current: 8.2695, Validation Loss AVG: 8.2695, lr: 0.001
Epoch [110/30], Training Loss: 29.0971, Validation Loss Current: 8.2349, Validation Loss AVG: 8.2349, lr: 0.001
Epoch [111/30], Training Loss: 28.6597, Validation Loss Current: 8.5960, Validation Loss AVG: 8.5960, lr: 0.001
Epoch [112/30], Training Loss: 32.8399, Validation Loss Current: 8.7290, Validation Loss AVG: 8.7290, lr: 0.001
Epoch [113/30], Training Loss: 32.4348, Validation Loss Current: 8.4320, Validation Loss AVG: 8.4320, lr: 0.001
Epoch [114/30], Training Loss: 29.5863, Validation Loss Current: 8.3839, Validation Loss AVG: 8.3839, lr: 0.001
Epoch [115/30], Training Loss: 27.7256, Validation Loss Current: 8.2916, Validation Loss AVG: 8.2916, lr: 0.001
Epoch [116/30], Training Loss: 27.7779, Validation Loss Current: 8.4936, Validation Loss AVG: 8.4936, lr: 0.001
Epoch [117/30], Training Loss: 27.7214, Validation Loss Current: 8.1528, Validation Loss AVG: 8.1528, lr: 0.001
Epoch [118/30], Training Loss: 27.5050, Validation Loss Current: 8.3171, Validation Loss AVG: 8.3171, lr: 0.001
Epoch [119/30], Training Loss: 27.2304, Validation Loss Current: 8.2916, Validation Loss AVG: 8.2916, lr: 0.001
Epoch [120/30], Training Loss: 27.7483, Validation Loss Current: 8.4054, Validation Loss AVG: 8.4054, lr: 0.001
Epoch [121/30], Training Loss: 27.7925, Validation Loss Current: 8.2703, Validation Loss AVG: 8.2703, lr: 0.001
Epoch [122/30], Training Loss: 26.2498, Validation Loss Current: 8.1982, Validation Loss AVG: 8.1982, lr: 0.001
Epoch [123/30], Training Loss: 26.0154, Validation Loss Current: 8.5404, Validation Loss AVG: 8.5404, lr: 0.001
Epoch [124/30], Training Loss: 27.9204, Validation Loss Current: 8.2935, Validation Loss AVG: 8.2935, lr: 0.001
Epoch [125/30], Training Loss: 27.6033, Validation Loss Current: 8.1548, Validation Loss AVG: 8.1548, lr: 0.001
Epoch [126/30], Training Loss: 25.5653, Validation Loss Current: 8.3047, Validation Loss AVG: 8.3047, lr: 0.001
Epoch [127/30], Training Loss: 27.4731, Validation Loss Current: 8.1554, Validation Loss AVG: 8.1554, lr: 0.001
Epoch [128/30], Training Loss: 25.3997, Validation Loss Current: 8.3327, Validation Loss AVG: 8.3327, lr: 0.001
Epoch [129/30], Training Loss: 24.9336, Validation Loss Current: 8.1005, Validation Loss AVG: 8.1005, lr: 0.001
Epoch [130/30], Training Loss: 24.6301, Validation Loss Current: 8.2541, Validation Loss AVG: 8.2541, lr: 0.001
Epoch [131/30], Training Loss: 24.6059, Validation Loss Current: 8.4244, Validation Loss AVG: 8.4244, lr: 0.001
Epoch [132/30], Training Loss: 24.8980, Validation Loss Current: 8.6969, Validation Loss AVG: 8.6969, lr: 0.001
Epoch [133/30], Training Loss: 24.0906, Validation Loss Current: 8.5041, Validation Loss AVG: 8.5041, lr: 0.001
Epoch [134/30], Training Loss: 26.0081, Validation Loss Current: 8.3004, Validation Loss AVG: 8.3004, lr: 0.001
Epoch [135/30], Training Loss: 25.1868, Validation Loss Current: 8.1954, Validation Loss AVG: 8.1954, lr: 0.001
Epoch [136/30], Training Loss: 24.2178, Validation Loss Current: 8.6206, Validation Loss AVG: 8.6206, lr: 0.001
Epoch [137/30], Training Loss: 25.7456, Validation Loss Current: 8.6001, Validation Loss AVG: 8.6001, lr: 0.001
Epoch [138/30], Training Loss: 24.9198, Validation Loss Current: 8.3406, Validation Loss AVG: 8.3406, lr: 0.001
Epoch [139/30], Training Loss: 23.5059, Validation Loss Current: 8.7319, Validation Loss AVG: 8.7319, lr: 0.001
Epoch [140/30], Training Loss: 24.4169, Validation Loss Current: 8.6178, Validation Loss AVG: 8.6178, lr: 0.001
Epoch [141/30], Training Loss: 23.2861, Validation Loss Current: 8.8923, Validation Loss AVG: 8.8923, lr: 0.001
Epoch [142/30], Training Loss: 25.0898, Validation Loss Current: 9.0066, Validation Loss AVG: 9.0066, lr: 0.001
Epoch [143/30], Training Loss: 25.8771, Validation Loss Current: 8.5731, Validation Loss AVG: 8.5731, lr: 0.001
Epoch [144/30], Training Loss: 26.0427, Validation Loss Current: 7.9940, Validation Loss AVG: 7.9940, lr: 0.001
Epoch [145/30], Training Loss: 23.3519, Validation Loss Current: 8.7803, Validation Loss AVG: 8.7803, lr: 0.001
Epoch [146/30], Training Loss: 23.6950, Validation Loss Current: 8.4816, Validation Loss AVG: 8.4816, lr: 0.001
Epoch [147/30], Training Loss: 22.2612, Validation Loss Current: 8.6203, Validation Loss AVG: 8.6203, lr: 0.001
Epoch [148/30], Training Loss: 22.1924, Validation Loss Current: 9.8297, Validation Loss AVG: 9.8297, lr: 0.001
Epoch [149/30], Training Loss: 25.2790, Validation Loss Current: 8.3129, Validation Loss AVG: 8.3129, lr: 0.001
Epoch [150/30], Training Loss: 22.9948, Validation Loss Current: 8.8088, Validation Loss AVG: 8.8088, lr: 0.001
Patch distance: 0.4 finished training. Best epoch: 144 Best val accuracy: [0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.2539473684210526, 0.2654605263157895, 0.2763157894736842, 0.2674342105263158, 0.2671052631578947, 0.26184210526315793, 0.27072368421052634, 0.2746710526315789, 0.268421052631579, 0.28388157894736843, 0.2878289473684211, 0.28125, 0.2950657894736842, 0.29210526315789476, 0.29375000000000007, 0.2950657894736842, 0.2970394736842105, 0.2967105263157895, 0.2970394736842105, 0.30460526315789477, 0.30131578947368426, 0.1960526315789474, 0.12006578947368421, 0.15131578947368424, 0.17664473684210527, 0.14638157894736842, 0.19046052631578947, 0.21447368421052632, 0.23157894736842102, 0.23190789473684212, 0.24967105263157893, 0.25, 0.2480263157894737, 0.25361842105263155, 0.24440789473684213, 0.25921052631578945, 0.2618421052631579, 0.26776315789473687, 0.2638157894736842, 0.2736842105263158, 0.2779605263157895, 0.28388157894736843, 0.29243421052631574, 0.2819078947368421, 0.3082236842105263, 0.3115131578947368, 0.32434210526315793, 0.33092105263157895, 0.3279605263157895, 0.3207236842105263, 0.34210526315789475, 0.33552631578947373, 0.3427631578947369, 0.30657894736842106, 0.3470394736842105, 0.3447368421052632, 0.3414473684210526, 0.356578947368421, 0.3559210526315789, 0.3585526315789474, 0.36348684210526316, 0.3115131578947368, 0.28092105263157896, 0.3539473684210527, 0.3536184210526315, 0.3707236842105263, 0.3476973684210526, 0.35888157894736844, 0.34539473684210525, 0.29309210526315793, 0.3628289473684211, 0.3526315789473684, 0.36546052631578946, 0.3648026315789473, 0.3819078947368421, 0.37072368421052626, 0.37434210526315786, 0.36414473684210524, 0.3506578947368421, 0.3680921052631579, 0.3828947368421053, 0.38125, 0.38421052631578945, 0.40164473684210533, 0.37532894736842104, 0.3368421052631579, 0.36282894736842103, 0.38453947368421054, 0.40493421052631573, 0.41480263157894737, 0.3947368421052631, 0.35, 0.3848684210526316, 0.41052631578947374, 0.40625, 0.3782894736842105, 0.41578947368421043, 0.3904605263157895, 0.40888157894736843, 0.39407894736842103, 0.40625, 0.4338815789473685, 0.40690789473684214, 0.42105263157894735, 0.4338815789473684, 0.44506578947368425, 0.41217105263157894, 0.42467105263157895, 0.43947368421052635, 0.43717105263157896, 0.4305921052631579, 0.4424342105263158, 0.43552631578947365, 0.4391447368421053, 0.4180921052631579, 0.4582236842105263, 0.41578947368421054, 0.42072368421052636, 0.40756578947368427, 0.3993421052631579, 0.41118421052631576, 0.3700657894736842, 0.40427631578947365, 0.43519736842105267, 0.42993421052631586, 0.42072368421052636, 0.4240131578947369, 0.36019736842105265, 0.4276315789473685, 0.4233552631578948] Best val loss: 7.993968343734741


Loaded best state dict for [0.2, 0.4]
Current group: 0.6
Epoch [1/30], Training Loss: 27.6202, Validation Loss Current: 8.5073, Validation Loss AVG: 8.5073, lr: 0.001
Epoch [2/30], Training Loss: 29.2232, Validation Loss Current: 8.1845, Validation Loss AVG: 8.1845, lr: 0.001
Epoch [3/30], Training Loss: 25.6601, Validation Loss Current: 8.5622, Validation Loss AVG: 8.5622, lr: 0.001
Epoch [4/30], Training Loss: 25.8600, Validation Loss Current: 8.1169, Validation Loss AVG: 8.1169, lr: 0.001
Epoch [5/30], Training Loss: 23.8097, Validation Loss Current: 7.9344, Validation Loss AVG: 7.9344, lr: 0.001
Epoch [6/30], Training Loss: 24.1013, Validation Loss Current: 8.8272, Validation Loss AVG: 8.8272, lr: 0.001
Epoch [7/30], Training Loss: 24.4044, Validation Loss Current: 7.7640, Validation Loss AVG: 7.7640, lr: 0.001
Epoch [8/30], Training Loss: 22.1854, Validation Loss Current: 7.9139, Validation Loss AVG: 7.9139, lr: 0.001
Epoch [9/30], Training Loss: 24.1326, Validation Loss Current: 8.5669, Validation Loss AVG: 8.5669, lr: 0.001
Epoch [10/30], Training Loss: 22.9816, Validation Loss Current: 7.8701, Validation Loss AVG: 7.8701, lr: 0.001
Epoch [11/30], Training Loss: 22.3603, Validation Loss Current: 7.7097, Validation Loss AVG: 7.7097, lr: 0.001
Epoch [12/30], Training Loss: 21.8957, Validation Loss Current: 8.8213, Validation Loss AVG: 8.8213, lr: 0.001
Epoch [13/30], Training Loss: 25.8330, Validation Loss Current: 7.9792, Validation Loss AVG: 7.9792, lr: 0.001
Epoch [14/30], Training Loss: 24.9758, Validation Loss Current: 7.8554, Validation Loss AVG: 7.8554, lr: 0.001
Epoch [15/30], Training Loss: 21.9377, Validation Loss Current: 8.1119, Validation Loss AVG: 8.1119, lr: 0.001
Epoch [16/30], Training Loss: 20.1739, Validation Loss Current: 7.9186, Validation Loss AVG: 7.9186, lr: 0.001
Epoch [17/30], Training Loss: 20.7042, Validation Loss Current: 9.0894, Validation Loss AVG: 9.0894, lr: 0.001
Epoch [18/30], Training Loss: 23.6662, Validation Loss Current: 8.5003, Validation Loss AVG: 8.5003, lr: 0.001
Epoch [19/30], Training Loss: 24.7582, Validation Loss Current: 7.6392, Validation Loss AVG: 7.6392, lr: 0.001
Epoch [20/30], Training Loss: 22.9566, Validation Loss Current: 8.0520, Validation Loss AVG: 8.0520, lr: 0.001
Epoch [21/30], Training Loss: 22.5755, Validation Loss Current: 9.2290, Validation Loss AVG: 9.2290, lr: 0.001
Epoch [22/30], Training Loss: 24.2719, Validation Loss Current: 8.2627, Validation Loss AVG: 8.2627, lr: 0.001
Epoch [23/30], Training Loss: 21.1371, Validation Loss Current: 8.0232, Validation Loss AVG: 8.0232, lr: 0.001
Epoch [24/30], Training Loss: 19.4262, Validation Loss Current: 8.3678, Validation Loss AVG: 8.3678, lr: 0.001
Epoch [25/30], Training Loss: 19.8703, Validation Loss Current: 8.7403, Validation Loss AVG: 8.7403, lr: 0.001
Epoch [26/30], Training Loss: 19.3497, Validation Loss Current: 8.7922, Validation Loss AVG: 8.7922, lr: 0.001
Epoch [27/30], Training Loss: 17.6094, Validation Loss Current: 8.2474, Validation Loss AVG: 8.2474, lr: 0.001
Epoch [28/30], Training Loss: 17.6648, Validation Loss Current: 9.3001, Validation Loss AVG: 9.3001, lr: 0.001
Epoch [29/30], Training Loss: 19.4602, Validation Loss Current: 9.4552, Validation Loss AVG: 9.4552, lr: 0.001
Epoch [30/30], Training Loss: 21.5503, Validation Loss Current: 8.7497, Validation Loss AVG: 8.7497, lr: 0.001
Epoch [31/30], Training Loss: 19.7115, Validation Loss Current: 8.6679, Validation Loss AVG: 8.6679, lr: 0.001
Epoch [32/30], Training Loss: 18.0125, Validation Loss Current: 8.8708, Validation Loss AVG: 8.8708, lr: 0.001
Epoch [33/30], Training Loss: 17.1874, Validation Loss Current: 9.2894, Validation Loss AVG: 9.2894, lr: 0.001
Epoch [34/30], Training Loss: 18.4741, Validation Loss Current: 8.6777, Validation Loss AVG: 8.6777, lr: 0.001
Epoch [35/30], Training Loss: 17.3647, Validation Loss Current: 8.8039, Validation Loss AVG: 8.8039, lr: 0.001
Epoch [36/30], Training Loss: 15.7538, Validation Loss Current: 8.5905, Validation Loss AVG: 8.5905, lr: 0.001
Epoch [37/30], Training Loss: 15.5204, Validation Loss Current: 10.2116, Validation Loss AVG: 10.2116, lr: 0.001
Epoch [38/30], Training Loss: 17.6025, Validation Loss Current: 8.9573, Validation Loss AVG: 8.9573, lr: 0.001
Epoch [39/30], Training Loss: 16.5059, Validation Loss Current: 8.5416, Validation Loss AVG: 8.5416, lr: 0.001
Epoch [40/30], Training Loss: 17.0587, Validation Loss Current: 9.1961, Validation Loss AVG: 9.1961, lr: 0.001
Epoch [41/30], Training Loss: 18.7466, Validation Loss Current: 10.0297, Validation Loss AVG: 10.0297, lr: 0.001
Epoch [42/30], Training Loss: 16.1570, Validation Loss Current: 9.2956, Validation Loss AVG: 9.2956, lr: 0.001
Epoch [43/30], Training Loss: 14.4444, Validation Loss Current: 8.9643, Validation Loss AVG: 8.9643, lr: 0.001
Epoch [44/30], Training Loss: 14.0991, Validation Loss Current: 9.4171, Validation Loss AVG: 9.4171, lr: 0.001
Epoch [45/30], Training Loss: 13.3056, Validation Loss Current: 9.3682, Validation Loss AVG: 9.3682, lr: 0.001
Epoch [46/30], Training Loss: 13.4868, Validation Loss Current: 10.9752, Validation Loss AVG: 10.9752, lr: 0.001
Epoch [47/30], Training Loss: 19.3825, Validation Loss Current: 9.1407, Validation Loss AVG: 9.1407, lr: 0.001
Epoch [48/30], Training Loss: 15.2299, Validation Loss Current: 10.2926, Validation Loss AVG: 10.2926, lr: 0.001
Epoch [49/30], Training Loss: 16.7357, Validation Loss Current: 9.4885, Validation Loss AVG: 9.4885, lr: 0.001
 --- Early Stopped ---
Patch distance: 0.6 finished training. Best epoch: 19 Best val accuracy: [0.4197368421052632, 0.41940789473684215, 0.43322368421052637, 0.4414473684210526, 0.4565789473684211, 0.3907894736842105, 0.47072368421052635, 0.4947368421052631, 0.4233552631578947, 0.4707236842105263, 0.4736842105263158, 0.4236842105263158, 0.43519736842105267, 0.4411184210526316, 0.4625, 0.4934210526315789, 0.4269736842105264, 0.4480263157894737, 0.47171052631578947, 0.4766447368421053, 0.41776315789473684, 0.4444078947368421, 0.46940789473684214, 0.46875, 0.4598684210526316, 0.4848684210526316, 0.48190789473684215, 0.425, 0.40921052631578947, 0.4125, 0.45592105263157895, 0.46414473684210533, 0.4496710526315789, 0.4743421052631579, 0.4756578947368421, 0.46644736842105267, 0.4509868421052632, 0.4773026315789474, 0.46875, 0.4740131578947368, 0.4217105263157895, 0.4598684210526316, 0.4832236842105263, 0.4858552631578947, 0.47631578947368414, 0.4151315789473685, 0.4467105263157894, 0.41217105263157894, 0.45789473684210524] Best val loss: 7.639243030548096


Loaded best state dict for [0.2, 0.4, 0.6]
Current group: 0.8
Epoch [1/30], Training Loss: 21.2765, Validation Loss Current: 10.1211, Validation Loss AVG: 10.1211, lr: 0.001
Epoch [2/30], Training Loss: 26.8387, Validation Loss Current: 7.9015, Validation Loss AVG: 7.9015, lr: 0.001
Epoch [3/30], Training Loss: 21.1910, Validation Loss Current: 8.6404, Validation Loss AVG: 8.6404, lr: 0.001
Epoch [4/30], Training Loss: 19.0489, Validation Loss Current: 8.9487, Validation Loss AVG: 8.9487, lr: 0.001
Epoch [5/30], Training Loss: 19.5610, Validation Loss Current: 10.2490, Validation Loss AVG: 10.2490, lr: 0.001
Epoch [6/30], Training Loss: 22.7298, Validation Loss Current: 8.2367, Validation Loss AVG: 8.2367, lr: 0.001
Epoch [7/30], Training Loss: 19.7311, Validation Loss Current: 8.7566, Validation Loss AVG: 8.7566, lr: 0.001
Epoch [8/30], Training Loss: 17.9651, Validation Loss Current: 8.2913, Validation Loss AVG: 8.2913, lr: 0.001
Epoch [9/30], Training Loss: 18.4058, Validation Loss Current: 11.1909, Validation Loss AVG: 11.1909, lr: 0.001
Epoch [10/30], Training Loss: 21.6387, Validation Loss Current: 8.5936, Validation Loss AVG: 8.5936, lr: 0.001
Epoch [11/30], Training Loss: 18.5762, Validation Loss Current: 9.1778, Validation Loss AVG: 9.1778, lr: 0.001
Epoch [12/30], Training Loss: 17.2875, Validation Loss Current: 9.2656, Validation Loss AVG: 9.2656, lr: 0.001
Epoch [13/30], Training Loss: 17.2724, Validation Loss Current: 9.5464, Validation Loss AVG: 9.5464, lr: 0.001
Epoch [14/30], Training Loss: 18.7037, Validation Loss Current: 9.2318, Validation Loss AVG: 9.2318, lr: 0.001
Epoch [15/30], Training Loss: 19.0091, Validation Loss Current: 9.1895, Validation Loss AVG: 9.1895, lr: 0.001
Epoch [16/30], Training Loss: 16.9760, Validation Loss Current: 9.3480, Validation Loss AVG: 9.3480, lr: 0.001
Epoch [17/30], Training Loss: 15.0642, Validation Loss Current: 10.0480, Validation Loss AVG: 10.0480, lr: 0.001
Epoch [18/30], Training Loss: 13.5208, Validation Loss Current: 10.4977, Validation Loss AVG: 10.4977, lr: 0.001
Epoch [19/30], Training Loss: 13.0297, Validation Loss Current: 9.7476, Validation Loss AVG: 9.7476, lr: 0.001
Epoch [20/30], Training Loss: 13.6168, Validation Loss Current: 10.4385, Validation Loss AVG: 10.4385, lr: 0.001
Epoch [21/30], Training Loss: 15.1203, Validation Loss Current: 11.1439, Validation Loss AVG: 11.1439, lr: 0.001
Epoch [22/30], Training Loss: 14.7891, Validation Loss Current: 10.6655, Validation Loss AVG: 10.6655, lr: 0.001
Epoch [23/30], Training Loss: 12.6324, Validation Loss Current: 10.7486, Validation Loss AVG: 10.7486, lr: 0.001
Epoch [24/30], Training Loss: 11.3432, Validation Loss Current: 11.1199, Validation Loss AVG: 11.1199, lr: 0.001
Epoch [25/30], Training Loss: 10.9217, Validation Loss Current: 11.7140, Validation Loss AVG: 11.7140, lr: 0.001
Epoch [26/30], Training Loss: 11.4574, Validation Loss Current: 10.4685, Validation Loss AVG: 10.4685, lr: 0.001
Epoch [27/30], Training Loss: 11.4318, Validation Loss Current: 12.1170, Validation Loss AVG: 12.1170, lr: 0.001
Epoch [28/30], Training Loss: 9.9604, Validation Loss Current: 11.7511, Validation Loss AVG: 11.7511, lr: 0.001
Epoch [29/30], Training Loss: 9.2961, Validation Loss Current: 11.9189, Validation Loss AVG: 11.9189, lr: 0.001
Epoch [30/30], Training Loss: 8.7143, Validation Loss Current: 12.9797, Validation Loss AVG: 12.9797, lr: 0.001
Epoch [31/30], Training Loss: 9.3144, Validation Loss Current: 20.2334, Validation Loss AVG: 20.2334, lr: 0.001
Epoch [32/30], Training Loss: 28.4091, Validation Loss Current: 10.0519, Validation Loss AVG: 10.0519, lr: 0.001
 --- Early Stopped ---
Patch distance: 0.8 finished training. Best epoch: 2 Best val accuracy: [0.45065789473684215, 0.46578947368421053, 0.4572368421052631, 0.4634868421052632, 0.39934210526315794, 0.4578947368421053, 0.4661184210526315, 0.48519736842105254, 0.4180921052631579, 0.46480263157894736, 0.4648026315789474, 0.45723684210526316, 0.4207236842105263, 0.45953947368421055, 0.4463815789473684, 0.4723684210526316, 0.46447368421052626, 0.4621710526315789, 0.4516447368421052, 0.4828947368421053, 0.43782894736842104, 0.4440789473684211, 0.4618421052631579, 0.47368421052631576, 0.4625, 0.4720394736842105, 0.46381578947368424, 0.4651315789473684, 0.4677631578947368, 0.45, 0.2888157894736842, 0.3726973684210526] Best val loss: 7.901472330093384


Loaded best state dict for [0.2, 0.4, 0.6, 0.8]
Current group: 1
Epoch [1/30], Training Loss: 22.6746, Validation Loss Current: 6.6647, Validation Loss AVG: 10.0373, lr: 0.001
Epoch [2/30], Training Loss: 20.3552, Validation Loss Current: 6.5957, Validation Loss AVG: 9.2166, lr: 0.001
Epoch [3/30], Training Loss: 20.9362, Validation Loss Current: 6.4695, Validation Loss AVG: 9.6372, lr: 0.001
Epoch [4/30], Training Loss: 18.1150, Validation Loss Current: 6.6813, Validation Loss AVG: 11.1578, lr: 0.001
Epoch [5/30], Training Loss: 16.7744, Validation Loss Current: 6.1391, Validation Loss AVG: 10.3571, lr: 0.001
Epoch [6/30], Training Loss: 17.0583, Validation Loss Current: 6.7469, Validation Loss AVG: 10.7700, lr: 0.001
Epoch [7/30], Training Loss: 18.3951, Validation Loss Current: 8.6095, Validation Loss AVG: 15.1776, lr: 0.001
Epoch [8/30], Training Loss: 22.4757, Validation Loss Current: 7.1291, Validation Loss AVG: 11.3905, lr: 0.001
Epoch [9/30], Training Loss: 17.1673, Validation Loss Current: 6.3897, Validation Loss AVG: 10.3969, lr: 0.001
Epoch [10/30], Training Loss: 16.8173, Validation Loss Current: 6.9732, Validation Loss AVG: 10.9783, lr: 0.001
Epoch [11/30], Training Loss: 18.6918, Validation Loss Current: 6.3195, Validation Loss AVG: 10.8627, lr: 0.001
Epoch [12/30], Training Loss: 15.7102, Validation Loss Current: 6.4613, Validation Loss AVG: 12.5752, lr: 0.001
Epoch [13/30], Training Loss: 14.5680, Validation Loss Current: 7.1998, Validation Loss AVG: 11.6169, lr: 0.001
Epoch [14/30], Training Loss: 12.7087, Validation Loss Current: 6.6722, Validation Loss AVG: 11.8774, lr: 0.001
Epoch [15/30], Training Loss: 12.2076, Validation Loss Current: 7.1562, Validation Loss AVG: 12.8511, lr: 0.001
Epoch [16/30], Training Loss: 15.1414, Validation Loss Current: 6.7872, Validation Loss AVG: 14.0049, lr: 0.001
Epoch [17/30], Training Loss: 13.6384, Validation Loss Current: 8.3953, Validation Loss AVG: 14.7260, lr: 0.001
Epoch [18/30], Training Loss: 17.3700, Validation Loss Current: 6.3917, Validation Loss AVG: 10.4695, lr: 0.001
Epoch [19/30], Training Loss: 12.7651, Validation Loss Current: 7.0473, Validation Loss AVG: 13.8887, lr: 0.001
Epoch [20/30], Training Loss: 13.8356, Validation Loss Current: 6.9221, Validation Loss AVG: 13.3140, lr: 0.001
Epoch [21/30], Training Loss: 15.7962, Validation Loss Current: 6.8621, Validation Loss AVG: 12.4317, lr: 0.001
Epoch [22/30], Training Loss: 12.0174, Validation Loss Current: 7.0576, Validation Loss AVG: 13.1664, lr: 0.001
Epoch [23/30], Training Loss: 11.1491, Validation Loss Current: 7.5012, Validation Loss AVG: 12.2781, lr: 0.001
Epoch [24/30], Training Loss: 9.9860, Validation Loss Current: 7.0977, Validation Loss AVG: 13.5033, lr: 0.001
Epoch [25/30], Training Loss: 9.3440, Validation Loss Current: 7.4496, Validation Loss AVG: 14.9682, lr: 0.001
Epoch [26/30], Training Loss: 8.8151, Validation Loss Current: 7.2259, Validation Loss AVG: 14.0131, lr: 0.001
Epoch [27/30], Training Loss: 7.4288, Validation Loss Current: 8.6197, Validation Loss AVG: 18.7833, lr: 0.001
Epoch [28/30], Training Loss: 7.0675, Validation Loss Current: 8.3005, Validation Loss AVG: 14.3116, lr: 0.001
Epoch [29/30], Training Loss: 10.5564, Validation Loss Current: 7.6370, Validation Loss AVG: 13.5700, lr: 0.001
Epoch [30/30], Training Loss: 13.7797, Validation Loss Current: 7.2068, Validation Loss AVG: 10.4112, lr: 0.001
Epoch [31/30], Training Loss: 8.6798, Validation Loss Current: 7.9157, Validation Loss AVG: 15.1149, lr: 0.001
Epoch [32/30], Training Loss: 11.2863, Validation Loss Current: 7.3479, Validation Loss AVG: 15.9489, lr: 0.001
Epoch [33/30], Training Loss: 9.1089, Validation Loss Current: 8.0979, Validation Loss AVG: 14.3607, lr: 0.001
Epoch [34/30], Training Loss: 6.3636, Validation Loss Current: 8.1292, Validation Loss AVG: 15.9506, lr: 0.001
Epoch [35/30], Training Loss: 5.1714, Validation Loss Current: 8.7566, Validation Loss AVG: 17.8336, lr: 0.001
 --- Early Stopped ---
Patch distance: 1 finished training. Best epoch: 5 Best val accuracy: [0.5460526315789473, 0.5838815789473685, 0.5542763157894737, 0.5641447368421053, 0.6019736842105263, 0.5740131578947368, 0.5131578947368421, 0.555921052631579, 0.5625, 0.5756578947368421, 0.569078947368421, 0.5904605263157895, 0.600328947368421, 0.625, 0.6233552631578947, 0.5822368421052632, 0.555921052631579, 0.6217105263157895, 0.6036184210526315, 0.5592105263157895, 0.5740131578947368, 0.6134868421052632, 0.6052631578947368, 0.618421052631579, 0.5805921052631579, 0.6266447368421053, 0.6085526315789473, 0.6299342105263158, 0.5641447368421053, 0.5904605263157895, 0.5953947368421053, 0.6134868421052632, 0.6036184210526315, 0.6282894736842105, 0.6282894736842105] Best val loss: 6.139115333557129


----- Training alexnet with sequence: [0.4, 0.6, 0.8, 1] -----
Current group: 0.4
Epoch [1/38], Training Loss: 41.5803, Validation Loss Current: 10.3787, Validation Loss AVG: 10.3787, lr: 0.001
Epoch [2/38], Training Loss: 41.4039, Validation Loss Current: 10.3444, Validation Loss AVG: 10.3444, lr: 0.001
Epoch [3/38], Training Loss: 41.1211, Validation Loss Current: 10.2948, Validation Loss AVG: 10.2948, lr: 0.001
Epoch [4/38], Training Loss: 40.8654, Validation Loss Current: 10.2167, Validation Loss AVG: 10.2167, lr: 0.001
Epoch [5/38], Training Loss: 40.6861, Validation Loss Current: 10.1076, Validation Loss AVG: 10.1076, lr: 0.001
Epoch [6/38], Training Loss: 39.9019, Validation Loss Current: 10.0690, Validation Loss AVG: 10.0690, lr: 0.001
Epoch [7/38], Training Loss: 39.7572, Validation Loss Current: 10.0499, Validation Loss AVG: 10.0499, lr: 0.001
Epoch [8/38], Training Loss: 39.6888, Validation Loss Current: 10.0252, Validation Loss AVG: 10.0252, lr: 0.001
Epoch [9/38], Training Loss: 40.3700, Validation Loss Current: 10.0199, Validation Loss AVG: 10.0199, lr: 0.001
Epoch [10/38], Training Loss: 39.7610, Validation Loss Current: 10.0255, Validation Loss AVG: 10.0255, lr: 0.001
Epoch [11/38], Training Loss: 39.9969, Validation Loss Current: 10.0142, Validation Loss AVG: 10.0142, lr: 0.001
Epoch [12/38], Training Loss: 39.6049, Validation Loss Current: 10.0072, Validation Loss AVG: 10.0072, lr: 0.001
Epoch [13/38], Training Loss: 40.4959, Validation Loss Current: 10.0225, Validation Loss AVG: 10.0225, lr: 0.001
Epoch [14/38], Training Loss: 40.4002, Validation Loss Current: 10.0487, Validation Loss AVG: 10.0487, lr: 0.001
Epoch [15/38], Training Loss: 40.1988, Validation Loss Current: 10.0330, Validation Loss AVG: 10.0330, lr: 0.001
Epoch [16/38], Training Loss: 39.5728, Validation Loss Current: 10.0140, Validation Loss AVG: 10.0140, lr: 0.001
Epoch [17/38], Training Loss: 39.6439, Validation Loss Current: 10.0105, Validation Loss AVG: 10.0105, lr: 0.001
Epoch [18/38], Training Loss: 40.6258, Validation Loss Current: 10.0020, Validation Loss AVG: 10.0020, lr: 0.001
Epoch [19/38], Training Loss: 40.2674, Validation Loss Current: 10.0729, Validation Loss AVG: 10.0729, lr: 0.001
Epoch [20/38], Training Loss: 39.9571, Validation Loss Current: 10.0139, Validation Loss AVG: 10.0139, lr: 0.001
Epoch [21/38], Training Loss: 40.2031, Validation Loss Current: 10.0276, Validation Loss AVG: 10.0276, lr: 0.001
Epoch [22/38], Training Loss: 40.0965, Validation Loss Current: 10.0478, Validation Loss AVG: 10.0478, lr: 0.001
Epoch [23/38], Training Loss: 39.8430, Validation Loss Current: 10.0187, Validation Loss AVG: 10.0187, lr: 0.001
Epoch [24/38], Training Loss: 39.9281, Validation Loss Current: 9.9950, Validation Loss AVG: 9.9950, lr: 0.001
Epoch [25/38], Training Loss: 39.8661, Validation Loss Current: 9.9850, Validation Loss AVG: 9.9850, lr: 0.001
Epoch [26/38], Training Loss: 39.8087, Validation Loss Current: 9.9753, Validation Loss AVG: 9.9753, lr: 0.001
Epoch [27/38], Training Loss: 39.5042, Validation Loss Current: 9.9629, Validation Loss AVG: 9.9629, lr: 0.001
Epoch [28/38], Training Loss: 39.5680, Validation Loss Current: 9.9734, Validation Loss AVG: 9.9734, lr: 0.001
Epoch [29/38], Training Loss: 39.4059, Validation Loss Current: 9.9956, Validation Loss AVG: 9.9956, lr: 0.001
Epoch [30/38], Training Loss: 39.5304, Validation Loss Current: 9.9828, Validation Loss AVG: 9.9828, lr: 0.001
Epoch [31/38], Training Loss: 39.9816, Validation Loss Current: 9.9786, Validation Loss AVG: 9.9786, lr: 0.001
Epoch [32/38], Training Loss: 39.9089, Validation Loss Current: 9.9820, Validation Loss AVG: 9.9820, lr: 0.001
Epoch [33/38], Training Loss: 39.7679, Validation Loss Current: 9.9758, Validation Loss AVG: 9.9758, lr: 0.001
Epoch [34/38], Training Loss: 40.2224, Validation Loss Current: 9.9764, Validation Loss AVG: 9.9764, lr: 0.001
Epoch [35/38], Training Loss: 39.6624, Validation Loss Current: 9.9752, Validation Loss AVG: 9.9752, lr: 0.001
Epoch [36/38], Training Loss: 39.7822, Validation Loss Current: 9.9800, Validation Loss AVG: 9.9800, lr: 0.001
Epoch [37/38], Training Loss: 39.1260, Validation Loss Current: 9.9666, Validation Loss AVG: 9.9666, lr: 0.001
Epoch [38/38], Training Loss: 39.5299, Validation Loss Current: 9.9857, Validation Loss AVG: 9.9857, lr: 0.001
Epoch [39/38], Training Loss: 39.4229, Validation Loss Current: 9.9670, Validation Loss AVG: 9.9670, lr: 0.001
Epoch [40/38], Training Loss: 39.6454, Validation Loss Current: 9.9835, Validation Loss AVG: 9.9835, lr: 0.001
Epoch [41/38], Training Loss: 39.2470, Validation Loss Current: 9.9514, Validation Loss AVG: 9.9514, lr: 0.001
Epoch [42/38], Training Loss: 39.6944, Validation Loss Current: 9.9700, Validation Loss AVG: 9.9700, lr: 0.001
Epoch [43/38], Training Loss: 39.2179, Validation Loss Current: 9.9490, Validation Loss AVG: 9.9490, lr: 0.001
Epoch [44/38], Training Loss: 39.6111, Validation Loss Current: 9.9304, Validation Loss AVG: 9.9304, lr: 0.001
Epoch [45/38], Training Loss: 39.1967, Validation Loss Current: 9.9283, Validation Loss AVG: 9.9283, lr: 0.001
Epoch [46/38], Training Loss: 39.7126, Validation Loss Current: 9.9348, Validation Loss AVG: 9.9348, lr: 0.001
Epoch [47/38], Training Loss: 39.4854, Validation Loss Current: 9.9119, Validation Loss AVG: 9.9119, lr: 0.001
Epoch [48/38], Training Loss: 39.2652, Validation Loss Current: 9.8934, Validation Loss AVG: 9.8934, lr: 0.001
Epoch [49/38], Training Loss: 39.7652, Validation Loss Current: 9.8857, Validation Loss AVG: 9.8857, lr: 0.001
Epoch [50/38], Training Loss: 39.7638, Validation Loss Current: 9.8724, Validation Loss AVG: 9.8724, lr: 0.001
Epoch [51/38], Training Loss: 39.5850, Validation Loss Current: 9.8631, Validation Loss AVG: 9.8631, lr: 0.001
Epoch [52/38], Training Loss: 39.3002, Validation Loss Current: 9.8546, Validation Loss AVG: 9.8546, lr: 0.001
Epoch [53/38], Training Loss: 38.9669, Validation Loss Current: 9.8540, Validation Loss AVG: 9.8540, lr: 0.001
Epoch [54/38], Training Loss: 39.0841, Validation Loss Current: 9.8729, Validation Loss AVG: 9.8729, lr: 0.001
Epoch [55/38], Training Loss: 38.9581, Validation Loss Current: 9.8321, Validation Loss AVG: 9.8321, lr: 0.001
Epoch [56/38], Training Loss: 38.0696, Validation Loss Current: 9.7990, Validation Loss AVG: 9.7990, lr: 0.001
Epoch [57/38], Training Loss: 39.4044, Validation Loss Current: 9.7939, Validation Loss AVG: 9.7939, lr: 0.001
Epoch [58/38], Training Loss: 38.0985, Validation Loss Current: 9.7633, Validation Loss AVG: 9.7633, lr: 0.001
Epoch [59/38], Training Loss: 38.4084, Validation Loss Current: 9.7600, Validation Loss AVG: 9.7600, lr: 0.001
Epoch [60/38], Training Loss: 38.3183, Validation Loss Current: 9.7451, Validation Loss AVG: 9.7451, lr: 0.001
Epoch [61/38], Training Loss: 38.3667, Validation Loss Current: 9.7042, Validation Loss AVG: 9.7042, lr: 0.001
Epoch [62/38], Training Loss: 38.5658, Validation Loss Current: 9.7215, Validation Loss AVG: 9.7215, lr: 0.001
Epoch [63/38], Training Loss: 38.9517, Validation Loss Current: 9.6519, Validation Loss AVG: 9.6519, lr: 0.001
Epoch [64/38], Training Loss: 37.5390, Validation Loss Current: 9.5758, Validation Loss AVG: 9.5758, lr: 0.001
Epoch [65/38], Training Loss: 37.7511, Validation Loss Current: 9.5136, Validation Loss AVG: 9.5136, lr: 0.001
Epoch [66/38], Training Loss: 36.8606, Validation Loss Current: 9.4425, Validation Loss AVG: 9.4425, lr: 0.001
Epoch [67/38], Training Loss: 36.7582, Validation Loss Current: 9.3288, Validation Loss AVG: 9.3288, lr: 0.001
Epoch [68/38], Training Loss: 36.2005, Validation Loss Current: 9.2651, Validation Loss AVG: 9.2651, lr: 0.001
Epoch [69/38], Training Loss: 35.6657, Validation Loss Current: 9.6232, Validation Loss AVG: 9.6232, lr: 0.001
Epoch [70/38], Training Loss: 35.5168, Validation Loss Current: 9.2321, Validation Loss AVG: 9.2321, lr: 0.001
Epoch [71/38], Training Loss: 35.1624, Validation Loss Current: 9.2158, Validation Loss AVG: 9.2158, lr: 0.001
Epoch [72/38], Training Loss: 34.3052, Validation Loss Current: 9.2493, Validation Loss AVG: 9.2493, lr: 0.001
Epoch [73/38], Training Loss: 33.7314, Validation Loss Current: 9.0061, Validation Loss AVG: 9.0061, lr: 0.001
Epoch [74/38], Training Loss: 32.9860, Validation Loss Current: 9.8714, Validation Loss AVG: 9.8714, lr: 0.001
Epoch [75/38], Training Loss: 34.1279, Validation Loss Current: 9.1428, Validation Loss AVG: 9.1428, lr: 0.001
Epoch [76/38], Training Loss: 33.9396, Validation Loss Current: 8.9482, Validation Loss AVG: 8.9482, lr: 0.001
Epoch [77/38], Training Loss: 34.0086, Validation Loss Current: 8.8244, Validation Loss AVG: 8.8244, lr: 0.001
Epoch [78/38], Training Loss: 33.8923, Validation Loss Current: 9.0065, Validation Loss AVG: 9.0065, lr: 0.001
Epoch [79/38], Training Loss: 33.8340, Validation Loss Current: 8.7594, Validation Loss AVG: 8.7594, lr: 0.001
Epoch [80/38], Training Loss: 32.0773, Validation Loss Current: 8.9159, Validation Loss AVG: 8.9159, lr: 0.001
Epoch [81/38], Training Loss: 32.5732, Validation Loss Current: 8.6986, Validation Loss AVG: 8.6986, lr: 0.001
Epoch [82/38], Training Loss: 33.1150, Validation Loss Current: 8.6450, Validation Loss AVG: 8.6450, lr: 0.001
Epoch [83/38], Training Loss: 32.5399, Validation Loss Current: 8.7010, Validation Loss AVG: 8.7010, lr: 0.001
Epoch [84/38], Training Loss: 33.7846, Validation Loss Current: 9.0777, Validation Loss AVG: 9.0777, lr: 0.001
Epoch [85/38], Training Loss: 34.6895, Validation Loss Current: 8.8442, Validation Loss AVG: 8.8442, lr: 0.001
Epoch [86/38], Training Loss: 33.6435, Validation Loss Current: 8.6501, Validation Loss AVG: 8.6501, lr: 0.001
Epoch [87/38], Training Loss: 34.8950, Validation Loss Current: 8.6082, Validation Loss AVG: 8.6082, lr: 0.001
Epoch [88/38], Training Loss: 32.7051, Validation Loss Current: 8.6081, Validation Loss AVG: 8.6081, lr: 0.001
Epoch [89/38], Training Loss: 31.7551, Validation Loss Current: 8.5655, Validation Loss AVG: 8.5655, lr: 0.001
Epoch [90/38], Training Loss: 30.9679, Validation Loss Current: 8.4545, Validation Loss AVG: 8.4545, lr: 0.001
Epoch [91/38], Training Loss: 30.5992, Validation Loss Current: 8.5251, Validation Loss AVG: 8.5251, lr: 0.001
Epoch [92/38], Training Loss: 30.0989, Validation Loss Current: 8.3209, Validation Loss AVG: 8.3209, lr: 0.001
Epoch [93/38], Training Loss: 31.0608, Validation Loss Current: 9.6659, Validation Loss AVG: 9.6659, lr: 0.001
Epoch [94/38], Training Loss: 40.0470, Validation Loss Current: 9.5872, Validation Loss AVG: 9.5872, lr: 0.001
Epoch [95/38], Training Loss: 35.9795, Validation Loss Current: 8.7746, Validation Loss AVG: 8.7746, lr: 0.001
Epoch [96/38], Training Loss: 34.2179, Validation Loss Current: 8.5802, Validation Loss AVG: 8.5802, lr: 0.001
Epoch [97/38], Training Loss: 32.2319, Validation Loss Current: 8.4495, Validation Loss AVG: 8.4495, lr: 0.001
Epoch [98/38], Training Loss: 32.1441, Validation Loss Current: 8.2579, Validation Loss AVG: 8.2579, lr: 0.001
Epoch [99/38], Training Loss: 30.7821, Validation Loss Current: 8.2273, Validation Loss AVG: 8.2273, lr: 0.001
Epoch [100/38], Training Loss: 31.7947, Validation Loss Current: 8.2374, Validation Loss AVG: 8.2374, lr: 0.001
Epoch [101/38], Training Loss: 30.5489, Validation Loss Current: 8.2544, Validation Loss AVG: 8.2544, lr: 0.001
Epoch [102/38], Training Loss: 29.2684, Validation Loss Current: 8.3645, Validation Loss AVG: 8.3645, lr: 0.001
Epoch [103/38], Training Loss: 30.6017, Validation Loss Current: 8.0988, Validation Loss AVG: 8.0988, lr: 0.001
Epoch [104/38], Training Loss: 31.1382, Validation Loss Current: 8.1603, Validation Loss AVG: 8.1603, lr: 0.001
Epoch [105/38], Training Loss: 29.9840, Validation Loss Current: 8.1532, Validation Loss AVG: 8.1532, lr: 0.001
Epoch [106/38], Training Loss: 30.6277, Validation Loss Current: 8.8751, Validation Loss AVG: 8.8751, lr: 0.001
Epoch [107/38], Training Loss: 38.0537, Validation Loss Current: 8.7880, Validation Loss AVG: 8.7880, lr: 0.001
Epoch [108/38], Training Loss: 33.3526, Validation Loss Current: 8.5890, Validation Loss AVG: 8.5890, lr: 0.001
Epoch [109/38], Training Loss: 30.4746, Validation Loss Current: 8.3393, Validation Loss AVG: 8.3393, lr: 0.001
Epoch [110/38], Training Loss: 29.3819, Validation Loss Current: 8.1241, Validation Loss AVG: 8.1241, lr: 0.001
Epoch [111/38], Training Loss: 28.6279, Validation Loss Current: 8.3050, Validation Loss AVG: 8.3050, lr: 0.001
Epoch [112/38], Training Loss: 28.0457, Validation Loss Current: 8.0534, Validation Loss AVG: 8.0534, lr: 0.001
Epoch [113/38], Training Loss: 27.6241, Validation Loss Current: 8.4215, Validation Loss AVG: 8.4215, lr: 0.001
Epoch [114/38], Training Loss: 29.2805, Validation Loss Current: 8.0121, Validation Loss AVG: 8.0121, lr: 0.001
Epoch [115/38], Training Loss: 27.4434, Validation Loss Current: 8.3322, Validation Loss AVG: 8.3322, lr: 0.001
Epoch [116/38], Training Loss: 28.3563, Validation Loss Current: 8.1431, Validation Loss AVG: 8.1431, lr: 0.001
Epoch [117/38], Training Loss: 27.3210, Validation Loss Current: 7.9380, Validation Loss AVG: 7.9380, lr: 0.001
Epoch [118/38], Training Loss: 26.4333, Validation Loss Current: 8.3527, Validation Loss AVG: 8.3527, lr: 0.001
Epoch [119/38], Training Loss: 27.6493, Validation Loss Current: 8.5579, Validation Loss AVG: 8.5579, lr: 0.001
Epoch [120/38], Training Loss: 28.9862, Validation Loss Current: 8.8314, Validation Loss AVG: 8.8314, lr: 0.001
Epoch [121/38], Training Loss: 32.6263, Validation Loss Current: 8.4065, Validation Loss AVG: 8.4065, lr: 0.001
Epoch [122/38], Training Loss: 28.3035, Validation Loss Current: 8.4182, Validation Loss AVG: 8.4182, lr: 0.001
Epoch [123/38], Training Loss: 27.1063, Validation Loss Current: 8.5540, Validation Loss AVG: 8.5540, lr: 0.001
Epoch [124/38], Training Loss: 26.9993, Validation Loss Current: 8.0664, Validation Loss AVG: 8.0664, lr: 0.001
Epoch [125/38], Training Loss: 26.8625, Validation Loss Current: 8.3061, Validation Loss AVG: 8.3061, lr: 0.001
Epoch [126/38], Training Loss: 26.4860, Validation Loss Current: 9.6258, Validation Loss AVG: 9.6258, lr: 0.001
Epoch [127/38], Training Loss: 37.0856, Validation Loss Current: 9.3817, Validation Loss AVG: 9.3817, lr: 0.001
Epoch [128/38], Training Loss: 33.8847, Validation Loss Current: 8.8486, Validation Loss AVG: 8.8486, lr: 0.001
Epoch [129/38], Training Loss: 31.8449, Validation Loss Current: 9.1884, Validation Loss AVG: 9.1884, lr: 0.001
Epoch [130/38], Training Loss: 34.1306, Validation Loss Current: 8.3258, Validation Loss AVG: 8.3258, lr: 0.001
Epoch [131/38], Training Loss: 27.6367, Validation Loss Current: 8.1259, Validation Loss AVG: 8.1259, lr: 0.001
Epoch [132/38], Training Loss: 27.3010, Validation Loss Current: 8.3134, Validation Loss AVG: 8.3134, lr: 0.001
Epoch [133/38], Training Loss: 29.1852, Validation Loss Current: 8.0204, Validation Loss AVG: 8.0204, lr: 0.001
Epoch [134/38], Training Loss: 26.0572, Validation Loss Current: 8.6107, Validation Loss AVG: 8.6107, lr: 0.001
Epoch [135/38], Training Loss: 24.9340, Validation Loss Current: 8.2271, Validation Loss AVG: 8.2271, lr: 0.001
Epoch [136/38], Training Loss: 26.5479, Validation Loss Current: 8.3179, Validation Loss AVG: 8.3179, lr: 0.001
Epoch [137/38], Training Loss: 26.1123, Validation Loss Current: 7.8734, Validation Loss AVG: 7.8734, lr: 0.001
Epoch [138/38], Training Loss: 24.1582, Validation Loss Current: 8.1870, Validation Loss AVG: 8.1870, lr: 0.001
Epoch [139/38], Training Loss: 24.7046, Validation Loss Current: 8.7632, Validation Loss AVG: 8.7632, lr: 0.001
Epoch [140/38], Training Loss: 26.8515, Validation Loss Current: 8.2480, Validation Loss AVG: 8.2480, lr: 0.001
Epoch [141/38], Training Loss: 24.9931, Validation Loss Current: 8.1161, Validation Loss AVG: 8.1161, lr: 0.001
Epoch [142/38], Training Loss: 24.1711, Validation Loss Current: 8.5421, Validation Loss AVG: 8.5421, lr: 0.001
Epoch [143/38], Training Loss: 24.4463, Validation Loss Current: 7.9767, Validation Loss AVG: 7.9767, lr: 0.001
Epoch [144/38], Training Loss: 22.9079, Validation Loss Current: 7.9178, Validation Loss AVG: 7.9178, lr: 0.001
Epoch [145/38], Training Loss: 23.6391, Validation Loss Current: 8.2907, Validation Loss AVG: 8.2907, lr: 0.001
Epoch [146/38], Training Loss: 24.0443, Validation Loss Current: 9.0801, Validation Loss AVG: 9.0801, lr: 0.001
Epoch [147/38], Training Loss: 25.8396, Validation Loss Current: 8.3099, Validation Loss AVG: 8.3099, lr: 0.001
Epoch [148/38], Training Loss: 24.1085, Validation Loss Current: 8.0128, Validation Loss AVG: 8.0128, lr: 0.001
Epoch [149/38], Training Loss: 22.2181, Validation Loss Current: 8.6749, Validation Loss AVG: 8.6749, lr: 0.001
Epoch [150/38], Training Loss: 25.8175, Validation Loss Current: 8.3514, Validation Loss AVG: 8.3514, lr: 0.001
Patch distance: 0.4 finished training. Best epoch: 137 Best val accuracy: [0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.23026315789473686, 0.23125, 0.23223684210526313, 0.23322368421052628, 0.2325657894736842, 0.2338815789473684, 0.2368421052631579, 0.24375, 0.24539473684210528, 0.2707236842105263, 0.28355263157894733, 0.2855263157894737, 0.29078947368421054, 0.29046052631578945, 0.29375, 0.2960526315789474, 0.30460526315789477, 0.3115131578947369, 0.30296052631578946, 0.3141447368421053, 0.32401315789473684, 0.32664473684210527, 0.3421052631578948, 0.3075657894736842, 0.3335526315789473, 0.3328947368421053, 0.3592105263157895, 0.33322368421052634, 0.3532894736842105, 0.35230263157894737, 0.3651315789473685, 0.37894736842105264, 0.3717105263157895, 0.3519736842105263, 0.35592105263157897, 0.3575657894736842, 0.3575657894736842, 0.3598684210526316, 0.3710526315789474, 0.38421052631578945, 0.3868421052631579, 0.4009868421052631, 0.3115131578947369, 0.25526315789473686, 0.35131578947368414, 0.36217105263157895, 0.3680921052631579, 0.39309210526315785, 0.40822368421052635, 0.3983552631578947, 0.4052631578947369, 0.39342105263157895, 0.42894736842105263, 0.4213815789473684, 0.41546052631578945, 0.35526315789473684, 0.34111842105263157, 0.3516447368421053, 0.4095394736842105, 0.42598684210526316, 0.4161184210526315, 0.4338815789473684, 0.40592105263157896, 0.4358552631578947, 0.4108552631578948, 0.43519736842105267, 0.4519736842105263, 0.4358552631578948, 0.40756578947368427, 0.38125, 0.39078947368421046, 0.41546052631578945, 0.42631578947368426, 0.43256578947368424, 0.4266447368421053, 0.33519736842105263, 0.2822368421052632, 0.32335526315789476, 0.36875, 0.3924342105263158, 0.43453947368421053, 0.4213815789473684, 0.42598684210526316, 0.3901315789473684, 0.42796052631578946, 0.40131578947368424, 0.4358552631578947, 0.43947368421052635, 0.41052631578947374, 0.39309210526315785, 0.43486842105263157, 0.40361842105263157, 0.43355263157894736, 0.44671052631578945, 0.4384868421052632, 0.3980263157894737, 0.4134868421052632, 0.44901315789473684, 0.44506578947368414, 0.42072368421052636] Best val loss: 7.873436450958252


Loaded best state dict for [0.4]
Current group: 0.6
Epoch [1/38], Training Loss: 24.8249, Validation Loss Current: 7.9026, Validation Loss AVG: 7.9026, lr: 0.001
Epoch [2/38], Training Loss: 23.4424, Validation Loss Current: 8.1541, Validation Loss AVG: 8.1541, lr: 0.001
Epoch [3/38], Training Loss: 23.4703, Validation Loss Current: 8.0637, Validation Loss AVG: 8.0637, lr: 0.001
Epoch [4/38], Training Loss: 22.8768, Validation Loss Current: 8.2214, Validation Loss AVG: 8.2214, lr: 0.001
Epoch [5/38], Training Loss: 23.1038, Validation Loss Current: 8.3732, Validation Loss AVG: 8.3732, lr: 0.001
Epoch [6/38], Training Loss: 22.6968, Validation Loss Current: 7.7690, Validation Loss AVG: 7.7690, lr: 0.001
Epoch [7/38], Training Loss: 22.4347, Validation Loss Current: 8.1147, Validation Loss AVG: 8.1147, lr: 0.001
Epoch [8/38], Training Loss: 23.1876, Validation Loss Current: 8.3553, Validation Loss AVG: 8.3553, lr: 0.001
Epoch [9/38], Training Loss: 23.6236, Validation Loss Current: 7.7282, Validation Loss AVG: 7.7282, lr: 0.001
Epoch [10/38], Training Loss: 22.5112, Validation Loss Current: 8.3772, Validation Loss AVG: 8.3772, lr: 0.001
Epoch [11/38], Training Loss: 24.3723, Validation Loss Current: 8.9081, Validation Loss AVG: 8.9081, lr: 0.001
Epoch [12/38], Training Loss: 25.3720, Validation Loss Current: 8.4585, Validation Loss AVG: 8.4585, lr: 0.001
Epoch [13/38], Training Loss: 22.5159, Validation Loss Current: 7.8724, Validation Loss AVG: 7.8724, lr: 0.001
Epoch [14/38], Training Loss: 21.0094, Validation Loss Current: 8.3095, Validation Loss AVG: 8.3095, lr: 0.001
Epoch [15/38], Training Loss: 22.0205, Validation Loss Current: 8.3031, Validation Loss AVG: 8.3031, lr: 0.001
Epoch [16/38], Training Loss: 21.7118, Validation Loss Current: 8.0439, Validation Loss AVG: 8.0439, lr: 0.001
Epoch [17/38], Training Loss: 21.2120, Validation Loss Current: 8.8105, Validation Loss AVG: 8.8105, lr: 0.001
Epoch [18/38], Training Loss: 22.2068, Validation Loss Current: 8.1392, Validation Loss AVG: 8.1392, lr: 0.001
Epoch [19/38], Training Loss: 22.8901, Validation Loss Current: 8.5053, Validation Loss AVG: 8.5053, lr: 0.001
Epoch [20/38], Training Loss: 20.6520, Validation Loss Current: 8.0420, Validation Loss AVG: 8.0420, lr: 0.001
Epoch [21/38], Training Loss: 20.4108, Validation Loss Current: 8.2331, Validation Loss AVG: 8.2331, lr: 0.001
Epoch [22/38], Training Loss: 18.3520, Validation Loss Current: 8.0445, Validation Loss AVG: 8.0445, lr: 0.001
Epoch [23/38], Training Loss: 18.9828, Validation Loss Current: 8.6415, Validation Loss AVG: 8.6415, lr: 0.001
Epoch [24/38], Training Loss: 18.8763, Validation Loss Current: 8.8669, Validation Loss AVG: 8.8669, lr: 0.001
Epoch [25/38], Training Loss: 21.3697, Validation Loss Current: 8.7684, Validation Loss AVG: 8.7684, lr: 0.001
Epoch [26/38], Training Loss: 22.0151, Validation Loss Current: 8.6279, Validation Loss AVG: 8.6279, lr: 0.001
Epoch [27/38], Training Loss: 19.5027, Validation Loss Current: 8.1476, Validation Loss AVG: 8.1476, lr: 0.001
Epoch [28/38], Training Loss: 20.3559, Validation Loss Current: 8.4964, Validation Loss AVG: 8.4964, lr: 0.001
Epoch [29/38], Training Loss: 18.3468, Validation Loss Current: 8.5222, Validation Loss AVG: 8.5222, lr: 0.001
Epoch [30/38], Training Loss: 17.2952, Validation Loss Current: 9.4390, Validation Loss AVG: 9.4390, lr: 0.001
Epoch [31/38], Training Loss: 18.6984, Validation Loss Current: 8.4352, Validation Loss AVG: 8.4352, lr: 0.001
Epoch [32/38], Training Loss: 18.5658, Validation Loss Current: 8.3824, Validation Loss AVG: 8.3824, lr: 0.001
Epoch [33/38], Training Loss: 18.4574, Validation Loss Current: 9.3525, Validation Loss AVG: 9.3525, lr: 0.001
Epoch [34/38], Training Loss: 18.1576, Validation Loss Current: 9.0665, Validation Loss AVG: 9.0665, lr: 0.001
Epoch [35/38], Training Loss: 19.9183, Validation Loss Current: 8.8210, Validation Loss AVG: 8.8210, lr: 0.001
Epoch [36/38], Training Loss: 18.9257, Validation Loss Current: 8.5031, Validation Loss AVG: 8.5031, lr: 0.001
Epoch [37/38], Training Loss: 18.8072, Validation Loss Current: 8.4428, Validation Loss AVG: 8.4428, lr: 0.001
Epoch [38/38], Training Loss: 18.3763, Validation Loss Current: 9.8594, Validation Loss AVG: 9.8594, lr: 0.001
Epoch [39/38], Training Loss: 20.8999, Validation Loss Current: 8.4426, Validation Loss AVG: 8.4426, lr: 0.001
 --- Early Stopped ---
Patch distance: 0.6 finished training. Best epoch: 9 Best val accuracy: [0.4621710526315789, 0.46447368421052626, 0.4631578947368421, 0.46578947368421053, 0.45789473684210524, 0.47993421052631574, 0.4519736842105263, 0.4430921052631579, 0.4756578947368421, 0.45032894736842105, 0.43881578947368427, 0.4631578947368421, 0.4740131578947368, 0.4565789473684211, 0.47072368421052635, 0.4671052631578948, 0.4536184210526316, 0.47006578947368427, 0.44671052631578945, 0.4809210526315789, 0.46940789473684214, 0.4828947368421053, 0.47730263157894737, 0.43157894736842106, 0.44703947368421054, 0.4625, 0.46019736842105263, 0.4223684210526316, 0.47269736842105264, 0.4569078947368421, 0.4759868421052632, 0.46611842105263157, 0.4444078947368421, 0.46743421052631573, 0.4608552631578947, 0.46019736842105263, 0.4766447368421053, 0.4197368421052632, 0.44013157894736843] Best val loss: 7.7282390117645265


Loaded best state dict for [0.4, 0.6]
Current group: 0.8
Epoch [1/38], Training Loss: 21.7532, Validation Loss Current: 8.1491, Validation Loss AVG: 8.1491, lr: 0.001
Epoch [2/38], Training Loss: 22.3872, Validation Loss Current: 10.1302, Validation Loss AVG: 10.1302, lr: 0.001
Epoch [3/38], Training Loss: 25.4518, Validation Loss Current: 8.3134, Validation Loss AVG: 8.3134, lr: 0.001
Epoch [4/38], Training Loss: 20.4918, Validation Loss Current: 8.4049, Validation Loss AVG: 8.4049, lr: 0.001
Epoch [5/38], Training Loss: 18.9408, Validation Loss Current: 8.3525, Validation Loss AVG: 8.3525, lr: 0.001
Epoch [6/38], Training Loss: 19.8671, Validation Loss Current: 9.4160, Validation Loss AVG: 9.4160, lr: 0.001
Epoch [7/38], Training Loss: 23.1448, Validation Loss Current: 7.8491, Validation Loss AVG: 7.8491, lr: 0.001
Epoch [8/38], Training Loss: 19.3395, Validation Loss Current: 8.6070, Validation Loss AVG: 8.6070, lr: 0.001
Epoch [9/38], Training Loss: 19.4658, Validation Loss Current: 9.0542, Validation Loss AVG: 9.0542, lr: 0.001
Epoch [10/38], Training Loss: 22.3580, Validation Loss Current: 8.8324, Validation Loss AVG: 8.8324, lr: 0.001
Epoch [11/38], Training Loss: 19.7227, Validation Loss Current: 9.1802, Validation Loss AVG: 9.1802, lr: 0.001
Epoch [12/38], Training Loss: 19.5975, Validation Loss Current: 8.5543, Validation Loss AVG: 8.5543, lr: 0.001
Epoch [13/38], Training Loss: 16.8980, Validation Loss Current: 9.1220, Validation Loss AVG: 9.1220, lr: 0.001
Epoch [14/38], Training Loss: 16.5267, Validation Loss Current: 9.3215, Validation Loss AVG: 9.3215, lr: 0.001
Epoch [15/38], Training Loss: 15.8650, Validation Loss Current: 9.4402, Validation Loss AVG: 9.4402, lr: 0.001
Epoch [16/38], Training Loss: 14.8384, Validation Loss Current: 9.6912, Validation Loss AVG: 9.6912, lr: 0.001
Epoch [17/38], Training Loss: 14.3716, Validation Loss Current: 10.0773, Validation Loss AVG: 10.0773, lr: 0.001
Epoch [18/38], Training Loss: 15.0480, Validation Loss Current: 10.5132, Validation Loss AVG: 10.5132, lr: 0.001
Epoch [19/38], Training Loss: 19.3776, Validation Loss Current: 12.6544, Validation Loss AVG: 12.6544, lr: 0.001
Epoch [20/38], Training Loss: 34.2730, Validation Loss Current: 9.5170, Validation Loss AVG: 9.5170, lr: 0.001
Epoch [21/38], Training Loss: 27.6150, Validation Loss Current: 8.3670, Validation Loss AVG: 8.3670, lr: 0.001
Epoch [22/38], Training Loss: 23.0368, Validation Loss Current: 8.4838, Validation Loss AVG: 8.4838, lr: 0.001
Epoch [23/38], Training Loss: 19.9469, Validation Loss Current: 8.5611, Validation Loss AVG: 8.5611, lr: 0.001
Epoch [24/38], Training Loss: 18.5332, Validation Loss Current: 8.6031, Validation Loss AVG: 8.6031, lr: 0.001
Epoch [25/38], Training Loss: 17.4271, Validation Loss Current: 8.6548, Validation Loss AVG: 8.6548, lr: 0.001
Epoch [26/38], Training Loss: 16.1958, Validation Loss Current: 9.4236, Validation Loss AVG: 9.4236, lr: 0.001
Epoch [27/38], Training Loss: 17.3339, Validation Loss Current: 9.8791, Validation Loss AVG: 9.8791, lr: 0.001
Epoch [28/38], Training Loss: 21.9724, Validation Loss Current: 10.2554, Validation Loss AVG: 10.2554, lr: 0.001
Epoch [29/38], Training Loss: 19.8135, Validation Loss Current: 8.5981, Validation Loss AVG: 8.5981, lr: 0.001
Epoch [30/38], Training Loss: 17.2826, Validation Loss Current: 9.1234, Validation Loss AVG: 9.1234, lr: 0.001
Epoch [31/38], Training Loss: 15.9887, Validation Loss Current: 8.8703, Validation Loss AVG: 8.8703, lr: 0.001
Epoch [32/38], Training Loss: 17.9790, Validation Loss Current: 10.0112, Validation Loss AVG: 10.0112, lr: 0.001
Epoch [33/38], Training Loss: 15.8841, Validation Loss Current: 10.2160, Validation Loss AVG: 10.2160, lr: 0.001
Epoch [34/38], Training Loss: 18.3610, Validation Loss Current: 9.3390, Validation Loss AVG: 9.3390, lr: 0.001
Epoch [35/38], Training Loss: 15.3684, Validation Loss Current: 10.6361, Validation Loss AVG: 10.6361, lr: 0.001
Epoch [36/38], Training Loss: 17.1294, Validation Loss Current: 9.8608, Validation Loss AVG: 9.8608, lr: 0.001
Epoch [37/38], Training Loss: 15.4625, Validation Loss Current: 9.6247, Validation Loss AVG: 9.6247, lr: 0.001
 --- Early Stopped ---
Patch distance: 0.8 finished training. Best epoch: 7 Best val accuracy: [0.4720394736842106, 0.393421052631579, 0.47730263157894737, 0.4769736842105264, 0.4884868421052631, 0.4184210526315789, 0.4865131578947368, 0.4983552631578948, 0.43519736842105267, 0.46743421052631584, 0.4375, 0.4648026315789474, 0.4625, 0.47006578947368427, 0.4858552631578948, 0.4756578947368421, 0.45953947368421055, 0.48190789473684215, 0.34638157894736843, 0.28651315789473686, 0.4134868421052632, 0.43717105263157896, 0.4342105263157895, 0.44868421052631574, 0.4743421052631579, 0.468421052631579, 0.3983552631578947, 0.4052631578947368, 0.4832236842105263, 0.4759868421052631, 0.4720394736842105, 0.43684210526315786, 0.4391447368421053, 0.45625, 0.47697368421052627, 0.47467105263157894, 0.45032894736842105] Best val loss: 7.84907603263855


Loaded best state dict for [0.4, 0.6, 0.8]
Current group: 1
Epoch [1/38], Training Loss: 18.4787, Validation Loss Current: 6.2031, Validation Loss AVG: 9.7067, lr: 0.001
Epoch [2/38], Training Loss: 17.7450, Validation Loss Current: 6.3324, Validation Loss AVG: 10.1753, lr: 0.001
Epoch [3/38], Training Loss: 15.9771, Validation Loss Current: 6.3996, Validation Loss AVG: 10.8465, lr: 0.001
Epoch [4/38], Training Loss: 15.1634, Validation Loss Current: 6.2725, Validation Loss AVG: 10.9969, lr: 0.001
Epoch [5/38], Training Loss: 14.6030, Validation Loss Current: 6.6076, Validation Loss AVG: 11.2372, lr: 0.001
Epoch [6/38], Training Loss: 15.9163, Validation Loss Current: 7.2693, Validation Loss AVG: 13.3044, lr: 0.001
Epoch [7/38], Training Loss: 23.6301, Validation Loss Current: 7.1274, Validation Loss AVG: 10.9257, lr: 0.001
Epoch [8/38], Training Loss: 21.7979, Validation Loss Current: 6.8539, Validation Loss AVG: 10.3080, lr: 0.001
Epoch [9/38], Training Loss: 17.9957, Validation Loss Current: 6.1275, Validation Loss AVG: 10.1827, lr: 0.001
Epoch [10/38], Training Loss: 15.7892, Validation Loss Current: 6.7903, Validation Loss AVG: 13.1872, lr: 0.001
Epoch [11/38], Training Loss: 14.8877, Validation Loss Current: 6.0457, Validation Loss AVG: 11.1549, lr: 0.001
Epoch [12/38], Training Loss: 13.8400, Validation Loss Current: 6.5021, Validation Loss AVG: 12.5944, lr: 0.001
Epoch [13/38], Training Loss: 13.7533, Validation Loss Current: 6.3813, Validation Loss AVG: 12.0691, lr: 0.001
Epoch [14/38], Training Loss: 12.6263, Validation Loss Current: 6.3162, Validation Loss AVG: 12.4124, lr: 0.001
Epoch [15/38], Training Loss: 11.3605, Validation Loss Current: 6.9271, Validation Loss AVG: 12.5417, lr: 0.001
Epoch [16/38], Training Loss: 12.5457, Validation Loss Current: 7.5250, Validation Loss AVG: 14.1931, lr: 0.001
Epoch [17/38], Training Loss: 13.8533, Validation Loss Current: 6.5137, Validation Loss AVG: 11.6168, lr: 0.001
Epoch [18/38], Training Loss: 12.4244, Validation Loss Current: 7.9893, Validation Loss AVG: 11.7464, lr: 0.001
Epoch [19/38], Training Loss: 13.5452, Validation Loss Current: 7.3063, Validation Loss AVG: 12.0628, lr: 0.001
Epoch [20/38], Training Loss: 11.3305, Validation Loss Current: 7.1706, Validation Loss AVG: 13.8808, lr: 0.001
Epoch [21/38], Training Loss: 15.3159, Validation Loss Current: 6.0939, Validation Loss AVG: 10.0398, lr: 0.001
Epoch [22/38], Training Loss: 12.7803, Validation Loss Current: 8.8628, Validation Loss AVG: 18.0659, lr: 0.001
Epoch [23/38], Training Loss: 19.6644, Validation Loss Current: 6.3551, Validation Loss AVG: 12.1824, lr: 0.001
Epoch [24/38], Training Loss: 15.4080, Validation Loss Current: 7.1844, Validation Loss AVG: 13.2890, lr: 0.001
Epoch [25/38], Training Loss: 14.3341, Validation Loss Current: 6.6293, Validation Loss AVG: 12.5963, lr: 0.001
Epoch [26/38], Training Loss: 10.8134, Validation Loss Current: 7.2930, Validation Loss AVG: 12.0526, lr: 0.001
Epoch [27/38], Training Loss: 9.8312, Validation Loss Current: 7.3535, Validation Loss AVG: 13.0297, lr: 0.001
Epoch [28/38], Training Loss: 8.7410, Validation Loss Current: 7.8076, Validation Loss AVG: 14.0739, lr: 0.001
Epoch [29/38], Training Loss: 8.9829, Validation Loss Current: 6.9488, Validation Loss AVG: 13.3006, lr: 0.001
Epoch [30/38], Training Loss: 8.3677, Validation Loss Current: 8.9767, Validation Loss AVG: 20.4673, lr: 0.001
Epoch [31/38], Training Loss: 15.2278, Validation Loss Current: 8.2443, Validation Loss AVG: 12.1049, lr: 0.001
Epoch [32/38], Training Loss: 11.8086, Validation Loss Current: 7.1797, Validation Loss AVG: 12.9635, lr: 0.001
Epoch [33/38], Training Loss: 8.8977, Validation Loss Current: 7.1607, Validation Loss AVG: 14.6149, lr: 0.001
Epoch [34/38], Training Loss: 8.0862, Validation Loss Current: 8.3023, Validation Loss AVG: 14.9313, lr: 0.001
Epoch [35/38], Training Loss: 10.3004, Validation Loss Current: 8.6968, Validation Loss AVG: 14.2363, lr: 0.001
Epoch [36/38], Training Loss: 10.0782, Validation Loss Current: 8.3841, Validation Loss AVG: 14.6889, lr: 0.001
Epoch [37/38], Training Loss: 13.6265, Validation Loss Current: 7.1751, Validation Loss AVG: 14.0854, lr: 0.001
Epoch [38/38], Training Loss: 14.6899, Validation Loss Current: 7.0790, Validation Loss AVG: 12.7091, lr: 0.001
Epoch [39/38], Training Loss: 10.0509, Validation Loss Current: 7.6250, Validation Loss AVG: 14.5360, lr: 0.001
Epoch [40/38], Training Loss: 7.6611, Validation Loss Current: 7.2700, Validation Loss AVG: 16.2405, lr: 0.001
Epoch [41/38], Training Loss: 6.3033, Validation Loss Current: 7.8369, Validation Loss AVG: 16.6695, lr: 0.001
 --- Early Stopped ---
Patch distance: 1 finished training. Best epoch: 11 Best val accuracy: [0.6167763157894737, 0.6134868421052632, 0.600328947368421, 0.6134868421052632, 0.6200657894736842, 0.569078947368421, 0.5641447368421053, 0.5542763157894737, 0.5855263157894737, 0.6069078947368421, 0.6266447368421053, 0.5789473684210527, 0.6167763157894737, 0.6052631578947368, 0.6233552631578947, 0.6200657894736842, 0.5855263157894737, 0.6134868421052632, 0.6200657894736842, 0.6282894736842105, 0.631578947368421, 0.5345394736842105, 0.6085526315789473, 0.6118421052631579, 0.6151315789473685, 0.6233552631578947, 0.618421052631579, 0.6299342105263158, 0.6463815789473685, 0.5953947368421053, 0.5444078947368421, 0.5805921052631579, 0.6167763157894737, 0.6200657894736842, 0.5575657894736842, 0.5427631578947368, 0.5657894736842105, 0.59375, 0.6134868421052632, 0.6118421052631579, 0.6282894736842105] Best val loss: 6.04572594165802


----- Training alexnet with sequence: [0.6, 0.8, 1] -----
Current group: 0.6
Epoch [1/50], Training Loss: 41.5862, Validation Loss Current: 10.3834, Validation Loss AVG: 10.3834, lr: 0.001
Epoch [2/50], Training Loss: 41.4354, Validation Loss Current: 10.3508, Validation Loss AVG: 10.3508, lr: 0.001
Epoch [3/50], Training Loss: 41.2446, Validation Loss Current: 10.3091, Validation Loss AVG: 10.3091, lr: 0.001
Epoch [4/50], Training Loss: 41.0326, Validation Loss Current: 10.2610, Validation Loss AVG: 10.2610, lr: 0.001
Epoch [5/50], Training Loss: 40.8858, Validation Loss Current: 10.2023, Validation Loss AVG: 10.2023, lr: 0.001
Epoch [6/50], Training Loss: 40.6151, Validation Loss Current: 10.1212, Validation Loss AVG: 10.1212, lr: 0.001
Epoch [7/50], Training Loss: 40.1874, Validation Loss Current: 10.0528, Validation Loss AVG: 10.0528, lr: 0.001
Epoch [8/50], Training Loss: 39.7949, Validation Loss Current: 10.0261, Validation Loss AVG: 10.0261, lr: 0.001
Epoch [9/50], Training Loss: 39.8598, Validation Loss Current: 10.0223, Validation Loss AVG: 10.0223, lr: 0.001
Epoch [10/50], Training Loss: 39.9314, Validation Loss Current: 10.0133, Validation Loss AVG: 10.0133, lr: 0.001
Epoch [11/50], Training Loss: 39.9444, Validation Loss Current: 9.9998, Validation Loss AVG: 9.9998, lr: 0.001
Epoch [12/50], Training Loss: 39.9630, Validation Loss Current: 10.0270, Validation Loss AVG: 10.0270, lr: 0.001
Epoch [13/50], Training Loss: 40.2250, Validation Loss Current: 10.0238, Validation Loss AVG: 10.0238, lr: 0.001
Epoch [14/50], Training Loss: 39.8847, Validation Loss Current: 9.9999, Validation Loss AVG: 9.9999, lr: 0.001
Epoch [15/50], Training Loss: 40.2348, Validation Loss Current: 9.9929, Validation Loss AVG: 9.9929, lr: 0.001
Epoch [16/50], Training Loss: 39.8162, Validation Loss Current: 10.0039, Validation Loss AVG: 10.0039, lr: 0.001
Epoch [17/50], Training Loss: 39.6396, Validation Loss Current: 9.9687, Validation Loss AVG: 9.9687, lr: 0.001
Epoch [18/50], Training Loss: 39.9226, Validation Loss Current: 9.9878, Validation Loss AVG: 9.9878, lr: 0.001
Epoch [19/50], Training Loss: 39.6675, Validation Loss Current: 10.0063, Validation Loss AVG: 10.0063, lr: 0.001
Epoch [20/50], Training Loss: 39.4786, Validation Loss Current: 9.9986, Validation Loss AVG: 9.9986, lr: 0.001
Epoch [21/50], Training Loss: 39.5586, Validation Loss Current: 9.9708, Validation Loss AVG: 9.9708, lr: 0.001
Epoch [22/50], Training Loss: 39.8217, Validation Loss Current: 9.9750, Validation Loss AVG: 9.9750, lr: 0.001
Epoch [23/50], Training Loss: 39.7866, Validation Loss Current: 9.9636, Validation Loss AVG: 9.9636, lr: 0.001
Epoch [24/50], Training Loss: 40.2295, Validation Loss Current: 9.9594, Validation Loss AVG: 9.9594, lr: 0.001
Epoch [25/50], Training Loss: 39.3011, Validation Loss Current: 9.9409, Validation Loss AVG: 9.9409, lr: 0.001
Epoch [26/50], Training Loss: 39.4286, Validation Loss Current: 9.9370, Validation Loss AVG: 9.9370, lr: 0.001
Epoch [27/50], Training Loss: 39.6640, Validation Loss Current: 9.9380, Validation Loss AVG: 9.9380, lr: 0.001
Epoch [28/50], Training Loss: 39.1494, Validation Loss Current: 9.9610, Validation Loss AVG: 9.9610, lr: 0.001
Epoch [29/50], Training Loss: 39.7138, Validation Loss Current: 9.9331, Validation Loss AVG: 9.9331, lr: 0.001
Epoch [30/50], Training Loss: 39.6266, Validation Loss Current: 9.9351, Validation Loss AVG: 9.9351, lr: 0.001
Epoch [31/50], Training Loss: 38.7892, Validation Loss Current: 9.9501, Validation Loss AVG: 9.9501, lr: 0.001
Epoch [32/50], Training Loss: 39.6349, Validation Loss Current: 9.9275, Validation Loss AVG: 9.9275, lr: 0.001
Epoch [33/50], Training Loss: 39.8369, Validation Loss Current: 9.9241, Validation Loss AVG: 9.9241, lr: 0.001
Epoch [34/50], Training Loss: 39.0295, Validation Loss Current: 9.9140, Validation Loss AVG: 9.9140, lr: 0.001
Epoch [35/50], Training Loss: 39.3336, Validation Loss Current: 9.8755, Validation Loss AVG: 9.8755, lr: 0.001
Epoch [36/50], Training Loss: 39.0864, Validation Loss Current: 9.8859, Validation Loss AVG: 9.8859, lr: 0.001
Epoch [37/50], Training Loss: 39.0102, Validation Loss Current: 9.8273, Validation Loss AVG: 9.8273, lr: 0.001
Epoch [38/50], Training Loss: 39.3677, Validation Loss Current: 9.8197, Validation Loss AVG: 9.8197, lr: 0.001
Epoch [39/50], Training Loss: 38.7892, Validation Loss Current: 9.8011, Validation Loss AVG: 9.8011, lr: 0.001
Epoch [40/50], Training Loss: 38.8583, Validation Loss Current: 9.7624, Validation Loss AVG: 9.7624, lr: 0.001
Epoch [41/50], Training Loss: 38.5289, Validation Loss Current: 9.7865, Validation Loss AVG: 9.7865, lr: 0.001
Epoch [42/50], Training Loss: 38.0531, Validation Loss Current: 9.7111, Validation Loss AVG: 9.7111, lr: 0.001
Epoch [43/50], Training Loss: 38.6305, Validation Loss Current: 9.7047, Validation Loss AVG: 9.7047, lr: 0.001
Epoch [44/50], Training Loss: 38.5054, Validation Loss Current: 9.6869, Validation Loss AVG: 9.6869, lr: 0.001
Epoch [45/50], Training Loss: 38.0979, Validation Loss Current: 9.5807, Validation Loss AVG: 9.5807, lr: 0.001
Epoch [46/50], Training Loss: 37.5450, Validation Loss Current: 9.5534, Validation Loss AVG: 9.5534, lr: 0.001
Epoch [47/50], Training Loss: 37.1576, Validation Loss Current: 9.7993, Validation Loss AVG: 9.7993, lr: 0.001
Epoch [48/50], Training Loss: 37.6854, Validation Loss Current: 9.4370, Validation Loss AVG: 9.4370, lr: 0.001
Epoch [49/50], Training Loss: 35.9400, Validation Loss Current: 9.4675, Validation Loss AVG: 9.4675, lr: 0.001
Epoch [50/50], Training Loss: 36.0964, Validation Loss Current: 9.1836, Validation Loss AVG: 9.1836, lr: 0.001
Epoch [51/50], Training Loss: 36.5023, Validation Loss Current: 9.0788, Validation Loss AVG: 9.0788, lr: 0.001
Epoch [52/50], Training Loss: 35.6606, Validation Loss Current: 9.1003, Validation Loss AVG: 9.1003, lr: 0.001
Epoch [53/50], Training Loss: 34.0601, Validation Loss Current: 9.0320, Validation Loss AVG: 9.0320, lr: 0.001
Epoch [54/50], Training Loss: 33.8746, Validation Loss Current: 13.6049, Validation Loss AVG: 13.6049, lr: 0.001
Epoch [55/50], Training Loss: 39.0558, Validation Loss Current: 9.1866, Validation Loss AVG: 9.1866, lr: 0.001
Epoch [56/50], Training Loss: 35.0287, Validation Loss Current: 9.0143, Validation Loss AVG: 9.0143, lr: 0.001
Epoch [57/50], Training Loss: 34.5262, Validation Loss Current: 8.7510, Validation Loss AVG: 8.7510, lr: 0.001
Epoch [58/50], Training Loss: 33.6089, Validation Loss Current: 9.2665, Validation Loss AVG: 9.2665, lr: 0.001
Epoch [59/50], Training Loss: 34.3925, Validation Loss Current: 8.7200, Validation Loss AVG: 8.7200, lr: 0.001
Epoch [60/50], Training Loss: 32.5905, Validation Loss Current: 8.6819, Validation Loss AVG: 8.6819, lr: 0.001
Epoch [61/50], Training Loss: 31.7692, Validation Loss Current: 8.6892, Validation Loss AVG: 8.6892, lr: 0.001
Epoch [62/50], Training Loss: 32.0873, Validation Loss Current: 8.8406, Validation Loss AVG: 8.8406, lr: 0.001
