Loading openmpi/cuda/64/3.1.4
  Loading requirement: hpcx/2.4.0
Loading pytorch-py36-cuda10.1-gcc/1.5.0
  Loading requirement: python36 ml-pythondeps-py36-cuda10.1-gcc/3.3.0
    openblas/dynamic/0.2.20 cudnn7.6-cuda10.1/7.6.5.32 hdf5_18/1.8.20
    nccl2-cuda10.1-gcc/2.7.8
Run:  0
 # ------------------ Running pipeline on stb_endsame color run_0 -------------------- #
cuda:0
 ------ Pipeline with following parameters ------
training_root_path :  /u/erdos/students/xcui32/cnslab/datasets/VOC2012/VOC2012_filtered/train
val_root_path :  /u/erdos/students/xcui32/cnslab/datasets/VOC2012/VOC2012_filtered/val
test_root_path :  /u/erdos/students/xcui32/cnslab/datasets/VOC2012/VOC2012_filtered/test
dataset_name :  VOC
target_distances :  [0.2, 0.4, 0.6, 0.8, 1]
training_mode :  stb_endsame
training_size :  None
background :  color
size :  (150, 150)
cls_to_use :  ['aeroplane', 'bicycle', 'bird', 'boat', 'car', 'cat', 'train', 'tvmonitor']
batch_size :  128
epochs :  150
resize_method :  long
n_folds :  5
num_workers :  16
model_name :  alexnet
device :  cuda:0
random_seed :  40
result_dirpath :  /u/erdos/students/xcui32/cnslab/results/VOC8AlexnetBlackCURstblr-adjusted
save_checkpoints :  False
save_progress_checkpoints :  False
verbose :  0
 ---  Loading datasets ---
 ---  Running  ---
Parameters: --------------------
{'scheduler_kwargs': {'mode': 'min', 'factor': 0.1, 'patience': 5}, 'optim_kwargs': {'lr': 0.1, 'momentum': 0.9}, 'max_norm': 2, 'val_target': 'current', 'patience': 30, 'early_stopping': True, 'scheduler_object': <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>, 'optimizer_object': <class 'torch.optim.sgd.SGD'>, 'criterion_object': <class 'torch.nn.modules.loss.CrossEntropyLoss'>, 'self': <pipelineCV2.RunModel object at 0x2aac786f5d68>}
--------------------
Fold: 0
----- Training alexnet with sequence: [0.2, 0.4, 0.6, 0.8, 1] -----
Current group: 0.2
Epoch [1/30], Training Loss: 40.1531, Validation Loss Current: 10.0162, Validation Loss AVG: 10.0162, lr: 0.1
Epoch [2/30], Training Loss: 40.5697, Validation Loss Current: 10.0532, Validation Loss AVG: 10.0532, lr: 0.1
Epoch [3/30], Training Loss: 40.6321, Validation Loss Current: 10.0914, Validation Loss AVG: 10.0914, lr: 0.1
Epoch [4/30], Training Loss: 40.2586, Validation Loss Current: 10.0602, Validation Loss AVG: 10.0602, lr: 0.1
Epoch [5/30], Training Loss: 39.9407, Validation Loss Current: 10.0301, Validation Loss AVG: 10.0301, lr: 0.1
Epoch [6/30], Training Loss: 40.5244, Validation Loss Current: 10.0221, Validation Loss AVG: 10.0221, lr: 0.1
Epoch [7/30], Training Loss: 40.7822, Validation Loss Current: 10.0648, Validation Loss AVG: 10.0648, lr: 0.1
Epoch [8/30], Training Loss: 40.0433, Validation Loss Current: 10.0602, Validation Loss AVG: 10.0602, lr: 0.010000000000000002
Epoch [9/30], Training Loss: 39.9998, Validation Loss Current: 10.0308, Validation Loss AVG: 10.0308, lr: 0.010000000000000002
Epoch [10/30], Training Loss: 39.3465, Validation Loss Current: 9.9382, Validation Loss AVG: 9.9382, lr: 0.010000000000000002
Epoch [11/30], Training Loss: 39.1531, Validation Loss Current: 9.9533, Validation Loss AVG: 9.9533, lr: 0.010000000000000002
Epoch [12/30], Training Loss: 38.8341, Validation Loss Current: 9.8965, Validation Loss AVG: 9.8965, lr: 0.010000000000000002
Epoch [13/30], Training Loss: 37.6816, Validation Loss Current: 10.1642, Validation Loss AVG: 10.1642, lr: 0.010000000000000002
Epoch [14/30], Training Loss: 35.4005, Validation Loss Current: 9.9905, Validation Loss AVG: 9.9905, lr: 0.010000000000000002
Epoch [15/30], Training Loss: 36.1319, Validation Loss Current: 10.2151, Validation Loss AVG: 10.2151, lr: 0.010000000000000002
Epoch [16/30], Training Loss: 35.1722, Validation Loss Current: 9.8844, Validation Loss AVG: 9.8844, lr: 0.010000000000000002
Epoch [17/30], Training Loss: 34.6391, Validation Loss Current: 9.9860, Validation Loss AVG: 9.9860, lr: 0.010000000000000002
Epoch [18/30], Training Loss: 33.4335, Validation Loss Current: 10.1622, Validation Loss AVG: 10.1622, lr: 0.010000000000000002
Epoch [19/30], Training Loss: 33.6841, Validation Loss Current: 10.1370, Validation Loss AVG: 10.1370, lr: 0.010000000000000002
Epoch [20/30], Training Loss: 33.9651, Validation Loss Current: 9.9806, Validation Loss AVG: 9.9806, lr: 0.010000000000000002
Epoch [21/30], Training Loss: 34.4251, Validation Loss Current: 10.7029, Validation Loss AVG: 10.7029, lr: 0.010000000000000002
Epoch [22/30], Training Loss: 33.1381, Validation Loss Current: 10.8443, Validation Loss AVG: 10.8443, lr: 0.010000000000000002
Epoch [23/30], Training Loss: 31.4434, Validation Loss Current: 9.9311, Validation Loss AVG: 9.9311, lr: 0.0010000000000000002
Epoch [24/30], Training Loss: 31.7125, Validation Loss Current: 10.3631, Validation Loss AVG: 10.3631, lr: 0.0010000000000000002
Epoch [25/30], Training Loss: 30.5987, Validation Loss Current: 10.0917, Validation Loss AVG: 10.0917, lr: 0.0010000000000000002
Epoch [26/30], Training Loss: 30.1815, Validation Loss Current: 10.3547, Validation Loss AVG: 10.3547, lr: 0.0010000000000000002
Epoch [27/30], Training Loss: 29.9418, Validation Loss Current: 10.3266, Validation Loss AVG: 10.3266, lr: 0.0010000000000000002
Epoch [28/30], Training Loss: 30.3985, Validation Loss Current: 10.6312, Validation Loss AVG: 10.6312, lr: 0.0010000000000000002
Epoch [29/30], Training Loss: 31.4121, Validation Loss Current: 10.5111, Validation Loss AVG: 10.5111, lr: 0.00010000000000000003
Epoch [30/30], Training Loss: 31.3837, Validation Loss Current: 10.5042, Validation Loss AVG: 10.5042, lr: 0.00010000000000000003
Epoch [31/30], Training Loss: 30.3634, Validation Loss Current: 10.4676, Validation Loss AVG: 10.4676, lr: 0.00010000000000000003
Epoch [32/30], Training Loss: 29.4622, Validation Loss Current: 10.4596, Validation Loss AVG: 10.4596, lr: 0.00010000000000000003
Epoch [33/30], Training Loss: 30.6804, Validation Loss Current: 10.4666, Validation Loss AVG: 10.4666, lr: 0.00010000000000000003
Epoch [34/30], Training Loss: 30.4964, Validation Loss Current: 10.4678, Validation Loss AVG: 10.4678, lr: 0.00010000000000000003
Epoch [35/30], Training Loss: 30.1175, Validation Loss Current: 10.5009, Validation Loss AVG: 10.5009, lr: 1.0000000000000004e-05
Epoch [36/30], Training Loss: 30.1756, Validation Loss Current: 10.4479, Validation Loss AVG: 10.4479, lr: 1.0000000000000004e-05
Epoch [37/30], Training Loss: 31.0996, Validation Loss Current: 10.4480, Validation Loss AVG: 10.4480, lr: 1.0000000000000004e-05
Epoch [38/30], Training Loss: 31.3698, Validation Loss Current: 10.4564, Validation Loss AVG: 10.4564, lr: 1.0000000000000004e-05
Epoch [39/30], Training Loss: 30.3885, Validation Loss Current: 10.4678, Validation Loss AVG: 10.4678, lr: 1.0000000000000004e-05
Epoch [40/30], Training Loss: 29.5383, Validation Loss Current: 10.4698, Validation Loss AVG: 10.4698, lr: 1.0000000000000004e-05
Epoch [41/30], Training Loss: 29.9783, Validation Loss Current: 10.4768, Validation Loss AVG: 10.4768, lr: 1.0000000000000004e-06
Epoch [42/30], Training Loss: 29.1247, Validation Loss Current: 10.4887, Validation Loss AVG: 10.4887, lr: 1.0000000000000004e-06
Epoch [43/30], Training Loss: 30.4256, Validation Loss Current: 10.4502, Validation Loss AVG: 10.4502, lr: 1.0000000000000004e-06
Epoch [44/30], Training Loss: 31.1032, Validation Loss Current: 10.4652, Validation Loss AVG: 10.4652, lr: 1.0000000000000004e-06
Epoch [45/30], Training Loss: 29.9123, Validation Loss Current: 10.4614, Validation Loss AVG: 10.4614, lr: 1.0000000000000004e-06
Epoch [46/30], Training Loss: 30.5568, Validation Loss Current: 10.4595, Validation Loss AVG: 10.4595, lr: 1.0000000000000004e-06
 --- Early Stopped ---
Patch distance: 0.2 finished training. Best epoch: 16 Best val accuracy: [0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.23421052631578948, 0.2638157894736842, 0.23223684210526313, 0.2967105263157895, 0.3003289473684211, 0.26184210526315793, 0.2509868421052632, 0.2582236842105263, 0.2536184210526316, 0.2555921052631579, 0.2476973684210526, 0.22532894736842102, 0.28157894736842104, 0.2625, 0.2825657894736842, 0.27401315789473685, 0.26875, 0.2588815789473684, 0.26348684210526313, 0.26414473684210527, 0.26611842105263156, 0.2641447368421052, 0.26480263157894735, 0.2654605263157895, 0.2654605263157895, 0.2657894736842105, 0.2651315789473684, 0.2661184210526316, 0.2651315789473684, 0.2654605263157895, 0.2654605263157895, 0.2654605263157895, 0.2654605263157895, 0.2654605263157895, 0.2654605263157895, 0.2654605263157895] Best val loss: 9.88441960811615


Loaded best state dict for [0.2]
Current group: 0.4
Epoch [1/30], Training Loss: 41.4107, Validation Loss Current: 10.0194, Validation Loss AVG: 10.0194, lr: 0.1
Epoch [2/30], Training Loss: 39.3547, Validation Loss Current: 10.6260, Validation Loss AVG: 10.6260, lr: 0.1
Epoch [3/30], Training Loss: 38.7138, Validation Loss Current: 14.6307, Validation Loss AVG: 14.6307, lr: 0.1
Epoch [4/30], Training Loss: 39.3694, Validation Loss Current: 11.4402, Validation Loss AVG: 11.4402, lr: 0.1
Epoch [5/30], Training Loss: 38.1285, Validation Loss Current: 9.8151, Validation Loss AVG: 9.8151, lr: 0.1
Epoch [6/30], Training Loss: 37.6833, Validation Loss Current: 10.3795, Validation Loss AVG: 10.3795, lr: 0.1
Epoch [7/30], Training Loss: 38.2062, Validation Loss Current: 11.4825, Validation Loss AVG: 11.4825, lr: 0.1
Epoch [8/30], Training Loss: 37.5709, Validation Loss Current: 9.6953, Validation Loss AVG: 9.6953, lr: 0.1
Epoch [9/30], Training Loss: 35.2640, Validation Loss Current: 11.7471, Validation Loss AVG: 11.7471, lr: 0.1
Epoch [10/30], Training Loss: 36.7896, Validation Loss Current: 9.4110, Validation Loss AVG: 9.4110, lr: 0.1
Epoch [11/30], Training Loss: 34.6597, Validation Loss Current: 12.9540, Validation Loss AVG: 12.9540, lr: 0.1
Epoch [12/30], Training Loss: 37.2111, Validation Loss Current: 9.6016, Validation Loss AVG: 9.6016, lr: 0.1
Epoch [13/30], Training Loss: 35.3407, Validation Loss Current: 10.5709, Validation Loss AVG: 10.5709, lr: 0.1
Epoch [14/30], Training Loss: 35.5336, Validation Loss Current: 9.5826, Validation Loss AVG: 9.5826, lr: 0.1
Epoch [15/30], Training Loss: 35.1819, Validation Loss Current: 9.2965, Validation Loss AVG: 9.2965, lr: 0.1
Epoch [16/30], Training Loss: 33.8722, Validation Loss Current: 9.4658, Validation Loss AVG: 9.4658, lr: 0.1
Epoch [17/30], Training Loss: 34.1142, Validation Loss Current: 9.5669, Validation Loss AVG: 9.5669, lr: 0.1
Epoch [18/30], Training Loss: 35.0565, Validation Loss Current: 9.8822, Validation Loss AVG: 9.8822, lr: 0.1
Epoch [19/30], Training Loss: 35.2026, Validation Loss Current: 9.3825, Validation Loss AVG: 9.3825, lr: 0.1
Epoch [20/30], Training Loss: 33.6742, Validation Loss Current: 9.3151, Validation Loss AVG: 9.3151, lr: 0.1
Epoch [21/30], Training Loss: 35.0346, Validation Loss Current: 9.0581, Validation Loss AVG: 9.0581, lr: 0.1
Epoch [22/30], Training Loss: 32.9957, Validation Loss Current: 8.9168, Validation Loss AVG: 8.9168, lr: 0.1
Epoch [23/30], Training Loss: 33.0565, Validation Loss Current: 9.1201, Validation Loss AVG: 9.1201, lr: 0.1
Epoch [24/30], Training Loss: 33.2885, Validation Loss Current: 9.4027, Validation Loss AVG: 9.4027, lr: 0.1
Epoch [25/30], Training Loss: 33.2104, Validation Loss Current: 10.8044, Validation Loss AVG: 10.8044, lr: 0.1
Epoch [26/30], Training Loss: 32.2059, Validation Loss Current: 14.0821, Validation Loss AVG: 14.0821, lr: 0.1
Epoch [27/30], Training Loss: 34.2783, Validation Loss Current: 9.1019, Validation Loss AVG: 9.1019, lr: 0.1
Epoch [28/30], Training Loss: 31.7091, Validation Loss Current: 9.8697, Validation Loss AVG: 9.8697, lr: 0.1
Epoch [29/30], Training Loss: 29.4215, Validation Loss Current: 9.4025, Validation Loss AVG: 9.4025, lr: 0.010000000000000002
Epoch [30/30], Training Loss: 27.3244, Validation Loss Current: 9.0050, Validation Loss AVG: 9.0050, lr: 0.010000000000000002
Epoch [31/30], Training Loss: 25.6626, Validation Loss Current: 8.8466, Validation Loss AVG: 8.8466, lr: 0.010000000000000002
Epoch [32/30], Training Loss: 25.6780, Validation Loss Current: 9.3080, Validation Loss AVG: 9.3080, lr: 0.010000000000000002
Epoch [33/30], Training Loss: 23.0646, Validation Loss Current: 9.1793, Validation Loss AVG: 9.1793, lr: 0.010000000000000002
Epoch [34/30], Training Loss: 22.8225, Validation Loss Current: 9.7781, Validation Loss AVG: 9.7781, lr: 0.010000000000000002
Epoch [35/30], Training Loss: 23.1922, Validation Loss Current: 9.5534, Validation Loss AVG: 9.5534, lr: 0.010000000000000002
Epoch [36/30], Training Loss: 22.4373, Validation Loss Current: 9.6577, Validation Loss AVG: 9.6577, lr: 0.010000000000000002
Epoch [37/30], Training Loss: 21.6798, Validation Loss Current: 10.0806, Validation Loss AVG: 10.0806, lr: 0.010000000000000002
Epoch [38/30], Training Loss: 20.2737, Validation Loss Current: 10.1077, Validation Loss AVG: 10.1077, lr: 0.0010000000000000002
Epoch [39/30], Training Loss: 19.3763, Validation Loss Current: 10.1273, Validation Loss AVG: 10.1273, lr: 0.0010000000000000002
Epoch [40/30], Training Loss: 18.7934, Validation Loss Current: 10.2633, Validation Loss AVG: 10.2633, lr: 0.0010000000000000002
Epoch [41/30], Training Loss: 18.6952, Validation Loss Current: 10.3289, Validation Loss AVG: 10.3289, lr: 0.0010000000000000002
Epoch [42/30], Training Loss: 19.2124, Validation Loss Current: 10.3258, Validation Loss AVG: 10.3258, lr: 0.0010000000000000002
Epoch [43/30], Training Loss: 19.3078, Validation Loss Current: 10.3954, Validation Loss AVG: 10.3954, lr: 0.0010000000000000002
Epoch [44/30], Training Loss: 18.7129, Validation Loss Current: 10.3753, Validation Loss AVG: 10.3753, lr: 0.00010000000000000003
Epoch [45/30], Training Loss: 20.2319, Validation Loss Current: 10.4033, Validation Loss AVG: 10.4033, lr: 0.00010000000000000003
Epoch [46/30], Training Loss: 19.1990, Validation Loss Current: 10.3997, Validation Loss AVG: 10.3997, lr: 0.00010000000000000003
Epoch [47/30], Training Loss: 17.9750, Validation Loss Current: 10.4321, Validation Loss AVG: 10.4321, lr: 0.00010000000000000003
Epoch [48/30], Training Loss: 19.0554, Validation Loss Current: 10.4588, Validation Loss AVG: 10.4588, lr: 0.00010000000000000003
Epoch [49/30], Training Loss: 17.6871, Validation Loss Current: 10.4509, Validation Loss AVG: 10.4509, lr: 0.00010000000000000003
Epoch [50/30], Training Loss: 18.2469, Validation Loss Current: 10.4387, Validation Loss AVG: 10.4387, lr: 1.0000000000000004e-05
Epoch [51/30], Training Loss: 18.0821, Validation Loss Current: 10.4334, Validation Loss AVG: 10.4334, lr: 1.0000000000000004e-05
Epoch [52/30], Training Loss: 18.8359, Validation Loss Current: 10.4446, Validation Loss AVG: 10.4446, lr: 1.0000000000000004e-05
Epoch [53/30], Training Loss: 18.5537, Validation Loss Current: 10.4494, Validation Loss AVG: 10.4494, lr: 1.0000000000000004e-05
Epoch [54/30], Training Loss: 17.7821, Validation Loss Current: 10.4298, Validation Loss AVG: 10.4298, lr: 1.0000000000000004e-05
Epoch [55/30], Training Loss: 17.8925, Validation Loss Current: 10.4258, Validation Loss AVG: 10.4258, lr: 1.0000000000000004e-05
Epoch [56/30], Training Loss: 18.2341, Validation Loss Current: 10.4286, Validation Loss AVG: 10.4286, lr: 1.0000000000000004e-06
Epoch [57/30], Training Loss: 17.6556, Validation Loss Current: 10.4081, Validation Loss AVG: 10.4081, lr: 1.0000000000000004e-06
Epoch [58/30], Training Loss: 18.6444, Validation Loss Current: 10.4369, Validation Loss AVG: 10.4369, lr: 1.0000000000000004e-06
Epoch [59/30], Training Loss: 18.1135, Validation Loss Current: 10.4313, Validation Loss AVG: 10.4313, lr: 1.0000000000000004e-06
Epoch [60/30], Training Loss: 18.2715, Validation Loss Current: 10.4127, Validation Loss AVG: 10.4127, lr: 1.0000000000000004e-06
Epoch [61/30], Training Loss: 17.6789, Validation Loss Current: 10.4372, Validation Loss AVG: 10.4372, lr: 1.0000000000000004e-06
 --- Early Stopped ---
Patch distance: 0.4 finished training. Best epoch: 31 Best val accuracy: [0.22532894736842107, 0.2236842105263158, 0.22434210526315787, 0.28421052631578947, 0.26611842105263156, 0.22532894736842107, 0.24276315789473685, 0.28717105263157894, 0.262828947368421, 0.27401315789473685, 0.2621710526315789, 0.28289473684210525, 0.2832236842105263, 0.2546052631578947, 0.3207236842105264, 0.26875, 0.3111842105263158, 0.28421052631578947, 0.2875, 0.2963815789473684, 0.33125, 0.3286184210526316, 0.3526315789473684, 0.33552631578947373, 0.26348684210526313, 0.31611842105263155, 0.32697368421052636, 0.29210526315789476, 0.3457236842105263, 0.34802631578947374, 0.3572368421052632, 0.35230263157894737, 0.3506578947368421, 0.3526315789473684, 0.3539473684210526, 0.36118421052631583, 0.3486842105263158, 0.356578947368421, 0.35460526315789476, 0.3549342105263158, 0.3542763157894737, 0.3555921052631579, 0.35328947368421054, 0.3549342105263158, 0.35624999999999996, 0.35460526315789476, 0.35460526315789476, 0.35460526315789476, 0.35427631578947366, 0.3542763157894737, 0.3542763157894737, 0.3542763157894737, 0.35460526315789476, 0.35493421052631585, 0.35493421052631585, 0.35493421052631585, 0.35493421052631585, 0.35460526315789476, 0.35493421052631585, 0.35493421052631585, 0.35493421052631585] Best val loss: 8.846565461158752


Loaded best state dict for [0.2, 0.4]
Current group: 0.6
Epoch [1/30], Training Loss: 30.1759, Validation Loss Current: 14.3886, Validation Loss AVG: 14.3886, lr: 0.1
Epoch [2/30], Training Loss: 34.1673, Validation Loss Current: 9.5738, Validation Loss AVG: 9.5738, lr: 0.1
Epoch [3/30], Training Loss: 31.9713, Validation Loss Current: 9.0008, Validation Loss AVG: 9.0008, lr: 0.1
Epoch [4/30], Training Loss: 30.6756, Validation Loss Current: 9.4114, Validation Loss AVG: 9.4114, lr: 0.1
Epoch [5/30], Training Loss: 29.8265, Validation Loss Current: 8.8202, Validation Loss AVG: 8.8202, lr: 0.1
Epoch [6/30], Training Loss: 30.3624, Validation Loss Current: 9.3983, Validation Loss AVG: 9.3983, lr: 0.1
Epoch [7/30], Training Loss: 30.2905, Validation Loss Current: 9.3878, Validation Loss AVG: 9.3878, lr: 0.1
Epoch [8/30], Training Loss: 28.3579, Validation Loss Current: 9.9613, Validation Loss AVG: 9.9613, lr: 0.1
Epoch [9/30], Training Loss: 30.5572, Validation Loss Current: 9.1669, Validation Loss AVG: 9.1669, lr: 0.1
Epoch [10/30], Training Loss: 30.0346, Validation Loss Current: 10.3683, Validation Loss AVG: 10.3683, lr: 0.1
Epoch [11/30], Training Loss: 30.4834, Validation Loss Current: 11.1933, Validation Loss AVG: 11.1933, lr: 0.1
Epoch [12/30], Training Loss: 27.5895, Validation Loss Current: 9.0273, Validation Loss AVG: 9.0273, lr: 0.010000000000000002
Epoch [13/30], Training Loss: 25.1504, Validation Loss Current: 8.8009, Validation Loss AVG: 8.8009, lr: 0.010000000000000002
Epoch [14/30], Training Loss: 23.4564, Validation Loss Current: 9.1309, Validation Loss AVG: 9.1309, lr: 0.010000000000000002
Epoch [15/30], Training Loss: 22.6399, Validation Loss Current: 9.3561, Validation Loss AVG: 9.3561, lr: 0.010000000000000002
Epoch [16/30], Training Loss: 23.9507, Validation Loss Current: 9.4970, Validation Loss AVG: 9.4970, lr: 0.010000000000000002
Epoch [17/30], Training Loss: 20.6333, Validation Loss Current: 9.7049, Validation Loss AVG: 9.7049, lr: 0.010000000000000002
Epoch [18/30], Training Loss: 19.1063, Validation Loss Current: 9.6120, Validation Loss AVG: 9.6120, lr: 0.010000000000000002
Epoch [19/30], Training Loss: 18.1431, Validation Loss Current: 10.5478, Validation Loss AVG: 10.5478, lr: 0.010000000000000002
Epoch [20/30], Training Loss: 17.5214, Validation Loss Current: 10.4301, Validation Loss AVG: 10.4301, lr: 0.0010000000000000002
Epoch [21/30], Training Loss: 16.4954, Validation Loss Current: 10.5378, Validation Loss AVG: 10.5378, lr: 0.0010000000000000002
Epoch [22/30], Training Loss: 16.0812, Validation Loss Current: 10.4843, Validation Loss AVG: 10.4843, lr: 0.0010000000000000002
Epoch [23/30], Training Loss: 16.2938, Validation Loss Current: 10.4938, Validation Loss AVG: 10.4938, lr: 0.0010000000000000002
Epoch [24/30], Training Loss: 17.4090, Validation Loss Current: 10.4934, Validation Loss AVG: 10.4934, lr: 0.0010000000000000002
Epoch [25/30], Training Loss: 16.0350, Validation Loss Current: 10.5989, Validation Loss AVG: 10.5989, lr: 0.0010000000000000002
Epoch [26/30], Training Loss: 16.0536, Validation Loss Current: 10.5037, Validation Loss AVG: 10.5037, lr: 0.00010000000000000003
Epoch [27/30], Training Loss: 15.6116, Validation Loss Current: 10.5962, Validation Loss AVG: 10.5962, lr: 0.00010000000000000003
Epoch [28/30], Training Loss: 15.4518, Validation Loss Current: 10.5773, Validation Loss AVG: 10.5773, lr: 0.00010000000000000003
Epoch [29/30], Training Loss: 15.2505, Validation Loss Current: 10.6392, Validation Loss AVG: 10.6392, lr: 0.00010000000000000003
Epoch [30/30], Training Loss: 15.7602, Validation Loss Current: 10.5820, Validation Loss AVG: 10.5820, lr: 0.00010000000000000003
Epoch [31/30], Training Loss: 16.2155, Validation Loss Current: 10.6442, Validation Loss AVG: 10.6442, lr: 0.00010000000000000003
Epoch [32/30], Training Loss: 16.5942, Validation Loss Current: 10.6481, Validation Loss AVG: 10.6481, lr: 1.0000000000000004e-05
Epoch [33/30], Training Loss: 15.6364, Validation Loss Current: 10.6243, Validation Loss AVG: 10.6243, lr: 1.0000000000000004e-05
Epoch [34/30], Training Loss: 16.1799, Validation Loss Current: 10.6273, Validation Loss AVG: 10.6273, lr: 1.0000000000000004e-05
Epoch [35/30], Training Loss: 15.4501, Validation Loss Current: 10.6012, Validation Loss AVG: 10.6012, lr: 1.0000000000000004e-05
Epoch [36/30], Training Loss: 15.9226, Validation Loss Current: 10.6562, Validation Loss AVG: 10.6562, lr: 1.0000000000000004e-05
Epoch [37/30], Training Loss: 15.5433, Validation Loss Current: 10.6169, Validation Loss AVG: 10.6169, lr: 1.0000000000000004e-05
Epoch [38/30], Training Loss: 15.6643, Validation Loss Current: 10.5888, Validation Loss AVG: 10.5888, lr: 1.0000000000000004e-06
Epoch [39/30], Training Loss: 16.5628, Validation Loss Current: 10.6344, Validation Loss AVG: 10.6344, lr: 1.0000000000000004e-06
Epoch [40/30], Training Loss: 15.4726, Validation Loss Current: 10.6133, Validation Loss AVG: 10.6133, lr: 1.0000000000000004e-06
Epoch [41/30], Training Loss: 15.7477, Validation Loss Current: 10.6055, Validation Loss AVG: 10.6055, lr: 1.0000000000000004e-06
Epoch [42/30], Training Loss: 15.1376, Validation Loss Current: 10.6318, Validation Loss AVG: 10.6318, lr: 1.0000000000000004e-06
Epoch [43/30], Training Loss: 15.0785, Validation Loss Current: 10.6055, Validation Loss AVG: 10.6055, lr: 1.0000000000000004e-06
 --- Early Stopped ---
Patch distance: 0.6 finished training. Best epoch: 13 Best val accuracy: [0.24276315789473685, 0.33289473684210524, 0.28190789473684214, 0.3536184210526316, 0.3674342105263158, 0.3105263157894737, 0.35460526315789476, 0.3088815789473684, 0.3388157894736842, 0.2740131578947369, 0.2621710526315789, 0.36348684210526316, 0.3828947368421053, 0.39342105263157895, 0.38322368421052627, 0.39013157894736844, 0.37302631578947365, 0.3815789473684211, 0.38552631578947366, 0.387828947368421, 0.38651315789473684, 0.3851973684210527, 0.38355263157894737, 0.38421052631578945, 0.37861842105263166, 0.3815789473684211, 0.3805921052631579, 0.37993421052631576, 0.37993421052631576, 0.38125, 0.38223684210526315, 0.38190789473684206, 0.38223684210526315, 0.3825657894736842, 0.38223684210526315, 0.38223684210526315, 0.38223684210526315, 0.3825657894736842, 0.3825657894736842, 0.38190789473684206, 0.3825657894736842, 0.38190789473684206, 0.38190789473684206] Best val loss: 8.800858759880066


Loaded best state dict for [0.2, 0.4, 0.6]
Current group: 0.8
Epoch [1/30], Training Loss: 28.8062, Validation Loss Current: 9.1814, Validation Loss AVG: 9.1814, lr: 0.1
Epoch [2/30], Training Loss: 30.9528, Validation Loss Current: 9.1514, Validation Loss AVG: 9.1514, lr: 0.1
Epoch [3/30], Training Loss: 33.3266, Validation Loss Current: 9.4503, Validation Loss AVG: 9.4503, lr: 0.1
Epoch [4/30], Training Loss: 29.1481, Validation Loss Current: 8.7091, Validation Loss AVG: 8.7091, lr: 0.1
Epoch [5/30], Training Loss: 28.3000, Validation Loss Current: 9.5088, Validation Loss AVG: 9.5088, lr: 0.1
Epoch [6/30], Training Loss: 29.4364, Validation Loss Current: 9.5655, Validation Loss AVG: 9.5655, lr: 0.1
Epoch [7/30], Training Loss: 27.8309, Validation Loss Current: 10.7950, Validation Loss AVG: 10.7950, lr: 0.1
Epoch [8/30], Training Loss: 31.1980, Validation Loss Current: 9.2387, Validation Loss AVG: 9.2387, lr: 0.1
Epoch [9/30], Training Loss: 30.2841, Validation Loss Current: 9.0802, Validation Loss AVG: 9.0802, lr: 0.1
Epoch [10/30], Training Loss: 28.8039, Validation Loss Current: 10.3934, Validation Loss AVG: 10.3934, lr: 0.1
Epoch [11/30], Training Loss: 28.4061, Validation Loss Current: 9.2656, Validation Loss AVG: 9.2656, lr: 0.010000000000000002
Epoch [12/30], Training Loss: 24.2215, Validation Loss Current: 9.4164, Validation Loss AVG: 9.4164, lr: 0.010000000000000002
Epoch [13/30], Training Loss: 22.9477, Validation Loss Current: 9.8230, Validation Loss AVG: 9.8230, lr: 0.010000000000000002
Epoch [14/30], Training Loss: 22.6614, Validation Loss Current: 9.6820, Validation Loss AVG: 9.6820, lr: 0.010000000000000002
Epoch [15/30], Training Loss: 19.5598, Validation Loss Current: 10.0428, Validation Loss AVG: 10.0428, lr: 0.010000000000000002
Epoch [16/30], Training Loss: 20.0110, Validation Loss Current: 10.3890, Validation Loss AVG: 10.3890, lr: 0.010000000000000002
Epoch [17/30], Training Loss: 19.2216, Validation Loss Current: 10.2308, Validation Loss AVG: 10.2308, lr: 0.0010000000000000002
Epoch [18/30], Training Loss: 17.0630, Validation Loss Current: 10.4491, Validation Loss AVG: 10.4491, lr: 0.0010000000000000002
Epoch [19/30], Training Loss: 17.8261, Validation Loss Current: 10.6015, Validation Loss AVG: 10.6015, lr: 0.0010000000000000002
Epoch [20/30], Training Loss: 18.1770, Validation Loss Current: 10.5732, Validation Loss AVG: 10.5732, lr: 0.0010000000000000002
Epoch [21/30], Training Loss: 18.7181, Validation Loss Current: 10.7301, Validation Loss AVG: 10.7301, lr: 0.0010000000000000002
Epoch [22/30], Training Loss: 17.4721, Validation Loss Current: 10.6036, Validation Loss AVG: 10.6036, lr: 0.0010000000000000002
Epoch [23/30], Training Loss: 16.3394, Validation Loss Current: 10.7088, Validation Loss AVG: 10.7088, lr: 0.00010000000000000003
Epoch [24/30], Training Loss: 16.4204, Validation Loss Current: 10.6810, Validation Loss AVG: 10.6810, lr: 0.00010000000000000003
Epoch [25/30], Training Loss: 17.5144, Validation Loss Current: 10.7068, Validation Loss AVG: 10.7068, lr: 0.00010000000000000003
Epoch [26/30], Training Loss: 16.2017, Validation Loss Current: 10.7494, Validation Loss AVG: 10.7494, lr: 0.00010000000000000003
Epoch [27/30], Training Loss: 16.9928, Validation Loss Current: 10.7733, Validation Loss AVG: 10.7733, lr: 0.00010000000000000003
Epoch [28/30], Training Loss: 16.6546, Validation Loss Current: 10.7545, Validation Loss AVG: 10.7545, lr: 0.00010000000000000003
Epoch [29/30], Training Loss: 16.7975, Validation Loss Current: 10.7694, Validation Loss AVG: 10.7694, lr: 1.0000000000000004e-05
Epoch [30/30], Training Loss: 17.0951, Validation Loss Current: 10.7816, Validation Loss AVG: 10.7816, lr: 1.0000000000000004e-05
Epoch [31/30], Training Loss: 16.6652, Validation Loss Current: 10.7580, Validation Loss AVG: 10.7580, lr: 1.0000000000000004e-05
Epoch [32/30], Training Loss: 16.2445, Validation Loss Current: 10.7916, Validation Loss AVG: 10.7916, lr: 1.0000000000000004e-05
Epoch [33/30], Training Loss: 17.4493, Validation Loss Current: 10.7623, Validation Loss AVG: 10.7623, lr: 1.0000000000000004e-05
Epoch [34/30], Training Loss: 17.8878, Validation Loss Current: 10.7522, Validation Loss AVG: 10.7522, lr: 1.0000000000000004e-05
 --- Early Stopped ---
Patch distance: 0.8 finished training. Best epoch: 4 Best val accuracy: [0.33421052631578946, 0.33782894736842106, 0.31644736842105264, 0.3598684210526316, 0.3861842105263158, 0.3559210526315789, 0.3450657894736842, 0.32796052631578954, 0.3213815789473684, 0.2986842105263158, 0.3786184210526316, 0.3805921052631579, 0.37697368421052635, 0.3819078947368421, 0.38848684210526313, 0.35855263157894735, 0.3713815789473684, 0.3703947368421052, 0.37236842105263157, 0.37697368421052635, 0.3697368421052632, 0.37236842105263157, 0.3726973684210526, 0.3713815789473684, 0.3710526315789474, 0.3713815789473684, 0.3710526315789474, 0.3703947368421053, 0.3703947368421053, 0.3703947368421053, 0.37006578947368424, 0.3703947368421053, 0.3703947368421053, 0.3703947368421053] Best val loss: 8.709086894989014


Loaded best state dict for [0.2, 0.4, 0.6, 0.8]
Current group: 1
Epoch [1/30], Training Loss: 29.3300, Validation Loss Current: 8.3244, Validation Loss AVG: 10.8756, lr: 0.1
Epoch [2/30], Training Loss: 31.5778, Validation Loss Current: 8.0390, Validation Loss AVG: 9.8382, lr: 0.1
Epoch [3/30], Training Loss: 29.6712, Validation Loss Current: 7.8037, Validation Loss AVG: 10.3797, lr: 0.1
Epoch [4/30], Training Loss: 28.4068, Validation Loss Current: 8.4435, Validation Loss AVG: 10.9693, lr: 0.1
Epoch [5/30], Training Loss: 28.4158, Validation Loss Current: 7.6186, Validation Loss AVG: 9.2990, lr: 0.1
Epoch [6/30], Training Loss: 28.5711, Validation Loss Current: 9.0005, Validation Loss AVG: 10.4242, lr: 0.1
Epoch [7/30], Training Loss: 29.7210, Validation Loss Current: 9.1187, Validation Loss AVG: 9.6168, lr: 0.1
Epoch [8/30], Training Loss: 30.6519, Validation Loss Current: 11.1465, Validation Loss AVG: 12.8873, lr: 0.1
Epoch [9/30], Training Loss: 30.1315, Validation Loss Current: 7.7520, Validation Loss AVG: 9.9400, lr: 0.1
Epoch [10/30], Training Loss: 28.7677, Validation Loss Current: 7.9962, Validation Loss AVG: 9.4229, lr: 0.1
Epoch [11/30], Training Loss: 28.3174, Validation Loss Current: 9.4871, Validation Loss AVG: 10.1866, lr: 0.1
Epoch [12/30], Training Loss: 27.7739, Validation Loss Current: 7.3445, Validation Loss AVG: 9.3826, lr: 0.010000000000000002
Epoch [13/30], Training Loss: 23.7430, Validation Loss Current: 6.9250, Validation Loss AVG: 8.7162, lr: 0.010000000000000002
Epoch [14/30], Training Loss: 23.0940, Validation Loss Current: 7.0386, Validation Loss AVG: 9.0661, lr: 0.010000000000000002
Epoch [15/30], Training Loss: 22.3483, Validation Loss Current: 6.9711, Validation Loss AVG: 9.1665, lr: 0.010000000000000002
Epoch [16/30], Training Loss: 20.1862, Validation Loss Current: 7.0342, Validation Loss AVG: 9.2584, lr: 0.010000000000000002
Epoch [17/30], Training Loss: 22.0141, Validation Loss Current: 7.1227, Validation Loss AVG: 9.5557, lr: 0.010000000000000002
Epoch [18/30], Training Loss: 19.0984, Validation Loss Current: 7.1359, Validation Loss AVG: 9.7537, lr: 0.010000000000000002
Epoch [19/30], Training Loss: 18.1765, Validation Loss Current: 7.1955, Validation Loss AVG: 9.9722, lr: 0.010000000000000002
Epoch [20/30], Training Loss: 16.7590, Validation Loss Current: 7.1577, Validation Loss AVG: 9.9518, lr: 0.0010000000000000002
Epoch [21/30], Training Loss: 17.3507, Validation Loss Current: 7.2162, Validation Loss AVG: 10.3394, lr: 0.0010000000000000002
Epoch [22/30], Training Loss: 15.9614, Validation Loss Current: 7.1676, Validation Loss AVG: 10.1912, lr: 0.0010000000000000002
Epoch [23/30], Training Loss: 16.6733, Validation Loss Current: 7.1801, Validation Loss AVG: 10.4190, lr: 0.0010000000000000002
Epoch [24/30], Training Loss: 18.3481, Validation Loss Current: 7.2391, Validation Loss AVG: 10.5290, lr: 0.0010000000000000002
Epoch [25/30], Training Loss: 17.8137, Validation Loss Current: 7.2419, Validation Loss AVG: 10.4766, lr: 0.0010000000000000002
Epoch [26/30], Training Loss: 18.6336, Validation Loss Current: 7.2731, Validation Loss AVG: 10.4508, lr: 0.00010000000000000003
Epoch [27/30], Training Loss: 15.6184, Validation Loss Current: 7.2938, Validation Loss AVG: 10.5542, lr: 0.00010000000000000003
Epoch [28/30], Training Loss: 15.7997, Validation Loss Current: 7.2653, Validation Loss AVG: 10.5198, lr: 0.00010000000000000003
Epoch [29/30], Training Loss: 16.9408, Validation Loss Current: 7.3401, Validation Loss AVG: 10.4847, lr: 0.00010000000000000003
Epoch [30/30], Training Loss: 15.2818, Validation Loss Current: 7.2660, Validation Loss AVG: 10.5150, lr: 0.00010000000000000003
Epoch [31/30], Training Loss: 15.8852, Validation Loss Current: 7.2450, Validation Loss AVG: 10.4991, lr: 0.00010000000000000003
Epoch [32/30], Training Loss: 15.5357, Validation Loss Current: 7.3014, Validation Loss AVG: 10.5286, lr: 1.0000000000000004e-05
Epoch [33/30], Training Loss: 16.0433, Validation Loss Current: 7.2836, Validation Loss AVG: 10.5108, lr: 1.0000000000000004e-05
Epoch [34/30], Training Loss: 15.3192, Validation Loss Current: 7.2947, Validation Loss AVG: 10.4937, lr: 1.0000000000000004e-05
Epoch [35/30], Training Loss: 15.7930, Validation Loss Current: 7.2719, Validation Loss AVG: 10.5113, lr: 1.0000000000000004e-05
Epoch [36/30], Training Loss: 15.3886, Validation Loss Current: 7.3185, Validation Loss AVG: 10.5032, lr: 1.0000000000000004e-05
Epoch [37/30], Training Loss: 15.7736, Validation Loss Current: 7.2681, Validation Loss AVG: 10.5002, lr: 1.0000000000000004e-05
Epoch [38/30], Training Loss: 15.6450, Validation Loss Current: 7.2698, Validation Loss AVG: 10.5118, lr: 1.0000000000000004e-06
Epoch [39/30], Training Loss: 16.6087, Validation Loss Current: 7.2990, Validation Loss AVG: 10.4485, lr: 1.0000000000000004e-06
Epoch [40/30], Training Loss: 16.1106, Validation Loss Current: 7.2534, Validation Loss AVG: 10.4832, lr: 1.0000000000000004e-06
Epoch [41/30], Training Loss: 15.6011, Validation Loss Current: 7.3280, Validation Loss AVG: 10.5305, lr: 1.0000000000000004e-06
Epoch [42/30], Training Loss: 15.3825, Validation Loss Current: 7.3520, Validation Loss AVG: 10.5014, lr: 1.0000000000000004e-06
Epoch [43/30], Training Loss: 15.8008, Validation Loss Current: 7.2698, Validation Loss AVG: 10.4835, lr: 1.0000000000000004e-06
 --- Early Stopped ---
Patch distance: 1 finished training. Best epoch: 13 Best val accuracy: [0.41118421052631576, 0.4161184210526316, 0.41118421052631576, 0.4375, 0.42269736842105265, 0.3782894736842105, 0.3355263157894737, 0.375, 0.4342105263157895, 0.4095394736842105, 0.35526315789473684, 0.4769736842105263, 0.5, 0.4934210526315789, 0.4967105263157895, 0.5082236842105263, 0.48848684210526316, 0.5032894736842105, 0.5115131578947368, 0.5213815789473685, 0.5263157894736842, 0.5164473684210527, 0.5230263157894737, 0.5197368421052632, 0.5180921052631579, 0.5180921052631579, 0.5213815789473685, 0.5180921052631579, 0.5180921052631579, 0.5180921052631579, 0.5180921052631579, 0.5180921052631579, 0.5180921052631579, 0.5180921052631579, 0.5180921052631579, 0.5180921052631579, 0.5180921052631579, 0.5180921052631579, 0.5180921052631579, 0.5180921052631579, 0.5180921052631579, 0.5180921052631579, 0.5180921052631579] Best val loss: 6.9250019788742065


----- Training alexnet with sequence: [0.4, 0.6, 0.8, 1] -----
Current group: 0.4
Epoch [1/38], Training Loss: 40.7742, Validation Loss Current: 10.0406, Validation Loss AVG: 10.0406, lr: 0.1
Epoch [2/38], Training Loss: 39.9394, Validation Loss Current: 10.0449, Validation Loss AVG: 10.0449, lr: 0.1
Epoch [3/38], Training Loss: 40.1379, Validation Loss Current: 9.9809, Validation Loss AVG: 9.9809, lr: 0.1
Epoch [4/38], Training Loss: 39.8763, Validation Loss Current: 10.0341, Validation Loss AVG: 10.0341, lr: 0.1
Epoch [5/38], Training Loss: 40.2224, Validation Loss Current: 9.9115, Validation Loss AVG: 9.9115, lr: 0.1
Epoch [6/38], Training Loss: 39.8675, Validation Loss Current: 10.0544, Validation Loss AVG: 10.0544, lr: 0.1
Epoch [7/38], Training Loss: 39.1389, Validation Loss Current: 9.5740, Validation Loss AVG: 9.5740, lr: 0.1
Epoch [8/38], Training Loss: 37.6664, Validation Loss Current: 9.7596, Validation Loss AVG: 9.7596, lr: 0.1
Epoch [9/38], Training Loss: 37.8524, Validation Loss Current: 11.4942, Validation Loss AVG: 11.4942, lr: 0.1
Epoch [10/38], Training Loss: 37.1334, Validation Loss Current: 12.2888, Validation Loss AVG: 12.2888, lr: 0.1
Epoch [11/38], Training Loss: 35.7158, Validation Loss Current: 19.6516, Validation Loss AVG: 19.6516, lr: 0.1
Epoch [12/38], Training Loss: 39.1666, Validation Loss Current: 9.3235, Validation Loss AVG: 9.3235, lr: 0.1
Epoch [13/38], Training Loss: 33.9578, Validation Loss Current: 10.1688, Validation Loss AVG: 10.1688, lr: 0.1
Epoch [14/38], Training Loss: 36.0021, Validation Loss Current: 9.7267, Validation Loss AVG: 9.7267, lr: 0.1
Epoch [15/38], Training Loss: 34.1611, Validation Loss Current: 9.7758, Validation Loss AVG: 9.7758, lr: 0.1
Epoch [16/38], Training Loss: 34.2409, Validation Loss Current: 9.5780, Validation Loss AVG: 9.5780, lr: 0.1
Epoch [17/38], Training Loss: 33.3152, Validation Loss Current: 16.4337, Validation Loss AVG: 16.4337, lr: 0.1
Epoch [18/38], Training Loss: 35.1358, Validation Loss Current: 10.4201, Validation Loss AVG: 10.4201, lr: 0.1
Epoch [19/38], Training Loss: 32.8713, Validation Loss Current: 8.9579, Validation Loss AVG: 8.9579, lr: 0.010000000000000002
Epoch [20/38], Training Loss: 30.6431, Validation Loss Current: 8.9413, Validation Loss AVG: 8.9413, lr: 0.010000000000000002
Epoch [21/38], Training Loss: 28.1767, Validation Loss Current: 8.8155, Validation Loss AVG: 8.8155, lr: 0.010000000000000002
Epoch [22/38], Training Loss: 26.4381, Validation Loss Current: 8.9603, Validation Loss AVG: 8.9603, lr: 0.010000000000000002
Epoch [23/38], Training Loss: 26.0408, Validation Loss Current: 9.0139, Validation Loss AVG: 9.0139, lr: 0.010000000000000002
Epoch [24/38], Training Loss: 26.5029, Validation Loss Current: 9.0868, Validation Loss AVG: 9.0868, lr: 0.010000000000000002
Epoch [25/38], Training Loss: 27.1137, Validation Loss Current: 8.9128, Validation Loss AVG: 8.9128, lr: 0.010000000000000002
Epoch [26/38], Training Loss: 25.3556, Validation Loss Current: 9.1577, Validation Loss AVG: 9.1577, lr: 0.010000000000000002
Epoch [27/38], Training Loss: 24.4188, Validation Loss Current: 9.2134, Validation Loss AVG: 9.2134, lr: 0.010000000000000002
Epoch [28/38], Training Loss: 25.1741, Validation Loss Current: 9.1634, Validation Loss AVG: 9.1634, lr: 0.0010000000000000002
Epoch [29/38], Training Loss: 24.2073, Validation Loss Current: 9.1740, Validation Loss AVG: 9.1740, lr: 0.0010000000000000002
Epoch [30/38], Training Loss: 23.0063, Validation Loss Current: 9.2771, Validation Loss AVG: 9.2771, lr: 0.0010000000000000002
Epoch [31/38], Training Loss: 23.0214, Validation Loss Current: 9.2617, Validation Loss AVG: 9.2617, lr: 0.0010000000000000002
Epoch [32/38], Training Loss: 22.4608, Validation Loss Current: 9.2958, Validation Loss AVG: 9.2958, lr: 0.0010000000000000002
Epoch [33/38], Training Loss: 24.0837, Validation Loss Current: 9.3979, Validation Loss AVG: 9.3979, lr: 0.0010000000000000002
Epoch [34/38], Training Loss: 22.3011, Validation Loss Current: 9.3846, Validation Loss AVG: 9.3846, lr: 0.00010000000000000003
Epoch [35/38], Training Loss: 22.3799, Validation Loss Current: 9.3507, Validation Loss AVG: 9.3507, lr: 0.00010000000000000003
Epoch [36/38], Training Loss: 22.4840, Validation Loss Current: 9.3371, Validation Loss AVG: 9.3371, lr: 0.00010000000000000003
Epoch [37/38], Training Loss: 23.6578, Validation Loss Current: 9.3566, Validation Loss AVG: 9.3566, lr: 0.00010000000000000003
Epoch [38/38], Training Loss: 23.0843, Validation Loss Current: 9.3604, Validation Loss AVG: 9.3604, lr: 0.00010000000000000003
Epoch [39/38], Training Loss: 22.4988, Validation Loss Current: 9.3672, Validation Loss AVG: 9.3672, lr: 0.00010000000000000003
Epoch [40/38], Training Loss: 22.3136, Validation Loss Current: 9.3579, Validation Loss AVG: 9.3579, lr: 1.0000000000000004e-05
Epoch [41/38], Training Loss: 24.3557, Validation Loss Current: 9.3400, Validation Loss AVG: 9.3400, lr: 1.0000000000000004e-05
Epoch [42/38], Training Loss: 22.8075, Validation Loss Current: 9.3444, Validation Loss AVG: 9.3444, lr: 1.0000000000000004e-05
Epoch [43/38], Training Loss: 23.0103, Validation Loss Current: 9.3719, Validation Loss AVG: 9.3719, lr: 1.0000000000000004e-05
Epoch [44/38], Training Loss: 22.7232, Validation Loss Current: 9.3595, Validation Loss AVG: 9.3595, lr: 1.0000000000000004e-05
Epoch [45/38], Training Loss: 22.8609, Validation Loss Current: 9.3346, Validation Loss AVG: 9.3346, lr: 1.0000000000000004e-05
Epoch [46/38], Training Loss: 23.1886, Validation Loss Current: 9.3331, Validation Loss AVG: 9.3331, lr: 1.0000000000000004e-06
Epoch [47/38], Training Loss: 23.1854, Validation Loss Current: 9.3726, Validation Loss AVG: 9.3726, lr: 1.0000000000000004e-06
Epoch [48/38], Training Loss: 23.3942, Validation Loss Current: 9.3616, Validation Loss AVG: 9.3616, lr: 1.0000000000000004e-06
Epoch [49/38], Training Loss: 22.1598, Validation Loss Current: 9.3537, Validation Loss AVG: 9.3537, lr: 1.0000000000000004e-06
Epoch [50/38], Training Loss: 21.9882, Validation Loss Current: 9.3442, Validation Loss AVG: 9.3442, lr: 1.0000000000000004e-06
Epoch [51/38], Training Loss: 22.3806, Validation Loss Current: 9.3912, Validation Loss AVG: 9.3912, lr: 1.0000000000000004e-06
 --- Early Stopped ---
Patch distance: 0.4 finished training. Best epoch: 21 Best val accuracy: [0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22664473684210526, 0.22532894736842107, 0.275, 0.26151315789473684, 0.2881578947368421, 0.24276315789473682, 0.23651315789473687, 0.28289473684210525, 0.29210526315789476, 0.2865131578947368, 0.29210526315789476, 0.2713815789473684, 0.24835526315789475, 0.3003289473684211, 0.35098684210526315, 0.3506578947368421, 0.36546052631578946, 0.3569078947368421, 0.35197368421052627, 0.3358552631578947, 0.35526315789473684, 0.36546052631578946, 0.34967105263157894, 0.35230263157894737, 0.36151315789473687, 0.3578947368421052, 0.3572368421052632, 0.3569078947368421, 0.356578947368421, 0.35526315789473684, 0.35789473684210527, 0.3582236842105263, 0.35625, 0.35690789473684215, 0.3559210526315789, 0.3559210526315789, 0.3559210526315789, 0.3559210526315789, 0.3559210526315789, 0.35625, 0.3559210526315789, 0.3559210526315789, 0.3559210526315789, 0.3559210526315789, 0.3559210526315789, 0.3559210526315789, 0.3559210526315789] Best val loss: 8.815520119667053


Loaded best state dict for [0.4]
Current group: 0.6
Epoch [1/38], Training Loss: 32.6856, Validation Loss Current: 9.2524, Validation Loss AVG: 9.2524, lr: 0.1
Epoch [2/38], Training Loss: 32.6883, Validation Loss Current: 11.1097, Validation Loss AVG: 11.1097, lr: 0.1
Epoch [3/38], Training Loss: 33.3842, Validation Loss Current: 9.2581, Validation Loss AVG: 9.2581, lr: 0.1
Epoch [4/38], Training Loss: 31.1509, Validation Loss Current: 9.9158, Validation Loss AVG: 9.9158, lr: 0.1
Epoch [5/38], Training Loss: 31.2837, Validation Loss Current: 9.6642, Validation Loss AVG: 9.6642, lr: 0.1
Epoch [6/38], Training Loss: 31.3889, Validation Loss Current: 9.1526, Validation Loss AVG: 9.1526, lr: 0.1
Epoch [7/38], Training Loss: 29.8047, Validation Loss Current: 10.5699, Validation Loss AVG: 10.5699, lr: 0.1
Epoch [8/38], Training Loss: 31.4192, Validation Loss Current: 9.3156, Validation Loss AVG: 9.3156, lr: 0.1
Epoch [9/38], Training Loss: 31.9853, Validation Loss Current: 9.0466, Validation Loss AVG: 9.0466, lr: 0.1
Epoch [10/38], Training Loss: 29.1056, Validation Loss Current: 9.2456, Validation Loss AVG: 9.2456, lr: 0.1
Epoch [11/38], Training Loss: 29.5002, Validation Loss Current: 11.5576, Validation Loss AVG: 11.5576, lr: 0.1
Epoch [12/38], Training Loss: 31.6958, Validation Loss Current: 9.3002, Validation Loss AVG: 9.3002, lr: 0.1
Epoch [13/38], Training Loss: 30.9817, Validation Loss Current: 9.2548, Validation Loss AVG: 9.2548, lr: 0.1
Epoch [14/38], Training Loss: 29.4461, Validation Loss Current: 10.8371, Validation Loss AVG: 10.8371, lr: 0.1
Epoch [15/38], Training Loss: 28.6817, Validation Loss Current: 10.8095, Validation Loss AVG: 10.8095, lr: 0.1
Epoch [16/38], Training Loss: 26.8374, Validation Loss Current: 9.2133, Validation Loss AVG: 9.2133, lr: 0.010000000000000002
Epoch [17/38], Training Loss: 23.3518, Validation Loss Current: 9.1125, Validation Loss AVG: 9.1125, lr: 0.010000000000000002
Epoch [18/38], Training Loss: 22.9019, Validation Loss Current: 9.1944, Validation Loss AVG: 9.1944, lr: 0.010000000000000002
Epoch [19/38], Training Loss: 21.9242, Validation Loss Current: 9.4425, Validation Loss AVG: 9.4425, lr: 0.010000000000000002
Epoch [20/38], Training Loss: 20.6331, Validation Loss Current: 9.8804, Validation Loss AVG: 9.8804, lr: 0.010000000000000002
Epoch [21/38], Training Loss: 20.4422, Validation Loss Current: 9.5752, Validation Loss AVG: 9.5752, lr: 0.010000000000000002
Epoch [22/38], Training Loss: 19.6861, Validation Loss Current: 9.5747, Validation Loss AVG: 9.5747, lr: 0.0010000000000000002
Epoch [23/38], Training Loss: 19.9154, Validation Loss Current: 9.8783, Validation Loss AVG: 9.8783, lr: 0.0010000000000000002
Epoch [24/38], Training Loss: 19.8429, Validation Loss Current: 9.9147, Validation Loss AVG: 9.9147, lr: 0.0010000000000000002
Epoch [25/38], Training Loss: 18.9327, Validation Loss Current: 9.9462, Validation Loss AVG: 9.9462, lr: 0.0010000000000000002
Epoch [26/38], Training Loss: 19.0069, Validation Loss Current: 9.9749, Validation Loss AVG: 9.9749, lr: 0.0010000000000000002
Epoch [27/38], Training Loss: 18.2978, Validation Loss Current: 10.0134, Validation Loss AVG: 10.0134, lr: 0.0010000000000000002
Epoch [28/38], Training Loss: 18.0919, Validation Loss Current: 10.0429, Validation Loss AVG: 10.0429, lr: 0.00010000000000000003
Epoch [29/38], Training Loss: 18.0653, Validation Loss Current: 10.0615, Validation Loss AVG: 10.0615, lr: 0.00010000000000000003
Epoch [30/38], Training Loss: 17.7629, Validation Loss Current: 10.0242, Validation Loss AVG: 10.0242, lr: 0.00010000000000000003
Epoch [31/38], Training Loss: 18.6498, Validation Loss Current: 10.0519, Validation Loss AVG: 10.0519, lr: 0.00010000000000000003
Epoch [32/38], Training Loss: 18.2379, Validation Loss Current: 10.0397, Validation Loss AVG: 10.0397, lr: 0.00010000000000000003
Epoch [33/38], Training Loss: 17.9185, Validation Loss Current: 10.0783, Validation Loss AVG: 10.0783, lr: 0.00010000000000000003
Epoch [34/38], Training Loss: 17.9964, Validation Loss Current: 10.0603, Validation Loss AVG: 10.0603, lr: 1.0000000000000004e-05
Epoch [35/38], Training Loss: 18.8229, Validation Loss Current: 10.0595, Validation Loss AVG: 10.0595, lr: 1.0000000000000004e-05
Epoch [36/38], Training Loss: 19.3570, Validation Loss Current: 10.0753, Validation Loss AVG: 10.0753, lr: 1.0000000000000004e-05
Epoch [37/38], Training Loss: 18.0211, Validation Loss Current: 10.0692, Validation Loss AVG: 10.0692, lr: 1.0000000000000004e-05
Epoch [38/38], Training Loss: 18.1923, Validation Loss Current: 10.0737, Validation Loss AVG: 10.0737, lr: 1.0000000000000004e-05
Epoch [39/38], Training Loss: 19.3900, Validation Loss Current: 10.0585, Validation Loss AVG: 10.0585, lr: 1.0000000000000004e-05
 --- Early Stopped ---
Patch distance: 0.6 finished training. Best epoch: 9 Best val accuracy: [0.3164473684210526, 0.2625, 0.350328947368421, 0.2845394736842105, 0.3223684210526316, 0.2970394736842105, 0.24276315789473682, 0.3032894736842105, 0.3358552631578947, 0.34605263157894733, 0.32565789473684215, 0.3157894736842105, 0.3338815789473684, 0.31315789473684214, 0.2875, 0.37434210526315786, 0.37039473684210533, 0.3763157894736842, 0.3911184210526316, 0.36907894736842106, 0.37467105263157896, 0.38453947368421054, 0.3799342105263158, 0.3848684210526316, 0.3825657894736842, 0.38421052631578945, 0.38125, 0.3828947368421053, 0.3832236842105263, 0.3828947368421053, 0.38355263157894737, 0.38421052631578945, 0.3838815789473684, 0.3838815789473684, 0.3838815789473684, 0.3832236842105264, 0.3835526315789474, 0.3835526315789474, 0.3835526315789474] Best val loss: 9.04655032157898


Loaded best state dict for [0.4, 0.6]
Current group: 0.8
Epoch [1/38], Training Loss: 28.7185, Validation Loss Current: 13.4675, Validation Loss AVG: 13.4675, lr: 0.1
Epoch [2/38], Training Loss: 30.0166, Validation Loss Current: 9.2046, Validation Loss AVG: 9.2046, lr: 0.1
Epoch [3/38], Training Loss: 29.5041, Validation Loss Current: 9.3433, Validation Loss AVG: 9.3433, lr: 0.1
Epoch [4/38], Training Loss: 28.6610, Validation Loss Current: 9.1514, Validation Loss AVG: 9.1514, lr: 0.1
Epoch [5/38], Training Loss: 28.4492, Validation Loss Current: 10.6155, Validation Loss AVG: 10.6155, lr: 0.1
Epoch [6/38], Training Loss: 29.3724, Validation Loss Current: 9.6020, Validation Loss AVG: 9.6020, lr: 0.1
Epoch [7/38], Training Loss: 28.5085, Validation Loss Current: 9.8674, Validation Loss AVG: 9.8674, lr: 0.1
Epoch [8/38], Training Loss: 29.8856, Validation Loss Current: 11.0084, Validation Loss AVG: 11.0084, lr: 0.1
Epoch [9/38], Training Loss: 30.5629, Validation Loss Current: 9.9695, Validation Loss AVG: 9.9695, lr: 0.1
Epoch [10/38], Training Loss: 27.2008, Validation Loss Current: 9.7507, Validation Loss AVG: 9.7507, lr: 0.1
Epoch [11/38], Training Loss: 25.4208, Validation Loss Current: 10.2065, Validation Loss AVG: 10.2065, lr: 0.010000000000000002
Epoch [12/38], Training Loss: 22.7299, Validation Loss Current: 9.6759, Validation Loss AVG: 9.6759, lr: 0.010000000000000002
Epoch [13/38], Training Loss: 21.0183, Validation Loss Current: 10.4505, Validation Loss AVG: 10.4505, lr: 0.010000000000000002
Epoch [14/38], Training Loss: 19.9079, Validation Loss Current: 10.1417, Validation Loss AVG: 10.1417, lr: 0.010000000000000002
Epoch [15/38], Training Loss: 20.3703, Validation Loss Current: 10.3195, Validation Loss AVG: 10.3195, lr: 0.010000000000000002
Epoch [16/38], Training Loss: 18.1980, Validation Loss Current: 10.6348, Validation Loss AVG: 10.6348, lr: 0.010000000000000002
Epoch [17/38], Training Loss: 17.2328, Validation Loss Current: 10.7865, Validation Loss AVG: 10.7865, lr: 0.0010000000000000002
Epoch [18/38], Training Loss: 17.3746, Validation Loss Current: 10.9975, Validation Loss AVG: 10.9975, lr: 0.0010000000000000002
Epoch [19/38], Training Loss: 16.9944, Validation Loss Current: 11.0161, Validation Loss AVG: 11.0161, lr: 0.0010000000000000002
Epoch [20/38], Training Loss: 17.9198, Validation Loss Current: 11.1261, Validation Loss AVG: 11.1261, lr: 0.0010000000000000002
Epoch [21/38], Training Loss: 16.3510, Validation Loss Current: 11.0645, Validation Loss AVG: 11.0645, lr: 0.0010000000000000002
Epoch [22/38], Training Loss: 18.0418, Validation Loss Current: 11.2392, Validation Loss AVG: 11.2392, lr: 0.0010000000000000002
Epoch [23/38], Training Loss: 17.8518, Validation Loss Current: 11.2155, Validation Loss AVG: 11.2155, lr: 0.00010000000000000003
Epoch [24/38], Training Loss: 16.7875, Validation Loss Current: 11.2757, Validation Loss AVG: 11.2757, lr: 0.00010000000000000003
Epoch [25/38], Training Loss: 16.3444, Validation Loss Current: 11.3241, Validation Loss AVG: 11.3241, lr: 0.00010000000000000003
Epoch [26/38], Training Loss: 16.6337, Validation Loss Current: 11.2426, Validation Loss AVG: 11.2426, lr: 0.00010000000000000003
Epoch [27/38], Training Loss: 17.1079, Validation Loss Current: 11.3594, Validation Loss AVG: 11.3594, lr: 0.00010000000000000003
Epoch [28/38], Training Loss: 16.2073, Validation Loss Current: 11.2953, Validation Loss AVG: 11.2953, lr: 0.00010000000000000003
Epoch [29/38], Training Loss: 16.3025, Validation Loss Current: 11.2418, Validation Loss AVG: 11.2418, lr: 1.0000000000000004e-05
Epoch [30/38], Training Loss: 16.6405, Validation Loss Current: 11.3063, Validation Loss AVG: 11.3063, lr: 1.0000000000000004e-05
Epoch [31/38], Training Loss: 15.6909, Validation Loss Current: 11.2377, Validation Loss AVG: 11.2377, lr: 1.0000000000000004e-05
Epoch [32/38], Training Loss: 16.8483, Validation Loss Current: 11.3290, Validation Loss AVG: 11.3290, lr: 1.0000000000000004e-05
Epoch [33/38], Training Loss: 16.4288, Validation Loss Current: 11.3466, Validation Loss AVG: 11.3466, lr: 1.0000000000000004e-05
Epoch [34/38], Training Loss: 15.6897, Validation Loss Current: 11.3063, Validation Loss AVG: 11.3063, lr: 1.0000000000000004e-05
 --- Early Stopped ---
Patch distance: 0.8 finished training. Best epoch: 4 Best val accuracy: [0.32434210526315793, 0.36282894736842103, 0.28717105263157894, 0.3460526315789474, 0.33322368421052634, 0.3503289473684211, 0.35789473684210527, 0.3342105263157895, 0.3391447368421053, 0.33223684210526316, 0.3796052631578948, 0.3855263157894737, 0.3792763157894737, 0.3819078947368421, 0.38322368421052627, 0.38421052631578945, 0.3855263157894736, 0.3805921052631579, 0.3805921052631579, 0.37894736842105264, 0.3848684210526315, 0.3805921052631579, 0.38125, 0.3792763157894737, 0.3792763157894737, 0.3789473684210526, 0.3796052631578947, 0.3796052631578947, 0.3796052631578947, 0.3792763157894737, 0.3792763157894737, 0.3796052631578947, 0.3792763157894737, 0.3792763157894737] Best val loss: 9.151372122764588


Loaded best state dict for [0.4, 0.6, 0.8]
Current group: 1
Epoch [1/38], Training Loss: 28.0796, Validation Loss Current: 7.9234, Validation Loss AVG: 9.2735, lr: 0.1
Epoch [2/38], Training Loss: 30.5230, Validation Loss Current: 7.9853, Validation Loss AVG: 9.6508, lr: 0.1
Epoch [3/38], Training Loss: 27.8175, Validation Loss Current: 7.8955, Validation Loss AVG: 9.4811, lr: 0.1
Epoch [4/38], Training Loss: 28.1941, Validation Loss Current: 10.1483, Validation Loss AVG: 11.4492, lr: 0.1
Epoch [5/38], Training Loss: 28.4067, Validation Loss Current: 7.5739, Validation Loss AVG: 10.8168, lr: 0.1
Epoch [6/38], Training Loss: 28.4813, Validation Loss Current: 8.4019, Validation Loss AVG: 9.8484, lr: 0.1
Epoch [7/38], Training Loss: 29.2608, Validation Loss Current: 8.6194, Validation Loss AVG: 9.2799, lr: 0.1
Epoch [8/38], Training Loss: 30.0580, Validation Loss Current: 8.3990, Validation Loss AVG: 9.5511, lr: 0.1
Epoch [9/38], Training Loss: 28.1775, Validation Loss Current: 7.7974, Validation Loss AVG: 9.3844, lr: 0.1
Epoch [10/38], Training Loss: 27.3545, Validation Loss Current: 7.8456, Validation Loss AVG: 10.3503, lr: 0.1
Epoch [11/38], Training Loss: 27.7021, Validation Loss Current: 7.9843, Validation Loss AVG: 8.8474, lr: 0.1
Epoch [12/38], Training Loss: 24.5248, Validation Loss Current: 7.1994, Validation Loss AVG: 9.3958, lr: 0.010000000000000002
Epoch [13/38], Training Loss: 22.1455, Validation Loss Current: 6.8329, Validation Loss AVG: 8.8417, lr: 0.010000000000000002
Epoch [14/38], Training Loss: 19.9917, Validation Loss Current: 7.1070, Validation Loss AVG: 9.4506, lr: 0.010000000000000002
Epoch [15/38], Training Loss: 19.7544, Validation Loss Current: 6.9842, Validation Loss AVG: 9.7344, lr: 0.010000000000000002
Epoch [16/38], Training Loss: 18.6541, Validation Loss Current: 7.2203, Validation Loss AVG: 10.1870, lr: 0.010000000000000002
Epoch [17/38], Training Loss: 18.2997, Validation Loss Current: 7.3422, Validation Loss AVG: 11.4620, lr: 0.010000000000000002
Epoch [18/38], Training Loss: 17.1685, Validation Loss Current: 7.4471, Validation Loss AVG: 10.8487, lr: 0.010000000000000002
Epoch [19/38], Training Loss: 16.6353, Validation Loss Current: 7.5547, Validation Loss AVG: 11.0356, lr: 0.010000000000000002
Epoch [20/38], Training Loss: 15.6602, Validation Loss Current: 7.4544, Validation Loss AVG: 11.5451, lr: 0.0010000000000000002
Epoch [21/38], Training Loss: 16.0013, Validation Loss Current: 7.4799, Validation Loss AVG: 11.3744, lr: 0.0010000000000000002
Epoch [22/38], Training Loss: 15.0765, Validation Loss Current: 7.5389, Validation Loss AVG: 11.2712, lr: 0.0010000000000000002
Epoch [23/38], Training Loss: 14.5439, Validation Loss Current: 7.5711, Validation Loss AVG: 11.5139, lr: 0.0010000000000000002
Epoch [24/38], Training Loss: 14.9184, Validation Loss Current: 7.5985, Validation Loss AVG: 11.5612, lr: 0.0010000000000000002
Epoch [25/38], Training Loss: 14.5395, Validation Loss Current: 7.6277, Validation Loss AVG: 11.6944, lr: 0.0010000000000000002
Epoch [26/38], Training Loss: 14.1351, Validation Loss Current: 7.6643, Validation Loss AVG: 11.7655, lr: 0.00010000000000000003
Epoch [27/38], Training Loss: 15.4056, Validation Loss Current: 7.7424, Validation Loss AVG: 11.7256, lr: 0.00010000000000000003
Epoch [28/38], Training Loss: 14.2053, Validation Loss Current: 7.7845, Validation Loss AVG: 11.7267, lr: 0.00010000000000000003
Epoch [29/38], Training Loss: 13.8782, Validation Loss Current: 7.6575, Validation Loss AVG: 11.6429, lr: 0.00010000000000000003
Epoch [30/38], Training Loss: 17.2500, Validation Loss Current: 7.7958, Validation Loss AVG: 11.7134, lr: 0.00010000000000000003
Epoch [31/38], Training Loss: 14.4667, Validation Loss Current: 7.7607, Validation Loss AVG: 11.6777, lr: 0.00010000000000000003
Epoch [32/38], Training Loss: 14.4224, Validation Loss Current: 7.6379, Validation Loss AVG: 11.6664, lr: 1.0000000000000004e-05
Epoch [33/38], Training Loss: 15.0061, Validation Loss Current: 7.6235, Validation Loss AVG: 11.6550, lr: 1.0000000000000004e-05
Epoch [34/38], Training Loss: 13.7990, Validation Loss Current: 7.8104, Validation Loss AVG: 11.6961, lr: 1.0000000000000004e-05
Epoch [35/38], Training Loss: 14.0559, Validation Loss Current: 7.7521, Validation Loss AVG: 11.7064, lr: 1.0000000000000004e-05
Epoch [36/38], Training Loss: 14.0398, Validation Loss Current: 7.7792, Validation Loss AVG: 11.7236, lr: 1.0000000000000004e-05
Epoch [37/38], Training Loss: 14.4946, Validation Loss Current: 7.6519, Validation Loss AVG: 11.6920, lr: 1.0000000000000004e-05
Epoch [38/38], Training Loss: 14.7880, Validation Loss Current: 7.6993, Validation Loss AVG: 11.6682, lr: 1.0000000000000004e-06
Epoch [39/38], Training Loss: 14.1724, Validation Loss Current: 7.6511, Validation Loss AVG: 11.6841, lr: 1.0000000000000004e-06
Epoch [40/38], Training Loss: 13.9945, Validation Loss Current: 7.7856, Validation Loss AVG: 11.7402, lr: 1.0000000000000004e-06
Epoch [41/38], Training Loss: 14.3226, Validation Loss Current: 7.6753, Validation Loss AVG: 11.6833, lr: 1.0000000000000004e-06
Epoch [42/38], Training Loss: 14.2726, Validation Loss Current: 7.6841, Validation Loss AVG: 11.6696, lr: 1.0000000000000004e-06
Epoch [43/38], Training Loss: 14.3738, Validation Loss Current: 7.7842, Validation Loss AVG: 11.6789, lr: 1.0000000000000004e-06
 --- Early Stopped ---
Patch distance: 1 finished training. Best epoch: 13 Best val accuracy: [0.37006578947368424, 0.4605263157894737, 0.42105263157894735, 0.4243421052631579, 0.46875, 0.39473684210526316, 0.3305921052631579, 0.3717105263157895, 0.46875, 0.44901315789473684, 0.4095394736842105, 0.4901315789473684, 0.5263157894736842, 0.53125, 0.5460526315789473, 0.5345394736842105, 0.5345394736842105, 0.5328947368421053, 0.5328947368421053, 0.5460526315789473, 0.5427631578947368, 0.5394736842105263, 0.5476973684210527, 0.5493421052631579, 0.5444078947368421, 0.5444078947368421, 0.5444078947368421, 0.5460526315789473, 0.5476973684210527, 0.5493421052631579, 0.5493421052631579, 0.5493421052631579, 0.5476973684210527, 0.5493421052631579, 0.5509868421052632, 0.5493421052631579, 0.5509868421052632, 0.5493421052631579, 0.5509868421052632, 0.5509868421052632, 0.5493421052631579, 0.5493421052631579, 0.5493421052631579] Best val loss: 6.832937598228455


----- Training alexnet with sequence: [0.6, 0.8, 1] -----
Current group: 0.6
Epoch [1/50], Training Loss: 40.6114, Validation Loss Current: 10.0340, Validation Loss AVG: 10.0340, lr: 0.1
Epoch [2/50], Training Loss: 40.5987, Validation Loss Current: 10.0358, Validation Loss AVG: 10.0358, lr: 0.1
Epoch [3/50], Training Loss: 40.1626, Validation Loss Current: 10.0511, Validation Loss AVG: 10.0511, lr: 0.1
Epoch [4/50], Training Loss: 40.1419, Validation Loss Current: 9.9551, Validation Loss AVG: 9.9551, lr: 0.1
Epoch [5/50], Training Loss: 40.0388, Validation Loss Current: 9.8092, Validation Loss AVG: 9.8092, lr: 0.1
Epoch [6/50], Training Loss: 37.9858, Validation Loss Current: 13.5278, Validation Loss AVG: 13.5278, lr: 0.1
Epoch [7/50], Training Loss: 39.0616, Validation Loss Current: 9.4652, Validation Loss AVG: 9.4652, lr: 0.1
Epoch [8/50], Training Loss: 38.5678, Validation Loss Current: 9.5925, Validation Loss AVG: 9.5925, lr: 0.1
Epoch [9/50], Training Loss: 36.4592, Validation Loss Current: 9.8010, Validation Loss AVG: 9.8010, lr: 0.1
Epoch [10/50], Training Loss: 36.4790, Validation Loss Current: 9.6493, Validation Loss AVG: 9.6493, lr: 0.1
Epoch [11/50], Training Loss: 35.9618, Validation Loss Current: 10.2369, Validation Loss AVG: 10.2369, lr: 0.1
Epoch [12/50], Training Loss: 33.1866, Validation Loss Current: 9.5807, Validation Loss AVG: 9.5807, lr: 0.1
Epoch [13/50], Training Loss: 36.3116, Validation Loss Current: 9.8682, Validation Loss AVG: 9.8682, lr: 0.1
Epoch [14/50], Training Loss: 34.6013, Validation Loss Current: 8.8609, Validation Loss AVG: 8.8609, lr: 0.010000000000000002
Epoch [15/50], Training Loss: 32.1822, Validation Loss Current: 8.6546, Validation Loss AVG: 8.6546, lr: 0.010000000000000002
Epoch [16/50], Training Loss: 30.0921, Validation Loss Current: 8.5657, Validation Loss AVG: 8.5657, lr: 0.010000000000000002
Epoch [17/50], Training Loss: 29.9973, Validation Loss Current: 8.3810, Validation Loss AVG: 8.3810, lr: 0.010000000000000002
Epoch [18/50], Training Loss: 29.1632, Validation Loss Current: 8.6274, Validation Loss AVG: 8.6274, lr: 0.010000000000000002
Epoch [19/50], Training Loss: 29.4089, Validation Loss Current: 9.0003, Validation Loss AVG: 9.0003, lr: 0.010000000000000002
Epoch [20/50], Training Loss: 27.7059, Validation Loss Current: 8.3616, Validation Loss AVG: 8.3616, lr: 0.010000000000000002
Epoch [21/50], Training Loss: 27.4909, Validation Loss Current: 8.3332, Validation Loss AVG: 8.3332, lr: 0.010000000000000002
Epoch [22/50], Training Loss: 28.1923, Validation Loss Current: 8.2903, Validation Loss AVG: 8.2903, lr: 0.010000000000000002
Epoch [23/50], Training Loss: 26.3516, Validation Loss Current: 8.2425, Validation Loss AVG: 8.2425, lr: 0.010000000000000002
Epoch [24/50], Training Loss: 26.8908, Validation Loss Current: 8.3526, Validation Loss AVG: 8.3526, lr: 0.010000000000000002
Epoch [25/50], Training Loss: 25.9965, Validation Loss Current: 8.2552, Validation Loss AVG: 8.2552, lr: 0.010000000000000002
Epoch [26/50], Training Loss: 25.4143, Validation Loss Current: 8.3955, Validation Loss AVG: 8.3955, lr: 0.010000000000000002
Epoch [27/50], Training Loss: 25.6410, Validation Loss Current: 8.3188, Validation Loss AVG: 8.3188, lr: 0.010000000000000002
Epoch [28/50], Training Loss: 25.5918, Validation Loss Current: 8.3164, Validation Loss AVG: 8.3164, lr: 0.010000000000000002
Epoch [29/50], Training Loss: 24.2915, Validation Loss Current: 8.4952, Validation Loss AVG: 8.4952, lr: 0.010000000000000002
Epoch [30/50], Training Loss: 24.5627, Validation Loss Current: 8.2602, Validation Loss AVG: 8.2602, lr: 0.0010000000000000002
Epoch [31/50], Training Loss: 24.0044, Validation Loss Current: 8.1832, Validation Loss AVG: 8.1832, lr: 0.0010000000000000002
Epoch [32/50], Training Loss: 22.6190, Validation Loss Current: 8.2178, Validation Loss AVG: 8.2178, lr: 0.0010000000000000002
Epoch [33/50], Training Loss: 22.2888, Validation Loss Current: 8.1964, Validation Loss AVG: 8.1964, lr: 0.0010000000000000002
Epoch [34/50], Training Loss: 22.4577, Validation Loss Current: 8.2309, Validation Loss AVG: 8.2309, lr: 0.0010000000000000002
Epoch [35/50], Training Loss: 22.4944, Validation Loss Current: 8.2101, Validation Loss AVG: 8.2101, lr: 0.0010000000000000002
Epoch [36/50], Training Loss: 22.7512, Validation Loss Current: 8.2537, Validation Loss AVG: 8.2537, lr: 0.0010000000000000002
Epoch [37/50], Training Loss: 22.6791, Validation Loss Current: 8.2452, Validation Loss AVG: 8.2452, lr: 0.0010000000000000002
Epoch [38/50], Training Loss: 22.5386, Validation Loss Current: 8.2507, Validation Loss AVG: 8.2507, lr: 0.00010000000000000003
Epoch [39/50], Training Loss: 22.5428, Validation Loss Current: 8.2559, Validation Loss AVG: 8.2559, lr: 0.00010000000000000003
Epoch [40/50], Training Loss: 23.5250, Validation Loss Current: 8.2257, Validation Loss AVG: 8.2257, lr: 0.00010000000000000003
Epoch [41/50], Training Loss: 21.6182, Validation Loss Current: 8.2326, Validation Loss AVG: 8.2326, lr: 0.00010000000000000003
Epoch [42/50], Training Loss: 23.4884, Validation Loss Current: 8.2654, Validation Loss AVG: 8.2654, lr: 0.00010000000000000003
Epoch [43/50], Training Loss: 21.9859, Validation Loss Current: 8.2595, Validation Loss AVG: 8.2595, lr: 0.00010000000000000003
Epoch [44/50], Training Loss: 22.4751, Validation Loss Current: 8.2352, Validation Loss AVG: 8.2352, lr: 1.0000000000000004e-05
Epoch [45/50], Training Loss: 22.3564, Validation Loss Current: 8.2668, Validation Loss AVG: 8.2668, lr: 1.0000000000000004e-05
Epoch [46/50], Training Loss: 23.9887, Validation Loss Current: 8.2764, Validation Loss AVG: 8.2764, lr: 1.0000000000000004e-05
Epoch [47/50], Training Loss: 22.0599, Validation Loss Current: 8.2675, Validation Loss AVG: 8.2675, lr: 1.0000000000000004e-05
Epoch [48/50], Training Loss: 22.0846, Validation Loss Current: 8.2579, Validation Loss AVG: 8.2579, lr: 1.0000000000000004e-05
Epoch [49/50], Training Loss: 21.4734, Validation Loss Current: 8.2376, Validation Loss AVG: 8.2376, lr: 1.0000000000000004e-05
Epoch [50/50], Training Loss: 23.7838, Validation Loss Current: 8.2481, Validation Loss AVG: 8.2481, lr: 1.0000000000000004e-06
Epoch [51/50], Training Loss: 22.8257, Validation Loss Current: 8.2318, Validation Loss AVG: 8.2318, lr: 1.0000000000000004e-06
Epoch [52/50], Training Loss: 22.7186, Validation Loss Current: 8.2565, Validation Loss AVG: 8.2565, lr: 1.0000000000000004e-06
Epoch [53/50], Training Loss: 22.0777, Validation Loss Current: 8.2488, Validation Loss AVG: 8.2488, lr: 1.0000000000000004e-06
Epoch [54/50], Training Loss: 22.1692, Validation Loss Current: 8.2702, Validation Loss AVG: 8.2702, lr: 1.0000000000000004e-06
Epoch [55/50], Training Loss: 21.7376, Validation Loss Current: 8.2582, Validation Loss AVG: 8.2582, lr: 1.0000000000000004e-06
Epoch [56/50], Training Loss: 22.1007, Validation Loss Current: 8.2557, Validation Loss AVG: 8.2557, lr: 1.0000000000000005e-07
Epoch [57/50], Training Loss: 22.5083, Validation Loss Current: 8.2570, Validation Loss AVG: 8.2570, lr: 1.0000000000000005e-07
Epoch [58/50], Training Loss: 21.6109, Validation Loss Current: 8.2494, Validation Loss AVG: 8.2494, lr: 1.0000000000000005e-07
Epoch [59/50], Training Loss: 22.0374, Validation Loss Current: 8.2478, Validation Loss AVG: 8.2478, lr: 1.0000000000000005e-07
Epoch [60/50], Training Loss: 21.7991, Validation Loss Current: 8.2598, Validation Loss AVG: 8.2598, lr: 1.0000000000000005e-07
Epoch [61/50], Training Loss: 22.0205, Validation Loss Current: 8.2570, Validation Loss AVG: 8.2570, lr: 1.0000000000000005e-07
 --- Early Stopped ---
Patch distance: 0.6 finished training. Best epoch: 31 Best val accuracy: [0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.23355263157894735, 0.23190789473684212, 0.19111842105263158, 0.2901315789473684, 0.26907894736842103, 0.24046052631578946, 0.27171052631578946, 0.24769736842105267, 0.3296052631578948, 0.23782894736842106, 0.3348684210526316, 0.3582236842105263, 0.3730263157894737, 0.37631578947368416, 0.36776315789473685, 0.36151315789473687, 0.38585526315789476, 0.3815789473684211, 0.38848684210526313, 0.3924342105263158, 0.3983552631578947, 0.3973684210526316, 0.4006578947368421, 0.38092105263157894, 0.4016447368421052, 0.39342105263157895, 0.3960526315789473, 0.40493421052631573, 0.40230263157894736, 0.40657894736842104, 0.40197368421052626, 0.41019736842105264, 0.4052631578947368, 0.4029605263157895, 0.40625, 0.40756578947368427, 0.40921052631578947, 0.4082236842105263, 0.4072368421052632, 0.4095394736842105, 0.40888157894736843, 0.4082236842105263, 0.40855263157894733, 0.4078947368421052, 0.4078947368421052, 0.4078947368421052, 0.4078947368421052, 0.4078947368421052, 0.4078947368421052, 0.4078947368421052, 0.4078947368421052, 0.4078947368421052, 0.4078947368421052, 0.4078947368421052, 0.4078947368421052, 0.4078947368421052, 0.4078947368421052, 0.4078947368421052] Best val loss: 8.183244729042054


Loaded best state dict for [0.6]
Current group: 0.8
Epoch [1/50], Training Loss: 31.6492, Validation Loss Current: 9.3870, Validation Loss AVG: 9.3870, lr: 0.1
Epoch [2/50], Training Loss: 32.1868, Validation Loss Current: 11.4338, Validation Loss AVG: 11.4338, lr: 0.1
Epoch [3/50], Training Loss: 34.7322, Validation Loss Current: 9.3068, Validation Loss AVG: 9.3068, lr: 0.1
Epoch [4/50], Training Loss: 32.6750, Validation Loss Current: 9.9667, Validation Loss AVG: 9.9667, lr: 0.1
Epoch [5/50], Training Loss: 33.6517, Validation Loss Current: 9.2579, Validation Loss AVG: 9.2579, lr: 0.1
Epoch [6/50], Training Loss: 33.3111, Validation Loss Current: 9.9331, Validation Loss AVG: 9.9331, lr: 0.1
Epoch [7/50], Training Loss: 34.3919, Validation Loss Current: 9.9697, Validation Loss AVG: 9.9697, lr: 0.1
Epoch [8/50], Training Loss: 31.5989, Validation Loss Current: 9.8912, Validation Loss AVG: 9.8912, lr: 0.1
Epoch [9/50], Training Loss: 31.0078, Validation Loss Current: 9.5945, Validation Loss AVG: 9.5945, lr: 0.1
Epoch [10/50], Training Loss: 31.5053, Validation Loss Current: 10.5487, Validation Loss AVG: 10.5487, lr: 0.1
Epoch [11/50], Training Loss: 32.1032, Validation Loss Current: 9.7505, Validation Loss AVG: 9.7505, lr: 0.1
Epoch [12/50], Training Loss: 29.8642, Validation Loss Current: 8.5859, Validation Loss AVG: 8.5859, lr: 0.010000000000000002
Epoch [13/50], Training Loss: 27.5022, Validation Loss Current: 8.4874, Validation Loss AVG: 8.4874, lr: 0.010000000000000002
Epoch [14/50], Training Loss: 27.5206, Validation Loss Current: 8.6367, Validation Loss AVG: 8.6367, lr: 0.010000000000000002
Epoch [15/50], Training Loss: 25.8155, Validation Loss Current: 8.6979, Validation Loss AVG: 8.6979, lr: 0.010000000000000002
Epoch [16/50], Training Loss: 25.2660, Validation Loss Current: 8.7551, Validation Loss AVG: 8.7551, lr: 0.010000000000000002
Epoch [17/50], Training Loss: 24.0540, Validation Loss Current: 8.8541, Validation Loss AVG: 8.8541, lr: 0.010000000000000002
Epoch [18/50], Training Loss: 23.5115, Validation Loss Current: 8.9897, Validation Loss AVG: 8.9897, lr: 0.010000000000000002
Epoch [19/50], Training Loss: 23.1997, Validation Loss Current: 9.2239, Validation Loss AVG: 9.2239, lr: 0.010000000000000002
Epoch [20/50], Training Loss: 22.0861, Validation Loss Current: 9.0331, Validation Loss AVG: 9.0331, lr: 0.0010000000000000002
Epoch [21/50], Training Loss: 23.4429, Validation Loss Current: 8.9568, Validation Loss AVG: 8.9568, lr: 0.0010000000000000002
Epoch [22/50], Training Loss: 23.2112, Validation Loss Current: 8.9519, Validation Loss AVG: 8.9519, lr: 0.0010000000000000002
Epoch [23/50], Training Loss: 20.9750, Validation Loss Current: 9.0019, Validation Loss AVG: 9.0019, lr: 0.0010000000000000002
Epoch [24/50], Training Loss: 23.1930, Validation Loss Current: 9.0415, Validation Loss AVG: 9.0415, lr: 0.0010000000000000002
Epoch [25/50], Training Loss: 21.1585, Validation Loss Current: 9.0850, Validation Loss AVG: 9.0850, lr: 0.0010000000000000002
Epoch [26/50], Training Loss: 20.6733, Validation Loss Current: 9.0795, Validation Loss AVG: 9.0795, lr: 0.00010000000000000003
Epoch [27/50], Training Loss: 21.4906, Validation Loss Current: 9.0655, Validation Loss AVG: 9.0655, lr: 0.00010000000000000003
Epoch [28/50], Training Loss: 21.6175, Validation Loss Current: 9.0928, Validation Loss AVG: 9.0928, lr: 0.00010000000000000003
Epoch [29/50], Training Loss: 21.3477, Validation Loss Current: 9.0673, Validation Loss AVG: 9.0673, lr: 0.00010000000000000003
Epoch [30/50], Training Loss: 21.2161, Validation Loss Current: 9.0639, Validation Loss AVG: 9.0639, lr: 0.00010000000000000003
Epoch [31/50], Training Loss: 21.0652, Validation Loss Current: 9.1041, Validation Loss AVG: 9.1041, lr: 0.00010000000000000003
Epoch [32/50], Training Loss: 20.4708, Validation Loss Current: 9.0797, Validation Loss AVG: 9.0797, lr: 1.0000000000000004e-05
Epoch [33/50], Training Loss: 20.6128, Validation Loss Current: 9.0438, Validation Loss AVG: 9.0438, lr: 1.0000000000000004e-05
Epoch [34/50], Training Loss: 20.5946, Validation Loss Current: 9.0516, Validation Loss AVG: 9.0516, lr: 1.0000000000000004e-05
Epoch [35/50], Training Loss: 22.5162, Validation Loss Current: 9.0717, Validation Loss AVG: 9.0717, lr: 1.0000000000000004e-05
Epoch [36/50], Training Loss: 21.6555, Validation Loss Current: 9.0819, Validation Loss AVG: 9.0819, lr: 1.0000000000000004e-05
Epoch [37/50], Training Loss: 20.8916, Validation Loss Current: 9.0904, Validation Loss AVG: 9.0904, lr: 1.0000000000000004e-05
Epoch [38/50], Training Loss: 21.7827, Validation Loss Current: 9.0804, Validation Loss AVG: 9.0804, lr: 1.0000000000000004e-06
Epoch [39/50], Training Loss: 22.1130, Validation Loss Current: 9.1108, Validation Loss AVG: 9.1108, lr: 1.0000000000000004e-06
Epoch [40/50], Training Loss: 20.3216, Validation Loss Current: 9.0637, Validation Loss AVG: 9.0637, lr: 1.0000000000000004e-06
Epoch [41/50], Training Loss: 20.2007, Validation Loss Current: 9.0741, Validation Loss AVG: 9.0741, lr: 1.0000000000000004e-06
Epoch [42/50], Training Loss: 21.5553, Validation Loss Current: 9.0754, Validation Loss AVG: 9.0754, lr: 1.0000000000000004e-06
Epoch [43/50], Training Loss: 22.3726, Validation Loss Current: 9.1052, Validation Loss AVG: 9.1052, lr: 1.0000000000000004e-06
 --- Early Stopped ---
Patch distance: 0.8 finished training. Best epoch: 13 Best val accuracy: [0.3259868421052632, 0.33322368421052634, 0.30296052631578946, 0.2930921052631579, 0.32664473684210527, 0.2815789473684211, 0.3075657894736842, 0.29901315789473687, 0.2878289473684211, 0.2957236842105263, 0.3105263157894737, 0.3796052631578947, 0.39342105263157895, 0.39736842105263154, 0.39769736842105263, 0.4026315789473684, 0.38421052631578945, 0.39375, 0.38782894736842105, 0.3973684210526316, 0.40032894736842106, 0.3996710526315789, 0.39671052631578946, 0.39934210526315794, 0.4, 0.40032894736842095, 0.3990131578947368, 0.3993421052631579, 0.39934210526315783, 0.39868421052631575, 0.3990131578947368, 0.3990131578947368, 0.39868421052631575, 0.39868421052631575, 0.39868421052631575, 0.39868421052631575, 0.3990131578947368, 0.3990131578947368, 0.3990131578947368, 0.3990131578947368, 0.3990131578947368, 0.3990131578947368, 0.3990131578947368] Best val loss: 8.487425136566163


Loaded best state dict for [0.6, 0.8]
Current group: 1
Epoch [1/50], Training Loss: 30.1680, Validation Loss Current: 11.3129, Validation Loss AVG: 15.7398, lr: 0.1
Epoch [2/50], Training Loss: 32.1130, Validation Loss Current: 8.6209, Validation Loss AVG: 9.7186, lr: 0.1
Epoch [3/50], Training Loss: 30.7863, Validation Loss Current: 9.1909, Validation Loss AVG: 9.6842, lr: 0.1
Epoch [4/50], Training Loss: 31.5697, Validation Loss Current: 9.4667, Validation Loss AVG: 11.9158, lr: 0.1
Epoch [5/50], Training Loss: 33.2986, Validation Loss Current: 9.1964, Validation Loss AVG: 9.9248, lr: 0.1
Epoch [6/50], Training Loss: 32.4509, Validation Loss Current: 8.0955, Validation Loss AVG: 11.2412, lr: 0.1
Epoch [7/50], Training Loss: 32.1636, Validation Loss Current: 8.9709, Validation Loss AVG: 10.2026, lr: 0.1
Epoch [8/50], Training Loss: 31.5460, Validation Loss Current: 8.1271, Validation Loss AVG: 9.3729, lr: 0.1
Epoch [9/50], Training Loss: 32.2559, Validation Loss Current: 9.5404, Validation Loss AVG: 10.9614, lr: 0.1
Epoch [10/50], Training Loss: 30.7605, Validation Loss Current: 7.8748, Validation Loss AVG: 11.5841, lr: 0.1
Epoch [11/50], Training Loss: 31.4016, Validation Loss Current: 7.8388, Validation Loss AVG: 9.2454, lr: 0.1
Epoch [12/50], Training Loss: 28.5889, Validation Loss Current: 8.1184, Validation Loss AVG: 9.2640, lr: 0.1
Epoch [13/50], Training Loss: 28.7532, Validation Loss Current: 8.6150, Validation Loss AVG: 9.5082, lr: 0.1
Epoch [14/50], Training Loss: 29.6321, Validation Loss Current: 8.6902, Validation Loss AVG: 11.1699, lr: 0.1
Epoch [15/50], Training Loss: 32.3515, Validation Loss Current: 7.9602, Validation Loss AVG: 9.1118, lr: 0.1
Epoch [16/50], Training Loss: 30.1196, Validation Loss Current: 8.9033, Validation Loss AVG: 9.9407, lr: 0.1
Epoch [17/50], Training Loss: 30.8817, Validation Loss Current: 8.2065, Validation Loss AVG: 9.4066, lr: 0.1
Epoch [18/50], Training Loss: 27.5132, Validation Loss Current: 7.5068, Validation Loss AVG: 9.4417, lr: 0.010000000000000002
Epoch [19/50], Training Loss: 26.2386, Validation Loss Current: 7.4551, Validation Loss AVG: 9.1256, lr: 0.010000000000000002
Epoch [20/50], Training Loss: 27.1894, Validation Loss Current: 7.3337, Validation Loss AVG: 9.3533, lr: 0.010000000000000002
Epoch [21/50], Training Loss: 25.5613, Validation Loss Current: 7.2918, Validation Loss AVG: 9.5324, lr: 0.010000000000000002
Epoch [22/50], Training Loss: 23.6824, Validation Loss Current: 7.3380, Validation Loss AVG: 9.5749, lr: 0.010000000000000002
Epoch [23/50], Training Loss: 22.8325, Validation Loss Current: 7.1733, Validation Loss AVG: 9.7769, lr: 0.010000000000000002
Epoch [24/50], Training Loss: 23.0080, Validation Loss Current: 7.2561, Validation Loss AVG: 9.8349, lr: 0.010000000000000002
Epoch [25/50], Training Loss: 22.4219, Validation Loss Current: 7.3450, Validation Loss AVG: 10.7091, lr: 0.010000000000000002
Epoch [26/50], Training Loss: 21.1128, Validation Loss Current: 7.4340, Validation Loss AVG: 10.7234, lr: 0.010000000000000002
Epoch [27/50], Training Loss: 21.6144, Validation Loss Current: 7.5780, Validation Loss AVG: 11.8855, lr: 0.010000000000000002
Epoch [28/50], Training Loss: 20.8292, Validation Loss Current: 7.3924, Validation Loss AVG: 11.3441, lr: 0.010000000000000002
Epoch [29/50], Training Loss: 19.2185, Validation Loss Current: 7.5467, Validation Loss AVG: 11.2566, lr: 0.010000000000000002
Epoch [30/50], Training Loss: 18.5376, Validation Loss Current: 7.5293, Validation Loss AVG: 11.3020, lr: 0.0010000000000000002
Epoch [31/50], Training Loss: 17.9519, Validation Loss Current: 7.5638, Validation Loss AVG: 11.4870, lr: 0.0010000000000000002
Epoch [32/50], Training Loss: 19.0809, Validation Loss Current: 7.5668, Validation Loss AVG: 11.5993, lr: 0.0010000000000000002
Epoch [33/50], Training Loss: 18.6402, Validation Loss Current: 7.6323, Validation Loss AVG: 11.8719, lr: 0.0010000000000000002
Epoch [34/50], Training Loss: 19.4790, Validation Loss Current: 7.5620, Validation Loss AVG: 11.7731, lr: 0.0010000000000000002
Epoch [35/50], Training Loss: 18.1833, Validation Loss Current: 7.6648, Validation Loss AVG: 11.8635, lr: 0.0010000000000000002
Epoch [36/50], Training Loss: 18.3811, Validation Loss Current: 7.6594, Validation Loss AVG: 11.9050, lr: 0.00010000000000000003
Epoch [37/50], Training Loss: 17.5493, Validation Loss Current: 7.6439, Validation Loss AVG: 11.9204, lr: 0.00010000000000000003
Epoch [38/50], Training Loss: 18.0596, Validation Loss Current: 7.7609, Validation Loss AVG: 11.9107, lr: 0.00010000000000000003
Epoch [39/50], Training Loss: 17.8048, Validation Loss Current: 7.6074, Validation Loss AVG: 11.9238, lr: 0.00010000000000000003
Epoch [40/50], Training Loss: 17.2671, Validation Loss Current: 7.6416, Validation Loss AVG: 11.9468, lr: 0.00010000000000000003
Epoch [41/50], Training Loss: 17.8604, Validation Loss Current: 7.7224, Validation Loss AVG: 11.9322, lr: 0.00010000000000000003
Epoch [42/50], Training Loss: 17.9304, Validation Loss Current: 7.6700, Validation Loss AVG: 11.9074, lr: 1.0000000000000004e-05
Epoch [43/50], Training Loss: 17.8340, Validation Loss Current: 7.6616, Validation Loss AVG: 11.8899, lr: 1.0000000000000004e-05
Epoch [44/50], Training Loss: 17.9792, Validation Loss Current: 7.7262, Validation Loss AVG: 11.9291, lr: 1.0000000000000004e-05
Epoch [45/50], Training Loss: 17.1928, Validation Loss Current: 7.7100, Validation Loss AVG: 11.9597, lr: 1.0000000000000004e-05
Epoch [46/50], Training Loss: 17.8933, Validation Loss Current: 7.6321, Validation Loss AVG: 11.9650, lr: 1.0000000000000004e-05
Epoch [47/50], Training Loss: 17.5080, Validation Loss Current: 7.6474, Validation Loss AVG: 11.9200, lr: 1.0000000000000004e-05
Epoch [48/50], Training Loss: 17.5350, Validation Loss Current: 7.6645, Validation Loss AVG: 11.9201, lr: 1.0000000000000004e-06
Epoch [49/50], Training Loss: 17.4666, Validation Loss Current: 7.6736, Validation Loss AVG: 11.9149, lr: 1.0000000000000004e-06
Epoch [50/50], Training Loss: 17.7487, Validation Loss Current: 7.6833, Validation Loss AVG: 11.9181, lr: 1.0000000000000004e-06
Epoch [51/50], Training Loss: 17.7997, Validation Loss Current: 7.6645, Validation Loss AVG: 11.9318, lr: 1.0000000000000004e-06
Epoch [52/50], Training Loss: 18.3476, Validation Loss Current: 7.7739, Validation Loss AVG: 11.9560, lr: 1.0000000000000004e-06
Epoch [53/50], Training Loss: 17.4437, Validation Loss Current: 7.6579, Validation Loss AVG: 11.9231, lr: 1.0000000000000004e-06
 --- Early Stopped ---
Patch distance: 1 finished training. Best epoch: 23 Best val accuracy: [0.3371710526315789, 0.3256578947368421, 0.32401315789473684, 0.3355263157894737, 0.31743421052631576, 0.42598684210526316, 0.3963815789473684, 0.3832236842105263, 0.3536184210526316, 0.3963815789473684, 0.4243421052631579, 0.3930921052631579, 0.3996710526315789, 0.3355263157894737, 0.4095394736842105, 0.37335526315789475, 0.4194078947368421, 0.4440789473684211, 0.45394736842105265, 0.46710526315789475, 0.4753289473684211, 0.48519736842105265, 0.5032894736842105, 0.4967105263157895, 0.4868421052631579, 0.5032894736842105, 0.5, 0.4868421052631579, 0.5016447368421053, 0.4917763157894737, 0.4901315789473684, 0.4917763157894737, 0.4917763157894737, 0.4868421052631579, 0.4868421052631579, 0.4868421052631579, 0.4868421052631579, 0.4868421052631579, 0.4868421052631579, 0.4868421052631579, 0.4868421052631579, 0.4868421052631579, 0.4868421052631579, 0.4868421052631579, 0.4868421052631579, 0.4868421052631579, 0.4868421052631579, 0.4868421052631579, 0.4868421052631579, 0.4868421052631579, 0.4868421052631579, 0.4868421052631579, 0.4868421052631579] Best val loss: 7.17328155040741


----- Training alexnet with sequence: [0.8, 1] -----
Current group: 0.8
Epoch [1/75], Training Loss: 40.1943, Validation Loss Current: 9.9717, Validation Loss AVG: 9.9717, lr: 0.1
Epoch [2/75], Training Loss: 39.9899, Validation Loss Current: 9.8672, Validation Loss AVG: 9.8672, lr: 0.1
Epoch [3/75], Training Loss: 37.8354, Validation Loss Current: 11.5872, Validation Loss AVG: 11.5872, lr: 0.1
Epoch [4/75], Training Loss: 36.9508, Validation Loss Current: 9.7536, Validation Loss AVG: 9.7536, lr: 0.1
Epoch [5/75], Training Loss: 35.6987, Validation Loss Current: 14.9861, Validation Loss AVG: 14.9861, lr: 0.1
Epoch [6/75], Training Loss: 40.0758, Validation Loss Current: 11.0792, Validation Loss AVG: 11.0792, lr: 0.1
Epoch [7/75], Training Loss: 35.3150, Validation Loss Current: 9.5411, Validation Loss AVG: 9.5411, lr: 0.1
Epoch [8/75], Training Loss: 33.1371, Validation Loss Current: 10.2905, Validation Loss AVG: 10.2905, lr: 0.1
Epoch [9/75], Training Loss: 34.4758, Validation Loss Current: 9.3873, Validation Loss AVG: 9.3873, lr: 0.1
Epoch [10/75], Training Loss: 33.9399, Validation Loss Current: 12.1090, Validation Loss AVG: 12.1090, lr: 0.1
Epoch [11/75], Training Loss: 33.0987, Validation Loss Current: 10.6927, Validation Loss AVG: 10.6927, lr: 0.1
Epoch [12/75], Training Loss: 33.1371, Validation Loss Current: 9.2894, Validation Loss AVG: 9.2894, lr: 0.1
Epoch [13/75], Training Loss: 34.1301, Validation Loss Current: 9.8822, Validation Loss AVG: 9.8822, lr: 0.1
Epoch [14/75], Training Loss: 32.9056, Validation Loss Current: 9.7685, Validation Loss AVG: 9.7685, lr: 0.1
Epoch [15/75], Training Loss: 31.8281, Validation Loss Current: 9.1904, Validation Loss AVG: 9.1904, lr: 0.1
Epoch [16/75], Training Loss: 32.0668, Validation Loss Current: 9.3434, Validation Loss AVG: 9.3434, lr: 0.1
Epoch [17/75], Training Loss: 32.5691, Validation Loss Current: 9.9583, Validation Loss AVG: 9.9583, lr: 0.1
Epoch [18/75], Training Loss: 33.8978, Validation Loss Current: 9.6543, Validation Loss AVG: 9.6543, lr: 0.1
Epoch [19/75], Training Loss: 32.1831, Validation Loss Current: 11.2037, Validation Loss AVG: 11.2037, lr: 0.1
Epoch [20/75], Training Loss: 30.7255, Validation Loss Current: 10.2091, Validation Loss AVG: 10.2091, lr: 0.1
Epoch [21/75], Training Loss: 32.6590, Validation Loss Current: 9.3907, Validation Loss AVG: 9.3907, lr: 0.1
Epoch [22/75], Training Loss: 27.2568, Validation Loss Current: 8.8097, Validation Loss AVG: 8.8097, lr: 0.010000000000000002
Epoch [23/75], Training Loss: 25.3001, Validation Loss Current: 8.5362, Validation Loss AVG: 8.5362, lr: 0.010000000000000002
Epoch [24/75], Training Loss: 24.0910, Validation Loss Current: 8.5934, Validation Loss AVG: 8.5934, lr: 0.010000000000000002
Epoch [25/75], Training Loss: 22.2521, Validation Loss Current: 9.1723, Validation Loss AVG: 9.1723, lr: 0.010000000000000002
Epoch [26/75], Training Loss: 20.9815, Validation Loss Current: 9.9734, Validation Loss AVG: 9.9734, lr: 0.010000000000000002
Epoch [27/75], Training Loss: 22.7217, Validation Loss Current: 9.3700, Validation Loss AVG: 9.3700, lr: 0.010000000000000002
Epoch [28/75], Training Loss: 20.7791, Validation Loss Current: 11.0964, Validation Loss AVG: 11.0964, lr: 0.010000000000000002
Epoch [29/75], Training Loss: 19.5525, Validation Loss Current: 10.7603, Validation Loss AVG: 10.7603, lr: 0.010000000000000002
Epoch [30/75], Training Loss: 18.2699, Validation Loss Current: 10.3928, Validation Loss AVG: 10.3928, lr: 0.0010000000000000002
Epoch [31/75], Training Loss: 17.5357, Validation Loss Current: 10.5424, Validation Loss AVG: 10.5424, lr: 0.0010000000000000002
Epoch [32/75], Training Loss: 18.1973, Validation Loss Current: 10.7696, Validation Loss AVG: 10.7696, lr: 0.0010000000000000002
Epoch [33/75], Training Loss: 17.4160, Validation Loss Current: 10.8191, Validation Loss AVG: 10.8191, lr: 0.0010000000000000002
Epoch [34/75], Training Loss: 17.4460, Validation Loss Current: 10.6799, Validation Loss AVG: 10.6799, lr: 0.0010000000000000002
Epoch [35/75], Training Loss: 19.8409, Validation Loss Current: 10.9318, Validation Loss AVG: 10.9318, lr: 0.0010000000000000002
Epoch [36/75], Training Loss: 18.6240, Validation Loss Current: 10.9724, Validation Loss AVG: 10.9724, lr: 0.00010000000000000003
Epoch [37/75], Training Loss: 16.6733, Validation Loss Current: 10.9142, Validation Loss AVG: 10.9142, lr: 0.00010000000000000003
Epoch [38/75], Training Loss: 18.2683, Validation Loss Current: 10.8707, Validation Loss AVG: 10.8707, lr: 0.00010000000000000003
Epoch [39/75], Training Loss: 17.5243, Validation Loss Current: 10.8140, Validation Loss AVG: 10.8140, lr: 0.00010000000000000003
Epoch [40/75], Training Loss: 17.6410, Validation Loss Current: 10.8490, Validation Loss AVG: 10.8490, lr: 0.00010000000000000003
Epoch [41/75], Training Loss: 17.7478, Validation Loss Current: 10.8871, Validation Loss AVG: 10.8871, lr: 0.00010000000000000003
Epoch [42/75], Training Loss: 17.1523, Validation Loss Current: 10.8690, Validation Loss AVG: 10.8690, lr: 1.0000000000000004e-05
Epoch [43/75], Training Loss: 17.4666, Validation Loss Current: 10.8670, Validation Loss AVG: 10.8670, lr: 1.0000000000000004e-05
Epoch [44/75], Training Loss: 16.9804, Validation Loss Current: 10.8803, Validation Loss AVG: 10.8803, lr: 1.0000000000000004e-05
Epoch [45/75], Training Loss: 16.4689, Validation Loss Current: 10.8427, Validation Loss AVG: 10.8427, lr: 1.0000000000000004e-05
Epoch [46/75], Training Loss: 16.6741, Validation Loss Current: 10.8447, Validation Loss AVG: 10.8447, lr: 1.0000000000000004e-05
Epoch [47/75], Training Loss: 16.5533, Validation Loss Current: 10.8273, Validation Loss AVG: 10.8273, lr: 1.0000000000000004e-05
Epoch [48/75], Training Loss: 16.9942, Validation Loss Current: 10.8334, Validation Loss AVG: 10.8334, lr: 1.0000000000000004e-06
Epoch [49/75], Training Loss: 18.6445, Validation Loss Current: 10.8271, Validation Loss AVG: 10.8271, lr: 1.0000000000000004e-06
Epoch [50/75], Training Loss: 17.4660, Validation Loss Current: 10.8677, Validation Loss AVG: 10.8677, lr: 1.0000000000000004e-06
Epoch [51/75], Training Loss: 16.7979, Validation Loss Current: 10.8944, Validation Loss AVG: 10.8944, lr: 1.0000000000000004e-06
Epoch [52/75], Training Loss: 17.4913, Validation Loss Current: 10.8353, Validation Loss AVG: 10.8353, lr: 1.0000000000000004e-06
Epoch [53/75], Training Loss: 16.5741, Validation Loss Current: 10.8715, Validation Loss AVG: 10.8715, lr: 1.0000000000000004e-06
 --- Early Stopped ---
Patch distance: 0.8 finished training. Best epoch: 23 Best val accuracy: [0.22532894736842107, 0.2273026315789474, 0.2993421052631579, 0.28125, 0.2763157894736842, 0.15394736842105264, 0.2953947368421053, 0.33092105263157895, 0.27302631578947373, 0.2595394736842105, 0.2325657894736842, 0.2888157894736842, 0.275, 0.31052631578947365, 0.3519736842105264, 0.349671052631579, 0.3447368421052632, 0.2963815789473684, 0.30493421052631575, 0.31973684210526315, 0.3375, 0.38651315789473684, 0.39802631578947373, 0.39111842105263156, 0.38453947368421054, 0.3710526315789474, 0.3996710526315789, 0.3822368421052632, 0.375, 0.3819078947368421, 0.38125, 0.3786184210526316, 0.3796052631578947, 0.38092105263157894, 0.3815789473684211, 0.3819078947368421, 0.3822368421052632, 0.3832236842105264, 0.3838815789473684, 0.3828947368421053, 0.38355263157894737, 0.38355263157894737, 0.38322368421052627, 0.38322368421052627, 0.38322368421052627, 0.38322368421052627, 0.38355263157894737, 0.38355263157894737, 0.38355263157894737, 0.38355263157894737, 0.38355263157894737, 0.38355263157894737, 0.38355263157894737] Best val loss: 8.536188960075378


Loaded best state dict for [0.8]
Current group: 1
Epoch [1/75], Training Loss: 26.9631, Validation Loss Current: 10.1594, Validation Loss AVG: 15.6144, lr: 0.1
Epoch [2/75], Training Loss: 29.9783, Validation Loss Current: 8.3668, Validation Loss AVG: 8.9943, lr: 0.1
Epoch [3/75], Training Loss: 29.0527, Validation Loss Current: 8.1629, Validation Loss AVG: 9.3103, lr: 0.1
Epoch [4/75], Training Loss: 29.9193, Validation Loss Current: 8.5111, Validation Loss AVG: 10.9249, lr: 0.1
Epoch [5/75], Training Loss: 30.9539, Validation Loss Current: 7.9745, Validation Loss AVG: 9.3699, lr: 0.1
Epoch [6/75], Training Loss: 29.2941, Validation Loss Current: 8.7848, Validation Loss AVG: 10.1741, lr: 0.1
Epoch [7/75], Training Loss: 30.2674, Validation Loss Current: 8.2574, Validation Loss AVG: 9.3653, lr: 0.1
Epoch [8/75], Training Loss: 28.0290, Validation Loss Current: 8.2742, Validation Loss AVG: 9.5839, lr: 0.1
Epoch [9/75], Training Loss: 29.2491, Validation Loss Current: 7.8324, Validation Loss AVG: 9.7593, lr: 0.1
Epoch [10/75], Training Loss: 28.7415, Validation Loss Current: 8.5149, Validation Loss AVG: 10.4969, lr: 0.1
Epoch [11/75], Training Loss: 27.1141, Validation Loss Current: 9.3202, Validation Loss AVG: 11.9572, lr: 0.1
Epoch [12/75], Training Loss: 27.0006, Validation Loss Current: 7.9399, Validation Loss AVG: 9.0908, lr: 0.1
Epoch [13/75], Training Loss: 26.4295, Validation Loss Current: 7.5632, Validation Loss AVG: 9.0775, lr: 0.1
Epoch [14/75], Training Loss: 26.4552, Validation Loss Current: 8.0750, Validation Loss AVG: 9.3681, lr: 0.1
Epoch [15/75], Training Loss: 25.3664, Validation Loss Current: 17.1089, Validation Loss AVG: 35.3614, lr: 0.1
Epoch [16/75], Training Loss: 28.3247, Validation Loss Current: 8.6988, Validation Loss AVG: 10.4787, lr: 0.1
Epoch [17/75], Training Loss: 27.5701, Validation Loss Current: 7.7526, Validation Loss AVG: 9.5857, lr: 0.1
Epoch [18/75], Training Loss: 27.1840, Validation Loss Current: 7.8687, Validation Loss AVG: 9.5742, lr: 0.1
Epoch [19/75], Training Loss: 26.4324, Validation Loss Current: 9.8169, Validation Loss AVG: 15.7158, lr: 0.1
Epoch [20/75], Training Loss: 25.8979, Validation Loss Current: 7.3665, Validation Loss AVG: 10.4203, lr: 0.010000000000000002
Epoch [21/75], Training Loss: 22.0126, Validation Loss Current: 6.9124, Validation Loss AVG: 10.7219, lr: 0.010000000000000002
Epoch [22/75], Training Loss: 18.9739, Validation Loss Current: 7.0405, Validation Loss AVG: 10.9846, lr: 0.010000000000000002
Epoch [23/75], Training Loss: 17.8338, Validation Loss Current: 6.9906, Validation Loss AVG: 10.6508, lr: 0.010000000000000002
Epoch [24/75], Training Loss: 16.1977, Validation Loss Current: 7.4816, Validation Loss AVG: 11.5956, lr: 0.010000000000000002
Epoch [25/75], Training Loss: 16.0942, Validation Loss Current: 7.3490, Validation Loss AVG: 12.2368, lr: 0.010000000000000002
Epoch [26/75], Training Loss: 13.9153, Validation Loss Current: 7.6160, Validation Loss AVG: 12.7902, lr: 0.010000000000000002
Epoch [27/75], Training Loss: 13.7069, Validation Loss Current: 7.7296, Validation Loss AVG: 12.8092, lr: 0.010000000000000002
Epoch [28/75], Training Loss: 13.4914, Validation Loss Current: 7.6113, Validation Loss AVG: 13.2405, lr: 0.0010000000000000002
Epoch [29/75], Training Loss: 12.2643, Validation Loss Current: 7.7018, Validation Loss AVG: 13.5414, lr: 0.0010000000000000002
Epoch [30/75], Training Loss: 11.5684, Validation Loss Current: 7.7616, Validation Loss AVG: 13.6806, lr: 0.0010000000000000002
Epoch [31/75], Training Loss: 11.5381, Validation Loss Current: 7.8014, Validation Loss AVG: 13.6885, lr: 0.0010000000000000002
Epoch [32/75], Training Loss: 12.1033, Validation Loss Current: 7.8802, Validation Loss AVG: 13.6846, lr: 0.0010000000000000002
Epoch [33/75], Training Loss: 12.6645, Validation Loss Current: 7.9384, Validation Loss AVG: 13.9713, lr: 0.0010000000000000002
Epoch [34/75], Training Loss: 11.3309, Validation Loss Current: 8.0073, Validation Loss AVG: 14.0704, lr: 0.00010000000000000003
Epoch [35/75], Training Loss: 14.3572, Validation Loss Current: 7.8747, Validation Loss AVG: 13.9562, lr: 0.00010000000000000003
Epoch [36/75], Training Loss: 11.6892, Validation Loss Current: 7.9143, Validation Loss AVG: 13.8399, lr: 0.00010000000000000003
Epoch [37/75], Training Loss: 11.0979, Validation Loss Current: 7.9229, Validation Loss AVG: 13.8600, lr: 0.00010000000000000003
Epoch [38/75], Training Loss: 11.3158, Validation Loss Current: 7.8521, Validation Loss AVG: 13.8458, lr: 0.00010000000000000003
Epoch [39/75], Training Loss: 11.1305, Validation Loss Current: 7.9454, Validation Loss AVG: 13.8867, lr: 0.00010000000000000003
Epoch [40/75], Training Loss: 11.3332, Validation Loss Current: 7.9615, Validation Loss AVG: 13.8254, lr: 1.0000000000000004e-05
Epoch [41/75], Training Loss: 12.2741, Validation Loss Current: 7.9957, Validation Loss AVG: 13.8778, lr: 1.0000000000000004e-05
Epoch [42/75], Training Loss: 13.6857, Validation Loss Current: 7.9095, Validation Loss AVG: 13.9114, lr: 1.0000000000000004e-05
Epoch [43/75], Training Loss: 11.0580, Validation Loss Current: 7.9497, Validation Loss AVG: 13.8532, lr: 1.0000000000000004e-05
Epoch [44/75], Training Loss: 12.0834, Validation Loss Current: 7.9627, Validation Loss AVG: 13.8423, lr: 1.0000000000000004e-05
Epoch [45/75], Training Loss: 12.0993, Validation Loss Current: 8.0243, Validation Loss AVG: 13.8414, lr: 1.0000000000000004e-05
Epoch [46/75], Training Loss: 11.1065, Validation Loss Current: 7.9308, Validation Loss AVG: 13.8104, lr: 1.0000000000000004e-06
Epoch [47/75], Training Loss: 10.9367, Validation Loss Current: 7.9924, Validation Loss AVG: 13.8130, lr: 1.0000000000000004e-06
Epoch [48/75], Training Loss: 11.4307, Validation Loss Current: 7.9582, Validation Loss AVG: 13.8965, lr: 1.0000000000000004e-06
Epoch [49/75], Training Loss: 14.1020, Validation Loss Current: 7.9074, Validation Loss AVG: 13.8558, lr: 1.0000000000000004e-06
Epoch [50/75], Training Loss: 11.4673, Validation Loss Current: 7.9006, Validation Loss AVG: 13.8189, lr: 1.0000000000000004e-06
Epoch [51/75], Training Loss: 12.8541, Validation Loss Current: 7.9752, Validation Loss AVG: 13.9053, lr: 1.0000000000000004e-06
 --- Early Stopped ---
Patch distance: 1 finished training. Best epoch: 21 Best val accuracy: [0.40789473684210525, 0.4029605263157895, 0.4407894736842105, 0.41118421052631576, 0.4375, 0.44901315789473684, 0.40625, 0.4144736842105263, 0.4555921052631579, 0.4243421052631579, 0.42598684210526316, 0.4309210526315789, 0.4720394736842105, 0.4786184210526316, 0.29605263157894735, 0.3717105263157895, 0.47368421052631576, 0.4440789473684211, 0.4375, 0.5460526315789473, 0.5460526315789473, 0.5411184210526315, 0.5444078947368421, 0.5509868421052632, 0.5460526315789473, 0.5526315789473685, 0.5575657894736842, 0.5444078947368421, 0.555921052631579, 0.5575657894736842, 0.5657894736842105, 0.5608552631578947, 0.5592105263157895, 0.5575657894736842, 0.555921052631579, 0.555921052631579, 0.5575657894736842, 0.5657894736842105, 0.5592105263157895, 0.5592105263157895, 0.5592105263157895, 0.5592105263157895, 0.5592105263157895, 0.5592105263157895, 0.5592105263157895, 0.5592105263157895, 0.5592105263157895, 0.5592105263157895, 0.5592105263157895, 0.5592105263157895, 0.5592105263157895] Best val loss: 6.912434101104736


----- Training alexnet with sequence: [1] -----
Current group: 1
Epoch [1/150], Training Loss: 40.7150, Validation Loss Current: 9.9281, Validation Loss AVG: 10.0269, lr: 0.1
Epoch [2/150], Training Loss: 39.6676, Validation Loss Current: 10.8867, Validation Loss AVG: 11.2398, lr: 0.1
Epoch [3/150], Training Loss: 39.6508, Validation Loss Current: 9.9711, Validation Loss AVG: 10.0135, lr: 0.1
Epoch [4/150], Training Loss: 39.5543, Validation Loss Current: 9.5166, Validation Loss AVG: 10.0756, lr: 0.1
Epoch [5/150], Training Loss: 37.4119, Validation Loss Current: 9.1103, Validation Loss AVG: 9.8107, lr: 0.1
Epoch [6/150], Training Loss: 36.8843, Validation Loss Current: 10.0155, Validation Loss AVG: 10.9858, lr: 0.1
Epoch [7/150], Training Loss: 35.7398, Validation Loss Current: 8.8944, Validation Loss AVG: 10.0752, lr: 0.1
Epoch [8/150], Training Loss: 35.4743, Validation Loss Current: 16.0979, Validation Loss AVG: 16.4899, lr: 0.1
Epoch [9/150], Training Loss: 37.9133, Validation Loss Current: 8.8229, Validation Loss AVG: 9.4204, lr: 0.1
Epoch [10/150], Training Loss: 35.3324, Validation Loss Current: 9.7180, Validation Loss AVG: 12.8104, lr: 0.1
Epoch [11/150], Training Loss: 33.4750, Validation Loss Current: 8.2999, Validation Loss AVG: 10.3971, lr: 0.1
Epoch [12/150], Training Loss: 32.4098, Validation Loss Current: 8.5677, Validation Loss AVG: 10.6217, lr: 0.1
Epoch [13/150], Training Loss: 33.3037, Validation Loss Current: 9.5934, Validation Loss AVG: 12.3318, lr: 0.1
Epoch [14/150], Training Loss: 32.3544, Validation Loss Current: 8.7046, Validation Loss AVG: 10.8136, lr: 0.1
Epoch [15/150], Training Loss: 34.0575, Validation Loss Current: 9.1966, Validation Loss AVG: 10.0358, lr: 0.1
Epoch [16/150], Training Loss: 32.9582, Validation Loss Current: 10.3980, Validation Loss AVG: 11.9196, lr: 0.1
Epoch [17/150], Training Loss: 33.1530, Validation Loss Current: 7.7344, Validation Loss AVG: 10.8568, lr: 0.1
Epoch [18/150], Training Loss: 30.2699, Validation Loss Current: 8.1410, Validation Loss AVG: 11.5090, lr: 0.1
Epoch [19/150], Training Loss: 32.2424, Validation Loss Current: 8.9291, Validation Loss AVG: 11.1346, lr: 0.1
Epoch [20/150], Training Loss: 32.1153, Validation Loss Current: 8.1964, Validation Loss AVG: 9.3015, lr: 0.1
Epoch [21/150], Training Loss: 30.3521, Validation Loss Current: 8.6594, Validation Loss AVG: 11.4437, lr: 0.1
Epoch [22/150], Training Loss: 30.4446, Validation Loss Current: 7.9545, Validation Loss AVG: 9.5650, lr: 0.1
Epoch [23/150], Training Loss: 28.7212, Validation Loss Current: 7.8945, Validation Loss AVG: 11.0265, lr: 0.1
Epoch [24/150], Training Loss: 25.5735, Validation Loss Current: 7.8126, Validation Loss AVG: 10.9704, lr: 0.010000000000000002
Epoch [25/150], Training Loss: 23.5446, Validation Loss Current: 7.4752, Validation Loss AVG: 10.6779, lr: 0.010000000000000002
Epoch [26/150], Training Loss: 23.2511, Validation Loss Current: 7.0888, Validation Loss AVG: 10.3675, lr: 0.010000000000000002
Epoch [27/150], Training Loss: 20.7092, Validation Loss Current: 7.2854, Validation Loss AVG: 10.6876, lr: 0.010000000000000002
Epoch [28/150], Training Loss: 22.2504, Validation Loss Current: 6.9605, Validation Loss AVG: 10.7876, lr: 0.010000000000000002
Epoch [29/150], Training Loss: 20.0022, Validation Loss Current: 7.0567, Validation Loss AVG: 11.4017, lr: 0.010000000000000002
Epoch [30/150], Training Loss: 19.0364, Validation Loss Current: 6.9438, Validation Loss AVG: 11.1558, lr: 0.010000000000000002
Epoch [31/150], Training Loss: 18.1863, Validation Loss Current: 7.0541, Validation Loss AVG: 11.1747, lr: 0.010000000000000002
Epoch [32/150], Training Loss: 17.0749, Validation Loss Current: 7.1208, Validation Loss AVG: 10.7902, lr: 0.010000000000000002
Epoch [33/150], Training Loss: 17.8360, Validation Loss Current: 7.8742, Validation Loss AVG: 12.2257, lr: 0.010000000000000002
Epoch [34/150], Training Loss: 18.0303, Validation Loss Current: 7.6498, Validation Loss AVG: 12.3485, lr: 0.010000000000000002
Epoch [35/150], Training Loss: 16.5597, Validation Loss Current: 7.7929, Validation Loss AVG: 13.0383, lr: 0.010000000000000002
Epoch [36/150], Training Loss: 15.2113, Validation Loss Current: 7.4754, Validation Loss AVG: 12.6676, lr: 0.010000000000000002
Epoch [37/150], Training Loss: 14.6941, Validation Loss Current: 7.6098, Validation Loss AVG: 12.6977, lr: 0.0010000000000000002
Epoch [38/150], Training Loss: 13.4383, Validation Loss Current: 7.7850, Validation Loss AVG: 12.9525, lr: 0.0010000000000000002
Epoch [39/150], Training Loss: 13.3986, Validation Loss Current: 7.8942, Validation Loss AVG: 13.0891, lr: 0.0010000000000000002
Epoch [40/150], Training Loss: 12.8717, Validation Loss Current: 8.0477, Validation Loss AVG: 13.3164, lr: 0.0010000000000000002
Epoch [41/150], Training Loss: 12.6554, Validation Loss Current: 7.9050, Validation Loss AVG: 13.4188, lr: 0.0010000000000000002
Epoch [42/150], Training Loss: 13.6751, Validation Loss Current: 7.8758, Validation Loss AVG: 13.3275, lr: 0.0010000000000000002
Epoch [43/150], Training Loss: 13.7063, Validation Loss Current: 8.0067, Validation Loss AVG: 13.3397, lr: 0.00010000000000000003
Epoch [44/150], Training Loss: 12.2649, Validation Loss Current: 7.9853, Validation Loss AVG: 13.3356, lr: 0.00010000000000000003
Epoch [45/150], Training Loss: 12.8246, Validation Loss Current: 8.0325, Validation Loss AVG: 13.4380, lr: 0.00010000000000000003
Epoch [46/150], Training Loss: 12.2554, Validation Loss Current: 8.0310, Validation Loss AVG: 13.4394, lr: 0.00010000000000000003
Epoch [47/150], Training Loss: 12.5124, Validation Loss Current: 7.9885, Validation Loss AVG: 13.4425, lr: 0.00010000000000000003
Epoch [48/150], Training Loss: 12.8328, Validation Loss Current: 8.0432, Validation Loss AVG: 13.4923, lr: 0.00010000000000000003
Epoch [49/150], Training Loss: 12.0921, Validation Loss Current: 7.9983, Validation Loss AVG: 13.5132, lr: 1.0000000000000004e-05
Epoch [50/150], Training Loss: 12.6222, Validation Loss Current: 8.0003, Validation Loss AVG: 13.5536, lr: 1.0000000000000004e-05
Epoch [51/150], Training Loss: 12.9333, Validation Loss Current: 8.0944, Validation Loss AVG: 13.5063, lr: 1.0000000000000004e-05
Epoch [52/150], Training Loss: 11.9581, Validation Loss Current: 8.0129, Validation Loss AVG: 13.5334, lr: 1.0000000000000004e-05
Epoch [53/150], Training Loss: 12.5792, Validation Loss Current: 8.0280, Validation Loss AVG: 13.5157, lr: 1.0000000000000004e-05
Epoch [54/150], Training Loss: 13.1202, Validation Loss Current: 8.0142, Validation Loss AVG: 13.5502, lr: 1.0000000000000004e-05
Epoch [55/150], Training Loss: 12.5326, Validation Loss Current: 8.2082, Validation Loss AVG: 13.6031, lr: 1.0000000000000004e-06
Epoch [56/150], Training Loss: 13.1480, Validation Loss Current: 8.0541, Validation Loss AVG: 13.5424, lr: 1.0000000000000004e-06
Epoch [57/150], Training Loss: 13.5550, Validation Loss Current: 8.1627, Validation Loss AVG: 13.5654, lr: 1.0000000000000004e-06
Epoch [58/150], Training Loss: 13.6219, Validation Loss Current: 8.0515, Validation Loss AVG: 13.5328, lr: 1.0000000000000004e-06
Epoch [59/150], Training Loss: 12.0615, Validation Loss Current: 8.0112, Validation Loss AVG: 13.4836, lr: 1.0000000000000004e-06
Epoch [60/150], Training Loss: 12.2217, Validation Loss Current: 8.1026, Validation Loss AVG: 13.5881, lr: 1.0000000000000004e-06
 --- Early Stopped ---
Patch distance: 1 finished training. Best epoch: 30 Best val accuracy: [0.22697368421052633, 0.2713815789473684, 0.22532894736842105, 0.3026315789473684, 0.3371710526315789, 0.26644736842105265, 0.3305921052631579, 0.2746710526315789, 0.3355263157894737, 0.3651315789473684, 0.3996710526315789, 0.4128289473684211, 0.40460526315789475, 0.37006578947368424, 0.34210526315789475, 0.3223684210526316, 0.4506578947368421, 0.4342105263157895, 0.33881578947368424, 0.4095394736842105, 0.4128289473684211, 0.4095394736842105, 0.44901315789473684, 0.48026315789473684, 0.4769736842105263, 0.5213815789473685, 0.5164473684210527, 0.5296052631578947, 0.5296052631578947, 0.506578947368421, 0.5148026315789473, 0.524671052631579, 0.5082236842105263, 0.5197368421052632, 0.5263157894736842, 0.53125, 0.5328947368421053, 0.5345394736842105, 0.53125, 0.5444078947368421, 0.5263157894736842, 0.5197368421052632, 0.5197368421052632, 0.5197368421052632, 0.5197368421052632, 0.5213815789473685, 0.524671052631579, 0.5230263157894737, 0.5230263157894737, 0.5230263157894737, 0.5230263157894737, 0.5230263157894737, 0.5230263157894737, 0.5230263157894737, 0.5230263157894737, 0.5230263157894737, 0.5230263157894737, 0.5230263157894737, 0.5230263157894737, 0.5230263157894737] Best val loss: 6.943769097328186


Fold: 1
----- Training alexnet with sequence: [0.2, 0.4, 0.6, 0.8, 1] -----
Current group: 0.2
Epoch [1/30], Training Loss: 40.6436, Validation Loss Current: 9.9784, Validation Loss AVG: 9.9784, lr: 0.1
Epoch [2/30], Training Loss: 40.3967, Validation Loss Current: 10.0188, Validation Loss AVG: 10.0188, lr: 0.1
Epoch [3/30], Training Loss: 40.6287, Validation Loss Current: 9.9959, Validation Loss AVG: 9.9959, lr: 0.1
Epoch [4/30], Training Loss: 40.0830, Validation Loss Current: 9.9833, Validation Loss AVG: 9.9833, lr: 0.1
Epoch [5/30], Training Loss: 40.4236, Validation Loss Current: 9.9871, Validation Loss AVG: 9.9871, lr: 0.1
Epoch [6/30], Training Loss: 40.6232, Validation Loss Current: 9.9974, Validation Loss AVG: 9.9974, lr: 0.1
Epoch [7/30], Training Loss: 40.1935, Validation Loss Current: 10.0391, Validation Loss AVG: 10.0391, lr: 0.1
Epoch [8/30], Training Loss: 40.1826, Validation Loss Current: 10.0302, Validation Loss AVG: 10.0302, lr: 0.010000000000000002
Epoch [9/30], Training Loss: 40.2679, Validation Loss Current: 10.0128, Validation Loss AVG: 10.0128, lr: 0.010000000000000002
Epoch [10/30], Training Loss: 39.8071, Validation Loss Current: 9.9874, Validation Loss AVG: 9.9874, lr: 0.010000000000000002
Epoch [11/30], Training Loss: 40.0295, Validation Loss Current: 9.9896, Validation Loss AVG: 9.9896, lr: 0.010000000000000002
Epoch [12/30], Training Loss: 40.2402, Validation Loss Current: 9.9871, Validation Loss AVG: 9.9871, lr: 0.010000000000000002
Epoch [13/30], Training Loss: 40.2919, Validation Loss Current: 9.9789, Validation Loss AVG: 9.9789, lr: 0.010000000000000002
Epoch [14/30], Training Loss: 40.4884, Validation Loss Current: 9.9820, Validation Loss AVG: 9.9820, lr: 0.0010000000000000002
Epoch [15/30], Training Loss: 39.5324, Validation Loss Current: 9.9854, Validation Loss AVG: 9.9854, lr: 0.0010000000000000002
Epoch [16/30], Training Loss: 40.4781, Validation Loss Current: 9.9822, Validation Loss AVG: 9.9822, lr: 0.0010000000000000002
Epoch [17/30], Training Loss: 40.2107, Validation Loss Current: 9.9771, Validation Loss AVG: 9.9771, lr: 0.0010000000000000002
Epoch [18/30], Training Loss: 39.8446, Validation Loss Current: 9.9778, Validation Loss AVG: 9.9778, lr: 0.0010000000000000002
Epoch [19/30], Training Loss: 39.8390, Validation Loss Current: 9.9790, Validation Loss AVG: 9.9790, lr: 0.0010000000000000002
Epoch [20/30], Training Loss: 40.3859, Validation Loss Current: 9.9778, Validation Loss AVG: 9.9778, lr: 0.0010000000000000002
Epoch [21/30], Training Loss: 39.9845, Validation Loss Current: 9.9853, Validation Loss AVG: 9.9853, lr: 0.0010000000000000002
Epoch [22/30], Training Loss: 39.7481, Validation Loss Current: 9.9743, Validation Loss AVG: 9.9743, lr: 0.0010000000000000002
Epoch [23/30], Training Loss: 40.4792, Validation Loss Current: 9.9856, Validation Loss AVG: 9.9856, lr: 0.0010000000000000002
Epoch [24/30], Training Loss: 39.9513, Validation Loss Current: 9.9783, Validation Loss AVG: 9.9783, lr: 0.0010000000000000002
Epoch [25/30], Training Loss: 39.8785, Validation Loss Current: 9.9783, Validation Loss AVG: 9.9783, lr: 0.0010000000000000002
Epoch [26/30], Training Loss: 40.2310, Validation Loss Current: 9.9813, Validation Loss AVG: 9.9813, lr: 0.0010000000000000002
Epoch [27/30], Training Loss: 40.4882, Validation Loss Current: 9.9810, Validation Loss AVG: 9.9810, lr: 0.0010000000000000002
Epoch [28/30], Training Loss: 39.9499, Validation Loss Current: 9.9740, Validation Loss AVG: 9.9740, lr: 0.0010000000000000002
Epoch [29/30], Training Loss: 39.8570, Validation Loss Current: 9.9888, Validation Loss AVG: 9.9888, lr: 0.00010000000000000003
Epoch [30/30], Training Loss: 40.0549, Validation Loss Current: 9.9827, Validation Loss AVG: 9.9827, lr: 0.00010000000000000003
Epoch [31/30], Training Loss: 40.2155, Validation Loss Current: 9.9766, Validation Loss AVG: 9.9766, lr: 0.00010000000000000003
Epoch [32/30], Training Loss: 40.1764, Validation Loss Current: 9.9859, Validation Loss AVG: 9.9859, lr: 0.00010000000000000003
Epoch [33/30], Training Loss: 39.9426, Validation Loss Current: 9.9685, Validation Loss AVG: 9.9685, lr: 0.00010000000000000003
Epoch [34/30], Training Loss: 40.3787, Validation Loss Current: 9.9803, Validation Loss AVG: 9.9803, lr: 0.00010000000000000003
Epoch [35/30], Training Loss: 39.9431, Validation Loss Current: 9.9781, Validation Loss AVG: 9.9781, lr: 0.00010000000000000003
Epoch [36/30], Training Loss: 39.8625, Validation Loss Current: 9.9862, Validation Loss AVG: 9.9862, lr: 0.00010000000000000003
Epoch [37/30], Training Loss: 39.5017, Validation Loss Current: 9.9816, Validation Loss AVG: 9.9816, lr: 0.00010000000000000003
Epoch [38/30], Training Loss: 40.0930, Validation Loss Current: 9.9761, Validation Loss AVG: 9.9761, lr: 0.00010000000000000003
Epoch [39/30], Training Loss: 39.7327, Validation Loss Current: 9.9771, Validation Loss AVG: 9.9771, lr: 0.00010000000000000003
Epoch [40/30], Training Loss: 39.7368, Validation Loss Current: 9.9818, Validation Loss AVG: 9.9818, lr: 1.0000000000000004e-05
Epoch [41/30], Training Loss: 40.2929, Validation Loss Current: 9.9805, Validation Loss AVG: 9.9805, lr: 1.0000000000000004e-05
Epoch [42/30], Training Loss: 39.7405, Validation Loss Current: 9.9857, Validation Loss AVG: 9.9857, lr: 1.0000000000000004e-05
Epoch [43/30], Training Loss: 39.9428, Validation Loss Current: 9.9879, Validation Loss AVG: 9.9879, lr: 1.0000000000000004e-05
Epoch [44/30], Training Loss: 39.8578, Validation Loss Current: 9.9740, Validation Loss AVG: 9.9740, lr: 1.0000000000000004e-05
Epoch [45/30], Training Loss: 39.9441, Validation Loss Current: 9.9780, Validation Loss AVG: 9.9780, lr: 1.0000000000000004e-05
Epoch [46/30], Training Loss: 40.2504, Validation Loss Current: 9.9769, Validation Loss AVG: 9.9769, lr: 1.0000000000000004e-06
Epoch [47/30], Training Loss: 40.0692, Validation Loss Current: 9.9810, Validation Loss AVG: 9.9810, lr: 1.0000000000000004e-06
Epoch [48/30], Training Loss: 40.1540, Validation Loss Current: 9.9832, Validation Loss AVG: 9.9832, lr: 1.0000000000000004e-06
Epoch [49/30], Training Loss: 40.3861, Validation Loss Current: 9.9799, Validation Loss AVG: 9.9799, lr: 1.0000000000000004e-06
Epoch [50/30], Training Loss: 40.1439, Validation Loss Current: 9.9826, Validation Loss AVG: 9.9826, lr: 1.0000000000000004e-06
Epoch [51/30], Training Loss: 39.7379, Validation Loss Current: 9.9800, Validation Loss AVG: 9.9800, lr: 1.0000000000000004e-06
Epoch [52/30], Training Loss: 40.3783, Validation Loss Current: 9.9794, Validation Loss AVG: 9.9794, lr: 1.0000000000000005e-07
Epoch [53/30], Training Loss: 39.5072, Validation Loss Current: 9.9813, Validation Loss AVG: 9.9813, lr: 1.0000000000000005e-07
Epoch [54/30], Training Loss: 40.1675, Validation Loss Current: 9.9819, Validation Loss AVG: 9.9819, lr: 1.0000000000000005e-07
Epoch [55/30], Training Loss: 39.7436, Validation Loss Current: 9.9766, Validation Loss AVG: 9.9766, lr: 1.0000000000000005e-07
Epoch [56/30], Training Loss: 40.1744, Validation Loss Current: 9.9791, Validation Loss AVG: 9.9791, lr: 1.0000000000000005e-07
Epoch [57/30], Training Loss: 39.9746, Validation Loss Current: 9.9780, Validation Loss AVG: 9.9780, lr: 1.0000000000000005e-07
Epoch [58/30], Training Loss: 40.0688, Validation Loss Current: 9.9814, Validation Loss AVG: 9.9814, lr: 1.0000000000000005e-08
Epoch [59/30], Training Loss: 39.5144, Validation Loss Current: 9.9822, Validation Loss AVG: 9.9822, lr: 1.0000000000000005e-08
Epoch [60/30], Training Loss: 39.8291, Validation Loss Current: 9.9716, Validation Loss AVG: 9.9716, lr: 1.0000000000000005e-08
Epoch [61/30], Training Loss: 40.4225, Validation Loss Current: 9.9809, Validation Loss AVG: 9.9809, lr: 1.0000000000000005e-08
Epoch [62/30], Training Loss: 40.0581, Validation Loss Current: 9.9785, Validation Loss AVG: 9.9785, lr: 1.0000000000000005e-08
Epoch [63/30], Training Loss: 39.7395, Validation Loss Current: 9.9855, Validation Loss AVG: 9.9855, lr: 1.0000000000000005e-08
 --- Early Stopped ---
Patch distance: 0.2 finished training. Best epoch: 33 Best val accuracy: [0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842] Best val loss: 9.96847038269043


Loaded best state dict for [0.2]
Current group: 0.4
Epoch [1/30], Training Loss: 40.0603, Validation Loss Current: 9.8878, Validation Loss AVG: 9.8878, lr: 0.1
Epoch [2/30], Training Loss: 39.6628, Validation Loss Current: 10.6055, Validation Loss AVG: 10.6055, lr: 0.1
Epoch [3/30], Training Loss: 39.3148, Validation Loss Current: 10.1290, Validation Loss AVG: 10.1290, lr: 0.1
Epoch [4/30], Training Loss: 39.7014, Validation Loss Current: 10.3645, Validation Loss AVG: 10.3645, lr: 0.1
Epoch [5/30], Training Loss: 39.3215, Validation Loss Current: 9.3341, Validation Loss AVG: 9.3341, lr: 0.1
Epoch [6/30], Training Loss: 38.3907, Validation Loss Current: 10.2316, Validation Loss AVG: 10.2316, lr: 0.1
Epoch [7/30], Training Loss: 37.3337, Validation Loss Current: 9.3031, Validation Loss AVG: 9.3031, lr: 0.1
Epoch [8/30], Training Loss: 35.6000, Validation Loss Current: 10.4019, Validation Loss AVG: 10.4019, lr: 0.1
Epoch [9/30], Training Loss: 35.4138, Validation Loss Current: 9.2882, Validation Loss AVG: 9.2882, lr: 0.1
Epoch [10/30], Training Loss: 34.4503, Validation Loss Current: 9.0747, Validation Loss AVG: 9.0747, lr: 0.1
Epoch [11/30], Training Loss: 35.7677, Validation Loss Current: 9.7938, Validation Loss AVG: 9.7938, lr: 0.1
Epoch [12/30], Training Loss: 35.0417, Validation Loss Current: 9.2359, Validation Loss AVG: 9.2359, lr: 0.1
Epoch [13/30], Training Loss: 33.5087, Validation Loss Current: 9.2058, Validation Loss AVG: 9.2058, lr: 0.1
Epoch [14/30], Training Loss: 34.2181, Validation Loss Current: 9.1019, Validation Loss AVG: 9.1019, lr: 0.1
Epoch [15/30], Training Loss: 32.6765, Validation Loss Current: 8.9067, Validation Loss AVG: 8.9067, lr: 0.1
Epoch [16/30], Training Loss: 33.2871, Validation Loss Current: 9.3135, Validation Loss AVG: 9.3135, lr: 0.1
Epoch [17/30], Training Loss: 34.8798, Validation Loss Current: 10.4827, Validation Loss AVG: 10.4827, lr: 0.1
Epoch [18/30], Training Loss: 34.5469, Validation Loss Current: 9.3434, Validation Loss AVG: 9.3434, lr: 0.1
Epoch [19/30], Training Loss: 33.2629, Validation Loss Current: 9.5576, Validation Loss AVG: 9.5576, lr: 0.1
Epoch [20/30], Training Loss: 32.6207, Validation Loss Current: 10.1617, Validation Loss AVG: 10.1617, lr: 0.1
Epoch [21/30], Training Loss: 33.7322, Validation Loss Current: 11.2669, Validation Loss AVG: 11.2669, lr: 0.1
Epoch [22/30], Training Loss: 31.5074, Validation Loss Current: 9.0951, Validation Loss AVG: 9.0951, lr: 0.010000000000000002
Epoch [23/30], Training Loss: 27.9143, Validation Loss Current: 9.3433, Validation Loss AVG: 9.3433, lr: 0.010000000000000002
Epoch [24/30], Training Loss: 26.6889, Validation Loss Current: 9.0524, Validation Loss AVG: 9.0524, lr: 0.010000000000000002
Epoch [25/30], Training Loss: 26.5826, Validation Loss Current: 9.1088, Validation Loss AVG: 9.1088, lr: 0.010000000000000002
Epoch [26/30], Training Loss: 25.8682, Validation Loss Current: 9.0258, Validation Loss AVG: 9.0258, lr: 0.010000000000000002
Epoch [27/30], Training Loss: 24.4899, Validation Loss Current: 9.4317, Validation Loss AVG: 9.4317, lr: 0.010000000000000002
Epoch [28/30], Training Loss: 22.1558, Validation Loss Current: 9.3902, Validation Loss AVG: 9.3902, lr: 0.0010000000000000002
Epoch [29/30], Training Loss: 22.2412, Validation Loss Current: 9.4366, Validation Loss AVG: 9.4366, lr: 0.0010000000000000002
Epoch [30/30], Training Loss: 23.4617, Validation Loss Current: 9.4742, Validation Loss AVG: 9.4742, lr: 0.0010000000000000002
Epoch [31/30], Training Loss: 22.4006, Validation Loss Current: 9.5026, Validation Loss AVG: 9.5026, lr: 0.0010000000000000002
Epoch [32/30], Training Loss: 21.6859, Validation Loss Current: 9.5389, Validation Loss AVG: 9.5389, lr: 0.0010000000000000002
Epoch [33/30], Training Loss: 21.4470, Validation Loss Current: 9.5453, Validation Loss AVG: 9.5453, lr: 0.0010000000000000002
Epoch [34/30], Training Loss: 22.1765, Validation Loss Current: 9.5506, Validation Loss AVG: 9.5506, lr: 0.00010000000000000003
Epoch [35/30], Training Loss: 21.5724, Validation Loss Current: 9.5499, Validation Loss AVG: 9.5499, lr: 0.00010000000000000003
Epoch [36/30], Training Loss: 22.7247, Validation Loss Current: 9.5683, Validation Loss AVG: 9.5683, lr: 0.00010000000000000003
Epoch [37/30], Training Loss: 21.3653, Validation Loss Current: 9.5594, Validation Loss AVG: 9.5594, lr: 0.00010000000000000003
Epoch [38/30], Training Loss: 21.5705, Validation Loss Current: 9.5550, Validation Loss AVG: 9.5550, lr: 0.00010000000000000003
Epoch [39/30], Training Loss: 22.7058, Validation Loss Current: 9.5475, Validation Loss AVG: 9.5475, lr: 0.00010000000000000003
Epoch [40/30], Training Loss: 21.2789, Validation Loss Current: 9.5248, Validation Loss AVG: 9.5248, lr: 1.0000000000000004e-05
Epoch [41/30], Training Loss: 21.9880, Validation Loss Current: 9.5611, Validation Loss AVG: 9.5611, lr: 1.0000000000000004e-05
Epoch [42/30], Training Loss: 21.1439, Validation Loss Current: 9.5486, Validation Loss AVG: 9.5486, lr: 1.0000000000000004e-05
Epoch [43/30], Training Loss: 21.6328, Validation Loss Current: 9.5376, Validation Loss AVG: 9.5376, lr: 1.0000000000000004e-05
Epoch [44/30], Training Loss: 22.7751, Validation Loss Current: 9.5605, Validation Loss AVG: 9.5605, lr: 1.0000000000000004e-05
Epoch [45/30], Training Loss: 22.1471, Validation Loss Current: 9.5716, Validation Loss AVG: 9.5716, lr: 1.0000000000000004e-05
 --- Early Stopped ---
Patch distance: 0.4 finished training. Best epoch: 15 Best val accuracy: [0.2450657894736842, 0.2555921052631579, 0.15460526315789475, 0.18453947368421053, 0.31118421052631584, 0.22796052631578947, 0.3167763157894737, 0.2414473684210526, 0.31249999999999994, 0.3375, 0.2601973684210527, 0.32171052631578945, 0.2746710526315789, 0.35789473684210527, 0.3404605263157895, 0.28190789473684214, 0.26644736842105265, 0.3167763157894737, 0.33125, 0.18125, 0.29046052631578945, 0.35, 0.3625, 0.3608552631578947, 0.37664473684210525, 0.37565789473684214, 0.3549342105263158, 0.37236842105263157, 0.3759868421052631, 0.37006578947368424, 0.3694078947368421, 0.3710526315789474, 0.3703947368421053, 0.36973684210526314, 0.36973684210526314, 0.3713815789473684, 0.3700657894736842, 0.3694078947368421, 0.3697368421052632, 0.3697368421052632, 0.37006578947368424, 0.37006578947368424, 0.37006578947368424, 0.37006578947368424, 0.37006578947368424] Best val loss: 8.906736660003663


Loaded best state dict for [0.2, 0.4]
Current group: 0.6
Epoch [1/30], Training Loss: 31.3175, Validation Loss Current: 9.6933, Validation Loss AVG: 9.6933, lr: 0.1
Epoch [2/30], Training Loss: 32.9104, Validation Loss Current: 8.7919, Validation Loss AVG: 8.7919, lr: 0.1
Epoch [3/30], Training Loss: 33.7709, Validation Loss Current: 9.9476, Validation Loss AVG: 9.9476, lr: 0.1
Epoch [4/30], Training Loss: 32.4102, Validation Loss Current: 8.8862, Validation Loss AVG: 8.8862, lr: 0.1
Epoch [5/30], Training Loss: 31.9621, Validation Loss Current: 9.6328, Validation Loss AVG: 9.6328, lr: 0.1
Epoch [6/30], Training Loss: 32.2073, Validation Loss Current: 9.5891, Validation Loss AVG: 9.5891, lr: 0.1
Epoch [7/30], Training Loss: 30.6520, Validation Loss Current: 9.1561, Validation Loss AVG: 9.1561, lr: 0.1
Epoch [8/30], Training Loss: 31.3437, Validation Loss Current: 9.6271, Validation Loss AVG: 9.6271, lr: 0.1
Epoch [9/30], Training Loss: 29.9234, Validation Loss Current: 8.4899, Validation Loss AVG: 8.4899, lr: 0.010000000000000002
Epoch [10/30], Training Loss: 26.1339, Validation Loss Current: 8.4377, Validation Loss AVG: 8.4377, lr: 0.010000000000000002
Epoch [11/30], Training Loss: 26.7522, Validation Loss Current: 8.4525, Validation Loss AVG: 8.4525, lr: 0.010000000000000002
Epoch [12/30], Training Loss: 24.3029, Validation Loss Current: 8.5614, Validation Loss AVG: 8.5614, lr: 0.010000000000000002
Epoch [13/30], Training Loss: 24.4148, Validation Loss Current: 8.4466, Validation Loss AVG: 8.4466, lr: 0.010000000000000002
Epoch [14/30], Training Loss: 22.6940, Validation Loss Current: 9.2694, Validation Loss AVG: 9.2694, lr: 0.010000000000000002
Epoch [15/30], Training Loss: 22.6061, Validation Loss Current: 8.7782, Validation Loss AVG: 8.7782, lr: 0.010000000000000002
Epoch [16/30], Training Loss: 22.8476, Validation Loss Current: 8.8409, Validation Loss AVG: 8.8409, lr: 0.010000000000000002
Epoch [17/30], Training Loss: 21.5539, Validation Loss Current: 8.7061, Validation Loss AVG: 8.7061, lr: 0.0010000000000000002
Epoch [18/30], Training Loss: 20.7707, Validation Loss Current: 8.7090, Validation Loss AVG: 8.7090, lr: 0.0010000000000000002
Epoch [19/30], Training Loss: 20.3285, Validation Loss Current: 8.7756, Validation Loss AVG: 8.7756, lr: 0.0010000000000000002
Epoch [20/30], Training Loss: 20.9124, Validation Loss Current: 8.8031, Validation Loss AVG: 8.8031, lr: 0.0010000000000000002
Epoch [21/30], Training Loss: 20.4255, Validation Loss Current: 8.7874, Validation Loss AVG: 8.7874, lr: 0.0010000000000000002
Epoch [22/30], Training Loss: 20.3415, Validation Loss Current: 8.8659, Validation Loss AVG: 8.8659, lr: 0.0010000000000000002
Epoch [23/30], Training Loss: 19.8938, Validation Loss Current: 8.8408, Validation Loss AVG: 8.8408, lr: 0.00010000000000000003
Epoch [24/30], Training Loss: 19.4872, Validation Loss Current: 8.8494, Validation Loss AVG: 8.8494, lr: 0.00010000000000000003
Epoch [25/30], Training Loss: 20.2799, Validation Loss Current: 8.8417, Validation Loss AVG: 8.8417, lr: 0.00010000000000000003
Epoch [26/30], Training Loss: 20.6984, Validation Loss Current: 8.8999, Validation Loss AVG: 8.8999, lr: 0.00010000000000000003
Epoch [27/30], Training Loss: 19.8204, Validation Loss Current: 8.8787, Validation Loss AVG: 8.8787, lr: 0.00010000000000000003
Epoch [28/30], Training Loss: 19.7735, Validation Loss Current: 8.8663, Validation Loss AVG: 8.8663, lr: 0.00010000000000000003
Epoch [29/30], Training Loss: 19.4336, Validation Loss Current: 8.8602, Validation Loss AVG: 8.8602, lr: 1.0000000000000004e-05
Epoch [30/30], Training Loss: 20.1822, Validation Loss Current: 8.8598, Validation Loss AVG: 8.8598, lr: 1.0000000000000004e-05
Epoch [31/30], Training Loss: 21.1131, Validation Loss Current: 8.8562, Validation Loss AVG: 8.8562, lr: 1.0000000000000004e-05
Epoch [32/30], Training Loss: 20.0984, Validation Loss Current: 8.9031, Validation Loss AVG: 8.9031, lr: 1.0000000000000004e-05
Epoch [33/30], Training Loss: 19.8723, Validation Loss Current: 8.8423, Validation Loss AVG: 8.8423, lr: 1.0000000000000004e-05
Epoch [34/30], Training Loss: 19.7398, Validation Loss Current: 8.8452, Validation Loss AVG: 8.8452, lr: 1.0000000000000004e-05
Epoch [35/30], Training Loss: 20.4025, Validation Loss Current: 8.8480, Validation Loss AVG: 8.8480, lr: 1.0000000000000004e-06
Epoch [36/30], Training Loss: 19.1948, Validation Loss Current: 8.8422, Validation Loss AVG: 8.8422, lr: 1.0000000000000004e-06
Epoch [37/30], Training Loss: 20.2098, Validation Loss Current: 8.8549, Validation Loss AVG: 8.8549, lr: 1.0000000000000004e-06
Epoch [38/30], Training Loss: 20.3748, Validation Loss Current: 8.8475, Validation Loss AVG: 8.8475, lr: 1.0000000000000004e-06
Epoch [39/30], Training Loss: 20.4787, Validation Loss Current: 8.8549, Validation Loss AVG: 8.8549, lr: 1.0000000000000004e-06
Epoch [40/30], Training Loss: 20.0242, Validation Loss Current: 8.8705, Validation Loss AVG: 8.8705, lr: 1.0000000000000004e-06
 --- Early Stopped ---
Patch distance: 0.6 finished training. Best epoch: 10 Best val accuracy: [0.32598684210526313, 0.3736842105263158, 0.21315789473684213, 0.3661184210526315, 0.31085526315789475, 0.30855263157894736, 0.3605263157894737, 0.31973684210526315, 0.4046052631578948, 0.41052631578947363, 0.4036184210526315, 0.4, 0.4029605263157895, 0.3799342105263158, 0.39440789473684207, 0.40888157894736843, 0.4029605263157894, 0.4078947368421052, 0.40855263157894733, 0.40855263157894733, 0.4108552631578948, 0.40888157894736843, 0.40888157894736843, 0.40888157894736843, 0.4082236842105263, 0.41085526315789467, 0.41118421052631576, 0.41085526315789467, 0.41085526315789467, 0.40953947368421045, 0.40953947368421045, 0.4101973684210526, 0.40953947368421045, 0.4101973684210526, 0.4101973684210526, 0.4101973684210526, 0.4101973684210526, 0.4101973684210526, 0.4101973684210526, 0.4101973684210526] Best val loss: 8.437682867050171


Loaded best state dict for [0.2, 0.4, 0.6]
Current group: 0.8
Epoch [1/30], Training Loss: 29.3358, Validation Loss Current: 8.6532, Validation Loss AVG: 8.6532, lr: 0.1
Epoch [2/30], Training Loss: 29.9888, Validation Loss Current: 9.0226, Validation Loss AVG: 9.0226, lr: 0.1
Epoch [3/30], Training Loss: 32.4559, Validation Loss Current: 10.3632, Validation Loss AVG: 10.3632, lr: 0.1
Epoch [4/30], Training Loss: 30.5456, Validation Loss Current: 8.9411, Validation Loss AVG: 8.9411, lr: 0.1
Epoch [5/30], Training Loss: 31.7403, Validation Loss Current: 9.2769, Validation Loss AVG: 9.2769, lr: 0.1
Epoch [6/30], Training Loss: 31.0575, Validation Loss Current: 8.4286, Validation Loss AVG: 8.4286, lr: 0.1
Epoch [7/30], Training Loss: 29.9723, Validation Loss Current: 9.5773, Validation Loss AVG: 9.5773, lr: 0.1
Epoch [8/30], Training Loss: 31.1814, Validation Loss Current: 9.0277, Validation Loss AVG: 9.0277, lr: 0.1
Epoch [9/30], Training Loss: 30.9860, Validation Loss Current: 9.0923, Validation Loss AVG: 9.0923, lr: 0.1
Epoch [10/30], Training Loss: 29.7679, Validation Loss Current: 10.1642, Validation Loss AVG: 10.1642, lr: 0.1
Epoch [11/30], Training Loss: 37.0350, Validation Loss Current: 8.8820, Validation Loss AVG: 8.8820, lr: 0.1
Epoch [12/30], Training Loss: 31.4797, Validation Loss Current: 9.3211, Validation Loss AVG: 9.3211, lr: 0.1
Epoch [13/30], Training Loss: 28.2615, Validation Loss Current: 8.6026, Validation Loss AVG: 8.6026, lr: 0.010000000000000002
Epoch [14/30], Training Loss: 24.7351, Validation Loss Current: 8.3704, Validation Loss AVG: 8.3704, lr: 0.010000000000000002
Epoch [15/30], Training Loss: 23.8721, Validation Loss Current: 8.6036, Validation Loss AVG: 8.6036, lr: 0.010000000000000002
Epoch [16/30], Training Loss: 22.3430, Validation Loss Current: 8.3789, Validation Loss AVG: 8.3789, lr: 0.010000000000000002
Epoch [17/30], Training Loss: 21.5931, Validation Loss Current: 8.6967, Validation Loss AVG: 8.6967, lr: 0.010000000000000002
Epoch [18/30], Training Loss: 20.8212, Validation Loss Current: 8.7732, Validation Loss AVG: 8.7732, lr: 0.010000000000000002
Epoch [19/30], Training Loss: 20.0888, Validation Loss Current: 8.9961, Validation Loss AVG: 8.9961, lr: 0.010000000000000002
Epoch [20/30], Training Loss: 19.5148, Validation Loss Current: 8.7520, Validation Loss AVG: 8.7520, lr: 0.010000000000000002
Epoch [21/30], Training Loss: 19.7200, Validation Loss Current: 8.8947, Validation Loss AVG: 8.8947, lr: 0.0010000000000000002
Epoch [22/30], Training Loss: 19.7171, Validation Loss Current: 8.9729, Validation Loss AVG: 8.9729, lr: 0.0010000000000000002
Epoch [23/30], Training Loss: 18.5776, Validation Loss Current: 8.9947, Validation Loss AVG: 8.9947, lr: 0.0010000000000000002
Epoch [24/30], Training Loss: 18.0292, Validation Loss Current: 9.1031, Validation Loss AVG: 9.1031, lr: 0.0010000000000000002
Epoch [25/30], Training Loss: 17.8755, Validation Loss Current: 9.1335, Validation Loss AVG: 9.1335, lr: 0.0010000000000000002
Epoch [26/30], Training Loss: 17.3454, Validation Loss Current: 9.1640, Validation Loss AVG: 9.1640, lr: 0.0010000000000000002
Epoch [27/30], Training Loss: 18.1818, Validation Loss Current: 9.1885, Validation Loss AVG: 9.1885, lr: 0.00010000000000000003
Epoch [28/30], Training Loss: 18.9013, Validation Loss Current: 9.1578, Validation Loss AVG: 9.1578, lr: 0.00010000000000000003
Epoch [29/30], Training Loss: 18.4548, Validation Loss Current: 9.1495, Validation Loss AVG: 9.1495, lr: 0.00010000000000000003
Epoch [30/30], Training Loss: 17.2774, Validation Loss Current: 9.1826, Validation Loss AVG: 9.1826, lr: 0.00010000000000000003
Epoch [31/30], Training Loss: 17.1265, Validation Loss Current: 9.1589, Validation Loss AVG: 9.1589, lr: 0.00010000000000000003
Epoch [32/30], Training Loss: 18.6736, Validation Loss Current: 9.1971, Validation Loss AVG: 9.1971, lr: 0.00010000000000000003
Epoch [33/30], Training Loss: 17.4638, Validation Loss Current: 9.1607, Validation Loss AVG: 9.1607, lr: 1.0000000000000004e-05
Epoch [34/30], Training Loss: 17.8445, Validation Loss Current: 9.1550, Validation Loss AVG: 9.1550, lr: 1.0000000000000004e-05
Epoch [35/30], Training Loss: 18.1338, Validation Loss Current: 9.1552, Validation Loss AVG: 9.1552, lr: 1.0000000000000004e-05
Epoch [36/30], Training Loss: 17.1515, Validation Loss Current: 9.1898, Validation Loss AVG: 9.1898, lr: 1.0000000000000004e-05
Epoch [37/30], Training Loss: 17.4170, Validation Loss Current: 9.1509, Validation Loss AVG: 9.1509, lr: 1.0000000000000004e-05
Epoch [38/30], Training Loss: 17.6282, Validation Loss Current: 9.1672, Validation Loss AVG: 9.1672, lr: 1.0000000000000004e-05
Epoch [39/30], Training Loss: 18.1122, Validation Loss Current: 9.1898, Validation Loss AVG: 9.1898, lr: 1.0000000000000004e-06
Epoch [40/30], Training Loss: 17.9051, Validation Loss Current: 9.1650, Validation Loss AVG: 9.1650, lr: 1.0000000000000004e-06
Epoch [41/30], Training Loss: 17.3935, Validation Loss Current: 9.1912, Validation Loss AVG: 9.1912, lr: 1.0000000000000004e-06
Epoch [42/30], Training Loss: 17.9527, Validation Loss Current: 9.1553, Validation Loss AVG: 9.1553, lr: 1.0000000000000004e-06
Epoch [43/30], Training Loss: 17.1308, Validation Loss Current: 9.1795, Validation Loss AVG: 9.1795, lr: 1.0000000000000004e-06
Epoch [44/30], Training Loss: 17.6196, Validation Loss Current: 9.1703, Validation Loss AVG: 9.1703, lr: 1.0000000000000004e-06
 --- Early Stopped ---
Patch distance: 0.8 finished training. Best epoch: 14 Best val accuracy: [0.3592105263157895, 0.3292763157894737, 0.3358552631578947, 0.3677631578947368, 0.34440789473684214, 0.38651315789473684, 0.36019736842105254, 0.3805921052631579, 0.37565789473684214, 0.2789473684210526, 0.33059210526315785, 0.3569078947368421, 0.4095394736842105, 0.41052631578947374, 0.3963815789473684, 0.4256578947368421, 0.41940789473684215, 0.40690789473684214, 0.41052631578947374, 0.4184210526315789, 0.41940789473684215, 0.4151315789473684, 0.41447368421052627, 0.4125, 0.4128289473684211, 0.4115131578947368, 0.4118421052631579, 0.4125, 0.41414473684210523, 0.41381578947368414, 0.4118421052631579, 0.41381578947368414, 0.41381578947368414, 0.4131578947368421, 0.4134868421052631, 0.4134868421052631, 0.4134868421052631, 0.4134868421052631, 0.41315789473684206, 0.41315789473684206, 0.41315789473684206, 0.41315789473684206, 0.41315789473684206, 0.41315789473684206] Best val loss: 8.370353937149048


Loaded best state dict for [0.2, 0.4, 0.6, 0.8]
Current group: 1
Epoch [1/30], Training Loss: 30.8976, Validation Loss Current: 10.4531, Validation Loss AVG: 12.9541, lr: 0.1
Epoch [2/30], Training Loss: 32.3138, Validation Loss Current: 8.0493, Validation Loss AVG: 9.4340, lr: 0.1
Epoch [3/30], Training Loss: 32.0058, Validation Loss Current: 8.8075, Validation Loss AVG: 9.9441, lr: 0.1
Epoch [4/30], Training Loss: 30.3028, Validation Loss Current: 8.7748, Validation Loss AVG: 11.7491, lr: 0.1
Epoch [5/30], Training Loss: 30.7071, Validation Loss Current: 8.7776, Validation Loss AVG: 12.3044, lr: 0.1
Epoch [6/30], Training Loss: 28.4249, Validation Loss Current: 7.3342, Validation Loss AVG: 8.8808, lr: 0.1
Epoch [7/30], Training Loss: 29.2898, Validation Loss Current: 8.2033, Validation Loss AVG: 10.1528, lr: 0.1
Epoch [8/30], Training Loss: 29.3531, Validation Loss Current: 8.0524, Validation Loss AVG: 9.7521, lr: 0.1
Epoch [9/30], Training Loss: 28.7552, Validation Loss Current: 8.8042, Validation Loss AVG: 10.2796, lr: 0.1
Epoch [10/30], Training Loss: 29.3356, Validation Loss Current: 7.7261, Validation Loss AVG: 8.9243, lr: 0.1
Epoch [11/30], Training Loss: 31.1155, Validation Loss Current: 9.4185, Validation Loss AVG: 11.8628, lr: 0.1
Epoch [12/30], Training Loss: 31.2263, Validation Loss Current: 10.9254, Validation Loss AVG: 15.6327, lr: 0.1
Epoch [13/30], Training Loss: 30.0242, Validation Loss Current: 7.0023, Validation Loss AVG: 9.1043, lr: 0.010000000000000002
Epoch [14/30], Training Loss: 24.7306, Validation Loss Current: 6.6028, Validation Loss AVG: 8.6697, lr: 0.010000000000000002
Epoch [15/30], Training Loss: 22.7629, Validation Loss Current: 6.5700, Validation Loss AVG: 9.3749, lr: 0.010000000000000002
Epoch [16/30], Training Loss: 21.8273, Validation Loss Current: 6.3841, Validation Loss AVG: 9.3070, lr: 0.010000000000000002
Epoch [17/30], Training Loss: 21.0852, Validation Loss Current: 6.4954, Validation Loss AVG: 9.3842, lr: 0.010000000000000002
Epoch [18/30], Training Loss: 19.5821, Validation Loss Current: 6.4830, Validation Loss AVG: 9.2476, lr: 0.010000000000000002
Epoch [19/30], Training Loss: 18.8506, Validation Loss Current: 6.6247, Validation Loss AVG: 10.2989, lr: 0.010000000000000002
Epoch [20/30], Training Loss: 17.8213, Validation Loss Current: 6.4073, Validation Loss AVG: 9.4971, lr: 0.010000000000000002
Epoch [21/30], Training Loss: 17.5946, Validation Loss Current: 6.5808, Validation Loss AVG: 10.4362, lr: 0.010000000000000002
Epoch [22/30], Training Loss: 16.1067, Validation Loss Current: 6.5436, Validation Loss AVG: 10.6962, lr: 0.010000000000000002
Epoch [23/30], Training Loss: 15.6007, Validation Loss Current: 6.6218, Validation Loss AVG: 10.6933, lr: 0.0010000000000000002
Epoch [24/30], Training Loss: 14.8185, Validation Loss Current: 6.6175, Validation Loss AVG: 10.6354, lr: 0.0010000000000000002
Epoch [25/30], Training Loss: 14.6640, Validation Loss Current: 6.7351, Validation Loss AVG: 10.8836, lr: 0.0010000000000000002
Epoch [26/30], Training Loss: 14.4307, Validation Loss Current: 6.7385, Validation Loss AVG: 10.9668, lr: 0.0010000000000000002
Epoch [27/30], Training Loss: 15.1420, Validation Loss Current: 6.7846, Validation Loss AVG: 11.0015, lr: 0.0010000000000000002
Epoch [28/30], Training Loss: 15.0374, Validation Loss Current: 6.7028, Validation Loss AVG: 11.0088, lr: 0.0010000000000000002
Epoch [29/30], Training Loss: 14.0065, Validation Loss Current: 6.7685, Validation Loss AVG: 11.0622, lr: 0.00010000000000000003
Epoch [30/30], Training Loss: 14.0313, Validation Loss Current: 6.7649, Validation Loss AVG: 11.0652, lr: 0.00010000000000000003
Epoch [31/30], Training Loss: 14.6616, Validation Loss Current: 6.7220, Validation Loss AVG: 11.0693, lr: 0.00010000000000000003
Epoch [32/30], Training Loss: 14.0887, Validation Loss Current: 6.7125, Validation Loss AVG: 11.0478, lr: 0.00010000000000000003
Epoch [33/30], Training Loss: 14.5019, Validation Loss Current: 6.8320, Validation Loss AVG: 11.0809, lr: 0.00010000000000000003
Epoch [34/30], Training Loss: 14.2060, Validation Loss Current: 6.7651, Validation Loss AVG: 11.0865, lr: 0.00010000000000000003
Epoch [35/30], Training Loss: 14.7923, Validation Loss Current: 6.8200, Validation Loss AVG: 11.1180, lr: 1.0000000000000004e-05
Epoch [36/30], Training Loss: 13.9846, Validation Loss Current: 6.8803, Validation Loss AVG: 11.1152, lr: 1.0000000000000004e-05
Epoch [37/30], Training Loss: 14.2426, Validation Loss Current: 6.8530, Validation Loss AVG: 11.0768, lr: 1.0000000000000004e-05
Epoch [38/30], Training Loss: 14.5096, Validation Loss Current: 6.7927, Validation Loss AVG: 11.0854, lr: 1.0000000000000004e-05
Epoch [39/30], Training Loss: 14.7545, Validation Loss Current: 6.7517, Validation Loss AVG: 11.0333, lr: 1.0000000000000004e-05
Epoch [40/30], Training Loss: 14.1221, Validation Loss Current: 6.7448, Validation Loss AVG: 11.1251, lr: 1.0000000000000004e-05
Epoch [41/30], Training Loss: 15.5729, Validation Loss Current: 6.7440, Validation Loss AVG: 11.0953, lr: 1.0000000000000004e-06
Epoch [42/30], Training Loss: 14.0650, Validation Loss Current: 6.8313, Validation Loss AVG: 11.1062, lr: 1.0000000000000004e-06
Epoch [43/30], Training Loss: 14.3225, Validation Loss Current: 6.7886, Validation Loss AVG: 11.0963, lr: 1.0000000000000004e-06
Epoch [44/30], Training Loss: 13.9638, Validation Loss Current: 6.7435, Validation Loss AVG: 11.0592, lr: 1.0000000000000004e-06
Epoch [45/30], Training Loss: 14.2730, Validation Loss Current: 6.8537, Validation Loss AVG: 11.1240, lr: 1.0000000000000004e-06
Epoch [46/30], Training Loss: 14.0425, Validation Loss Current: 6.8439, Validation Loss AVG: 11.0900, lr: 1.0000000000000004e-06
 --- Early Stopped ---
Patch distance: 1 finished training. Best epoch: 16 Best val accuracy: [0.3338815789473684, 0.4194078947368421, 0.3963815789473684, 0.39144736842105265, 0.44901315789473684, 0.46710526315789475, 0.4243421052631579, 0.42269736842105265, 0.43256578947368424, 0.45230263157894735, 0.3519736842105263, 0.3651315789473684, 0.4901315789473684, 0.5394736842105263, 0.5509868421052632, 0.5674342105263158, 0.5476973684210527, 0.5723684210526315, 0.5625, 0.5904605263157895, 0.569078947368421, 0.5904605263157895, 0.5904605263157895, 0.5921052631578947, 0.5855263157894737, 0.5822368421052632, 0.5789473684210527, 0.5789473684210527, 0.5789473684210527, 0.5773026315789473, 0.5773026315789473, 0.5773026315789473, 0.5805921052631579, 0.5822368421052632, 0.5822368421052632, 0.5805921052631579, 0.5805921052631579, 0.5805921052631579, 0.5805921052631579, 0.5789473684210527, 0.5789473684210527, 0.5789473684210527, 0.5789473684210527, 0.5789473684210527, 0.5789473684210527, 0.5789473684210527] Best val loss: 6.384108781814575


----- Training alexnet with sequence: [0.4, 0.6, 0.8, 1] -----
Current group: 0.4
Epoch [1/38], Training Loss: 40.0676, Validation Loss Current: 9.9688, Validation Loss AVG: 9.9688, lr: 0.1
Epoch [2/38], Training Loss: 40.5696, Validation Loss Current: 10.0068, Validation Loss AVG: 10.0068, lr: 0.1
Epoch [3/38], Training Loss: 40.2658, Validation Loss Current: 9.9626, Validation Loss AVG: 9.9626, lr: 0.1
Epoch [4/38], Training Loss: 39.7708, Validation Loss Current: 9.9946, Validation Loss AVG: 9.9946, lr: 0.1
Epoch [5/38], Training Loss: 39.0808, Validation Loss Current: 11.0821, Validation Loss AVG: 11.0821, lr: 0.1
Epoch [6/38], Training Loss: 40.6430, Validation Loss Current: 10.0318, Validation Loss AVG: 10.0318, lr: 0.1
Epoch [7/38], Training Loss: 39.5161, Validation Loss Current: 9.9706, Validation Loss AVG: 9.9706, lr: 0.1
Epoch [8/38], Training Loss: 39.6338, Validation Loss Current: 9.6665, Validation Loss AVG: 9.6665, lr: 0.1
Epoch [9/38], Training Loss: 37.8026, Validation Loss Current: 9.4488, Validation Loss AVG: 9.4488, lr: 0.1
Epoch [10/38], Training Loss: 37.0744, Validation Loss Current: 9.2133, Validation Loss AVG: 9.2133, lr: 0.1
Epoch [11/38], Training Loss: 35.9629, Validation Loss Current: 9.6541, Validation Loss AVG: 9.6541, lr: 0.1
Epoch [12/38], Training Loss: 36.6611, Validation Loss Current: 11.2856, Validation Loss AVG: 11.2856, lr: 0.1
Epoch [13/38], Training Loss: 38.4679, Validation Loss Current: 9.2171, Validation Loss AVG: 9.2171, lr: 0.1
Epoch [14/38], Training Loss: 35.6420, Validation Loss Current: 9.1554, Validation Loss AVG: 9.1554, lr: 0.1
Epoch [15/38], Training Loss: 34.6877, Validation Loss Current: 9.3599, Validation Loss AVG: 9.3599, lr: 0.1
Epoch [16/38], Training Loss: 34.9213, Validation Loss Current: 9.3224, Validation Loss AVG: 9.3224, lr: 0.1
Epoch [17/38], Training Loss: 35.4965, Validation Loss Current: 9.1020, Validation Loss AVG: 9.1020, lr: 0.1
Epoch [18/38], Training Loss: 35.4860, Validation Loss Current: 8.9255, Validation Loss AVG: 8.9255, lr: 0.1
Epoch [19/38], Training Loss: 33.2154, Validation Loss Current: 9.2973, Validation Loss AVG: 9.2973, lr: 0.1
Epoch [20/38], Training Loss: 32.7314, Validation Loss Current: 9.6518, Validation Loss AVG: 9.6518, lr: 0.1
Epoch [21/38], Training Loss: 34.2096, Validation Loss Current: 9.7887, Validation Loss AVG: 9.7887, lr: 0.1
Epoch [22/38], Training Loss: 34.8230, Validation Loss Current: 10.7389, Validation Loss AVG: 10.7389, lr: 0.1
Epoch [23/38], Training Loss: 33.4130, Validation Loss Current: 9.2812, Validation Loss AVG: 9.2812, lr: 0.1
Epoch [24/38], Training Loss: 33.8586, Validation Loss Current: 9.2631, Validation Loss AVG: 9.2631, lr: 0.1
Epoch [25/38], Training Loss: 32.5896, Validation Loss Current: 8.7993, Validation Loss AVG: 8.7993, lr: 0.010000000000000002
Epoch [26/38], Training Loss: 30.2793, Validation Loss Current: 8.7303, Validation Loss AVG: 8.7303, lr: 0.010000000000000002
Epoch [27/38], Training Loss: 28.3068, Validation Loss Current: 8.7555, Validation Loss AVG: 8.7555, lr: 0.010000000000000002
Epoch [28/38], Training Loss: 27.9644, Validation Loss Current: 8.7771, Validation Loss AVG: 8.7771, lr: 0.010000000000000002
Epoch [29/38], Training Loss: 27.4622, Validation Loss Current: 8.7971, Validation Loss AVG: 8.7971, lr: 0.010000000000000002
Epoch [30/38], Training Loss: 27.8338, Validation Loss Current: 9.4574, Validation Loss AVG: 9.4574, lr: 0.010000000000000002
Epoch [31/38], Training Loss: 26.5173, Validation Loss Current: 9.0185, Validation Loss AVG: 9.0185, lr: 0.010000000000000002
Epoch [32/38], Training Loss: 27.9435, Validation Loss Current: 9.1896, Validation Loss AVG: 9.1896, lr: 0.010000000000000002
Epoch [33/38], Training Loss: 27.1500, Validation Loss Current: 8.8708, Validation Loss AVG: 8.8708, lr: 0.0010000000000000002
Epoch [34/38], Training Loss: 25.5900, Validation Loss Current: 8.9624, Validation Loss AVG: 8.9624, lr: 0.0010000000000000002
Epoch [35/38], Training Loss: 24.1331, Validation Loss Current: 9.0092, Validation Loss AVG: 9.0092, lr: 0.0010000000000000002
Epoch [36/38], Training Loss: 26.0620, Validation Loss Current: 9.0199, Validation Loss AVG: 9.0199, lr: 0.0010000000000000002
Epoch [37/38], Training Loss: 26.1159, Validation Loss Current: 9.0054, Validation Loss AVG: 9.0054, lr: 0.0010000000000000002
Epoch [38/38], Training Loss: 25.6045, Validation Loss Current: 9.0911, Validation Loss AVG: 9.0911, lr: 0.0010000000000000002
Epoch [39/38], Training Loss: 24.0176, Validation Loss Current: 9.0662, Validation Loss AVG: 9.0662, lr: 0.00010000000000000003
Epoch [40/38], Training Loss: 25.7045, Validation Loss Current: 9.0787, Validation Loss AVG: 9.0787, lr: 0.00010000000000000003
Epoch [41/38], Training Loss: 24.8420, Validation Loss Current: 9.0565, Validation Loss AVG: 9.0565, lr: 0.00010000000000000003
Epoch [42/38], Training Loss: 24.0022, Validation Loss Current: 9.0897, Validation Loss AVG: 9.0897, lr: 0.00010000000000000003
Epoch [43/38], Training Loss: 24.8367, Validation Loss Current: 9.0575, Validation Loss AVG: 9.0575, lr: 0.00010000000000000003
Epoch [44/38], Training Loss: 24.2054, Validation Loss Current: 9.0621, Validation Loss AVG: 9.0621, lr: 0.00010000000000000003
Epoch [45/38], Training Loss: 24.0985, Validation Loss Current: 9.0851, Validation Loss AVG: 9.0851, lr: 1.0000000000000004e-05
Epoch [46/38], Training Loss: 24.2134, Validation Loss Current: 9.0781, Validation Loss AVG: 9.0781, lr: 1.0000000000000004e-05
Epoch [47/38], Training Loss: 24.2525, Validation Loss Current: 9.0491, Validation Loss AVG: 9.0491, lr: 1.0000000000000004e-05
Epoch [48/38], Training Loss: 24.3730, Validation Loss Current: 9.0689, Validation Loss AVG: 9.0689, lr: 1.0000000000000004e-05
Epoch [49/38], Training Loss: 23.5539, Validation Loss Current: 9.0774, Validation Loss AVG: 9.0774, lr: 1.0000000000000004e-05
Epoch [50/38], Training Loss: 25.0585, Validation Loss Current: 9.0649, Validation Loss AVG: 9.0649, lr: 1.0000000000000004e-05
Epoch [51/38], Training Loss: 23.9593, Validation Loss Current: 9.0854, Validation Loss AVG: 9.0854, lr: 1.0000000000000004e-06
Epoch [52/38], Training Loss: 23.7458, Validation Loss Current: 9.0477, Validation Loss AVG: 9.0477, lr: 1.0000000000000004e-06
Epoch [53/38], Training Loss: 24.5040, Validation Loss Current: 9.0842, Validation Loss AVG: 9.0842, lr: 1.0000000000000004e-06
Epoch [54/38], Training Loss: 23.9619, Validation Loss Current: 9.0766, Validation Loss AVG: 9.0766, lr: 1.0000000000000004e-06
Epoch [55/38], Training Loss: 24.5212, Validation Loss Current: 9.0918, Validation Loss AVG: 9.0918, lr: 1.0000000000000004e-06
Epoch [56/38], Training Loss: 23.9383, Validation Loss Current: 9.0741, Validation Loss AVG: 9.0741, lr: 1.0000000000000004e-06
 --- Early Stopped ---
Patch distance: 0.4 finished training. Best epoch: 26 Best val accuracy: [0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.25230263157894733, 0.17401315789473684, 0.2450657894736842, 0.18980263157894736, 0.2552631578947368, 0.27203947368421055, 0.3115131578947368, 0.27730263157894736, 0.2924342105263158, 0.32763157894736844, 0.3427631578947369, 0.33092105263157895, 0.3069078947368421, 0.34572368421052635, 0.3588815789473684, 0.36710526315789477, 0.30164473684210524, 0.3174342105263158, 0.3006578947368421, 0.35230263157894737, 0.3233552631578947, 0.3595394736842105, 0.3677631578947369, 0.37467105263157896, 0.38585526315789476, 0.37434210526315786, 0.3509868421052631, 0.3549342105263158, 0.31184210526315786, 0.36381578947368426, 0.3657894736842105, 0.36578947368421055, 0.36743421052631586, 0.3661184210526316, 0.36348684210526316, 0.36513157894736836, 0.3648026315789474, 0.3648026315789473, 0.3648026315789474, 0.36546052631578946, 0.3674342105263158, 0.3674342105263158, 0.3674342105263158, 0.3674342105263158, 0.3677631578947368, 0.3674342105263158, 0.3677631578947368, 0.3677631578947368, 0.3677631578947368, 0.3677631578947368, 0.3677631578947368, 0.3677631578947368, 0.3674342105263158] Best val loss: 8.730344867706298


Loaded best state dict for [0.4]
Current group: 0.6
Epoch [1/38], Training Loss: 32.0994, Validation Loss Current: 9.0213, Validation Loss AVG: 9.0213, lr: 0.1
Epoch [2/38], Training Loss: 32.9053, Validation Loss Current: 8.8108, Validation Loss AVG: 8.8108, lr: 0.1
Epoch [3/38], Training Loss: 33.1201, Validation Loss Current: 8.8034, Validation Loss AVG: 8.8034, lr: 0.1
Epoch [4/38], Training Loss: 33.2560, Validation Loss Current: 9.8100, Validation Loss AVG: 9.8100, lr: 0.1
Epoch [5/38], Training Loss: 33.0371, Validation Loss Current: 9.7055, Validation Loss AVG: 9.7055, lr: 0.1
Epoch [6/38], Training Loss: 36.4672, Validation Loss Current: 10.2782, Validation Loss AVG: 10.2782, lr: 0.1
Epoch [7/38], Training Loss: 34.6883, Validation Loss Current: 9.2433, Validation Loss AVG: 9.2433, lr: 0.1
Epoch [8/38], Training Loss: 31.7525, Validation Loss Current: 8.9109, Validation Loss AVG: 8.9109, lr: 0.1
Epoch [9/38], Training Loss: 30.6328, Validation Loss Current: 10.5765, Validation Loss AVG: 10.5765, lr: 0.1
Epoch [10/38], Training Loss: 30.1694, Validation Loss Current: 9.3032, Validation Loss AVG: 9.3032, lr: 0.010000000000000002
Epoch [11/38], Training Loss: 27.4547, Validation Loss Current: 9.1097, Validation Loss AVG: 9.1097, lr: 0.010000000000000002
Epoch [12/38], Training Loss: 27.2582, Validation Loss Current: 8.9291, Validation Loss AVG: 8.9291, lr: 0.010000000000000002
Epoch [13/38], Training Loss: 25.6366, Validation Loss Current: 9.0173, Validation Loss AVG: 9.0173, lr: 0.010000000000000002
Epoch [14/38], Training Loss: 24.8358, Validation Loss Current: 8.9533, Validation Loss AVG: 8.9533, lr: 0.010000000000000002
Epoch [15/38], Training Loss: 25.6588, Validation Loss Current: 9.1273, Validation Loss AVG: 9.1273, lr: 0.010000000000000002
Epoch [16/38], Training Loss: 24.1599, Validation Loss Current: 9.1232, Validation Loss AVG: 9.1232, lr: 0.0010000000000000002
Epoch [17/38], Training Loss: 23.8857, Validation Loss Current: 9.1089, Validation Loss AVG: 9.1089, lr: 0.0010000000000000002
Epoch [18/38], Training Loss: 23.0774, Validation Loss Current: 9.1802, Validation Loss AVG: 9.1802, lr: 0.0010000000000000002
Epoch [19/38], Training Loss: 22.6953, Validation Loss Current: 9.1958, Validation Loss AVG: 9.1958, lr: 0.0010000000000000002
Epoch [20/38], Training Loss: 22.8246, Validation Loss Current: 9.2143, Validation Loss AVG: 9.2143, lr: 0.0010000000000000002
Epoch [21/38], Training Loss: 22.8679, Validation Loss Current: 9.1927, Validation Loss AVG: 9.1927, lr: 0.0010000000000000002
Epoch [22/38], Training Loss: 24.0520, Validation Loss Current: 9.1828, Validation Loss AVG: 9.1828, lr: 0.00010000000000000003
Epoch [23/38], Training Loss: 23.5605, Validation Loss Current: 9.2394, Validation Loss AVG: 9.2394, lr: 0.00010000000000000003
Epoch [24/38], Training Loss: 22.5674, Validation Loss Current: 9.2230, Validation Loss AVG: 9.2230, lr: 0.00010000000000000003
Epoch [25/38], Training Loss: 23.1948, Validation Loss Current: 9.2059, Validation Loss AVG: 9.2059, lr: 0.00010000000000000003
Epoch [26/38], Training Loss: 22.9625, Validation Loss Current: 9.2298, Validation Loss AVG: 9.2298, lr: 0.00010000000000000003
Epoch [27/38], Training Loss: 23.2071, Validation Loss Current: 9.2453, Validation Loss AVG: 9.2453, lr: 0.00010000000000000003
Epoch [28/38], Training Loss: 23.2127, Validation Loss Current: 9.2197, Validation Loss AVG: 9.2197, lr: 1.0000000000000004e-05
Epoch [29/38], Training Loss: 26.5191, Validation Loss Current: 9.2418, Validation Loss AVG: 9.2418, lr: 1.0000000000000004e-05
Epoch [30/38], Training Loss: 23.8053, Validation Loss Current: 9.2177, Validation Loss AVG: 9.2177, lr: 1.0000000000000004e-05
Epoch [31/38], Training Loss: 23.6151, Validation Loss Current: 9.2443, Validation Loss AVG: 9.2443, lr: 1.0000000000000004e-05
Epoch [32/38], Training Loss: 24.5698, Validation Loss Current: 9.1951, Validation Loss AVG: 9.1951, lr: 1.0000000000000004e-05
Epoch [33/38], Training Loss: 24.5363, Validation Loss Current: 9.2077, Validation Loss AVG: 9.2077, lr: 1.0000000000000004e-05
 --- Early Stopped ---
Patch distance: 0.6 finished training. Best epoch: 3 Best val accuracy: [0.3559210526315789, 0.39440789473684207, 0.34078947368421053, 0.35098684210526315, 0.32072368421052627, 0.29440789473684215, 0.29078947368421054, 0.3625, 0.37269736842105267, 0.3644736842105263, 0.3779605263157895, 0.3871710526315789, 0.3960526315789474, 0.3996710526315789, 0.3983552631578947, 0.3911184210526316, 0.3917763157894737, 0.39111842105263156, 0.39375, 0.38815789473684215, 0.39572368421052634, 0.3960526315789474, 0.3950657894736842, 0.3907894736842105, 0.3901315789473684, 0.3894736842105263, 0.3894736842105263, 0.3894736842105263, 0.38914473684210527, 0.38980263157894735, 0.3901315789473684, 0.3907894736842105, 0.3907894736842105] Best val loss: 8.803427124023438


Loaded best state dict for [0.4, 0.6]
Current group: 0.8
Epoch [1/38], Training Loss: 32.4360, Validation Loss Current: 9.2465, Validation Loss AVG: 9.2465, lr: 0.1
Epoch [2/38], Training Loss: 33.9002, Validation Loss Current: 9.8090, Validation Loss AVG: 9.8090, lr: 0.1
Epoch [3/38], Training Loss: 34.4512, Validation Loss Current: 9.1280, Validation Loss AVG: 9.1280, lr: 0.1
Epoch [4/38], Training Loss: 31.9920, Validation Loss Current: 8.4858, Validation Loss AVG: 8.4858, lr: 0.1
Epoch [5/38], Training Loss: 29.8643, Validation Loss Current: 9.5635, Validation Loss AVG: 9.5635, lr: 0.1
Epoch [6/38], Training Loss: 33.5220, Validation Loss Current: 9.7805, Validation Loss AVG: 9.7805, lr: 0.1
Epoch [7/38], Training Loss: 30.6564, Validation Loss Current: 11.0248, Validation Loss AVG: 11.0248, lr: 0.1
Epoch [8/38], Training Loss: 35.0703, Validation Loss Current: 11.6297, Validation Loss AVG: 11.6297, lr: 0.1
Epoch [9/38], Training Loss: 32.3704, Validation Loss Current: 9.7591, Validation Loss AVG: 9.7591, lr: 0.1
Epoch [10/38], Training Loss: 31.6384, Validation Loss Current: 9.6351, Validation Loss AVG: 9.6351, lr: 0.1
Epoch [11/38], Training Loss: 30.5229, Validation Loss Current: 8.8505, Validation Loss AVG: 8.8505, lr: 0.010000000000000002
Epoch [12/38], Training Loss: 27.6199, Validation Loss Current: 8.5326, Validation Loss AVG: 8.5326, lr: 0.010000000000000002
Epoch [13/38], Training Loss: 26.4179, Validation Loss Current: 8.3849, Validation Loss AVG: 8.3849, lr: 0.010000000000000002
Epoch [14/38], Training Loss: 24.4498, Validation Loss Current: 8.5955, Validation Loss AVG: 8.5955, lr: 0.010000000000000002
Epoch [15/38], Training Loss: 24.9373, Validation Loss Current: 8.4971, Validation Loss AVG: 8.4971, lr: 0.010000000000000002
Epoch [16/38], Training Loss: 25.6724, Validation Loss Current: 8.6355, Validation Loss AVG: 8.6355, lr: 0.010000000000000002
Epoch [17/38], Training Loss: 23.7845, Validation Loss Current: 8.5987, Validation Loss AVG: 8.5987, lr: 0.010000000000000002
Epoch [18/38], Training Loss: 23.7134, Validation Loss Current: 9.0871, Validation Loss AVG: 9.0871, lr: 0.010000000000000002
Epoch [19/38], Training Loss: 22.3463, Validation Loss Current: 8.7566, Validation Loss AVG: 8.7566, lr: 0.010000000000000002
Epoch [20/38], Training Loss: 22.2445, Validation Loss Current: 8.6286, Validation Loss AVG: 8.6286, lr: 0.0010000000000000002
Epoch [21/38], Training Loss: 21.8445, Validation Loss Current: 8.6791, Validation Loss AVG: 8.6791, lr: 0.0010000000000000002
Epoch [22/38], Training Loss: 20.9047, Validation Loss Current: 8.7407, Validation Loss AVG: 8.7407, lr: 0.0010000000000000002
Epoch [23/38], Training Loss: 20.9451, Validation Loss Current: 8.7822, Validation Loss AVG: 8.7822, lr: 0.0010000000000000002
Epoch [24/38], Training Loss: 20.9481, Validation Loss Current: 8.8058, Validation Loss AVG: 8.8058, lr: 0.0010000000000000002
Epoch [25/38], Training Loss: 21.6052, Validation Loss Current: 8.8301, Validation Loss AVG: 8.8301, lr: 0.0010000000000000002
Epoch [26/38], Training Loss: 21.3350, Validation Loss Current: 8.7970, Validation Loss AVG: 8.7970, lr: 0.00010000000000000003
Epoch [27/38], Training Loss: 21.2530, Validation Loss Current: 8.8009, Validation Loss AVG: 8.8009, lr: 0.00010000000000000003
Epoch [28/38], Training Loss: 21.1425, Validation Loss Current: 8.7991, Validation Loss AVG: 8.7991, lr: 0.00010000000000000003
Epoch [29/38], Training Loss: 21.2038, Validation Loss Current: 8.8301, Validation Loss AVG: 8.8301, lr: 0.00010000000000000003
Epoch [30/38], Training Loss: 19.8390, Validation Loss Current: 8.8475, Validation Loss AVG: 8.8475, lr: 0.00010000000000000003
Epoch [31/38], Training Loss: 21.2264, Validation Loss Current: 8.8369, Validation Loss AVG: 8.8369, lr: 0.00010000000000000003
Epoch [32/38], Training Loss: 21.2595, Validation Loss Current: 8.8125, Validation Loss AVG: 8.8125, lr: 1.0000000000000004e-05
Epoch [33/38], Training Loss: 21.2523, Validation Loss Current: 8.8425, Validation Loss AVG: 8.8425, lr: 1.0000000000000004e-05
Epoch [34/38], Training Loss: 20.6056, Validation Loss Current: 8.8196, Validation Loss AVG: 8.8196, lr: 1.0000000000000004e-05
Epoch [35/38], Training Loss: 21.1819, Validation Loss Current: 8.8108, Validation Loss AVG: 8.8108, lr: 1.0000000000000004e-05
Epoch [36/38], Training Loss: 20.7691, Validation Loss Current: 8.8273, Validation Loss AVG: 8.8273, lr: 1.0000000000000004e-05
Epoch [37/38], Training Loss: 20.7995, Validation Loss Current: 8.8014, Validation Loss AVG: 8.8014, lr: 1.0000000000000004e-05
Epoch [38/38], Training Loss: 20.6724, Validation Loss Current: 8.8017, Validation Loss AVG: 8.8017, lr: 1.0000000000000004e-06
Epoch [39/38], Training Loss: 19.9263, Validation Loss Current: 8.8123, Validation Loss AVG: 8.8123, lr: 1.0000000000000004e-06
Epoch [40/38], Training Loss: 20.6954, Validation Loss Current: 8.8233, Validation Loss AVG: 8.8233, lr: 1.0000000000000004e-06
Epoch [41/38], Training Loss: 20.7851, Validation Loss Current: 8.8008, Validation Loss AVG: 8.8008, lr: 1.0000000000000004e-06
Epoch [42/38], Training Loss: 19.9682, Validation Loss Current: 8.8214, Validation Loss AVG: 8.8214, lr: 1.0000000000000004e-06
Epoch [43/38], Training Loss: 20.6153, Validation Loss Current: 8.8333, Validation Loss AVG: 8.8333, lr: 1.0000000000000004e-06
 --- Early Stopped ---
Patch distance: 0.8 finished training. Best epoch: 13 Best val accuracy: [0.3684210526315789, 0.3638157894736842, 0.325328947368421, 0.3805921052631579, 0.33519736842105263, 0.3427631578947369, 0.29703947368421046, 0.34967105263157894, 0.27697368421052626, 0.35625, 0.37631578947368427, 0.4023026315789474, 0.40921052631578947, 0.40921052631578936, 0.4203947368421052, 0.39934210526315794, 0.41776315789473684, 0.3924342105263158, 0.40131578947368424, 0.4111842105263158, 0.4082236842105263, 0.4134868421052632, 0.40921052631578947, 0.4101973684210526, 0.40690789473684214, 0.40690789473684214, 0.40855263157894733, 0.40888157894736843, 0.40855263157894733, 0.40592105263157896, 0.40690789473684214, 0.4072368421052632, 0.4072368421052632, 0.4072368421052632, 0.40690789473684214, 0.40690789473684214, 0.40657894736842104, 0.40657894736842104, 0.40657894736842104, 0.40657894736842104, 0.40657894736842104, 0.40657894736842104, 0.40657894736842104] Best val loss: 8.384902811050415


Loaded best state dict for [0.4, 0.6, 0.8]
Current group: 1
Epoch [1/38], Training Loss: 31.4008, Validation Loss Current: 9.1457, Validation Loss AVG: 10.4689, lr: 0.1
Epoch [2/38], Training Loss: 32.5848, Validation Loss Current: 9.0058, Validation Loss AVG: 10.5024, lr: 0.1
Epoch [3/38], Training Loss: 33.9064, Validation Loss Current: 7.8817, Validation Loss AVG: 8.9428, lr: 0.1
Epoch [4/38], Training Loss: 30.7806, Validation Loss Current: 8.6673, Validation Loss AVG: 11.5655, lr: 0.1
Epoch [5/38], Training Loss: 31.4072, Validation Loss Current: 8.5254, Validation Loss AVG: 9.8300, lr: 0.1
Epoch [6/38], Training Loss: 32.0974, Validation Loss Current: 8.0728, Validation Loss AVG: 9.1594, lr: 0.1
Epoch [7/38], Training Loss: 30.8496, Validation Loss Current: 7.8348, Validation Loss AVG: 9.0150, lr: 0.1
Epoch [8/38], Training Loss: 30.6870, Validation Loss Current: 8.0903, Validation Loss AVG: 9.5081, lr: 0.1
Epoch [9/38], Training Loss: 30.3247, Validation Loss Current: 8.0730, Validation Loss AVG: 9.3583, lr: 0.1
Epoch [10/38], Training Loss: 32.2615, Validation Loss Current: 8.3476, Validation Loss AVG: 12.0136, lr: 0.1
Epoch [11/38], Training Loss: 30.7674, Validation Loss Current: 8.5780, Validation Loss AVG: 11.7516, lr: 0.1
Epoch [12/38], Training Loss: 30.5390, Validation Loss Current: 7.7452, Validation Loss AVG: 8.9931, lr: 0.1
Epoch [13/38], Training Loss: 30.3760, Validation Loss Current: 8.0179, Validation Loss AVG: 9.9369, lr: 0.1
Epoch [14/38], Training Loss: 28.3333, Validation Loss Current: 8.6106, Validation Loss AVG: 9.7059, lr: 0.1
Epoch [15/38], Training Loss: 29.9709, Validation Loss Current: 8.8374, Validation Loss AVG: 9.3881, lr: 0.1
Epoch [16/38], Training Loss: 31.2252, Validation Loss Current: 8.1526, Validation Loss AVG: 9.1271, lr: 0.1
Epoch [17/38], Training Loss: 31.7282, Validation Loss Current: 8.8875, Validation Loss AVG: 9.6427, lr: 0.1
Epoch [18/38], Training Loss: 30.5735, Validation Loss Current: 8.8751, Validation Loss AVG: 11.2320, lr: 0.1
Epoch [19/38], Training Loss: 29.3355, Validation Loss Current: 7.3208, Validation Loss AVG: 10.0302, lr: 0.010000000000000002
Epoch [20/38], Training Loss: 26.7275, Validation Loss Current: 7.0948, Validation Loss AVG: 9.0061, lr: 0.010000000000000002
Epoch [21/38], Training Loss: 24.1023, Validation Loss Current: 7.2564, Validation Loss AVG: 9.2694, lr: 0.010000000000000002
Epoch [22/38], Training Loss: 23.3822, Validation Loss Current: 7.1538, Validation Loss AVG: 9.3014, lr: 0.010000000000000002
Epoch [23/38], Training Loss: 22.7839, Validation Loss Current: 7.0652, Validation Loss AVG: 9.3935, lr: 0.010000000000000002
Epoch [24/38], Training Loss: 22.5080, Validation Loss Current: 7.1089, Validation Loss AVG: 9.6639, lr: 0.010000000000000002
Epoch [25/38], Training Loss: 21.1966, Validation Loss Current: 7.1786, Validation Loss AVG: 9.5597, lr: 0.010000000000000002
Epoch [26/38], Training Loss: 20.6645, Validation Loss Current: 7.2380, Validation Loss AVG: 10.1888, lr: 0.010000000000000002
Epoch [27/38], Training Loss: 18.7332, Validation Loss Current: 7.4907, Validation Loss AVG: 10.1977, lr: 0.010000000000000002
Epoch [28/38], Training Loss: 20.2872, Validation Loss Current: 7.3956, Validation Loss AVG: 10.4742, lr: 0.010000000000000002
Epoch [29/38], Training Loss: 17.4656, Validation Loss Current: 7.7143, Validation Loss AVG: 10.3820, lr: 0.010000000000000002
Epoch [30/38], Training Loss: 17.0977, Validation Loss Current: 7.4938, Validation Loss AVG: 10.4545, lr: 0.0010000000000000002
Epoch [31/38], Training Loss: 16.6176, Validation Loss Current: 7.6740, Validation Loss AVG: 10.5548, lr: 0.0010000000000000002
Epoch [32/38], Training Loss: 16.4286, Validation Loss Current: 7.6711, Validation Loss AVG: 10.6591, lr: 0.0010000000000000002
Epoch [33/38], Training Loss: 16.6943, Validation Loss Current: 7.5716, Validation Loss AVG: 10.5798, lr: 0.0010000000000000002
Epoch [34/38], Training Loss: 16.4179, Validation Loss Current: 7.7995, Validation Loss AVG: 10.7062, lr: 0.0010000000000000002
Epoch [35/38], Training Loss: 15.2573, Validation Loss Current: 7.6616, Validation Loss AVG: 10.7046, lr: 0.0010000000000000002
Epoch [36/38], Training Loss: 15.4750, Validation Loss Current: 7.6849, Validation Loss AVG: 10.6932, lr: 0.00010000000000000003
Epoch [37/38], Training Loss: 15.2311, Validation Loss Current: 7.7659, Validation Loss AVG: 10.7764, lr: 0.00010000000000000003
Epoch [38/38], Training Loss: 15.7701, Validation Loss Current: 7.7405, Validation Loss AVG: 10.7355, lr: 0.00010000000000000003
Epoch [39/38], Training Loss: 15.5453, Validation Loss Current: 7.7778, Validation Loss AVG: 10.7799, lr: 0.00010000000000000003
Epoch [40/38], Training Loss: 14.8769, Validation Loss Current: 7.7875, Validation Loss AVG: 10.7851, lr: 0.00010000000000000003
Epoch [41/38], Training Loss: 15.7345, Validation Loss Current: 7.7519, Validation Loss AVG: 10.7727, lr: 0.00010000000000000003
Epoch [42/38], Training Loss: 15.5402, Validation Loss Current: 7.8137, Validation Loss AVG: 10.8165, lr: 1.0000000000000004e-05
Epoch [43/38], Training Loss: 15.3622, Validation Loss Current: 7.7676, Validation Loss AVG: 10.8399, lr: 1.0000000000000004e-05
Epoch [44/38], Training Loss: 15.1624, Validation Loss Current: 7.8494, Validation Loss AVG: 10.8109, lr: 1.0000000000000004e-05
Epoch [45/38], Training Loss: 16.4145, Validation Loss Current: 7.7784, Validation Loss AVG: 10.8480, lr: 1.0000000000000004e-05
Epoch [46/38], Training Loss: 15.6186, Validation Loss Current: 7.7609, Validation Loss AVG: 10.8050, lr: 1.0000000000000004e-05
Epoch [47/38], Training Loss: 15.4962, Validation Loss Current: 7.8462, Validation Loss AVG: 10.8118, lr: 1.0000000000000004e-05
Epoch [48/38], Training Loss: 15.5518, Validation Loss Current: 7.7884, Validation Loss AVG: 10.7709, lr: 1.0000000000000004e-06
Epoch [49/38], Training Loss: 15.9875, Validation Loss Current: 7.8237, Validation Loss AVG: 10.8260, lr: 1.0000000000000004e-06
Epoch [50/38], Training Loss: 15.8211, Validation Loss Current: 7.7974, Validation Loss AVG: 10.8055, lr: 1.0000000000000004e-06
Epoch [51/38], Training Loss: 15.1049, Validation Loss Current: 7.8112, Validation Loss AVG: 10.8163, lr: 1.0000000000000004e-06
Epoch [52/38], Training Loss: 15.1539, Validation Loss Current: 7.7860, Validation Loss AVG: 10.8277, lr: 1.0000000000000004e-06
Epoch [53/38], Training Loss: 15.1958, Validation Loss Current: 7.7528, Validation Loss AVG: 10.8094, lr: 1.0000000000000004e-06
 --- Early Stopped ---
Patch distance: 1 finished training. Best epoch: 23 Best val accuracy: [0.32894736842105265, 0.34210526315789475, 0.3996710526315789, 0.40460526315789475, 0.3832236842105263, 0.39144736842105265, 0.4128289473684211, 0.4276315789473684, 0.42598684210526316, 0.46710526315789475, 0.42269736842105265, 0.43585526315789475, 0.46875, 0.4243421052631579, 0.4194078947368421, 0.40625, 0.38651315789473684, 0.3667763157894737, 0.4621710526315789, 0.48519736842105265, 0.49835526315789475, 0.5, 0.5180921052631579, 0.5197368421052632, 0.5115131578947368, 0.5180921052631579, 0.5131578947368421, 0.5032894736842105, 0.4967105263157895, 0.5098684210526315, 0.5115131578947368, 0.5115131578947368, 0.506578947368421, 0.506578947368421, 0.5098684210526315, 0.5098684210526315, 0.5098684210526315, 0.5098684210526315, 0.5082236842105263, 0.5082236842105263, 0.5082236842105263, 0.5082236842105263, 0.5082236842105263, 0.5082236842105263, 0.5082236842105263, 0.5082236842105263, 0.5082236842105263, 0.5082236842105263, 0.5082236842105263, 0.5082236842105263, 0.5082236842105263, 0.5082236842105263, 0.5082236842105263] Best val loss: 7.0651572942733765


----- Training alexnet with sequence: [0.6, 0.8, 1] -----
Current group: 0.6
Epoch [1/50], Training Loss: 40.6243, Validation Loss Current: 9.9623, Validation Loss AVG: 9.9623, lr: 0.1
Epoch [2/50], Training Loss: 39.5065, Validation Loss Current: 9.9247, Validation Loss AVG: 9.9247, lr: 0.1
Epoch [3/50], Training Loss: 39.9504, Validation Loss Current: 9.7350, Validation Loss AVG: 9.7350, lr: 0.1
Epoch [4/50], Training Loss: 39.9863, Validation Loss Current: 12.0410, Validation Loss AVG: 12.0410, lr: 0.1
Epoch [5/50], Training Loss: 36.7892, Validation Loss Current: 11.1602, Validation Loss AVG: 11.1602, lr: 0.1
Epoch [6/50], Training Loss: 36.2340, Validation Loss Current: 9.1640, Validation Loss AVG: 9.1640, lr: 0.1
Epoch [7/50], Training Loss: 34.1005, Validation Loss Current: 8.8761, Validation Loss AVG: 8.8761, lr: 0.1
Epoch [8/50], Training Loss: 34.8014, Validation Loss Current: 10.2502, Validation Loss AVG: 10.2502, lr: 0.1
Epoch [9/50], Training Loss: 36.3231, Validation Loss Current: 9.4331, Validation Loss AVG: 9.4331, lr: 0.1
Epoch [10/50], Training Loss: 34.2508, Validation Loss Current: 9.0835, Validation Loss AVG: 9.0835, lr: 0.1
Epoch [11/50], Training Loss: 33.8059, Validation Loss Current: 9.2422, Validation Loss AVG: 9.2422, lr: 0.1
Epoch [12/50], Training Loss: 33.0701, Validation Loss Current: 9.9394, Validation Loss AVG: 9.9394, lr: 0.1
Epoch [13/50], Training Loss: 35.8141, Validation Loss Current: 9.0282, Validation Loss AVG: 9.0282, lr: 0.1
Epoch [14/50], Training Loss: 31.1884, Validation Loss Current: 8.6941, Validation Loss AVG: 8.6941, lr: 0.010000000000000002
Epoch [15/50], Training Loss: 29.9758, Validation Loss Current: 8.4583, Validation Loss AVG: 8.4583, lr: 0.010000000000000002
Epoch [16/50], Training Loss: 27.0386, Validation Loss Current: 8.6186, Validation Loss AVG: 8.6186, lr: 0.010000000000000002
Epoch [17/50], Training Loss: 26.3383, Validation Loss Current: 8.3649, Validation Loss AVG: 8.3649, lr: 0.010000000000000002
Epoch [18/50], Training Loss: 27.2854, Validation Loss Current: 8.7501, Validation Loss AVG: 8.7501, lr: 0.010000000000000002
Epoch [19/50], Training Loss: 24.9747, Validation Loss Current: 8.5097, Validation Loss AVG: 8.5097, lr: 0.010000000000000002
Epoch [20/50], Training Loss: 25.3630, Validation Loss Current: 8.7027, Validation Loss AVG: 8.7027, lr: 0.010000000000000002
Epoch [21/50], Training Loss: 24.4840, Validation Loss Current: 8.9155, Validation Loss AVG: 8.9155, lr: 0.010000000000000002
Epoch [22/50], Training Loss: 24.1478, Validation Loss Current: 8.5033, Validation Loss AVG: 8.5033, lr: 0.010000000000000002
Epoch [23/50], Training Loss: 23.1343, Validation Loss Current: 9.6780, Validation Loss AVG: 9.6780, lr: 0.010000000000000002
Epoch [24/50], Training Loss: 22.4409, Validation Loss Current: 8.9922, Validation Loss AVG: 8.9922, lr: 0.0010000000000000002
Epoch [25/50], Training Loss: 21.0092, Validation Loss Current: 9.0996, Validation Loss AVG: 9.0996, lr: 0.0010000000000000002
Epoch [26/50], Training Loss: 21.1892, Validation Loss Current: 9.0163, Validation Loss AVG: 9.0163, lr: 0.0010000000000000002
Epoch [27/50], Training Loss: 20.5096, Validation Loss Current: 9.2494, Validation Loss AVG: 9.2494, lr: 0.0010000000000000002
Epoch [28/50], Training Loss: 21.1495, Validation Loss Current: 9.2143, Validation Loss AVG: 9.2143, lr: 0.0010000000000000002
Epoch [29/50], Training Loss: 20.8186, Validation Loss Current: 9.1994, Validation Loss AVG: 9.1994, lr: 0.0010000000000000002
Epoch [30/50], Training Loss: 21.4195, Validation Loss Current: 9.2332, Validation Loss AVG: 9.2332, lr: 0.00010000000000000003
Epoch [31/50], Training Loss: 20.5709, Validation Loss Current: 9.2336, Validation Loss AVG: 9.2336, lr: 0.00010000000000000003
Epoch [32/50], Training Loss: 20.8302, Validation Loss Current: 9.1917, Validation Loss AVG: 9.1917, lr: 0.00010000000000000003
Epoch [33/50], Training Loss: 20.2786, Validation Loss Current: 9.2341, Validation Loss AVG: 9.2341, lr: 0.00010000000000000003
Epoch [34/50], Training Loss: 20.9121, Validation Loss Current: 9.2916, Validation Loss AVG: 9.2916, lr: 0.00010000000000000003
Epoch [35/50], Training Loss: 20.7355, Validation Loss Current: 9.2721, Validation Loss AVG: 9.2721, lr: 0.00010000000000000003
Epoch [36/50], Training Loss: 21.0365, Validation Loss Current: 9.2378, Validation Loss AVG: 9.2378, lr: 1.0000000000000004e-05
Epoch [37/50], Training Loss: 21.0909, Validation Loss Current: 9.2795, Validation Loss AVG: 9.2795, lr: 1.0000000000000004e-05
Epoch [38/50], Training Loss: 20.3551, Validation Loss Current: 9.2667, Validation Loss AVG: 9.2667, lr: 1.0000000000000004e-05
Epoch [39/50], Training Loss: 20.7936, Validation Loss Current: 9.2594, Validation Loss AVG: 9.2594, lr: 1.0000000000000004e-05
Epoch [40/50], Training Loss: 23.9449, Validation Loss Current: 9.2588, Validation Loss AVG: 9.2588, lr: 1.0000000000000004e-05
Epoch [41/50], Training Loss: 21.2244, Validation Loss Current: 9.2753, Validation Loss AVG: 9.2753, lr: 1.0000000000000004e-05
Epoch [42/50], Training Loss: 20.2179, Validation Loss Current: 9.2422, Validation Loss AVG: 9.2422, lr: 1.0000000000000004e-06
Epoch [43/50], Training Loss: 21.3595, Validation Loss Current: 9.2583, Validation Loss AVG: 9.2583, lr: 1.0000000000000004e-06
Epoch [44/50], Training Loss: 20.8112, Validation Loss Current: 9.2621, Validation Loss AVG: 9.2621, lr: 1.0000000000000004e-06
Epoch [45/50], Training Loss: 21.8070, Validation Loss Current: 9.2615, Validation Loss AVG: 9.2615, lr: 1.0000000000000004e-06
Epoch [46/50], Training Loss: 21.6227, Validation Loss Current: 9.2368, Validation Loss AVG: 9.2368, lr: 1.0000000000000004e-06
Epoch [47/50], Training Loss: 21.2984, Validation Loss Current: 9.2592, Validation Loss AVG: 9.2592, lr: 1.0000000000000004e-06
 --- Early Stopped ---
Patch distance: 0.6 finished training. Best epoch: 17 Best val accuracy: [0.2450657894736842, 0.2450657894736842, 0.29046052631578945, 0.17763157894736842, 0.30559210526315794, 0.3453947368421053, 0.362171052631579, 0.29078947368421054, 0.32763157894736844, 0.29375, 0.35657894736842105, 0.28322368421052635, 0.3338815789473684, 0.37565789473684214, 0.40756578947368427, 0.4013157894736842, 0.40855263157894733, 0.3917763157894737, 0.41743421052631585, 0.4141447368421053, 0.40690789473684214, 0.41578947368421054, 0.3940789473684211, 0.4128289473684211, 0.4164473684210527, 0.4223684210526316, 0.4167763157894736, 0.41743421052631574, 0.41480263157894737, 0.41578947368421054, 0.4167763157894736, 0.41743421052631574, 0.4167763157894736, 0.4171052631578947, 0.4184210526315789, 0.4184210526315789, 0.41776315789473684, 0.4180921052631579, 0.4180921052631579, 0.4184210526315789, 0.41776315789473684, 0.4180921052631579, 0.4180921052631579, 0.41776315789473684, 0.4180921052631579, 0.4180921052631579, 0.41776315789473684] Best val loss: 8.364871335029601


Loaded best state dict for [0.6]
Current group: 0.8
Epoch [1/50], Training Loss: 31.1323, Validation Loss Current: 9.5512, Validation Loss AVG: 9.5512, lr: 0.1
Epoch [2/50], Training Loss: 33.0308, Validation Loss Current: 14.6111, Validation Loss AVG: 14.6111, lr: 0.1
Epoch [3/50], Training Loss: 35.1943, Validation Loss Current: 10.5829, Validation Loss AVG: 10.5829, lr: 0.1
Epoch [4/50], Training Loss: 34.5758, Validation Loss Current: 9.2397, Validation Loss AVG: 9.2397, lr: 0.1
Epoch [5/50], Training Loss: 32.7409, Validation Loss Current: 9.6847, Validation Loss AVG: 9.6847, lr: 0.1
Epoch [6/50], Training Loss: 32.2373, Validation Loss Current: 9.9976, Validation Loss AVG: 9.9976, lr: 0.1
Epoch [7/50], Training Loss: 31.5984, Validation Loss Current: 11.8855, Validation Loss AVG: 11.8855, lr: 0.1
Epoch [8/50], Training Loss: 36.0853, Validation Loss Current: 9.5762, Validation Loss AVG: 9.5762, lr: 0.1
Epoch [9/50], Training Loss: 31.6617, Validation Loss Current: 10.3575, Validation Loss AVG: 10.3575, lr: 0.1
Epoch [10/50], Training Loss: 32.9604, Validation Loss Current: 10.6202, Validation Loss AVG: 10.6202, lr: 0.1
Epoch [11/50], Training Loss: 30.2659, Validation Loss Current: 8.7596, Validation Loss AVG: 8.7596, lr: 0.010000000000000002
Epoch [12/50], Training Loss: 27.1231, Validation Loss Current: 8.6513, Validation Loss AVG: 8.6513, lr: 0.010000000000000002
Epoch [13/50], Training Loss: 26.0657, Validation Loss Current: 8.6846, Validation Loss AVG: 8.6846, lr: 0.010000000000000002
Epoch [14/50], Training Loss: 24.5901, Validation Loss Current: 8.7214, Validation Loss AVG: 8.7214, lr: 0.010000000000000002
Epoch [15/50], Training Loss: 24.6756, Validation Loss Current: 9.1054, Validation Loss AVG: 9.1054, lr: 0.010000000000000002
Epoch [16/50], Training Loss: 24.6143, Validation Loss Current: 8.8255, Validation Loss AVG: 8.8255, lr: 0.010000000000000002
Epoch [17/50], Training Loss: 22.9872, Validation Loss Current: 9.3146, Validation Loss AVG: 9.3146, lr: 0.010000000000000002
Epoch [18/50], Training Loss: 22.3411, Validation Loss Current: 8.9213, Validation Loss AVG: 8.9213, lr: 0.010000000000000002
Epoch [19/50], Training Loss: 21.1884, Validation Loss Current: 9.0959, Validation Loss AVG: 9.0959, lr: 0.0010000000000000002
Epoch [20/50], Training Loss: 20.6811, Validation Loss Current: 9.0730, Validation Loss AVG: 9.0730, lr: 0.0010000000000000002
Epoch [21/50], Training Loss: 21.2724, Validation Loss Current: 9.2338, Validation Loss AVG: 9.2338, lr: 0.0010000000000000002
Epoch [22/50], Training Loss: 20.1369, Validation Loss Current: 9.2053, Validation Loss AVG: 9.2053, lr: 0.0010000000000000002
Epoch [23/50], Training Loss: 20.7526, Validation Loss Current: 9.1538, Validation Loss AVG: 9.1538, lr: 0.0010000000000000002
Epoch [24/50], Training Loss: 20.6247, Validation Loss Current: 9.3397, Validation Loss AVG: 9.3397, lr: 0.0010000000000000002
Epoch [25/50], Training Loss: 20.8998, Validation Loss Current: 9.2992, Validation Loss AVG: 9.2992, lr: 0.00010000000000000003
Epoch [26/50], Training Loss: 21.1718, Validation Loss Current: 9.2526, Validation Loss AVG: 9.2526, lr: 0.00010000000000000003
Epoch [27/50], Training Loss: 20.8530, Validation Loss Current: 9.2549, Validation Loss AVG: 9.2549, lr: 0.00010000000000000003
Epoch [28/50], Training Loss: 20.2111, Validation Loss Current: 9.2730, Validation Loss AVG: 9.2730, lr: 0.00010000000000000003
Epoch [29/50], Training Loss: 21.5435, Validation Loss Current: 9.2697, Validation Loss AVG: 9.2697, lr: 0.00010000000000000003
Epoch [30/50], Training Loss: 19.9837, Validation Loss Current: 9.2438, Validation Loss AVG: 9.2438, lr: 0.00010000000000000003
Epoch [31/50], Training Loss: 21.1343, Validation Loss Current: 9.2388, Validation Loss AVG: 9.2388, lr: 1.0000000000000004e-05
Epoch [32/50], Training Loss: 19.6984, Validation Loss Current: 9.2594, Validation Loss AVG: 9.2594, lr: 1.0000000000000004e-05
Epoch [33/50], Training Loss: 20.2840, Validation Loss Current: 9.2518, Validation Loss AVG: 9.2518, lr: 1.0000000000000004e-05
Epoch [34/50], Training Loss: 20.5649, Validation Loss Current: 9.2481, Validation Loss AVG: 9.2481, lr: 1.0000000000000004e-05
Epoch [35/50], Training Loss: 19.6261, Validation Loss Current: 9.2461, Validation Loss AVG: 9.2461, lr: 1.0000000000000004e-05
Epoch [36/50], Training Loss: 20.3144, Validation Loss Current: 9.2228, Validation Loss AVG: 9.2228, lr: 1.0000000000000004e-05
Epoch [37/50], Training Loss: 22.8361, Validation Loss Current: 9.2715, Validation Loss AVG: 9.2715, lr: 1.0000000000000004e-06
Epoch [38/50], Training Loss: 20.4389, Validation Loss Current: 9.2383, Validation Loss AVG: 9.2383, lr: 1.0000000000000004e-06
Epoch [39/50], Training Loss: 19.7997, Validation Loss Current: 9.2712, Validation Loss AVG: 9.2712, lr: 1.0000000000000004e-06
Epoch [40/50], Training Loss: 20.1254, Validation Loss Current: 9.2573, Validation Loss AVG: 9.2573, lr: 1.0000000000000004e-06
Epoch [41/50], Training Loss: 21.5196, Validation Loss Current: 9.2789, Validation Loss AVG: 9.2789, lr: 1.0000000000000004e-06
Epoch [42/50], Training Loss: 20.9771, Validation Loss Current: 9.2660, Validation Loss AVG: 9.2660, lr: 1.0000000000000004e-06
 --- Early Stopped ---
Patch distance: 0.8 finished training. Best epoch: 12 Best val accuracy: [0.3677631578947368, 0.3101973684210526, 0.2901315789473684, 0.31184210526315786, 0.3361842105263158, 0.31184210526315786, 0.3003289473684211, 0.33026315789473687, 0.34309210526315786, 0.2924342105263158, 0.3815789473684211, 0.38782894736842105, 0.3940789473684211, 0.40625, 0.3868421052631579, 0.41118421052631576, 0.3990131578947368, 0.4167763157894737, 0.4078947368421053, 0.4115131578947368, 0.4095394736842105, 0.41217105263157894, 0.41447368421052627, 0.4131578947368421, 0.4128289473684211, 0.41414473684210523, 0.41480263157894737, 0.41546052631578945, 0.4164473684210527, 0.41578947368421054, 0.41710526315789476, 0.4164473684210527, 0.4161184210526316, 0.4161184210526316, 0.4164473684210527, 0.41578947368421054, 0.41578947368421054, 0.41578947368421054, 0.41578947368421054, 0.41578947368421054, 0.41578947368421054, 0.41578947368421054] Best val loss: 8.651261520385741


Loaded best state dict for [0.6, 0.8]
Current group: 1
Epoch [1/50], Training Loss: 28.9068, Validation Loss Current: 9.5402, Validation Loss AVG: 11.4388, lr: 0.1
Epoch [2/50], Training Loss: 32.0417, Validation Loss Current: 13.8130, Validation Loss AVG: 14.7247, lr: 0.1
Epoch [3/50], Training Loss: 37.1445, Validation Loss Current: 9.2984, Validation Loss AVG: 15.3732, lr: 0.1
Epoch [4/50], Training Loss: 33.6122, Validation Loss Current: 13.6518, Validation Loss AVG: 22.9862, lr: 0.1
Epoch [5/50], Training Loss: 33.8414, Validation Loss Current: 9.3577, Validation Loss AVG: 11.7601, lr: 0.1
Epoch [6/50], Training Loss: 31.7917, Validation Loss Current: 8.4027, Validation Loss AVG: 9.7168, lr: 0.1
Epoch [7/50], Training Loss: 30.0982, Validation Loss Current: 9.0056, Validation Loss AVG: 11.1402, lr: 0.1
Epoch [8/50], Training Loss: 29.8332, Validation Loss Current: 9.5482, Validation Loss AVG: 10.7512, lr: 0.1
Epoch [9/50], Training Loss: 31.4615, Validation Loss Current: 8.3229, Validation Loss AVG: 12.2614, lr: 0.1
Epoch [10/50], Training Loss: 31.3218, Validation Loss Current: 7.9154, Validation Loss AVG: 9.5521, lr: 0.1
Epoch [11/50], Training Loss: 30.0252, Validation Loss Current: 15.5075, Validation Loss AVG: 20.7777, lr: 0.1
Epoch [12/50], Training Loss: 32.6236, Validation Loss Current: 7.9850, Validation Loss AVG: 9.8436, lr: 0.1
Epoch [13/50], Training Loss: 29.8262, Validation Loss Current: 7.7244, Validation Loss AVG: 8.6882, lr: 0.1
Epoch [14/50], Training Loss: 28.7240, Validation Loss Current: 7.5205, Validation Loss AVG: 9.6745, lr: 0.1
Epoch [15/50], Training Loss: 28.8924, Validation Loss Current: 7.3512, Validation Loss AVG: 8.5435, lr: 0.1
Epoch [16/50], Training Loss: 26.6405, Validation Loss Current: 8.0308, Validation Loss AVG: 10.3759, lr: 0.1
Epoch [17/50], Training Loss: 28.4842, Validation Loss Current: 9.1116, Validation Loss AVG: 10.0670, lr: 0.1
Epoch [18/50], Training Loss: 28.5786, Validation Loss Current: 7.9487, Validation Loss AVG: 11.5158, lr: 0.1
Epoch [19/50], Training Loss: 30.4444, Validation Loss Current: 7.4496, Validation Loss AVG: 9.3109, lr: 0.1
Epoch [20/50], Training Loss: 29.2417, Validation Loss Current: 7.9404, Validation Loss AVG: 11.5705, lr: 0.1
Epoch [21/50], Training Loss: 30.4417, Validation Loss Current: 7.4551, Validation Loss AVG: 9.4768, lr: 0.1
Epoch [22/50], Training Loss: 25.9509, Validation Loss Current: 6.7919, Validation Loss AVG: 8.8068, lr: 0.010000000000000002
Epoch [23/50], Training Loss: 23.1338, Validation Loss Current: 6.5433, Validation Loss AVG: 9.1812, lr: 0.010000000000000002
Epoch [24/50], Training Loss: 23.0899, Validation Loss Current: 6.3882, Validation Loss AVG: 8.9904, lr: 0.010000000000000002
Epoch [25/50], Training Loss: 21.1656, Validation Loss Current: 6.4842, Validation Loss AVG: 9.7253, lr: 0.010000000000000002
Epoch [26/50], Training Loss: 19.8437, Validation Loss Current: 6.5659, Validation Loss AVG: 9.5910, lr: 0.010000000000000002
Epoch [27/50], Training Loss: 18.3026, Validation Loss Current: 6.5233, Validation Loss AVG: 9.9469, lr: 0.010000000000000002
Epoch [28/50], Training Loss: 20.4980, Validation Loss Current: 6.8389, Validation Loss AVG: 11.0050, lr: 0.010000000000000002
Epoch [29/50], Training Loss: 17.7175, Validation Loss Current: 6.7560, Validation Loss AVG: 10.6564, lr: 0.010000000000000002
Epoch [30/50], Training Loss: 16.2496, Validation Loss Current: 6.7343, Validation Loss AVG: 11.1877, lr: 0.010000000000000002
Epoch [31/50], Training Loss: 15.1727, Validation Loss Current: 6.8493, Validation Loss AVG: 11.0512, lr: 0.0010000000000000002
Epoch [32/50], Training Loss: 15.0556, Validation Loss Current: 6.8815, Validation Loss AVG: 11.2926, lr: 0.0010000000000000002
Epoch [33/50], Training Loss: 14.4881, Validation Loss Current: 6.8955, Validation Loss AVG: 11.2490, lr: 0.0010000000000000002
Epoch [34/50], Training Loss: 14.7461, Validation Loss Current: 6.8271, Validation Loss AVG: 11.3470, lr: 0.0010000000000000002
Epoch [35/50], Training Loss: 14.4999, Validation Loss Current: 6.9271, Validation Loss AVG: 11.4191, lr: 0.0010000000000000002
Epoch [36/50], Training Loss: 13.9142, Validation Loss Current: 6.9456, Validation Loss AVG: 11.4549, lr: 0.0010000000000000002
Epoch [37/50], Training Loss: 13.8692, Validation Loss Current: 7.0328, Validation Loss AVG: 11.4487, lr: 0.00010000000000000003
Epoch [38/50], Training Loss: 13.6226, Validation Loss Current: 7.0272, Validation Loss AVG: 11.4548, lr: 0.00010000000000000003
Epoch [39/50], Training Loss: 13.8936, Validation Loss Current: 6.9506, Validation Loss AVG: 11.5538, lr: 0.00010000000000000003
Epoch [40/50], Training Loss: 13.7291, Validation Loss Current: 6.9804, Validation Loss AVG: 11.5614, lr: 0.00010000000000000003
Epoch [41/50], Training Loss: 14.0968, Validation Loss Current: 6.9893, Validation Loss AVG: 11.5814, lr: 0.00010000000000000003
Epoch [42/50], Training Loss: 14.0075, Validation Loss Current: 7.0541, Validation Loss AVG: 11.5399, lr: 0.00010000000000000003
Epoch [43/50], Training Loss: 13.7671, Validation Loss Current: 6.9588, Validation Loss AVG: 11.5462, lr: 1.0000000000000004e-05
Epoch [44/50], Training Loss: 14.2831, Validation Loss Current: 6.9166, Validation Loss AVG: 11.5646, lr: 1.0000000000000004e-05
Epoch [45/50], Training Loss: 14.0335, Validation Loss Current: 6.9558, Validation Loss AVG: 11.5726, lr: 1.0000000000000004e-05
Epoch [46/50], Training Loss: 14.1544, Validation Loss Current: 6.9421, Validation Loss AVG: 11.5708, lr: 1.0000000000000004e-05
Epoch [47/50], Training Loss: 13.7687, Validation Loss Current: 6.9519, Validation Loss AVG: 11.5631, lr: 1.0000000000000004e-05
Epoch [48/50], Training Loss: 13.6509, Validation Loss Current: 6.9744, Validation Loss AVG: 11.5560, lr: 1.0000000000000004e-05
Epoch [49/50], Training Loss: 15.2042, Validation Loss Current: 6.9214, Validation Loss AVG: 11.5705, lr: 1.0000000000000004e-06
Epoch [50/50], Training Loss: 15.0668, Validation Loss Current: 6.9536, Validation Loss AVG: 11.5524, lr: 1.0000000000000004e-06
Epoch [51/50], Training Loss: 13.6246, Validation Loss Current: 6.9737, Validation Loss AVG: 11.5825, lr: 1.0000000000000004e-06
Epoch [52/50], Training Loss: 13.6563, Validation Loss Current: 6.9761, Validation Loss AVG: 11.5990, lr: 1.0000000000000004e-06
Epoch [53/50], Training Loss: 14.7153, Validation Loss Current: 6.9316, Validation Loss AVG: 11.6044, lr: 1.0000000000000004e-06
Epoch [54/50], Training Loss: 14.3650, Validation Loss Current: 6.9794, Validation Loss AVG: 11.5420, lr: 1.0000000000000004e-06
 --- Early Stopped ---
Patch distance: 1 finished training. Best epoch: 24 Best val accuracy: [0.3832236842105263, 0.3519736842105263, 0.3980263157894737, 0.39144736842105265, 0.33881578947368424, 0.39144736842105265, 0.40625, 0.36019736842105265, 0.4654605263157895, 0.43914473684210525, 0.2582236842105263, 0.4440789473684211, 0.4555921052631579, 0.47368421052631576, 0.48848684210526316, 0.44901315789473684, 0.4473684210526316, 0.4457236842105263, 0.5016447368421053, 0.44901315789473684, 0.4769736842105263, 0.5460526315789473, 0.5476973684210527, 0.555921052631579, 0.5460526315789473, 0.5641447368421053, 0.5509868421052632, 0.5476973684210527, 0.5740131578947368, 0.5427631578947368, 0.5592105263157895, 0.5575657894736842, 0.5608552631578947, 0.5657894736842105, 0.5592105263157895, 0.5625, 0.5608552631578947, 0.5608552631578947, 0.5575657894736842, 0.555921052631579, 0.555921052631579, 0.555921052631579, 0.555921052631579, 0.555921052631579, 0.555921052631579, 0.555921052631579, 0.555921052631579, 0.555921052631579, 0.555921052631579, 0.555921052631579, 0.555921052631579, 0.555921052631579, 0.555921052631579, 0.555921052631579] Best val loss: 6.38824737071991


----- Training alexnet with sequence: [0.8, 1] -----
Current group: 0.8
Epoch [1/75], Training Loss: 41.1264, Validation Loss Current: 9.9718, Validation Loss AVG: 9.9718, lr: 0.1
Epoch [2/75], Training Loss: 39.9008, Validation Loss Current: 9.9741, Validation Loss AVG: 9.9741, lr: 0.1
Epoch [3/75], Training Loss: 40.0870, Validation Loss Current: 9.6532, Validation Loss AVG: 9.6532, lr: 0.1
Epoch [4/75], Training Loss: 39.7952, Validation Loss Current: 9.8571, Validation Loss AVG: 9.8571, lr: 0.1
Epoch [5/75], Training Loss: 37.2658, Validation Loss Current: 11.5653, Validation Loss AVG: 11.5653, lr: 0.1
Epoch [6/75], Training Loss: 38.1564, Validation Loss Current: 9.5191, Validation Loss AVG: 9.5191, lr: 0.1
Epoch [7/75], Training Loss: 34.8031, Validation Loss Current: 8.7456, Validation Loss AVG: 8.7456, lr: 0.1
Epoch [8/75], Training Loss: 34.0706, Validation Loss Current: 17.8187, Validation Loss AVG: 17.8187, lr: 0.1
Epoch [9/75], Training Loss: 38.3787, Validation Loss Current: 8.6795, Validation Loss AVG: 8.6795, lr: 0.1
Epoch [10/75], Training Loss: 34.2846, Validation Loss Current: 9.2223, Validation Loss AVG: 9.2223, lr: 0.1
Epoch [11/75], Training Loss: 32.2304, Validation Loss Current: 18.3492, Validation Loss AVG: 18.3492, lr: 0.1
Epoch [12/75], Training Loss: 33.8248, Validation Loss Current: 10.0364, Validation Loss AVG: 10.0364, lr: 0.1
Epoch [13/75], Training Loss: 34.1139, Validation Loss Current: 9.3776, Validation Loss AVG: 9.3776, lr: 0.1
Epoch [14/75], Training Loss: 34.3073, Validation Loss Current: 9.0649, Validation Loss AVG: 9.0649, lr: 0.1
Epoch [15/75], Training Loss: 32.9703, Validation Loss Current: 12.0279, Validation Loss AVG: 12.0279, lr: 0.1
Epoch [16/75], Training Loss: 34.3063, Validation Loss Current: 8.5614, Validation Loss AVG: 8.5614, lr: 0.010000000000000002
Epoch [17/75], Training Loss: 28.1993, Validation Loss Current: 8.5397, Validation Loss AVG: 8.5397, lr: 0.010000000000000002
Epoch [18/75], Training Loss: 26.9068, Validation Loss Current: 8.0160, Validation Loss AVG: 8.0160, lr: 0.010000000000000002
Epoch [19/75], Training Loss: 24.8954, Validation Loss Current: 8.2332, Validation Loss AVG: 8.2332, lr: 0.010000000000000002
Epoch [20/75], Training Loss: 26.5042, Validation Loss Current: 8.4789, Validation Loss AVG: 8.4789, lr: 0.010000000000000002
Epoch [21/75], Training Loss: 24.4558, Validation Loss Current: 8.5571, Validation Loss AVG: 8.5571, lr: 0.010000000000000002
Epoch [22/75], Training Loss: 23.8984, Validation Loss Current: 8.0331, Validation Loss AVG: 8.0331, lr: 0.010000000000000002
Epoch [23/75], Training Loss: 23.8557, Validation Loss Current: 8.9785, Validation Loss AVG: 8.9785, lr: 0.010000000000000002
Epoch [24/75], Training Loss: 22.8388, Validation Loss Current: 9.1704, Validation Loss AVG: 9.1704, lr: 0.010000000000000002
Epoch [25/75], Training Loss: 23.1768, Validation Loss Current: 8.6004, Validation Loss AVG: 8.6004, lr: 0.0010000000000000002
Epoch [26/75], Training Loss: 21.7735, Validation Loss Current: 8.7168, Validation Loss AVG: 8.7168, lr: 0.0010000000000000002
Epoch [27/75], Training Loss: 21.4746, Validation Loss Current: 8.8638, Validation Loss AVG: 8.8638, lr: 0.0010000000000000002
Epoch [28/75], Training Loss: 21.0291, Validation Loss Current: 8.8150, Validation Loss AVG: 8.8150, lr: 0.0010000000000000002
Epoch [29/75], Training Loss: 22.4900, Validation Loss Current: 8.7515, Validation Loss AVG: 8.7515, lr: 0.0010000000000000002
Epoch [30/75], Training Loss: 20.9876, Validation Loss Current: 8.6857, Validation Loss AVG: 8.6857, lr: 0.0010000000000000002
Epoch [31/75], Training Loss: 21.2009, Validation Loss Current: 8.7530, Validation Loss AVG: 8.7530, lr: 0.00010000000000000003
Epoch [32/75], Training Loss: 23.8545, Validation Loss Current: 8.8422, Validation Loss AVG: 8.8422, lr: 0.00010000000000000003
Epoch [33/75], Training Loss: 20.2123, Validation Loss Current: 8.8827, Validation Loss AVG: 8.8827, lr: 0.00010000000000000003
Epoch [34/75], Training Loss: 20.4407, Validation Loss Current: 8.8911, Validation Loss AVG: 8.8911, lr: 0.00010000000000000003
Epoch [35/75], Training Loss: 21.9395, Validation Loss Current: 8.9077, Validation Loss AVG: 8.9077, lr: 0.00010000000000000003
Epoch [36/75], Training Loss: 21.2436, Validation Loss Current: 8.8522, Validation Loss AVG: 8.8522, lr: 0.00010000000000000003
Epoch [37/75], Training Loss: 20.9819, Validation Loss Current: 8.8628, Validation Loss AVG: 8.8628, lr: 1.0000000000000004e-05
Epoch [38/75], Training Loss: 20.3481, Validation Loss Current: 8.8614, Validation Loss AVG: 8.8614, lr: 1.0000000000000004e-05
Epoch [39/75], Training Loss: 20.8056, Validation Loss Current: 8.8923, Validation Loss AVG: 8.8923, lr: 1.0000000000000004e-05
Epoch [40/75], Training Loss: 21.3109, Validation Loss Current: 8.8720, Validation Loss AVG: 8.8720, lr: 1.0000000000000004e-05
Epoch [41/75], Training Loss: 20.8858, Validation Loss Current: 8.8640, Validation Loss AVG: 8.8640, lr: 1.0000000000000004e-05
Epoch [42/75], Training Loss: 20.2402, Validation Loss Current: 8.8577, Validation Loss AVG: 8.8577, lr: 1.0000000000000004e-05
Epoch [43/75], Training Loss: 20.1540, Validation Loss Current: 8.8556, Validation Loss AVG: 8.8556, lr: 1.0000000000000004e-06
Epoch [44/75], Training Loss: 21.0401, Validation Loss Current: 8.8587, Validation Loss AVG: 8.8587, lr: 1.0000000000000004e-06
Epoch [45/75], Training Loss: 20.7488, Validation Loss Current: 8.8735, Validation Loss AVG: 8.8735, lr: 1.0000000000000004e-06
Epoch [46/75], Training Loss: 20.3255, Validation Loss Current: 8.8791, Validation Loss AVG: 8.8791, lr: 1.0000000000000004e-06
Epoch [47/75], Training Loss: 21.5884, Validation Loss Current: 8.8671, Validation Loss AVG: 8.8671, lr: 1.0000000000000004e-06
Epoch [48/75], Training Loss: 20.7749, Validation Loss Current: 8.8710, Validation Loss AVG: 8.8710, lr: 1.0000000000000004e-06
 --- Early Stopped ---
Patch distance: 0.8 finished training. Best epoch: 18 Best val accuracy: [0.2450657894736842, 0.2450657894736842, 0.30657894736842106, 0.2674342105263158, 0.2450657894736842, 0.2654605263157895, 0.37434210526315786, 0.2950657894736842, 0.3680921052631579, 0.3226973684210526, 0.13914473684210527, 0.31875, 0.2858552631578947, 0.3555921052631579, 0.3078947368421053, 0.40493421052631573, 0.38618421052631574, 0.41776315789473684, 0.4108552631578948, 0.4115131578947368, 0.40888157894736843, 0.4266447368421053, 0.40625, 0.4016447368421052, 0.42269736842105265, 0.4269736842105264, 0.43125, 0.4266447368421053, 0.4286184210526315, 0.4309210526315789, 0.43125, 0.4319078947368421, 0.4319078947368421, 0.4322368421052631, 0.4319078947368421, 0.4322368421052632, 0.4319078947368421, 0.4322368421052632, 0.4322368421052632, 0.4322368421052632, 0.43157894736842106, 0.4322368421052632, 0.4322368421052632, 0.4322368421052632, 0.4322368421052632, 0.4322368421052632, 0.4322368421052632, 0.4322368421052632] Best val loss: 8.016032242774964


Loaded best state dict for [0.8]
Current group: 1
Epoch [1/75], Training Loss: 29.6700, Validation Loss Current: 11.1088, Validation Loss AVG: 12.3351, lr: 0.1
Epoch [2/75], Training Loss: 32.2675, Validation Loss Current: 7.7112, Validation Loss AVG: 8.7962, lr: 0.1
Epoch [3/75], Training Loss: 32.4623, Validation Loss Current: 11.5055, Validation Loss AVG: 11.2543, lr: 0.1
Epoch [4/75], Training Loss: 32.9360, Validation Loss Current: 7.9455, Validation Loss AVG: 9.0469, lr: 0.1
Epoch [5/75], Training Loss: 32.8268, Validation Loss Current: 8.0688, Validation Loss AVG: 9.5569, lr: 0.1
Epoch [6/75], Training Loss: 31.7111, Validation Loss Current: 7.8646, Validation Loss AVG: 8.9248, lr: 0.1
Epoch [7/75], Training Loss: 30.7319, Validation Loss Current: 23.8509, Validation Loss AVG: 25.7084, lr: 0.1
Epoch [8/75], Training Loss: 35.3992, Validation Loss Current: 14.6186, Validation Loss AVG: 19.4147, lr: 0.1
Epoch [9/75], Training Loss: 34.5036, Validation Loss Current: 7.3113, Validation Loss AVG: 8.4289, lr: 0.010000000000000002
Epoch [10/75], Training Loss: 26.9207, Validation Loss Current: 7.0038, Validation Loss AVG: 8.1647, lr: 0.010000000000000002
Epoch [11/75], Training Loss: 25.0093, Validation Loss Current: 6.8685, Validation Loss AVG: 8.2204, lr: 0.010000000000000002
Epoch [12/75], Training Loss: 23.6758, Validation Loss Current: 6.7385, Validation Loss AVG: 8.6754, lr: 0.010000000000000002
Epoch [13/75], Training Loss: 23.9007, Validation Loss Current: 6.6494, Validation Loss AVG: 8.6770, lr: 0.010000000000000002
Epoch [14/75], Training Loss: 22.1174, Validation Loss Current: 6.8046, Validation Loss AVG: 8.9850, lr: 0.010000000000000002
Epoch [15/75], Training Loss: 21.6365, Validation Loss Current: 6.7871, Validation Loss AVG: 8.6703, lr: 0.010000000000000002
Epoch [16/75], Training Loss: 23.1902, Validation Loss Current: 6.4837, Validation Loss AVG: 8.3050, lr: 0.010000000000000002
Epoch [17/75], Training Loss: 20.7136, Validation Loss Current: 6.4609, Validation Loss AVG: 8.5493, lr: 0.010000000000000002
Epoch [18/75], Training Loss: 21.0653, Validation Loss Current: 6.7449, Validation Loss AVG: 9.6620, lr: 0.010000000000000002
Epoch [19/75], Training Loss: 19.3020, Validation Loss Current: 6.6030, Validation Loss AVG: 9.4648, lr: 0.010000000000000002
Epoch [20/75], Training Loss: 19.3480, Validation Loss Current: 6.6693, Validation Loss AVG: 9.5893, lr: 0.010000000000000002
Epoch [21/75], Training Loss: 17.7093, Validation Loss Current: 6.6696, Validation Loss AVG: 9.9770, lr: 0.010000000000000002
Epoch [22/75], Training Loss: 16.6664, Validation Loss Current: 6.8557, Validation Loss AVG: 10.1423, lr: 0.010000000000000002
Epoch [23/75], Training Loss: 16.4511, Validation Loss Current: 6.7070, Validation Loss AVG: 9.8161, lr: 0.010000000000000002
Epoch [24/75], Training Loss: 14.9273, Validation Loss Current: 6.6789, Validation Loss AVG: 10.1156, lr: 0.0010000000000000002
Epoch [25/75], Training Loss: 15.3012, Validation Loss Current: 6.8248, Validation Loss AVG: 10.0257, lr: 0.0010000000000000002
Epoch [26/75], Training Loss: 15.1623, Validation Loss Current: 6.8386, Validation Loss AVG: 10.1529, lr: 0.0010000000000000002
Epoch [27/75], Training Loss: 14.3317, Validation Loss Current: 6.9369, Validation Loss AVG: 10.5137, lr: 0.0010000000000000002
Epoch [28/75], Training Loss: 14.5919, Validation Loss Current: 6.8901, Validation Loss AVG: 10.4533, lr: 0.0010000000000000002
Epoch [29/75], Training Loss: 13.9091, Validation Loss Current: 7.0389, Validation Loss AVG: 10.6451, lr: 0.0010000000000000002
Epoch [30/75], Training Loss: 14.0711, Validation Loss Current: 6.9495, Validation Loss AVG: 10.6171, lr: 0.00010000000000000003
Epoch [31/75], Training Loss: 13.9297, Validation Loss Current: 6.9400, Validation Loss AVG: 10.5475, lr: 0.00010000000000000003
Epoch [32/75], Training Loss: 13.7758, Validation Loss Current: 6.9569, Validation Loss AVG: 10.5724, lr: 0.00010000000000000003
Epoch [33/75], Training Loss: 13.2694, Validation Loss Current: 6.9575, Validation Loss AVG: 10.5441, lr: 0.00010000000000000003
Epoch [34/75], Training Loss: 14.5022, Validation Loss Current: 6.8926, Validation Loss AVG: 10.5413, lr: 0.00010000000000000003
Epoch [35/75], Training Loss: 13.7884, Validation Loss Current: 6.9258, Validation Loss AVG: 10.6138, lr: 0.00010000000000000003
Epoch [36/75], Training Loss: 13.7189, Validation Loss Current: 7.0835, Validation Loss AVG: 10.6397, lr: 1.0000000000000004e-05
Epoch [37/75], Training Loss: 13.4630, Validation Loss Current: 6.9538, Validation Loss AVG: 10.6251, lr: 1.0000000000000004e-05
Epoch [38/75], Training Loss: 14.1768, Validation Loss Current: 6.9774, Validation Loss AVG: 10.5979, lr: 1.0000000000000004e-05
Epoch [39/75], Training Loss: 13.4065, Validation Loss Current: 6.9664, Validation Loss AVG: 10.5981, lr: 1.0000000000000004e-05
Epoch [40/75], Training Loss: 13.4301, Validation Loss Current: 7.0193, Validation Loss AVG: 10.6034, lr: 1.0000000000000004e-05
Epoch [41/75], Training Loss: 13.5219, Validation Loss Current: 6.9977, Validation Loss AVG: 10.6323, lr: 1.0000000000000004e-05
Epoch [42/75], Training Loss: 14.5013, Validation Loss Current: 6.9254, Validation Loss AVG: 10.6249, lr: 1.0000000000000004e-06
Epoch [43/75], Training Loss: 14.2647, Validation Loss Current: 7.0318, Validation Loss AVG: 10.5767, lr: 1.0000000000000004e-06
Epoch [44/75], Training Loss: 14.1454, Validation Loss Current: 6.9836, Validation Loss AVG: 10.6442, lr: 1.0000000000000004e-06
Epoch [45/75], Training Loss: 13.2555, Validation Loss Current: 6.9133, Validation Loss AVG: 10.6336, lr: 1.0000000000000004e-06
Epoch [46/75], Training Loss: 13.7657, Validation Loss Current: 6.9938, Validation Loss AVG: 10.6385, lr: 1.0000000000000004e-06
Epoch [47/75], Training Loss: 13.6109, Validation Loss Current: 7.0265, Validation Loss AVG: 10.6281, lr: 1.0000000000000004e-06
 --- Early Stopped ---
Patch distance: 1 finished training. Best epoch: 17 Best val accuracy: [0.2598684210526316, 0.45394736842105265, 0.2582236842105263, 0.46875, 0.3832236842105263, 0.43914473684210525, 0.2565789473684211, 0.3536184210526316, 0.47039473684210525, 0.5049342105263158, 0.524671052631579, 0.53125, 0.5476973684210527, 0.537828947368421, 0.5493421052631579, 0.5509868421052632, 0.5592105263157895, 0.5526315789473685, 0.555921052631579, 0.5822368421052632, 0.5888157894736842, 0.5855263157894737, 0.5822368421052632, 0.5805921052631579, 0.5838815789473685, 0.5855263157894737, 0.5822368421052632, 0.5838815789473685, 0.5789473684210527, 0.587171052631579, 0.5855263157894737, 0.5904605263157895, 0.5888157894736842, 0.5855263157894737, 0.5822368421052632, 0.5822368421052632, 0.5822368421052632, 0.5822368421052632, 0.5822368421052632, 0.5822368421052632, 0.5822368421052632, 0.5822368421052632, 0.5822368421052632, 0.5822368421052632, 0.5822368421052632, 0.5822368421052632, 0.5822368421052632] Best val loss: 6.4608553647994995


----- Training alexnet with sequence: [1] -----
Current group: 1
Epoch [1/150], Training Loss: 40.4158, Validation Loss Current: 9.9226, Validation Loss AVG: 9.9566, lr: 0.1
Epoch [2/150], Training Loss: 39.4487, Validation Loss Current: 9.8407, Validation Loss AVG: 9.8573, lr: 0.1
Epoch [3/150], Training Loss: 40.0568, Validation Loss Current: 9.3959, Validation Loss AVG: 9.6661, lr: 0.1
Epoch [4/150], Training Loss: 40.3272, Validation Loss Current: 9.6400, Validation Loss AVG: 9.8119, lr: 0.1
Epoch [5/150], Training Loss: 39.7129, Validation Loss Current: 9.5170, Validation Loss AVG: 10.3064, lr: 0.1
Epoch [6/150], Training Loss: 39.3370, Validation Loss Current: 9.7018, Validation Loss AVG: 9.8933, lr: 0.1
Epoch [7/150], Training Loss: 38.1580, Validation Loss Current: 9.7801, Validation Loss AVG: 10.0142, lr: 0.1
Epoch [8/150], Training Loss: 37.4686, Validation Loss Current: 8.7162, Validation Loss AVG: 9.7189, lr: 0.1
Epoch [9/150], Training Loss: 35.0855, Validation Loss Current: 8.9938, Validation Loss AVG: 10.4328, lr: 0.1
Epoch [10/150], Training Loss: 34.9656, Validation Loss Current: 8.1852, Validation Loss AVG: 9.1462, lr: 0.1
Epoch [11/150], Training Loss: 33.7200, Validation Loss Current: 7.6962, Validation Loss AVG: 9.2695, lr: 0.1
Epoch [12/150], Training Loss: 33.7395, Validation Loss Current: 8.8397, Validation Loss AVG: 9.6323, lr: 0.1
Epoch [13/150], Training Loss: 33.7967, Validation Loss Current: 8.7863, Validation Loss AVG: 9.6938, lr: 0.1
Epoch [14/150], Training Loss: 33.8806, Validation Loss Current: 8.7936, Validation Loss AVG: 9.5102, lr: 0.1
Epoch [15/150], Training Loss: 33.3781, Validation Loss Current: 9.3906, Validation Loss AVG: 11.0059, lr: 0.1
Epoch [16/150], Training Loss: 34.0584, Validation Loss Current: 10.3375, Validation Loss AVG: 12.1531, lr: 0.1
Epoch [17/150], Training Loss: 34.0688, Validation Loss Current: 10.3817, Validation Loss AVG: 12.8859, lr: 0.1
Epoch [18/150], Training Loss: 33.5837, Validation Loss Current: 7.9771, Validation Loss AVG: 9.0332, lr: 0.010000000000000002
Epoch [19/150], Training Loss: 30.6061, Validation Loss Current: 7.5706, Validation Loss AVG: 8.8663, lr: 0.010000000000000002
Epoch [20/150], Training Loss: 29.6855, Validation Loss Current: 7.5180, Validation Loss AVG: 9.0141, lr: 0.010000000000000002
Epoch [21/150], Training Loss: 28.3563, Validation Loss Current: 7.0994, Validation Loss AVG: 8.6439, lr: 0.010000000000000002
Epoch [22/150], Training Loss: 26.5582, Validation Loss Current: 7.0217, Validation Loss AVG: 8.8082, lr: 0.010000000000000002
Epoch [23/150], Training Loss: 27.4451, Validation Loss Current: 7.0452, Validation Loss AVG: 8.9056, lr: 0.010000000000000002
Epoch [24/150], Training Loss: 25.9399, Validation Loss Current: 7.0179, Validation Loss AVG: 8.8490, lr: 0.010000000000000002
Epoch [25/150], Training Loss: 25.5358, Validation Loss Current: 7.0349, Validation Loss AVG: 8.7640, lr: 0.010000000000000002
Epoch [26/150], Training Loss: 25.1724, Validation Loss Current: 6.9220, Validation Loss AVG: 9.0697, lr: 0.010000000000000002
Epoch [27/150], Training Loss: 24.5117, Validation Loss Current: 6.8481, Validation Loss AVG: 8.8803, lr: 0.010000000000000002
Epoch [28/150], Training Loss: 24.8995, Validation Loss Current: 6.9566, Validation Loss AVG: 8.8792, lr: 0.010000000000000002
Epoch [29/150], Training Loss: 23.1933, Validation Loss Current: 7.0733, Validation Loss AVG: 9.2527, lr: 0.010000000000000002
Epoch [30/150], Training Loss: 22.7338, Validation Loss Current: 7.1318, Validation Loss AVG: 9.2697, lr: 0.010000000000000002
Epoch [31/150], Training Loss: 22.5628, Validation Loss Current: 6.9467, Validation Loss AVG: 9.3695, lr: 0.010000000000000002
Epoch [32/150], Training Loss: 24.2530, Validation Loss Current: 6.9080, Validation Loss AVG: 9.4168, lr: 0.010000000000000002
Epoch [33/150], Training Loss: 20.9490, Validation Loss Current: 7.1118, Validation Loss AVG: 10.3213, lr: 0.010000000000000002
Epoch [34/150], Training Loss: 19.8417, Validation Loss Current: 6.9259, Validation Loss AVG: 9.7316, lr: 0.0010000000000000002
Epoch [35/150], Training Loss: 19.7079, Validation Loss Current: 6.9906, Validation Loss AVG: 9.6819, lr: 0.0010000000000000002
Epoch [36/150], Training Loss: 19.8746, Validation Loss Current: 6.9318, Validation Loss AVG: 9.9569, lr: 0.0010000000000000002
Epoch [37/150], Training Loss: 19.6920, Validation Loss Current: 6.9391, Validation Loss AVG: 9.7885, lr: 0.0010000000000000002
Epoch [38/150], Training Loss: 19.3248, Validation Loss Current: 6.9010, Validation Loss AVG: 9.9433, lr: 0.0010000000000000002
Epoch [39/150], Training Loss: 19.3832, Validation Loss Current: 7.0166, Validation Loss AVG: 9.8369, lr: 0.0010000000000000002
Epoch [40/150], Training Loss: 19.2623, Validation Loss Current: 7.0235, Validation Loss AVG: 9.9096, lr: 0.00010000000000000003
Epoch [41/150], Training Loss: 20.1261, Validation Loss Current: 6.9601, Validation Loss AVG: 9.9077, lr: 0.00010000000000000003
Epoch [42/150], Training Loss: 19.3773, Validation Loss Current: 7.0267, Validation Loss AVG: 9.9232, lr: 0.00010000000000000003
Epoch [43/150], Training Loss: 19.1687, Validation Loss Current: 7.0309, Validation Loss AVG: 9.9128, lr: 0.00010000000000000003
Epoch [44/150], Training Loss: 19.0998, Validation Loss Current: 7.0124, Validation Loss AVG: 9.9404, lr: 0.00010000000000000003
Epoch [45/150], Training Loss: 18.7169, Validation Loss Current: 7.0042, Validation Loss AVG: 9.9319, lr: 0.00010000000000000003
Epoch [46/150], Training Loss: 20.2472, Validation Loss Current: 7.0097, Validation Loss AVG: 9.9299, lr: 1.0000000000000004e-05
Epoch [47/150], Training Loss: 19.5764, Validation Loss Current: 6.9656, Validation Loss AVG: 9.9385, lr: 1.0000000000000004e-05
Epoch [48/150], Training Loss: 20.5516, Validation Loss Current: 7.0045, Validation Loss AVG: 9.9416, lr: 1.0000000000000004e-05
Epoch [49/150], Training Loss: 20.0311, Validation Loss Current: 7.0673, Validation Loss AVG: 9.9489, lr: 1.0000000000000004e-05
Epoch [50/150], Training Loss: 19.3699, Validation Loss Current: 6.9689, Validation Loss AVG: 9.9482, lr: 1.0000000000000004e-05
Epoch [51/150], Training Loss: 19.0337, Validation Loss Current: 7.0134, Validation Loss AVG: 9.9543, lr: 1.0000000000000004e-05
Epoch [52/150], Training Loss: 19.6107, Validation Loss Current: 6.9088, Validation Loss AVG: 9.9321, lr: 1.0000000000000004e-06
Epoch [53/150], Training Loss: 18.9300, Validation Loss Current: 6.9717, Validation Loss AVG: 9.9568, lr: 1.0000000000000004e-06
Epoch [54/150], Training Loss: 19.6918, Validation Loss Current: 6.9661, Validation Loss AVG: 9.9367, lr: 1.0000000000000004e-06
Epoch [55/150], Training Loss: 20.8514, Validation Loss Current: 6.9357, Validation Loss AVG: 9.9564, lr: 1.0000000000000004e-06
Epoch [56/150], Training Loss: 19.1576, Validation Loss Current: 6.9717, Validation Loss AVG: 9.9538, lr: 1.0000000000000004e-06
Epoch [57/150], Training Loss: 19.2888, Validation Loss Current: 6.9712, Validation Loss AVG: 9.9415, lr: 1.0000000000000004e-06
 --- Early Stopped ---
Patch distance: 1 finished training. Best epoch: 27 Best val accuracy: [0.2450657894736842, 0.3092105263157895, 0.3092105263157895, 0.2713815789473684, 0.31085526315789475, 0.26480263157894735, 0.33881578947368424, 0.37006578947368424, 0.3651315789473684, 0.3996710526315789, 0.4243421052631579, 0.3536184210526316, 0.3782894736842105, 0.3503289473684211, 0.2993421052631579, 0.3651315789473684, 0.3569078947368421, 0.44243421052631576, 0.4440789473684211, 0.4588815789473684, 0.47039473684210525, 0.4769736842105263, 0.4769736842105263, 0.48026315789473684, 0.4786184210526316, 0.506578947368421, 0.49835526315789475, 0.48848684210526316, 0.4967105263157895, 0.5016447368421053, 0.49506578947368424, 0.4901315789473684, 0.5164473684210527, 0.5049342105263158, 0.5164473684210527, 0.5148026315789473, 0.5197368421052632, 0.5082236842105263, 0.5148026315789473, 0.5180921052631579, 0.5131578947368421, 0.5148026315789473, 0.5164473684210527, 0.5180921052631579, 0.5164473684210527, 0.5164473684210527, 0.5164473684210527, 0.5164473684210527, 0.5164473684210527, 0.5164473684210527, 0.5164473684210527, 0.5164473684210527, 0.5164473684210527, 0.5164473684210527, 0.5148026315789473, 0.5148026315789473, 0.5148026315789473] Best val loss: 6.848129630088806


Fold: 2
----- Training alexnet with sequence: [0.2, 0.4, 0.6, 0.8, 1] -----
Current group: 0.2
Epoch [1/30], Training Loss: 40.6660, Validation Loss Current: 10.0827, Validation Loss AVG: 10.0827, lr: 0.1
Epoch [2/30], Training Loss: 40.6267, Validation Loss Current: 10.1040, Validation Loss AVG: 10.1040, lr: 0.1
Epoch [3/30], Training Loss: 40.4050, Validation Loss Current: 10.1351, Validation Loss AVG: 10.1351, lr: 0.1
Epoch [4/30], Training Loss: 39.6421, Validation Loss Current: 10.1285, Validation Loss AVG: 10.1285, lr: 0.1
Epoch [5/30], Training Loss: 40.4564, Validation Loss Current: 10.1044, Validation Loss AVG: 10.1044, lr: 0.1
Epoch [6/30], Training Loss: 40.2148, Validation Loss Current: 10.1123, Validation Loss AVG: 10.1123, lr: 0.1
Epoch [7/30], Training Loss: 40.0615, Validation Loss Current: 10.1136, Validation Loss AVG: 10.1136, lr: 0.1
Epoch [8/30], Training Loss: 40.1027, Validation Loss Current: 10.1034, Validation Loss AVG: 10.1034, lr: 0.010000000000000002
Epoch [9/30], Training Loss: 40.3620, Validation Loss Current: 10.0915, Validation Loss AVG: 10.0915, lr: 0.010000000000000002
Epoch [10/30], Training Loss: 39.3644, Validation Loss Current: 10.0760, Validation Loss AVG: 10.0760, lr: 0.010000000000000002
Epoch [11/30], Training Loss: 39.6165, Validation Loss Current: 10.0534, Validation Loss AVG: 10.0534, lr: 0.010000000000000002
Epoch [12/30], Training Loss: 40.0407, Validation Loss Current: 9.9998, Validation Loss AVG: 9.9998, lr: 0.010000000000000002
Epoch [13/30], Training Loss: 39.2786, Validation Loss Current: 9.9178, Validation Loss AVG: 9.9178, lr: 0.010000000000000002
Epoch [14/30], Training Loss: 38.6485, Validation Loss Current: 9.7853, Validation Loss AVG: 9.7853, lr: 0.010000000000000002
Epoch [15/30], Training Loss: 37.0933, Validation Loss Current: 9.5828, Validation Loss AVG: 9.5828, lr: 0.010000000000000002
Epoch [16/30], Training Loss: 37.1253, Validation Loss Current: 9.5999, Validation Loss AVG: 9.5999, lr: 0.010000000000000002
Epoch [17/30], Training Loss: 37.1548, Validation Loss Current: 9.6353, Validation Loss AVG: 9.6353, lr: 0.010000000000000002
Epoch [18/30], Training Loss: 37.6655, Validation Loss Current: 9.3827, Validation Loss AVG: 9.3827, lr: 0.010000000000000002
Epoch [19/30], Training Loss: 35.9656, Validation Loss Current: 9.4542, Validation Loss AVG: 9.4542, lr: 0.010000000000000002
Epoch [20/30], Training Loss: 36.3552, Validation Loss Current: 9.3812, Validation Loss AVG: 9.3812, lr: 0.010000000000000002
Epoch [21/30], Training Loss: 35.1241, Validation Loss Current: 9.5396, Validation Loss AVG: 9.5396, lr: 0.010000000000000002
Epoch [22/30], Training Loss: 35.2217, Validation Loss Current: 9.5936, Validation Loss AVG: 9.5936, lr: 0.010000000000000002
Epoch [23/30], Training Loss: 34.9304, Validation Loss Current: 9.6027, Validation Loss AVG: 9.6027, lr: 0.010000000000000002
Epoch [24/30], Training Loss: 35.8712, Validation Loss Current: 9.5972, Validation Loss AVG: 9.5972, lr: 0.010000000000000002
Epoch [25/30], Training Loss: 35.6505, Validation Loss Current: 9.8760, Validation Loss AVG: 9.8760, lr: 0.010000000000000002
Epoch [26/30], Training Loss: 34.7431, Validation Loss Current: 10.2813, Validation Loss AVG: 10.2813, lr: 0.010000000000000002
Epoch [27/30], Training Loss: 34.2377, Validation Loss Current: 9.5926, Validation Loss AVG: 9.5926, lr: 0.0010000000000000002
Epoch [28/30], Training Loss: 33.2555, Validation Loss Current: 9.6772, Validation Loss AVG: 9.6772, lr: 0.0010000000000000002
Epoch [29/30], Training Loss: 33.1207, Validation Loss Current: 9.6543, Validation Loss AVG: 9.6543, lr: 0.0010000000000000002
Epoch [30/30], Training Loss: 32.3465, Validation Loss Current: 9.7368, Validation Loss AVG: 9.7368, lr: 0.0010000000000000002
Epoch [31/30], Training Loss: 33.3257, Validation Loss Current: 9.8681, Validation Loss AVG: 9.8681, lr: 0.0010000000000000002
Epoch [32/30], Training Loss: 33.3842, Validation Loss Current: 9.7335, Validation Loss AVG: 9.7335, lr: 0.0010000000000000002
Epoch [33/30], Training Loss: 32.1843, Validation Loss Current: 9.6617, Validation Loss AVG: 9.6617, lr: 0.00010000000000000003
Epoch [34/30], Training Loss: 32.4599, Validation Loss Current: 9.6566, Validation Loss AVG: 9.6566, lr: 0.00010000000000000003
Epoch [35/30], Training Loss: 32.7898, Validation Loss Current: 9.6675, Validation Loss AVG: 9.6675, lr: 0.00010000000000000003
Epoch [36/30], Training Loss: 32.0639, Validation Loss Current: 9.6637, Validation Loss AVG: 9.6637, lr: 0.00010000000000000003
Epoch [37/30], Training Loss: 31.9138, Validation Loss Current: 9.7026, Validation Loss AVG: 9.7026, lr: 0.00010000000000000003
Epoch [38/30], Training Loss: 32.3457, Validation Loss Current: 9.6697, Validation Loss AVG: 9.6697, lr: 0.00010000000000000003
Epoch [39/30], Training Loss: 31.4669, Validation Loss Current: 9.6736, Validation Loss AVG: 9.6736, lr: 1.0000000000000004e-05
Epoch [40/30], Training Loss: 32.7449, Validation Loss Current: 9.6669, Validation Loss AVG: 9.6669, lr: 1.0000000000000004e-05
Epoch [41/30], Training Loss: 32.7117, Validation Loss Current: 9.6677, Validation Loss AVG: 9.6677, lr: 1.0000000000000004e-05
Epoch [42/30], Training Loss: 32.3312, Validation Loss Current: 9.6625, Validation Loss AVG: 9.6625, lr: 1.0000000000000004e-05
Epoch [43/30], Training Loss: 32.8327, Validation Loss Current: 9.6773, Validation Loss AVG: 9.6773, lr: 1.0000000000000004e-05
Epoch [44/30], Training Loss: 32.9804, Validation Loss Current: 9.6718, Validation Loss AVG: 9.6718, lr: 1.0000000000000004e-05
Epoch [45/30], Training Loss: 32.5489, Validation Loss Current: 9.6717, Validation Loss AVG: 9.6717, lr: 1.0000000000000004e-06
Epoch [46/30], Training Loss: 31.9174, Validation Loss Current: 9.6726, Validation Loss AVG: 9.6726, lr: 1.0000000000000004e-06
Epoch [47/30], Training Loss: 32.6008, Validation Loss Current: 9.6821, Validation Loss AVG: 9.6821, lr: 1.0000000000000004e-06
Epoch [48/30], Training Loss: 32.7128, Validation Loss Current: 9.6734, Validation Loss AVG: 9.6734, lr: 1.0000000000000004e-06
Epoch [49/30], Training Loss: 32.1915, Validation Loss Current: 9.6717, Validation Loss AVG: 9.6717, lr: 1.0000000000000004e-06
Epoch [50/30], Training Loss: 32.7987, Validation Loss Current: 9.6825, Validation Loss AVG: 9.6825, lr: 1.0000000000000004e-06
 --- Early Stopped ---
Patch distance: 0.2 finished training. Best epoch: 20 Best val accuracy: [0.2302631578947368, 0.2302631578947368, 0.2302631578947368, 0.2302631578947368, 0.2302631578947368, 0.2302631578947368, 0.2302631578947368, 0.2302631578947368, 0.2302631578947368, 0.2302631578947368, 0.2302631578947368, 0.2302631578947368, 0.2302631578947368, 0.2302631578947368, 0.2302631578947368, 0.3134868421052632, 0.29539473684210527, 0.33421052631578946, 0.3276315789473684, 0.3266447368421053, 0.30526315789473685, 0.2914473684210526, 0.24440789473684213, 0.2700657894736842, 0.2546052631578947, 0.25789473684210523, 0.2875, 0.2661184210526316, 0.27302631578947373, 0.26940789473684207, 0.2582236842105263, 0.26710526315789473, 0.2677631578947368, 0.2667763157894737, 0.2667763157894737, 0.2680921052631579, 0.2651315789473684, 0.26809210526315785, 0.2667763157894737, 0.26644736842105265, 0.26644736842105265, 0.2657894736842105, 0.26644736842105265, 0.26644736842105265, 0.26644736842105265, 0.26644736842105265, 0.26644736842105265, 0.26644736842105265, 0.26644736842105265, 0.26644736842105265] Best val loss: 9.381164145469665


Loaded best state dict for [0.2]
Current group: 0.4
Epoch [1/30], Training Loss: 45.3424, Validation Loss Current: 10.0394, Validation Loss AVG: 10.0394, lr: 0.1
Epoch [2/30], Training Loss: 39.6680, Validation Loss Current: 9.9217, Validation Loss AVG: 9.9217, lr: 0.1
Epoch [3/30], Training Loss: 38.4764, Validation Loss Current: 10.1907, Validation Loss AVG: 10.1907, lr: 0.1
Epoch [4/30], Training Loss: 38.4571, Validation Loss Current: 10.3857, Validation Loss AVG: 10.3857, lr: 0.1
Epoch [5/30], Training Loss: 35.7481, Validation Loss Current: 9.7898, Validation Loss AVG: 9.7898, lr: 0.1
Epoch [6/30], Training Loss: 36.6913, Validation Loss Current: 12.2764, Validation Loss AVG: 12.2764, lr: 0.1
Epoch [7/30], Training Loss: 37.8019, Validation Loss Current: 9.3106, Validation Loss AVG: 9.3106, lr: 0.1
Epoch [8/30], Training Loss: 34.5221, Validation Loss Current: 9.6494, Validation Loss AVG: 9.6494, lr: 0.1
Epoch [9/30], Training Loss: 35.1954, Validation Loss Current: 9.3174, Validation Loss AVG: 9.3174, lr: 0.1
Epoch [10/30], Training Loss: 34.1177, Validation Loss Current: 11.3151, Validation Loss AVG: 11.3151, lr: 0.1
Epoch [11/30], Training Loss: 33.7108, Validation Loss Current: 9.3497, Validation Loss AVG: 9.3497, lr: 0.1
Epoch [12/30], Training Loss: 32.7897, Validation Loss Current: 10.0929, Validation Loss AVG: 10.0929, lr: 0.1
Epoch [13/30], Training Loss: 34.6767, Validation Loss Current: 14.5398, Validation Loss AVG: 14.5398, lr: 0.1
Epoch [14/30], Training Loss: 35.6960, Validation Loss Current: 9.1179, Validation Loss AVG: 9.1179, lr: 0.010000000000000002
Epoch [15/30], Training Loss: 28.3427, Validation Loss Current: 8.7829, Validation Loss AVG: 8.7829, lr: 0.010000000000000002
Epoch [16/30], Training Loss: 26.6206, Validation Loss Current: 8.7410, Validation Loss AVG: 8.7410, lr: 0.010000000000000002
Epoch [17/30], Training Loss: 26.4435, Validation Loss Current: 8.6675, Validation Loss AVG: 8.6675, lr: 0.010000000000000002
Epoch [18/30], Training Loss: 25.9483, Validation Loss Current: 8.8594, Validation Loss AVG: 8.8594, lr: 0.010000000000000002
Epoch [19/30], Training Loss: 26.2313, Validation Loss Current: 8.7191, Validation Loss AVG: 8.7191, lr: 0.010000000000000002
Epoch [20/30], Training Loss: 25.5151, Validation Loss Current: 9.4001, Validation Loss AVG: 9.4001, lr: 0.010000000000000002
Epoch [21/30], Training Loss: 23.8402, Validation Loss Current: 9.0369, Validation Loss AVG: 9.0369, lr: 0.010000000000000002
Epoch [22/30], Training Loss: 24.2996, Validation Loss Current: 8.8298, Validation Loss AVG: 8.8298, lr: 0.010000000000000002
Epoch [23/30], Training Loss: 22.8298, Validation Loss Current: 9.1354, Validation Loss AVG: 9.1354, lr: 0.010000000000000002
Epoch [24/30], Training Loss: 22.8505, Validation Loss Current: 8.9991, Validation Loss AVG: 8.9991, lr: 0.0010000000000000002
Epoch [25/30], Training Loss: 20.2290, Validation Loss Current: 9.1519, Validation Loss AVG: 9.1519, lr: 0.0010000000000000002
Epoch [26/30], Training Loss: 20.5957, Validation Loss Current: 9.0723, Validation Loss AVG: 9.0723, lr: 0.0010000000000000002
Epoch [27/30], Training Loss: 20.2147, Validation Loss Current: 9.2174, Validation Loss AVG: 9.2174, lr: 0.0010000000000000002
Epoch [28/30], Training Loss: 20.2026, Validation Loss Current: 9.1741, Validation Loss AVG: 9.1741, lr: 0.0010000000000000002
Epoch [29/30], Training Loss: 19.6402, Validation Loss Current: 9.2816, Validation Loss AVG: 9.2816, lr: 0.0010000000000000002
Epoch [30/30], Training Loss: 19.8497, Validation Loss Current: 9.2551, Validation Loss AVG: 9.2551, lr: 0.00010000000000000003
Epoch [31/30], Training Loss: 20.6936, Validation Loss Current: 9.2544, Validation Loss AVG: 9.2544, lr: 0.00010000000000000003
Epoch [32/30], Training Loss: 19.6376, Validation Loss Current: 9.2852, Validation Loss AVG: 9.2852, lr: 0.00010000000000000003
Epoch [33/30], Training Loss: 20.1030, Validation Loss Current: 9.2815, Validation Loss AVG: 9.2815, lr: 0.00010000000000000003
Epoch [34/30], Training Loss: 21.0385, Validation Loss Current: 9.2936, Validation Loss AVG: 9.2936, lr: 0.00010000000000000003
Epoch [35/30], Training Loss: 19.3631, Validation Loss Current: 9.2692, Validation Loss AVG: 9.2692, lr: 0.00010000000000000003
Epoch [36/30], Training Loss: 19.4545, Validation Loss Current: 9.3452, Validation Loss AVG: 9.3452, lr: 1.0000000000000004e-05
Epoch [37/30], Training Loss: 19.8163, Validation Loss Current: 9.2991, Validation Loss AVG: 9.2991, lr: 1.0000000000000004e-05
Epoch [38/30], Training Loss: 20.0196, Validation Loss Current: 9.3000, Validation Loss AVG: 9.3000, lr: 1.0000000000000004e-05
Epoch [39/30], Training Loss: 19.5314, Validation Loss Current: 9.2693, Validation Loss AVG: 9.2693, lr: 1.0000000000000004e-05
Epoch [40/30], Training Loss: 20.4923, Validation Loss Current: 9.2965, Validation Loss AVG: 9.2965, lr: 1.0000000000000004e-05
Epoch [41/30], Training Loss: 19.1474, Validation Loss Current: 9.2671, Validation Loss AVG: 9.2671, lr: 1.0000000000000004e-05
Epoch [42/30], Training Loss: 19.8014, Validation Loss Current: 9.3022, Validation Loss AVG: 9.3022, lr: 1.0000000000000004e-06
Epoch [43/30], Training Loss: 20.0391, Validation Loss Current: 9.2927, Validation Loss AVG: 9.2927, lr: 1.0000000000000004e-06
Epoch [44/30], Training Loss: 20.4386, Validation Loss Current: 9.2902, Validation Loss AVG: 9.2902, lr: 1.0000000000000004e-06
Epoch [45/30], Training Loss: 20.5650, Validation Loss Current: 9.2953, Validation Loss AVG: 9.2953, lr: 1.0000000000000004e-06
Epoch [46/30], Training Loss: 21.0064, Validation Loss Current: 9.3004, Validation Loss AVG: 9.3004, lr: 1.0000000000000004e-06
Epoch [47/30], Training Loss: 19.6879, Validation Loss Current: 9.2831, Validation Loss AVG: 9.2831, lr: 1.0000000000000004e-06
 --- Early Stopped ---
Patch distance: 0.4 finished training. Best epoch: 17 Best val accuracy: [0.2605263157894737, 0.2302631578947368, 0.1667763157894737, 0.23815789473684212, 0.305921052631579, 0.16644736842105262, 0.3174342105263158, 0.28157894736842104, 0.3098684210526316, 0.17532894736842103, 0.33453947368421055, 0.30164473684210524, 0.18355263157894736, 0.3503289473684211, 0.36743421052631575, 0.37532894736842104, 0.37335526315789475, 0.36644736842105263, 0.3759868421052631, 0.36282894736842103, 0.37335526315789475, 0.3694078947368421, 0.3717105263157895, 0.38125, 0.37434210526315786, 0.3796052631578947, 0.37105263157894736, 0.37664473684210525, 0.3697368421052632, 0.3703947368421052, 0.3694078947368421, 0.37072368421052626, 0.37072368421052626, 0.37138157894736834, 0.3697368421052631, 0.3697368421052631, 0.3697368421052631, 0.3694078947368421, 0.36907894736842106, 0.36907894736842106, 0.3694078947368421, 0.3694078947368421, 0.3694078947368421, 0.3694078947368421, 0.3694078947368421, 0.3694078947368421, 0.3694078947368421] Best val loss: 8.667542433738708


Loaded best state dict for [0.2, 0.4]
Current group: 0.6
Epoch [1/30], Training Loss: 30.9874, Validation Loss Current: 8.7429, Validation Loss AVG: 8.7429, lr: 0.1
Epoch [2/30], Training Loss: 31.2460, Validation Loss Current: 10.9927, Validation Loss AVG: 10.9927, lr: 0.1
Epoch [3/30], Training Loss: 31.7101, Validation Loss Current: 10.1190, Validation Loss AVG: 10.1190, lr: 0.1
Epoch [4/30], Training Loss: 31.8979, Validation Loss Current: 9.0921, Validation Loss AVG: 9.0921, lr: 0.1
Epoch [5/30], Training Loss: 29.4208, Validation Loss Current: 9.4630, Validation Loss AVG: 9.4630, lr: 0.1
Epoch [6/30], Training Loss: 30.1038, Validation Loss Current: 12.2538, Validation Loss AVG: 12.2538, lr: 0.1
Epoch [7/30], Training Loss: 32.2162, Validation Loss Current: 8.8667, Validation Loss AVG: 8.8667, lr: 0.1
Epoch [8/30], Training Loss: 27.6650, Validation Loss Current: 8.5005, Validation Loss AVG: 8.5005, lr: 0.010000000000000002
Epoch [9/30], Training Loss: 25.5312, Validation Loss Current: 8.2068, Validation Loss AVG: 8.2068, lr: 0.010000000000000002
Epoch [10/30], Training Loss: 23.1156, Validation Loss Current: 8.6236, Validation Loss AVG: 8.6236, lr: 0.010000000000000002
Epoch [11/30], Training Loss: 23.2599, Validation Loss Current: 8.0633, Validation Loss AVG: 8.0633, lr: 0.010000000000000002
Epoch [12/30], Training Loss: 21.6749, Validation Loss Current: 8.3389, Validation Loss AVG: 8.3389, lr: 0.010000000000000002
Epoch [13/30], Training Loss: 21.4042, Validation Loss Current: 8.4804, Validation Loss AVG: 8.4804, lr: 0.010000000000000002
Epoch [14/30], Training Loss: 20.7914, Validation Loss Current: 8.6605, Validation Loss AVG: 8.6605, lr: 0.010000000000000002
Epoch [15/30], Training Loss: 19.7876, Validation Loss Current: 8.4414, Validation Loss AVG: 8.4414, lr: 0.010000000000000002
Epoch [16/30], Training Loss: 19.3829, Validation Loss Current: 8.8154, Validation Loss AVG: 8.8154, lr: 0.010000000000000002
Epoch [17/30], Training Loss: 19.0513, Validation Loss Current: 9.0613, Validation Loss AVG: 9.0613, lr: 0.010000000000000002
Epoch [18/30], Training Loss: 17.6284, Validation Loss Current: 9.2242, Validation Loss AVG: 9.2242, lr: 0.0010000000000000002
Epoch [19/30], Training Loss: 16.9085, Validation Loss Current: 9.3223, Validation Loss AVG: 9.3223, lr: 0.0010000000000000002
Epoch [20/30], Training Loss: 16.8801, Validation Loss Current: 9.2884, Validation Loss AVG: 9.2884, lr: 0.0010000000000000002
Epoch [21/30], Training Loss: 16.9784, Validation Loss Current: 9.4870, Validation Loss AVG: 9.4870, lr: 0.0010000000000000002
Epoch [22/30], Training Loss: 16.2506, Validation Loss Current: 9.5289, Validation Loss AVG: 9.5289, lr: 0.0010000000000000002
Epoch [23/30], Training Loss: 16.3590, Validation Loss Current: 9.4484, Validation Loss AVG: 9.4484, lr: 0.0010000000000000002
Epoch [24/30], Training Loss: 15.6229, Validation Loss Current: 9.5091, Validation Loss AVG: 9.5091, lr: 0.00010000000000000003
Epoch [25/30], Training Loss: 16.6056, Validation Loss Current: 9.4965, Validation Loss AVG: 9.4965, lr: 0.00010000000000000003
Epoch [26/30], Training Loss: 16.2050, Validation Loss Current: 9.5497, Validation Loss AVG: 9.5497, lr: 0.00010000000000000003
Epoch [27/30], Training Loss: 15.8168, Validation Loss Current: 9.5211, Validation Loss AVG: 9.5211, lr: 0.00010000000000000003
Epoch [28/30], Training Loss: 16.2678, Validation Loss Current: 9.5545, Validation Loss AVG: 9.5545, lr: 0.00010000000000000003
Epoch [29/30], Training Loss: 15.4790, Validation Loss Current: 9.5282, Validation Loss AVG: 9.5282, lr: 0.00010000000000000003
Epoch [30/30], Training Loss: 16.3929, Validation Loss Current: 9.5324, Validation Loss AVG: 9.5324, lr: 1.0000000000000004e-05
Epoch [31/30], Training Loss: 15.5395, Validation Loss Current: 9.5511, Validation Loss AVG: 9.5511, lr: 1.0000000000000004e-05
Epoch [32/30], Training Loss: 17.5462, Validation Loss Current: 9.5913, Validation Loss AVG: 9.5913, lr: 1.0000000000000004e-05
Epoch [33/30], Training Loss: 16.2353, Validation Loss Current: 9.5459, Validation Loss AVG: 9.5459, lr: 1.0000000000000004e-05
Epoch [34/30], Training Loss: 15.3938, Validation Loss Current: 9.5698, Validation Loss AVG: 9.5698, lr: 1.0000000000000004e-05
Epoch [35/30], Training Loss: 15.4194, Validation Loss Current: 9.5531, Validation Loss AVG: 9.5531, lr: 1.0000000000000004e-05
Epoch [36/30], Training Loss: 16.1390, Validation Loss Current: 9.5737, Validation Loss AVG: 9.5737, lr: 1.0000000000000004e-06
Epoch [37/30], Training Loss: 15.9291, Validation Loss Current: 9.5920, Validation Loss AVG: 9.5920, lr: 1.0000000000000004e-06
Epoch [38/30], Training Loss: 16.2772, Validation Loss Current: 9.5602, Validation Loss AVG: 9.5602, lr: 1.0000000000000004e-06
Epoch [39/30], Training Loss: 15.9299, Validation Loss Current: 9.5929, Validation Loss AVG: 9.5929, lr: 1.0000000000000004e-06
Epoch [40/30], Training Loss: 15.5124, Validation Loss Current: 9.5883, Validation Loss AVG: 9.5883, lr: 1.0000000000000004e-06
Epoch [41/30], Training Loss: 15.5356, Validation Loss Current: 9.5780, Validation Loss AVG: 9.5780, lr: 1.0000000000000004e-06
 --- Early Stopped ---
Patch distance: 0.6 finished training. Best epoch: 11 Best val accuracy: [0.3513157894736842, 0.2934210526315789, 0.2736842105263158, 0.3940789473684211, 0.36809210526315794, 0.2700657894736842, 0.35723684210526313, 0.3996710526315789, 0.4151315789473684, 0.4098684210526316, 0.42796052631578946, 0.42269736842105254, 0.42467105263157895, 0.4184210526315789, 0.4184210526315789, 0.42467105263157895, 0.43355263157894736, 0.42105263157894746, 0.42730263157894743, 0.425, 0.4292763157894736, 0.4240131578947368, 0.4286184210526316, 0.4266447368421053, 0.4253289473684211, 0.425, 0.4240131578947368, 0.4230263157894737, 0.42467105263157895, 0.42467105263157895, 0.42434210526315785, 0.4240131578947368, 0.4240131578947368, 0.42434210526315785, 0.4240131578947368, 0.42434210526315785, 0.42434210526315785, 0.42434210526315785, 0.42434210526315785, 0.4240131578947368, 0.4240131578947368] Best val loss: 8.063329792022705


Loaded best state dict for [0.2, 0.4, 0.6]
Current group: 0.8
Epoch [1/30], Training Loss: 29.2031, Validation Loss Current: 12.7192, Validation Loss AVG: 12.7192, lr: 0.1
Epoch [2/30], Training Loss: 30.3342, Validation Loss Current: 10.8946, Validation Loss AVG: 10.8946, lr: 0.1
Epoch [3/30], Training Loss: 30.4813, Validation Loss Current: 12.0407, Validation Loss AVG: 12.0407, lr: 0.1
Epoch [4/30], Training Loss: 29.2850, Validation Loss Current: 8.9211, Validation Loss AVG: 8.9211, lr: 0.1
Epoch [5/30], Training Loss: 30.6529, Validation Loss Current: 9.4344, Validation Loss AVG: 9.4344, lr: 0.1
Epoch [6/30], Training Loss: 31.1613, Validation Loss Current: 9.6462, Validation Loss AVG: 9.6462, lr: 0.1
Epoch [7/30], Training Loss: 30.9951, Validation Loss Current: 9.7631, Validation Loss AVG: 9.7631, lr: 0.1
Epoch [8/30], Training Loss: 29.0484, Validation Loss Current: 9.5678, Validation Loss AVG: 9.5678, lr: 0.1
Epoch [9/30], Training Loss: 28.4580, Validation Loss Current: 9.1887, Validation Loss AVG: 9.1887, lr: 0.1
Epoch [10/30], Training Loss: 28.6270, Validation Loss Current: 9.3610, Validation Loss AVG: 9.3610, lr: 0.1
Epoch [11/30], Training Loss: 25.5367, Validation Loss Current: 8.5873, Validation Loss AVG: 8.5873, lr: 0.010000000000000002
Epoch [12/30], Training Loss: 23.2704, Validation Loss Current: 8.4195, Validation Loss AVG: 8.4195, lr: 0.010000000000000002
Epoch [13/30], Training Loss: 20.6743, Validation Loss Current: 8.8062, Validation Loss AVG: 8.8062, lr: 0.010000000000000002
Epoch [14/30], Training Loss: 21.7108, Validation Loss Current: 8.2549, Validation Loss AVG: 8.2549, lr: 0.010000000000000002
Epoch [15/30], Training Loss: 19.0197, Validation Loss Current: 9.3375, Validation Loss AVG: 9.3375, lr: 0.010000000000000002
Epoch [16/30], Training Loss: 19.0356, Validation Loss Current: 9.0842, Validation Loss AVG: 9.0842, lr: 0.010000000000000002
Epoch [17/30], Training Loss: 16.5794, Validation Loss Current: 9.4810, Validation Loss AVG: 9.4810, lr: 0.010000000000000002
Epoch [18/30], Training Loss: 15.5821, Validation Loss Current: 9.5459, Validation Loss AVG: 9.5459, lr: 0.010000000000000002
Epoch [19/30], Training Loss: 14.8636, Validation Loss Current: 9.6849, Validation Loss AVG: 9.6849, lr: 0.010000000000000002
Epoch [20/30], Training Loss: 14.5889, Validation Loss Current: 10.0996, Validation Loss AVG: 10.0996, lr: 0.010000000000000002
Epoch [21/30], Training Loss: 12.7860, Validation Loss Current: 10.8116, Validation Loss AVG: 10.8116, lr: 0.0010000000000000002
Epoch [22/30], Training Loss: 12.5905, Validation Loss Current: 10.8369, Validation Loss AVG: 10.8369, lr: 0.0010000000000000002
Epoch [23/30], Training Loss: 12.7642, Validation Loss Current: 10.9446, Validation Loss AVG: 10.9446, lr: 0.0010000000000000002
Epoch [24/30], Training Loss: 11.7797, Validation Loss Current: 10.9995, Validation Loss AVG: 10.9995, lr: 0.0010000000000000002
Epoch [25/30], Training Loss: 11.4077, Validation Loss Current: 11.1142, Validation Loss AVG: 11.1142, lr: 0.0010000000000000002
Epoch [26/30], Training Loss: 12.9702, Validation Loss Current: 11.4515, Validation Loss AVG: 11.4515, lr: 0.0010000000000000002
Epoch [27/30], Training Loss: 11.5182, Validation Loss Current: 11.4883, Validation Loss AVG: 11.4883, lr: 0.00010000000000000003
Epoch [28/30], Training Loss: 13.1118, Validation Loss Current: 11.3892, Validation Loss AVG: 11.3892, lr: 0.00010000000000000003
Epoch [29/30], Training Loss: 11.5058, Validation Loss Current: 11.4486, Validation Loss AVG: 11.4486, lr: 0.00010000000000000003
Epoch [30/30], Training Loss: 11.3172, Validation Loss Current: 11.4629, Validation Loss AVG: 11.4629, lr: 0.00010000000000000003
Epoch [31/30], Training Loss: 11.4567, Validation Loss Current: 11.4547, Validation Loss AVG: 11.4547, lr: 0.00010000000000000003
Epoch [32/30], Training Loss: 12.7206, Validation Loss Current: 11.4381, Validation Loss AVG: 11.4381, lr: 0.00010000000000000003
Epoch [33/30], Training Loss: 11.4847, Validation Loss Current: 11.4322, Validation Loss AVG: 11.4322, lr: 1.0000000000000004e-05
Epoch [34/30], Training Loss: 11.4492, Validation Loss Current: 11.4600, Validation Loss AVG: 11.4600, lr: 1.0000000000000004e-05
Epoch [35/30], Training Loss: 11.0174, Validation Loss Current: 11.4325, Validation Loss AVG: 11.4325, lr: 1.0000000000000004e-05
Epoch [36/30], Training Loss: 11.5613, Validation Loss Current: 11.4264, Validation Loss AVG: 11.4264, lr: 1.0000000000000004e-05
Epoch [37/30], Training Loss: 11.2770, Validation Loss Current: 11.4786, Validation Loss AVG: 11.4786, lr: 1.0000000000000004e-05
Epoch [38/30], Training Loss: 11.7234, Validation Loss Current: 11.4579, Validation Loss AVG: 11.4579, lr: 1.0000000000000004e-05
Epoch [39/30], Training Loss: 12.3556, Validation Loss Current: 11.4116, Validation Loss AVG: 11.4116, lr: 1.0000000000000004e-06
Epoch [40/30], Training Loss: 11.0537, Validation Loss Current: 11.4515, Validation Loss AVG: 11.4515, lr: 1.0000000000000004e-06
Epoch [41/30], Training Loss: 12.1500, Validation Loss Current: 11.4423, Validation Loss AVG: 11.4423, lr: 1.0000000000000004e-06
Epoch [42/30], Training Loss: 11.1729, Validation Loss Current: 11.4589, Validation Loss AVG: 11.4589, lr: 1.0000000000000004e-06
Epoch [43/30], Training Loss: 11.2553, Validation Loss Current: 11.4174, Validation Loss AVG: 11.4174, lr: 1.0000000000000004e-06
Epoch [44/30], Training Loss: 11.0618, Validation Loss Current: 11.4372, Validation Loss AVG: 11.4372, lr: 1.0000000000000004e-06
 --- Early Stopped ---
Patch distance: 0.8 finished training. Best epoch: 14 Best val accuracy: [0.3036184210526316, 0.30953947368421053, 0.31842105263157894, 0.3674342105263158, 0.3726973684210526, 0.32203947368421054, 0.3536184210526316, 0.3796052631578948, 0.3217105263157895, 0.3282894736842105, 0.40625, 0.4217105263157895, 0.41217105263157894, 0.43190789473684205, 0.4052631578947368, 0.40394736842105267, 0.4052631578947368, 0.41578947368421054, 0.4141447368421053, 0.40592105263157896, 0.3957236842105263, 0.3996710526315789, 0.3963815789473684, 0.4009868421052632, 0.40164473684210533, 0.39473684210526316, 0.3960526315789473, 0.3960526315789473, 0.39703947368421055, 0.3960526315789473, 0.39539473684210524, 0.3963815789473684, 0.3963815789473684, 0.3963815789473684, 0.3957236842105263, 0.3957236842105263, 0.3960526315789473, 0.3963815789473684, 0.3963815789473684, 0.3960526315789473, 0.3963815789473684, 0.3963815789473684, 0.3960526315789473, 0.3960526315789473] Best val loss: 8.254936933517456


Loaded best state dict for [0.2, 0.4, 0.6, 0.8]
Current group: 1
Epoch [1/30], Training Loss: 27.1347, Validation Loss Current: 9.4759, Validation Loss AVG: 11.9044, lr: 0.1
Epoch [2/30], Training Loss: 27.9315, Validation Loss Current: 7.5724, Validation Loss AVG: 8.9804, lr: 0.1
Epoch [3/30], Training Loss: 26.1885, Validation Loss Current: 8.9370, Validation Loss AVG: 12.1763, lr: 0.1
Epoch [4/30], Training Loss: 28.7904, Validation Loss Current: 8.2848, Validation Loss AVG: 9.2871, lr: 0.1
Epoch [5/30], Training Loss: 26.8905, Validation Loss Current: 8.7486, Validation Loss AVG: 11.8307, lr: 0.1
Epoch [6/30], Training Loss: 27.3008, Validation Loss Current: 7.5141, Validation Loss AVG: 9.3382, lr: 0.1
Epoch [7/30], Training Loss: 26.4797, Validation Loss Current: 7.6012, Validation Loss AVG: 9.0540, lr: 0.1
Epoch [8/30], Training Loss: 29.5709, Validation Loss Current: 7.6359, Validation Loss AVG: 9.5974, lr: 0.1
Epoch [9/30], Training Loss: 27.3770, Validation Loss Current: 7.6531, Validation Loss AVG: 9.3028, lr: 0.1
Epoch [10/30], Training Loss: 27.4135, Validation Loss Current: 8.1445, Validation Loss AVG: 9.9427, lr: 0.1
Epoch [11/30], Training Loss: 28.6292, Validation Loss Current: 7.5281, Validation Loss AVG: 9.5876, lr: 0.1
Epoch [12/30], Training Loss: 24.4912, Validation Loss Current: 8.2189, Validation Loss AVG: 10.0203, lr: 0.1
Epoch [13/30], Training Loss: 22.9361, Validation Loss Current: 6.5121, Validation Loss AVG: 8.4537, lr: 0.010000000000000002
Epoch [14/30], Training Loss: 19.7242, Validation Loss Current: 6.5750, Validation Loss AVG: 9.4935, lr: 0.010000000000000002
Epoch [15/30], Training Loss: 16.8814, Validation Loss Current: 6.5360, Validation Loss AVG: 9.4528, lr: 0.010000000000000002
Epoch [16/30], Training Loss: 15.7818, Validation Loss Current: 6.4620, Validation Loss AVG: 9.9856, lr: 0.010000000000000002
Epoch [17/30], Training Loss: 14.6667, Validation Loss Current: 6.8182, Validation Loss AVG: 10.8863, lr: 0.010000000000000002
Epoch [18/30], Training Loss: 13.4288, Validation Loss Current: 6.9770, Validation Loss AVG: 10.7825, lr: 0.010000000000000002
Epoch [19/30], Training Loss: 14.1162, Validation Loss Current: 7.0241, Validation Loss AVG: 10.6773, lr: 0.010000000000000002
Epoch [20/30], Training Loss: 13.1288, Validation Loss Current: 7.2026, Validation Loss AVG: 12.0885, lr: 0.010000000000000002
Epoch [21/30], Training Loss: 11.8911, Validation Loss Current: 7.3678, Validation Loss AVG: 11.7731, lr: 0.010000000000000002
Epoch [22/30], Training Loss: 10.0008, Validation Loss Current: 7.7485, Validation Loss AVG: 13.7565, lr: 0.010000000000000002
Epoch [23/30], Training Loss: 9.1102, Validation Loss Current: 7.7004, Validation Loss AVG: 13.1469, lr: 0.0010000000000000002
Epoch [24/30], Training Loss: 8.9454, Validation Loss Current: 7.8331, Validation Loss AVG: 13.0778, lr: 0.0010000000000000002
Epoch [25/30], Training Loss: 9.6092, Validation Loss Current: 7.9164, Validation Loss AVG: 13.3803, lr: 0.0010000000000000002
Epoch [26/30], Training Loss: 8.7074, Validation Loss Current: 7.9391, Validation Loss AVG: 13.5933, lr: 0.0010000000000000002
Epoch [27/30], Training Loss: 9.8514, Validation Loss Current: 8.0250, Validation Loss AVG: 13.5682, lr: 0.0010000000000000002
Epoch [28/30], Training Loss: 7.8923, Validation Loss Current: 8.0685, Validation Loss AVG: 13.8854, lr: 0.0010000000000000002
Epoch [29/30], Training Loss: 8.7534, Validation Loss Current: 7.9989, Validation Loss AVG: 13.9256, lr: 0.00010000000000000003
Epoch [30/30], Training Loss: 7.8492, Validation Loss Current: 8.1529, Validation Loss AVG: 14.0825, lr: 0.00010000000000000003
Epoch [31/30], Training Loss: 7.8927, Validation Loss Current: 8.0974, Validation Loss AVG: 13.9840, lr: 0.00010000000000000003
Epoch [32/30], Training Loss: 7.7670, Validation Loss Current: 8.1061, Validation Loss AVG: 14.0448, lr: 0.00010000000000000003
Epoch [33/30], Training Loss: 7.9759, Validation Loss Current: 8.1313, Validation Loss AVG: 14.1102, lr: 0.00010000000000000003
Epoch [34/30], Training Loss: 7.7257, Validation Loss Current: 8.1027, Validation Loss AVG: 14.0720, lr: 0.00010000000000000003
Epoch [35/30], Training Loss: 7.6914, Validation Loss Current: 8.1137, Validation Loss AVG: 14.1368, lr: 1.0000000000000004e-05
Epoch [36/30], Training Loss: 8.3497, Validation Loss Current: 8.0753, Validation Loss AVG: 14.1067, lr: 1.0000000000000004e-05
Epoch [37/30], Training Loss: 8.5519, Validation Loss Current: 8.1150, Validation Loss AVG: 14.1398, lr: 1.0000000000000004e-05
Epoch [38/30], Training Loss: 9.2699, Validation Loss Current: 8.1662, Validation Loss AVG: 14.0999, lr: 1.0000000000000004e-05
Epoch [39/30], Training Loss: 8.1314, Validation Loss Current: 8.0964, Validation Loss AVG: 14.0717, lr: 1.0000000000000004e-05
Epoch [40/30], Training Loss: 8.1360, Validation Loss Current: 8.2062, Validation Loss AVG: 14.0990, lr: 1.0000000000000004e-05
Epoch [41/30], Training Loss: 9.1939, Validation Loss Current: 8.1164, Validation Loss AVG: 14.0493, lr: 1.0000000000000004e-06
Epoch [42/30], Training Loss: 9.1522, Validation Loss Current: 8.1510, Validation Loss AVG: 14.1452, lr: 1.0000000000000004e-06
Epoch [43/30], Training Loss: 9.2965, Validation Loss Current: 8.1986, Validation Loss AVG: 14.1171, lr: 1.0000000000000004e-06
Epoch [44/30], Training Loss: 7.5364, Validation Loss Current: 8.1365, Validation Loss AVG: 14.0770, lr: 1.0000000000000004e-06
Epoch [45/30], Training Loss: 7.7558, Validation Loss Current: 7.9747, Validation Loss AVG: 13.9938, lr: 1.0000000000000004e-06
Epoch [46/30], Training Loss: 7.5658, Validation Loss Current: 8.2905, Validation Loss AVG: 14.1415, lr: 1.0000000000000004e-06
 --- Early Stopped ---
Patch distance: 1 finished training. Best epoch: 16 Best val accuracy: [0.3782894736842105, 0.4621710526315789, 0.3963815789473684, 0.40460526315789475, 0.42269736842105265, 0.48519736842105265, 0.4621710526315789, 0.46710526315789475, 0.4506578947368421, 0.4440789473684211, 0.4654605263157895, 0.5, 0.5509868421052632, 0.5592105263157895, 0.5476973684210527, 0.5707236842105263, 0.5657894736842105, 0.5575657894736842, 0.569078947368421, 0.5592105263157895, 0.5740131578947368, 0.5773026315789473, 0.5756578947368421, 0.5855263157894737, 0.5888157894736842, 0.5921052631578947, 0.587171052631579, 0.5904605263157895, 0.5921052631578947, 0.5904605263157895, 0.5904605263157895, 0.5904605263157895, 0.5888157894736842, 0.5904605263157895, 0.5888157894736842, 0.5888157894736842, 0.5888157894736842, 0.5888157894736842, 0.5888157894736842, 0.5888157894736842, 0.5888157894736842, 0.5888157894736842, 0.5888157894736842, 0.5888157894736842, 0.5888157894736842, 0.5888157894736842] Best val loss: 6.462017893791199


----- Training alexnet with sequence: [0.4, 0.6, 0.8, 1] -----
Current group: 0.4
Epoch [1/38], Training Loss: 40.4453, Validation Loss Current: 10.1213, Validation Loss AVG: 10.1213, lr: 0.1
Epoch [2/38], Training Loss: 40.9052, Validation Loss Current: 10.1095, Validation Loss AVG: 10.1095, lr: 0.1
Epoch [3/38], Training Loss: 40.3743, Validation Loss Current: 10.0826, Validation Loss AVG: 10.0826, lr: 0.1
Epoch [4/38], Training Loss: 39.9032, Validation Loss Current: 10.1016, Validation Loss AVG: 10.1016, lr: 0.1
Epoch [5/38], Training Loss: 40.1786, Validation Loss Current: 9.8358, Validation Loss AVG: 9.8358, lr: 0.1
Epoch [6/38], Training Loss: 38.5963, Validation Loss Current: 9.7835, Validation Loss AVG: 9.7835, lr: 0.1
Epoch [7/38], Training Loss: 37.1662, Validation Loss Current: 10.7635, Validation Loss AVG: 10.7635, lr: 0.1
Epoch [8/38], Training Loss: 36.5750, Validation Loss Current: 9.9690, Validation Loss AVG: 9.9690, lr: 0.1
Epoch [9/38], Training Loss: 38.7047, Validation Loss Current: 9.3434, Validation Loss AVG: 9.3434, lr: 0.1
Epoch [10/38], Training Loss: 37.0210, Validation Loss Current: 9.4704, Validation Loss AVG: 9.4704, lr: 0.1
Epoch [11/38], Training Loss: 35.9271, Validation Loss Current: 9.3450, Validation Loss AVG: 9.3450, lr: 0.1
Epoch [12/38], Training Loss: 35.9264, Validation Loss Current: 9.7298, Validation Loss AVG: 9.7298, lr: 0.1
Epoch [13/38], Training Loss: 35.5153, Validation Loss Current: 9.2169, Validation Loss AVG: 9.2169, lr: 0.1
Epoch [14/38], Training Loss: 34.3687, Validation Loss Current: 9.4205, Validation Loss AVG: 9.4205, lr: 0.1
Epoch [15/38], Training Loss: 33.4778, Validation Loss Current: 10.0890, Validation Loss AVG: 10.0890, lr: 0.1
Epoch [16/38], Training Loss: 34.1212, Validation Loss Current: 9.5150, Validation Loss AVG: 9.5150, lr: 0.1
Epoch [17/38], Training Loss: 33.9898, Validation Loss Current: 9.1290, Validation Loss AVG: 9.1290, lr: 0.1
Epoch [18/38], Training Loss: 33.2879, Validation Loss Current: 9.6040, Validation Loss AVG: 9.6040, lr: 0.1
Epoch [19/38], Training Loss: 34.7026, Validation Loss Current: 11.2529, Validation Loss AVG: 11.2529, lr: 0.1
Epoch [20/38], Training Loss: 34.8477, Validation Loss Current: 10.2982, Validation Loss AVG: 10.2982, lr: 0.1
Epoch [21/38], Training Loss: 36.7797, Validation Loss Current: 9.2030, Validation Loss AVG: 9.2030, lr: 0.1
Epoch [22/38], Training Loss: 34.8680, Validation Loss Current: 10.1629, Validation Loss AVG: 10.1629, lr: 0.1
Epoch [23/38], Training Loss: 35.8818, Validation Loss Current: 9.6992, Validation Loss AVG: 9.6992, lr: 0.1
Epoch [24/38], Training Loss: 32.3349, Validation Loss Current: 8.9426, Validation Loss AVG: 8.9426, lr: 0.010000000000000002
Epoch [25/38], Training Loss: 30.4904, Validation Loss Current: 8.7382, Validation Loss AVG: 8.7382, lr: 0.010000000000000002
Epoch [26/38], Training Loss: 29.2055, Validation Loss Current: 8.6858, Validation Loss AVG: 8.6858, lr: 0.010000000000000002
Epoch [27/38], Training Loss: 29.1424, Validation Loss Current: 8.6206, Validation Loss AVG: 8.6206, lr: 0.010000000000000002
Epoch [28/38], Training Loss: 27.8800, Validation Loss Current: 8.6381, Validation Loss AVG: 8.6381, lr: 0.010000000000000002
Epoch [29/38], Training Loss: 27.4579, Validation Loss Current: 8.5831, Validation Loss AVG: 8.5831, lr: 0.010000000000000002
Epoch [30/38], Training Loss: 27.1212, Validation Loss Current: 8.7688, Validation Loss AVG: 8.7688, lr: 0.010000000000000002
Epoch [31/38], Training Loss: 26.1522, Validation Loss Current: 8.6924, Validation Loss AVG: 8.6924, lr: 0.010000000000000002
Epoch [32/38], Training Loss: 27.3689, Validation Loss Current: 8.4274, Validation Loss AVG: 8.4274, lr: 0.010000000000000002
Epoch [33/38], Training Loss: 25.3890, Validation Loss Current: 8.6812, Validation Loss AVG: 8.6812, lr: 0.010000000000000002
Epoch [34/38], Training Loss: 24.9497, Validation Loss Current: 8.9107, Validation Loss AVG: 8.9107, lr: 0.010000000000000002
Epoch [35/38], Training Loss: 25.9724, Validation Loss Current: 8.6857, Validation Loss AVG: 8.6857, lr: 0.010000000000000002
Epoch [36/38], Training Loss: 24.2667, Validation Loss Current: 9.1804, Validation Loss AVG: 9.1804, lr: 0.010000000000000002
Epoch [37/38], Training Loss: 25.0580, Validation Loss Current: 8.8176, Validation Loss AVG: 8.8176, lr: 0.010000000000000002
Epoch [38/38], Training Loss: 25.4985, Validation Loss Current: 8.7766, Validation Loss AVG: 8.7766, lr: 0.010000000000000002
Epoch [39/38], Training Loss: 22.4585, Validation Loss Current: 8.7311, Validation Loss AVG: 8.7311, lr: 0.0010000000000000002
Epoch [40/38], Training Loss: 22.0637, Validation Loss Current: 8.8816, Validation Loss AVG: 8.8816, lr: 0.0010000000000000002
Epoch [41/38], Training Loss: 22.3520, Validation Loss Current: 8.8889, Validation Loss AVG: 8.8889, lr: 0.0010000000000000002
Epoch [42/38], Training Loss: 21.9170, Validation Loss Current: 8.9809, Validation Loss AVG: 8.9809, lr: 0.0010000000000000002
Epoch [43/38], Training Loss: 21.0947, Validation Loss Current: 9.0088, Validation Loss AVG: 9.0088, lr: 0.0010000000000000002
Epoch [44/38], Training Loss: 21.2494, Validation Loss Current: 8.9828, Validation Loss AVG: 8.9828, lr: 0.0010000000000000002
Epoch [45/38], Training Loss: 20.5877, Validation Loss Current: 8.9591, Validation Loss AVG: 8.9591, lr: 0.00010000000000000003
Epoch [46/38], Training Loss: 21.0931, Validation Loss Current: 8.9791, Validation Loss AVG: 8.9791, lr: 0.00010000000000000003
Epoch [47/38], Training Loss: 21.7418, Validation Loss Current: 8.9952, Validation Loss AVG: 8.9952, lr: 0.00010000000000000003
Epoch [48/38], Training Loss: 20.9194, Validation Loss Current: 8.9725, Validation Loss AVG: 8.9725, lr: 0.00010000000000000003
Epoch [49/38], Training Loss: 21.5015, Validation Loss Current: 9.0207, Validation Loss AVG: 9.0207, lr: 0.00010000000000000003
Epoch [50/38], Training Loss: 21.5435, Validation Loss Current: 8.9899, Validation Loss AVG: 8.9899, lr: 0.00010000000000000003
Epoch [51/38], Training Loss: 21.6367, Validation Loss Current: 9.0241, Validation Loss AVG: 9.0241, lr: 1.0000000000000004e-05
Epoch [52/38], Training Loss: 21.2885, Validation Loss Current: 8.9951, Validation Loss AVG: 8.9951, lr: 1.0000000000000004e-05
Epoch [53/38], Training Loss: 22.4765, Validation Loss Current: 9.0097, Validation Loss AVG: 9.0097, lr: 1.0000000000000004e-05
Epoch [54/38], Training Loss: 20.7191, Validation Loss Current: 9.0196, Validation Loss AVG: 9.0196, lr: 1.0000000000000004e-05
Epoch [55/38], Training Loss: 21.1576, Validation Loss Current: 8.9790, Validation Loss AVG: 8.9790, lr: 1.0000000000000004e-05
Epoch [56/38], Training Loss: 20.8201, Validation Loss Current: 8.9712, Validation Loss AVG: 8.9712, lr: 1.0000000000000004e-05
Epoch [57/38], Training Loss: 20.8186, Validation Loss Current: 9.0140, Validation Loss AVG: 9.0140, lr: 1.0000000000000004e-06
Epoch [58/38], Training Loss: 21.1037, Validation Loss Current: 9.0140, Validation Loss AVG: 9.0140, lr: 1.0000000000000004e-06
Epoch [59/38], Training Loss: 21.9488, Validation Loss Current: 9.0335, Validation Loss AVG: 9.0335, lr: 1.0000000000000004e-06
Epoch [60/38], Training Loss: 20.9888, Validation Loss Current: 8.9764, Validation Loss AVG: 8.9764, lr: 1.0000000000000004e-06
Epoch [61/38], Training Loss: 20.7614, Validation Loss Current: 9.0131, Validation Loss AVG: 9.0131, lr: 1.0000000000000004e-06
Epoch [62/38], Training Loss: 22.8159, Validation Loss Current: 9.0396, Validation Loss AVG: 9.0396, lr: 1.0000000000000004e-06
 --- Early Stopped ---
Patch distance: 0.4 finished training. Best epoch: 32 Best val accuracy: [0.2302631578947368, 0.2302631578947368, 0.2302631578947368, 0.2302631578947368, 0.249671052631579, 0.2828947368421052, 0.29375000000000007, 0.3131578947368421, 0.3263157894736842, 0.2953947368421053, 0.3394736842105263, 0.21875, 0.33059210526315785, 0.27598684210526314, 0.2414473684210526, 0.3075657894736842, 0.30526315789473685, 0.28421052631578947, 0.2825657894736842, 0.24013157894736842, 0.3282894736842105, 0.27960526315789475, 0.2611842105263158, 0.34111842105263157, 0.3582236842105263, 0.3651315789473684, 0.3625, 0.3651315789473684, 0.38585526315789476, 0.3648026315789474, 0.3684210526315789, 0.38651315789473684, 0.3759868421052632, 0.35197368421052627, 0.3782894736842105, 0.37763157894736843, 0.36546052631578946, 0.3667763157894737, 0.37467105263157896, 0.375, 0.37565789473684214, 0.37269736842105256, 0.37434210526315786, 0.37072368421052626, 0.3694078947368421, 0.36973684210526314, 0.3713815789473684, 0.3717105263157895, 0.37236842105263157, 0.37269736842105267, 0.37269736842105267, 0.37302631578947365, 0.3726973684210526, 0.3720394736842105, 0.37236842105263157, 0.3717105263157895, 0.3717105263157895, 0.37203947368421053, 0.3720394736842105, 0.3717105263157895, 0.37203947368421053, 0.37203947368421053] Best val loss: 8.4273606300354


Loaded best state dict for [0.4]
Current group: 0.6
Epoch [1/38], Training Loss: 33.0595, Validation Loss Current: 8.8772, Validation Loss AVG: 8.8772, lr: 0.1
Epoch [2/38], Training Loss: 31.6889, Validation Loss Current: 9.3481, Validation Loss AVG: 9.3481, lr: 0.1
Epoch [3/38], Training Loss: 30.1276, Validation Loss Current: 11.8128, Validation Loss AVG: 11.8128, lr: 0.1
Epoch [4/38], Training Loss: 33.3251, Validation Loss Current: 20.0848, Validation Loss AVG: 20.0848, lr: 0.1
Epoch [5/38], Training Loss: 36.8353, Validation Loss Current: 11.6927, Validation Loss AVG: 11.6927, lr: 0.1
Epoch [6/38], Training Loss: 34.6665, Validation Loss Current: 9.0007, Validation Loss AVG: 9.0007, lr: 0.1
Epoch [7/38], Training Loss: 31.2339, Validation Loss Current: 9.8705, Validation Loss AVG: 9.8705, lr: 0.1
Epoch [8/38], Training Loss: 30.9900, Validation Loss Current: 8.6653, Validation Loss AVG: 8.6653, lr: 0.010000000000000002
Epoch [9/38], Training Loss: 27.6695, Validation Loss Current: 8.3888, Validation Loss AVG: 8.3888, lr: 0.010000000000000002
Epoch [10/38], Training Loss: 27.0456, Validation Loss Current: 8.2340, Validation Loss AVG: 8.2340, lr: 0.010000000000000002
Epoch [11/38], Training Loss: 26.3035, Validation Loss Current: 8.5692, Validation Loss AVG: 8.5692, lr: 0.010000000000000002
Epoch [12/38], Training Loss: 25.0949, Validation Loss Current: 8.3483, Validation Loss AVG: 8.3483, lr: 0.010000000000000002
Epoch [13/38], Training Loss: 24.7012, Validation Loss Current: 8.6322, Validation Loss AVG: 8.6322, lr: 0.010000000000000002
Epoch [14/38], Training Loss: 24.4344, Validation Loss Current: 8.4555, Validation Loss AVG: 8.4555, lr: 0.010000000000000002
Epoch [15/38], Training Loss: 24.0531, Validation Loss Current: 8.6631, Validation Loss AVG: 8.6631, lr: 0.010000000000000002
Epoch [16/38], Training Loss: 24.4199, Validation Loss Current: 8.6275, Validation Loss AVG: 8.6275, lr: 0.010000000000000002
Epoch [17/38], Training Loss: 22.5473, Validation Loss Current: 8.5030, Validation Loss AVG: 8.5030, lr: 0.0010000000000000002
Epoch [18/38], Training Loss: 22.9662, Validation Loss Current: 8.6044, Validation Loss AVG: 8.6044, lr: 0.0010000000000000002
Epoch [19/38], Training Loss: 22.2824, Validation Loss Current: 8.6841, Validation Loss AVG: 8.6841, lr: 0.0010000000000000002
Epoch [20/38], Training Loss: 22.3671, Validation Loss Current: 8.6411, Validation Loss AVG: 8.6411, lr: 0.0010000000000000002
Epoch [21/38], Training Loss: 22.2944, Validation Loss Current: 8.7174, Validation Loss AVG: 8.7174, lr: 0.0010000000000000002
Epoch [22/38], Training Loss: 22.0899, Validation Loss Current: 8.6191, Validation Loss AVG: 8.6191, lr: 0.0010000000000000002
Epoch [23/38], Training Loss: 24.9125, Validation Loss Current: 8.6432, Validation Loss AVG: 8.6432, lr: 0.00010000000000000003
Epoch [24/38], Training Loss: 21.3988, Validation Loss Current: 8.6690, Validation Loss AVG: 8.6690, lr: 0.00010000000000000003
Epoch [25/38], Training Loss: 22.2883, Validation Loss Current: 8.6748, Validation Loss AVG: 8.6748, lr: 0.00010000000000000003
Epoch [26/38], Training Loss: 21.6576, Validation Loss Current: 8.6717, Validation Loss AVG: 8.6717, lr: 0.00010000000000000003
Epoch [27/38], Training Loss: 21.1931, Validation Loss Current: 8.6992, Validation Loss AVG: 8.6992, lr: 0.00010000000000000003
Epoch [28/38], Training Loss: 21.4883, Validation Loss Current: 8.6916, Validation Loss AVG: 8.6916, lr: 0.00010000000000000003
Epoch [29/38], Training Loss: 21.5805, Validation Loss Current: 8.7107, Validation Loss AVG: 8.7107, lr: 1.0000000000000004e-05
Epoch [30/38], Training Loss: 21.5535, Validation Loss Current: 8.7044, Validation Loss AVG: 8.7044, lr: 1.0000000000000004e-05
Epoch [31/38], Training Loss: 21.4205, Validation Loss Current: 8.7130, Validation Loss AVG: 8.7130, lr: 1.0000000000000004e-05
Epoch [32/38], Training Loss: 21.0194, Validation Loss Current: 8.6992, Validation Loss AVG: 8.6992, lr: 1.0000000000000004e-05
Epoch [33/38], Training Loss: 21.6178, Validation Loss Current: 8.6972, Validation Loss AVG: 8.6972, lr: 1.0000000000000004e-05
Epoch [34/38], Training Loss: 22.2140, Validation Loss Current: 8.7004, Validation Loss AVG: 8.7004, lr: 1.0000000000000004e-05
Epoch [35/38], Training Loss: 21.3509, Validation Loss Current: 8.6974, Validation Loss AVG: 8.6974, lr: 1.0000000000000004e-06
Epoch [36/38], Training Loss: 22.4965, Validation Loss Current: 8.7006, Validation Loss AVG: 8.7006, lr: 1.0000000000000004e-06
Epoch [37/38], Training Loss: 22.1778, Validation Loss Current: 8.6706, Validation Loss AVG: 8.6706, lr: 1.0000000000000004e-06
Epoch [38/38], Training Loss: 20.9974, Validation Loss Current: 8.6703, Validation Loss AVG: 8.6703, lr: 1.0000000000000004e-06
Epoch [39/38], Training Loss: 21.3547, Validation Loss Current: 8.6924, Validation Loss AVG: 8.6924, lr: 1.0000000000000004e-06
Epoch [40/38], Training Loss: 22.4460, Validation Loss Current: 8.7256, Validation Loss AVG: 8.7256, lr: 1.0000000000000004e-06
 --- Early Stopped ---
Patch distance: 0.6 finished training. Best epoch: 10 Best val accuracy: [0.34342105263157896, 0.3483552631578947, 0.31282894736842104, 0.23552631578947367, 0.3335526315789473, 0.35526315789473684, 0.32368421052631574, 0.3707236842105263, 0.39572368421052634, 0.4016447368421052, 0.3871710526315789, 0.39473684210526316, 0.39769736842105263, 0.41118421052631576, 0.3891447368421053, 0.3904605263157895, 0.4082236842105263, 0.40361842105263157, 0.4052631578947369, 0.40394736842105256, 0.4042763157894737, 0.4052631578947368, 0.4052631578947368, 0.40394736842105267, 0.4042763157894737, 0.40394736842105267, 0.40394736842105267, 0.4052631578947368, 0.40493421052631584, 0.40493421052631584, 0.40493421052631584, 0.40493421052631584, 0.4052631578947369, 0.4052631578947369, 0.4052631578947369, 0.4052631578947369, 0.4052631578947369, 0.4052631578947369, 0.4052631578947369, 0.4052631578947369] Best val loss: 8.234012961387634


Loaded best state dict for [0.4, 0.6]
Current group: 0.8
Epoch [1/38], Training Loss: 31.9129, Validation Loss Current: 9.1897, Validation Loss AVG: 9.1897, lr: 0.1
Epoch [2/38], Training Loss: 33.2728, Validation Loss Current: 9.0968, Validation Loss AVG: 9.0968, lr: 0.1
Epoch [3/38], Training Loss: 32.0235, Validation Loss Current: 9.5069, Validation Loss AVG: 9.5069, lr: 0.1
Epoch [4/38], Training Loss: 31.9532, Validation Loss Current: 9.1492, Validation Loss AVG: 9.1492, lr: 0.1
Epoch [5/38], Training Loss: 30.8377, Validation Loss Current: 11.7989, Validation Loss AVG: 11.7989, lr: 0.1
Epoch [6/38], Training Loss: 33.5526, Validation Loss Current: 18.2378, Validation Loss AVG: 18.2378, lr: 0.1
Epoch [7/38], Training Loss: 36.2103, Validation Loss Current: 9.3797, Validation Loss AVG: 9.3797, lr: 0.1
Epoch [8/38], Training Loss: 33.9736, Validation Loss Current: 9.5649, Validation Loss AVG: 9.5649, lr: 0.1
Epoch [9/38], Training Loss: 32.7447, Validation Loss Current: 9.0302, Validation Loss AVG: 9.0302, lr: 0.010000000000000002
Epoch [10/38], Training Loss: 29.8666, Validation Loss Current: 9.0272, Validation Loss AVG: 9.0272, lr: 0.010000000000000002
Epoch [11/38], Training Loss: 29.0772, Validation Loss Current: 8.7802, Validation Loss AVG: 8.7802, lr: 0.010000000000000002
Epoch [12/38], Training Loss: 27.6370, Validation Loss Current: 8.7899, Validation Loss AVG: 8.7899, lr: 0.010000000000000002
Epoch [13/38], Training Loss: 26.0059, Validation Loss Current: 8.8661, Validation Loss AVG: 8.8661, lr: 0.010000000000000002
Epoch [14/38], Training Loss: 26.0014, Validation Loss Current: 8.7274, Validation Loss AVG: 8.7274, lr: 0.010000000000000002
Epoch [15/38], Training Loss: 25.4402, Validation Loss Current: 8.7350, Validation Loss AVG: 8.7350, lr: 0.010000000000000002
Epoch [16/38], Training Loss: 24.5575, Validation Loss Current: 9.0765, Validation Loss AVG: 9.0765, lr: 0.010000000000000002
Epoch [17/38], Training Loss: 24.8889, Validation Loss Current: 8.7964, Validation Loss AVG: 8.7964, lr: 0.010000000000000002
Epoch [18/38], Training Loss: 24.7556, Validation Loss Current: 8.8372, Validation Loss AVG: 8.8372, lr: 0.010000000000000002
Epoch [19/38], Training Loss: 24.7930, Validation Loss Current: 9.0014, Validation Loss AVG: 9.0014, lr: 0.010000000000000002
Epoch [20/38], Training Loss: 22.4309, Validation Loss Current: 9.0455, Validation Loss AVG: 9.0455, lr: 0.010000000000000002
Epoch [21/38], Training Loss: 21.7532, Validation Loss Current: 8.8660, Validation Loss AVG: 8.8660, lr: 0.0010000000000000002
Epoch [22/38], Training Loss: 21.3436, Validation Loss Current: 8.9379, Validation Loss AVG: 8.9379, lr: 0.0010000000000000002
Epoch [23/38], Training Loss: 21.3334, Validation Loss Current: 8.9529, Validation Loss AVG: 8.9529, lr: 0.0010000000000000002
Epoch [24/38], Training Loss: 20.9363, Validation Loss Current: 8.9968, Validation Loss AVG: 8.9968, lr: 0.0010000000000000002
Epoch [25/38], Training Loss: 20.6425, Validation Loss Current: 9.0219, Validation Loss AVG: 9.0219, lr: 0.0010000000000000002
Epoch [26/38], Training Loss: 21.1364, Validation Loss Current: 9.0409, Validation Loss AVG: 9.0409, lr: 0.0010000000000000002
Epoch [27/38], Training Loss: 20.7781, Validation Loss Current: 9.0172, Validation Loss AVG: 9.0172, lr: 0.00010000000000000003
Epoch [28/38], Training Loss: 20.1601, Validation Loss Current: 9.0277, Validation Loss AVG: 9.0277, lr: 0.00010000000000000003
Epoch [29/38], Training Loss: 20.5049, Validation Loss Current: 9.0310, Validation Loss AVG: 9.0310, lr: 0.00010000000000000003
Epoch [30/38], Training Loss: 21.3466, Validation Loss Current: 9.0331, Validation Loss AVG: 9.0331, lr: 0.00010000000000000003
Epoch [31/38], Training Loss: 21.2119, Validation Loss Current: 9.0278, Validation Loss AVG: 9.0278, lr: 0.00010000000000000003
Epoch [32/38], Training Loss: 21.1388, Validation Loss Current: 9.0391, Validation Loss AVG: 9.0391, lr: 0.00010000000000000003
Epoch [33/38], Training Loss: 22.0040, Validation Loss Current: 9.0349, Validation Loss AVG: 9.0349, lr: 1.0000000000000004e-05
Epoch [34/38], Training Loss: 21.3506, Validation Loss Current: 9.0202, Validation Loss AVG: 9.0202, lr: 1.0000000000000004e-05
Epoch [35/38], Training Loss: 20.9536, Validation Loss Current: 9.0187, Validation Loss AVG: 9.0187, lr: 1.0000000000000004e-05
Epoch [36/38], Training Loss: 20.8115, Validation Loss Current: 9.0374, Validation Loss AVG: 9.0374, lr: 1.0000000000000004e-05
Epoch [37/38], Training Loss: 21.0508, Validation Loss Current: 9.0245, Validation Loss AVG: 9.0245, lr: 1.0000000000000004e-05
Epoch [38/38], Training Loss: 21.1211, Validation Loss Current: 9.0304, Validation Loss AVG: 9.0304, lr: 1.0000000000000004e-05
Epoch [39/38], Training Loss: 21.5696, Validation Loss Current: 9.0255, Validation Loss AVG: 9.0255, lr: 1.0000000000000004e-06
Epoch [40/38], Training Loss: 20.6006, Validation Loss Current: 9.0277, Validation Loss AVG: 9.0277, lr: 1.0000000000000004e-06
Epoch [41/38], Training Loss: 21.6024, Validation Loss Current: 9.0297, Validation Loss AVG: 9.0297, lr: 1.0000000000000004e-06
Epoch [42/38], Training Loss: 21.0981, Validation Loss Current: 9.0477, Validation Loss AVG: 9.0477, lr: 1.0000000000000004e-06
Epoch [43/38], Training Loss: 20.0492, Validation Loss Current: 9.0190, Validation Loss AVG: 9.0190, lr: 1.0000000000000004e-06
Epoch [44/38], Training Loss: 20.1121, Validation Loss Current: 9.0505, Validation Loss AVG: 9.0505, lr: 1.0000000000000004e-06
 --- Early Stopped ---
Patch distance: 0.8 finished training. Best epoch: 14 Best val accuracy: [0.31842105263157894, 0.30493421052631575, 0.33421052631578946, 0.3394736842105263, 0.21019736842105266, 0.2631578947368421, 0.28717105263157894, 0.3138157894736842, 0.32532894736842105, 0.34638157894736843, 0.36875, 0.3713815789473685, 0.3717105263157895, 0.3782894736842105, 0.36875, 0.3828947368421053, 0.37565789473684214, 0.35690789473684215, 0.36907894736842106, 0.38092105263157894, 0.3789473684210526, 0.3782894736842105, 0.37532894736842104, 0.37269736842105267, 0.3796052631578947, 0.3733552631578947, 0.37302631578947365, 0.375, 0.3740131578947369, 0.3740131578947368, 0.3743421052631579, 0.3740131578947369, 0.3736842105263158, 0.3736842105263158, 0.3736842105263158, 0.3736842105263158, 0.37335526315789475, 0.37335526315789475, 0.3736842105263158, 0.37335526315789475, 0.37335526315789475, 0.37335526315789475, 0.37335526315789475, 0.3736842105263158] Best val loss: 8.727435994148255


Loaded best state dict for [0.4, 0.6, 0.8]
Current group: 1
Epoch [1/38], Training Loss: 31.7928, Validation Loss Current: 8.7217, Validation Loss AVG: 9.5571, lr: 0.1
Epoch [2/38], Training Loss: 33.7166, Validation Loss Current: 8.2831, Validation Loss AVG: 9.4225, lr: 0.1
Epoch [3/38], Training Loss: 32.2284, Validation Loss Current: 10.5003, Validation Loss AVG: 14.4756, lr: 0.1
Epoch [4/38], Training Loss: 35.3998, Validation Loss Current: 8.6168, Validation Loss AVG: 9.7704, lr: 0.1
Epoch [5/38], Training Loss: 32.8861, Validation Loss Current: 8.3444, Validation Loss AVG: 9.8696, lr: 0.1
Epoch [6/38], Training Loss: 31.2479, Validation Loss Current: 8.8886, Validation Loss AVG: 9.8630, lr: 0.1
Epoch [7/38], Training Loss: 31.3771, Validation Loss Current: 8.3565, Validation Loss AVG: 10.0454, lr: 0.1
Epoch [8/38], Training Loss: 32.6326, Validation Loss Current: 8.8421, Validation Loss AVG: 9.5995, lr: 0.1
Epoch [9/38], Training Loss: 31.8865, Validation Loss Current: 7.6611, Validation Loss AVG: 8.9264, lr: 0.010000000000000002
Epoch [10/38], Training Loss: 29.3030, Validation Loss Current: 7.6852, Validation Loss AVG: 9.8797, lr: 0.010000000000000002
Epoch [11/38], Training Loss: 27.4075, Validation Loss Current: 7.3056, Validation Loss AVG: 8.8290, lr: 0.010000000000000002
Epoch [12/38], Training Loss: 27.2592, Validation Loss Current: 7.3749, Validation Loss AVG: 9.0730, lr: 0.010000000000000002
Epoch [13/38], Training Loss: 26.8459, Validation Loss Current: 7.1352, Validation Loss AVG: 8.9983, lr: 0.010000000000000002
Epoch [14/38], Training Loss: 24.6157, Validation Loss Current: 7.1201, Validation Loss AVG: 9.0413, lr: 0.010000000000000002
Epoch [15/38], Training Loss: 26.8030, Validation Loss Current: 7.1298, Validation Loss AVG: 8.7498, lr: 0.010000000000000002
Epoch [16/38], Training Loss: 24.0812, Validation Loss Current: 7.3826, Validation Loss AVG: 9.1719, lr: 0.010000000000000002
Epoch [17/38], Training Loss: 23.2869, Validation Loss Current: 7.2392, Validation Loss AVG: 9.0001, lr: 0.010000000000000002
Epoch [18/38], Training Loss: 22.5712, Validation Loss Current: 7.1456, Validation Loss AVG: 9.4272, lr: 0.010000000000000002
Epoch [19/38], Training Loss: 21.7566, Validation Loss Current: 7.0696, Validation Loss AVG: 9.3903, lr: 0.010000000000000002
Epoch [20/38], Training Loss: 21.9759, Validation Loss Current: 7.0625, Validation Loss AVG: 9.1717, lr: 0.010000000000000002
Epoch [21/38], Training Loss: 20.4711, Validation Loss Current: 7.2098, Validation Loss AVG: 9.6018, lr: 0.010000000000000002
Epoch [22/38], Training Loss: 20.6730, Validation Loss Current: 7.1638, Validation Loss AVG: 10.1393, lr: 0.010000000000000002
Epoch [23/38], Training Loss: 20.0981, Validation Loss Current: 7.2139, Validation Loss AVG: 9.9313, lr: 0.010000000000000002
Epoch [24/38], Training Loss: 19.7977, Validation Loss Current: 7.2999, Validation Loss AVG: 10.3185, lr: 0.010000000000000002
Epoch [25/38], Training Loss: 19.2447, Validation Loss Current: 7.1048, Validation Loss AVG: 9.7623, lr: 0.010000000000000002
Epoch [26/38], Training Loss: 18.7377, Validation Loss Current: 7.0708, Validation Loss AVG: 10.2118, lr: 0.010000000000000002
Epoch [27/38], Training Loss: 16.6738, Validation Loss Current: 7.1277, Validation Loss AVG: 9.8431, lr: 0.0010000000000000002
Epoch [28/38], Training Loss: 17.0993, Validation Loss Current: 7.2473, Validation Loss AVG: 9.9982, lr: 0.0010000000000000002
Epoch [29/38], Training Loss: 16.9645, Validation Loss Current: 7.3601, Validation Loss AVG: 10.0812, lr: 0.0010000000000000002
Epoch [30/38], Training Loss: 15.5009, Validation Loss Current: 7.3978, Validation Loss AVG: 10.0689, lr: 0.0010000000000000002
Epoch [31/38], Training Loss: 15.1328, Validation Loss Current: 7.4229, Validation Loss AVG: 10.2243, lr: 0.0010000000000000002
Epoch [32/38], Training Loss: 15.4002, Validation Loss Current: 7.4231, Validation Loss AVG: 10.3098, lr: 0.0010000000000000002
Epoch [33/38], Training Loss: 14.9784, Validation Loss Current: 7.4052, Validation Loss AVG: 10.2468, lr: 0.00010000000000000003
Epoch [34/38], Training Loss: 15.4592, Validation Loss Current: 7.4860, Validation Loss AVG: 10.2449, lr: 0.00010000000000000003
Epoch [35/38], Training Loss: 15.1315, Validation Loss Current: 7.4233, Validation Loss AVG: 10.2322, lr: 0.00010000000000000003
Epoch [36/38], Training Loss: 15.5640, Validation Loss Current: 7.5090, Validation Loss AVG: 10.2410, lr: 0.00010000000000000003
Epoch [37/38], Training Loss: 15.0639, Validation Loss Current: 7.4503, Validation Loss AVG: 10.2539, lr: 0.00010000000000000003
Epoch [38/38], Training Loss: 14.7144, Validation Loss Current: 7.5313, Validation Loss AVG: 10.2813, lr: 0.00010000000000000003
Epoch [39/38], Training Loss: 14.8327, Validation Loss Current: 7.4583, Validation Loss AVG: 10.2549, lr: 1.0000000000000004e-05
Epoch [40/38], Training Loss: 15.2226, Validation Loss Current: 7.4956, Validation Loss AVG: 10.2495, lr: 1.0000000000000004e-05
Epoch [41/38], Training Loss: 15.4583, Validation Loss Current: 7.4467, Validation Loss AVG: 10.2739, lr: 1.0000000000000004e-05
Epoch [42/38], Training Loss: 14.6824, Validation Loss Current: 7.4761, Validation Loss AVG: 10.2513, lr: 1.0000000000000004e-05
Epoch [43/38], Training Loss: 15.0358, Validation Loss Current: 7.4622, Validation Loss AVG: 10.2464, lr: 1.0000000000000004e-05
Epoch [44/38], Training Loss: 15.2361, Validation Loss Current: 7.4949, Validation Loss AVG: 10.2803, lr: 1.0000000000000004e-05
Epoch [45/38], Training Loss: 14.7265, Validation Loss Current: 7.4371, Validation Loss AVG: 10.2414, lr: 1.0000000000000004e-06
Epoch [46/38], Training Loss: 16.8158, Validation Loss Current: 7.5040, Validation Loss AVG: 10.2764, lr: 1.0000000000000004e-06
Epoch [47/38], Training Loss: 15.7689, Validation Loss Current: 7.5104, Validation Loss AVG: 10.2729, lr: 1.0000000000000004e-06
Epoch [48/38], Training Loss: 16.4974, Validation Loss Current: 7.5069, Validation Loss AVG: 10.2670, lr: 1.0000000000000004e-06
Epoch [49/38], Training Loss: 15.6623, Validation Loss Current: 7.5085, Validation Loss AVG: 10.2738, lr: 1.0000000000000004e-06
Epoch [50/38], Training Loss: 15.3665, Validation Loss Current: 7.4717, Validation Loss AVG: 10.2843, lr: 1.0000000000000004e-06
 --- Early Stopped ---
Patch distance: 1 finished training. Best epoch: 20 Best val accuracy: [0.3848684210526316, 0.40625, 0.2944078947368421, 0.3832236842105263, 0.36348684210526316, 0.3996710526315789, 0.3963815789473684, 0.39473684210526316, 0.4375, 0.4473684210526316, 0.4621710526315789, 0.4654605263157895, 0.4753289473684211, 0.4917763157894737, 0.48848684210526316, 0.4753289473684211, 0.49506578947368424, 0.4934210526315789, 0.4868421052631579, 0.5, 0.5049342105263158, 0.5082236842105263, 0.48355263157894735, 0.5049342105263158, 0.49835526315789475, 0.5115131578947368, 0.5164473684210527, 0.506578947368421, 0.5082236842105263, 0.5131578947368421, 0.5115131578947368, 0.5049342105263158, 0.5032894736842105, 0.5016447368421053, 0.5032894736842105, 0.506578947368421, 0.506578947368421, 0.5032894736842105, 0.5032894736842105, 0.5032894736842105, 0.5049342105263158, 0.5032894736842105, 0.5049342105263158, 0.5049342105263158, 0.5049342105263158, 0.5049342105263158, 0.5049342105263158, 0.5049342105263158, 0.5049342105263158, 0.5049342105263158] Best val loss: 7.0624531507492065


----- Training alexnet with sequence: [0.6, 0.8, 1] -----
Current group: 0.6
Epoch [1/50], Training Loss: 40.5493, Validation Loss Current: 10.0958, Validation Loss AVG: 10.0958, lr: 0.1
Epoch [2/50], Training Loss: 40.8212, Validation Loss Current: 10.1220, Validation Loss AVG: 10.1220, lr: 0.1
Epoch [3/50], Training Loss: 40.8343, Validation Loss Current: 10.1304, Validation Loss AVG: 10.1304, lr: 0.1
Epoch [4/50], Training Loss: 40.2275, Validation Loss Current: 10.0666, Validation Loss AVG: 10.0666, lr: 0.1
Epoch [5/50], Training Loss: 40.0904, Validation Loss Current: 10.0796, Validation Loss AVG: 10.0796, lr: 0.1
Epoch [6/50], Training Loss: 39.7541, Validation Loss Current: 9.5592, Validation Loss AVG: 9.5592, lr: 0.1
Epoch [7/50], Training Loss: 36.0479, Validation Loss Current: 36.4949, Validation Loss AVG: 36.4949, lr: 0.1
Epoch [8/50], Training Loss: 53.3541, Validation Loss Current: 11.9156, Validation Loss AVG: 11.9156, lr: 0.1
Epoch [9/50], Training Loss: 37.2374, Validation Loss Current: 9.3001, Validation Loss AVG: 9.3001, lr: 0.1
Epoch [10/50], Training Loss: 35.1525, Validation Loss Current: 9.1016, Validation Loss AVG: 9.1016, lr: 0.1
Epoch [11/50], Training Loss: 34.5826, Validation Loss Current: 9.9503, Validation Loss AVG: 9.9503, lr: 0.1
Epoch [12/50], Training Loss: 34.2303, Validation Loss Current: 9.3226, Validation Loss AVG: 9.3226, lr: 0.1
Epoch [13/50], Training Loss: 34.7817, Validation Loss Current: 11.3590, Validation Loss AVG: 11.3590, lr: 0.1
Epoch [14/50], Training Loss: 35.9923, Validation Loss Current: 10.7183, Validation Loss AVG: 10.7183, lr: 0.1
Epoch [15/50], Training Loss: 35.0694, Validation Loss Current: 9.5635, Validation Loss AVG: 9.5635, lr: 0.1
Epoch [16/50], Training Loss: 33.4601, Validation Loss Current: 9.8903, Validation Loss AVG: 9.8903, lr: 0.1
Epoch [17/50], Training Loss: 31.1986, Validation Loss Current: 8.4842, Validation Loss AVG: 8.4842, lr: 0.010000000000000002
Epoch [18/50], Training Loss: 28.7462, Validation Loss Current: 8.3528, Validation Loss AVG: 8.3528, lr: 0.010000000000000002
Epoch [19/50], Training Loss: 27.9018, Validation Loss Current: 8.3717, Validation Loss AVG: 8.3717, lr: 0.010000000000000002
Epoch [20/50], Training Loss: 26.8687, Validation Loss Current: 8.3350, Validation Loss AVG: 8.3350, lr: 0.010000000000000002
Epoch [21/50], Training Loss: 27.1108, Validation Loss Current: 8.3283, Validation Loss AVG: 8.3283, lr: 0.010000000000000002
Epoch [22/50], Training Loss: 26.3738, Validation Loss Current: 8.2807, Validation Loss AVG: 8.2807, lr: 0.010000000000000002
Epoch [23/50], Training Loss: 25.2436, Validation Loss Current: 8.2700, Validation Loss AVG: 8.2700, lr: 0.010000000000000002
Epoch [24/50], Training Loss: 24.3326, Validation Loss Current: 8.0645, Validation Loss AVG: 8.0645, lr: 0.010000000000000002
Epoch [25/50], Training Loss: 24.8727, Validation Loss Current: 8.4291, Validation Loss AVG: 8.4291, lr: 0.010000000000000002
Epoch [26/50], Training Loss: 23.7464, Validation Loss Current: 8.6578, Validation Loss AVG: 8.6578, lr: 0.010000000000000002
Epoch [27/50], Training Loss: 23.3249, Validation Loss Current: 8.7127, Validation Loss AVG: 8.7127, lr: 0.010000000000000002
Epoch [28/50], Training Loss: 22.0329, Validation Loss Current: 8.6397, Validation Loss AVG: 8.6397, lr: 0.010000000000000002
Epoch [29/50], Training Loss: 21.3142, Validation Loss Current: 8.6106, Validation Loss AVG: 8.6106, lr: 0.010000000000000002
Epoch [30/50], Training Loss: 20.8956, Validation Loss Current: 9.0507, Validation Loss AVG: 9.0507, lr: 0.010000000000000002
Epoch [31/50], Training Loss: 19.8554, Validation Loss Current: 8.6449, Validation Loss AVG: 8.6449, lr: 0.0010000000000000002
Epoch [32/50], Training Loss: 20.0996, Validation Loss Current: 8.5435, Validation Loss AVG: 8.5435, lr: 0.0010000000000000002
Epoch [33/50], Training Loss: 19.5631, Validation Loss Current: 8.6264, Validation Loss AVG: 8.6264, lr: 0.0010000000000000002
Epoch [34/50], Training Loss: 19.3482, Validation Loss Current: 8.7391, Validation Loss AVG: 8.7391, lr: 0.0010000000000000002
Epoch [35/50], Training Loss: 19.2154, Validation Loss Current: 8.6974, Validation Loss AVG: 8.6974, lr: 0.0010000000000000002
Epoch [36/50], Training Loss: 18.6053, Validation Loss Current: 8.7567, Validation Loss AVG: 8.7567, lr: 0.0010000000000000002
Epoch [37/50], Training Loss: 18.4530, Validation Loss Current: 8.7461, Validation Loss AVG: 8.7461, lr: 0.00010000000000000003
Epoch [38/50], Training Loss: 18.4980, Validation Loss Current: 8.7822, Validation Loss AVG: 8.7822, lr: 0.00010000000000000003
Epoch [39/50], Training Loss: 18.2618, Validation Loss Current: 8.7694, Validation Loss AVG: 8.7694, lr: 0.00010000000000000003
Epoch [40/50], Training Loss: 18.7276, Validation Loss Current: 8.7515, Validation Loss AVG: 8.7515, lr: 0.00010000000000000003
Epoch [41/50], Training Loss: 18.6092, Validation Loss Current: 8.8019, Validation Loss AVG: 8.8019, lr: 0.00010000000000000003
Epoch [42/50], Training Loss: 18.3885, Validation Loss Current: 8.7861, Validation Loss AVG: 8.7861, lr: 0.00010000000000000003
Epoch [43/50], Training Loss: 18.2538, Validation Loss Current: 8.8033, Validation Loss AVG: 8.8033, lr: 1.0000000000000004e-05
Epoch [44/50], Training Loss: 18.2947, Validation Loss Current: 8.7966, Validation Loss AVG: 8.7966, lr: 1.0000000000000004e-05
Epoch [45/50], Training Loss: 17.8903, Validation Loss Current: 8.8437, Validation Loss AVG: 8.8437, lr: 1.0000000000000004e-05
Epoch [46/50], Training Loss: 18.7399, Validation Loss Current: 8.8108, Validation Loss AVG: 8.8108, lr: 1.0000000000000004e-05
Epoch [47/50], Training Loss: 20.7195, Validation Loss Current: 8.8306, Validation Loss AVG: 8.8306, lr: 1.0000000000000004e-05
Epoch [48/50], Training Loss: 18.4675, Validation Loss Current: 8.7971, Validation Loss AVG: 8.7971, lr: 1.0000000000000004e-05
Epoch [49/50], Training Loss: 19.2461, Validation Loss Current: 8.8241, Validation Loss AVG: 8.8241, lr: 1.0000000000000004e-06
Epoch [50/50], Training Loss: 19.3894, Validation Loss Current: 8.8287, Validation Loss AVG: 8.8287, lr: 1.0000000000000004e-06
Epoch [51/50], Training Loss: 18.9673, Validation Loss Current: 8.8078, Validation Loss AVG: 8.8078, lr: 1.0000000000000004e-06
Epoch [52/50], Training Loss: 19.0181, Validation Loss Current: 8.8355, Validation Loss AVG: 8.8355, lr: 1.0000000000000004e-06
Epoch [53/50], Training Loss: 18.0756, Validation Loss Current: 8.8245, Validation Loss AVG: 8.8245, lr: 1.0000000000000004e-06
Epoch [54/50], Training Loss: 19.8659, Validation Loss Current: 8.8015, Validation Loss AVG: 8.8015, lr: 1.0000000000000004e-06
 --- Early Stopped ---
Patch distance: 0.6 finished training. Best epoch: 24 Best val accuracy: [0.2302631578947368, 0.2302631578947368, 0.2302631578947368, 0.2302631578947368, 0.2302631578947368, 0.3092105263157895, 0.1986842105263158, 0.24901315789473685, 0.3269736842105263, 0.3223684210526315, 0.3194078947368421, 0.3203947368421053, 0.23552631578947372, 0.3190789473684211, 0.28388157894736843, 0.27697368421052626, 0.36249999999999993, 0.3871710526315789, 0.3615131578947368, 0.37697368421052635, 0.3894736842105263, 0.4036184210526315, 0.40690789473684214, 0.4167763157894736, 0.40230263157894736, 0.38749999999999996, 0.4026315789473684, 0.40592105263157896, 0.3950657894736842, 0.41052631578947363, 0.42302631578947364, 0.4164473684210527, 0.4167763157894736, 0.41644736842105257, 0.4240131578947368, 0.4167763157894737, 0.4197368421052632, 0.41875, 0.41940789473684215, 0.4200657894736842, 0.41875, 0.41907894736842105, 0.41907894736842105, 0.41907894736842105, 0.41940789473684215, 0.41940789473684215, 0.41940789473684215, 0.41940789473684215, 0.4197368421052632, 0.4197368421052632, 0.4197368421052632, 0.4197368421052632, 0.4197368421052632, 0.4197368421052632] Best val loss: 8.064533734321595


Loaded best state dict for [0.6]
Current group: 0.8
Epoch [1/50], Training Loss: 30.6879, Validation Loss Current: 10.8771, Validation Loss AVG: 10.8771, lr: 0.1
Epoch [2/50], Training Loss: 34.5496, Validation Loss Current: 14.5690, Validation Loss AVG: 14.5690, lr: 0.1
Epoch [3/50], Training Loss: 35.5847, Validation Loss Current: 9.8144, Validation Loss AVG: 9.8144, lr: 0.1
Epoch [4/50], Training Loss: 32.9631, Validation Loss Current: 9.0566, Validation Loss AVG: 9.0566, lr: 0.1
Epoch [5/50], Training Loss: 32.4165, Validation Loss Current: 9.8610, Validation Loss AVG: 9.8610, lr: 0.1
Epoch [6/50], Training Loss: 33.2982, Validation Loss Current: 9.1977, Validation Loss AVG: 9.1977, lr: 0.1
Epoch [7/50], Training Loss: 31.3072, Validation Loss Current: 10.3159, Validation Loss AVG: 10.3159, lr: 0.1
Epoch [8/50], Training Loss: 32.2175, Validation Loss Current: 9.1798, Validation Loss AVG: 9.1798, lr: 0.1
Epoch [9/50], Training Loss: 30.5608, Validation Loss Current: 9.0772, Validation Loss AVG: 9.0772, lr: 0.1
Epoch [10/50], Training Loss: 31.3497, Validation Loss Current: 10.5149, Validation Loss AVG: 10.5149, lr: 0.1
Epoch [11/50], Training Loss: 29.8000, Validation Loss Current: 8.3666, Validation Loss AVG: 8.3666, lr: 0.010000000000000002
Epoch [12/50], Training Loss: 25.2620, Validation Loss Current: 8.3251, Validation Loss AVG: 8.3251, lr: 0.010000000000000002
Epoch [13/50], Training Loss: 24.5703, Validation Loss Current: 8.5118, Validation Loss AVG: 8.5118, lr: 0.010000000000000002
Epoch [14/50], Training Loss: 23.9062, Validation Loss Current: 8.5042, Validation Loss AVG: 8.5042, lr: 0.010000000000000002
Epoch [15/50], Training Loss: 23.6181, Validation Loss Current: 8.9890, Validation Loss AVG: 8.9890, lr: 0.010000000000000002
Epoch [16/50], Training Loss: 22.8684, Validation Loss Current: 8.6699, Validation Loss AVG: 8.6699, lr: 0.010000000000000002
Epoch [17/50], Training Loss: 20.9037, Validation Loss Current: 8.8635, Validation Loss AVG: 8.8635, lr: 0.010000000000000002
Epoch [18/50], Training Loss: 20.6823, Validation Loss Current: 8.9025, Validation Loss AVG: 8.9025, lr: 0.010000000000000002
Epoch [19/50], Training Loss: 20.9139, Validation Loss Current: 9.1822, Validation Loss AVG: 9.1822, lr: 0.0010000000000000002
Epoch [20/50], Training Loss: 18.6843, Validation Loss Current: 9.0856, Validation Loss AVG: 9.0856, lr: 0.0010000000000000002
Epoch [21/50], Training Loss: 20.7795, Validation Loss Current: 9.1992, Validation Loss AVG: 9.1992, lr: 0.0010000000000000002
Epoch [22/50], Training Loss: 19.7177, Validation Loss Current: 9.2567, Validation Loss AVG: 9.2567, lr: 0.0010000000000000002
Epoch [23/50], Training Loss: 19.0337, Validation Loss Current: 9.4218, Validation Loss AVG: 9.4218, lr: 0.0010000000000000002
Epoch [24/50], Training Loss: 18.4405, Validation Loss Current: 9.3317, Validation Loss AVG: 9.3317, lr: 0.0010000000000000002
Epoch [25/50], Training Loss: 17.8767, Validation Loss Current: 9.3068, Validation Loss AVG: 9.3068, lr: 0.00010000000000000003
Epoch [26/50], Training Loss: 18.0248, Validation Loss Current: 9.2764, Validation Loss AVG: 9.2764, lr: 0.00010000000000000003
Epoch [27/50], Training Loss: 18.3567, Validation Loss Current: 9.3113, Validation Loss AVG: 9.3113, lr: 0.00010000000000000003
Epoch [28/50], Training Loss: 18.9135, Validation Loss Current: 9.3257, Validation Loss AVG: 9.3257, lr: 0.00010000000000000003
Epoch [29/50], Training Loss: 19.3606, Validation Loss Current: 9.3590, Validation Loss AVG: 9.3590, lr: 0.00010000000000000003
Epoch [30/50], Training Loss: 17.9034, Validation Loss Current: 9.3349, Validation Loss AVG: 9.3349, lr: 0.00010000000000000003
Epoch [31/50], Training Loss: 18.6882, Validation Loss Current: 9.3305, Validation Loss AVG: 9.3305, lr: 1.0000000000000004e-05
Epoch [32/50], Training Loss: 17.9650, Validation Loss Current: 9.3530, Validation Loss AVG: 9.3530, lr: 1.0000000000000004e-05
Epoch [33/50], Training Loss: 18.4722, Validation Loss Current: 9.3409, Validation Loss AVG: 9.3409, lr: 1.0000000000000004e-05
Epoch [34/50], Training Loss: 18.3412, Validation Loss Current: 9.3891, Validation Loss AVG: 9.3891, lr: 1.0000000000000004e-05
Epoch [35/50], Training Loss: 18.2520, Validation Loss Current: 9.3138, Validation Loss AVG: 9.3138, lr: 1.0000000000000004e-05
Epoch [36/50], Training Loss: 18.8051, Validation Loss Current: 9.3825, Validation Loss AVG: 9.3825, lr: 1.0000000000000004e-05
Epoch [37/50], Training Loss: 18.3512, Validation Loss Current: 9.3747, Validation Loss AVG: 9.3747, lr: 1.0000000000000004e-06
Epoch [38/50], Training Loss: 18.5863, Validation Loss Current: 9.3339, Validation Loss AVG: 9.3339, lr: 1.0000000000000004e-06
Epoch [39/50], Training Loss: 19.2966, Validation Loss Current: 9.3351, Validation Loss AVG: 9.3351, lr: 1.0000000000000004e-06
Epoch [40/50], Training Loss: 19.4843, Validation Loss Current: 9.3590, Validation Loss AVG: 9.3590, lr: 1.0000000000000004e-06
Epoch [41/50], Training Loss: 18.7998, Validation Loss Current: 9.3360, Validation Loss AVG: 9.3360, lr: 1.0000000000000004e-06
Epoch [42/50], Training Loss: 18.0824, Validation Loss Current: 9.3688, Validation Loss AVG: 9.3688, lr: 1.0000000000000004e-06
 --- Early Stopped ---
Patch distance: 0.8 finished training. Best epoch: 12 Best val accuracy: [0.21348684210526314, 0.2644736842105263, 0.2950657894736842, 0.3384868421052632, 0.3190789473684211, 0.3483552631578947, 0.2480263157894737, 0.3493421052631579, 0.3355263157894737, 0.34078947368421053, 0.39309210526315785, 0.3927631578947368, 0.38914473684210527, 0.39539473684210524, 0.3960526315789473, 0.4, 0.4016447368421052, 0.39999999999999997, 0.40592105263157896, 0.41118421052631576, 0.41447368421052627, 0.40855263157894744, 0.4072368421052632, 0.40921052631578947, 0.40822368421052635, 0.4078947368421053, 0.40756578947368427, 0.40756578947368427, 0.40592105263157896, 0.4072368421052632, 0.40822368421052635, 0.4078947368421053, 0.40822368421052635, 0.40855263157894744, 0.4088815789473685, 0.40855263157894744, 0.40855263157894744, 0.40855263157894744, 0.40855263157894744, 0.40855263157894744, 0.40855263157894744, 0.40855263157894744] Best val loss: 8.325123596191407


Loaded best state dict for [0.6, 0.8]
Current group: 1
Epoch [1/50], Training Loss: 28.6469, Validation Loss Current: 8.0725, Validation Loss AVG: 8.8556, lr: 0.1
Epoch [2/50], Training Loss: 29.7988, Validation Loss Current: 10.9295, Validation Loss AVG: 12.8254, lr: 0.1
Epoch [3/50], Training Loss: 32.0831, Validation Loss Current: 8.4149, Validation Loss AVG: 11.0750, lr: 0.1
Epoch [4/50], Training Loss: 29.9467, Validation Loss Current: 8.9178, Validation Loss AVG: 9.9484, lr: 0.1
Epoch [5/50], Training Loss: 29.5525, Validation Loss Current: 8.7270, Validation Loss AVG: 10.0249, lr: 0.1
Epoch [6/50], Training Loss: 30.8870, Validation Loss Current: 8.8323, Validation Loss AVG: 9.5870, lr: 0.1
Epoch [7/50], Training Loss: 31.4809, Validation Loss Current: 8.4487, Validation Loss AVG: 10.5804, lr: 0.1
Epoch [8/50], Training Loss: 28.3831, Validation Loss Current: 7.4194, Validation Loss AVG: 8.6713, lr: 0.010000000000000002
Epoch [9/50], Training Loss: 26.6307, Validation Loss Current: 7.1366, Validation Loss AVG: 8.5628, lr: 0.010000000000000002
Epoch [10/50], Training Loss: 24.3641, Validation Loss Current: 7.2293, Validation Loss AVG: 8.8383, lr: 0.010000000000000002
Epoch [11/50], Training Loss: 23.1584, Validation Loss Current: 7.1212, Validation Loss AVG: 9.2103, lr: 0.010000000000000002
Epoch [12/50], Training Loss: 22.3222, Validation Loss Current: 7.0580, Validation Loss AVG: 8.7885, lr: 0.010000000000000002
Epoch [13/50], Training Loss: 21.1951, Validation Loss Current: 6.9506, Validation Loss AVG: 8.9376, lr: 0.010000000000000002
Epoch [14/50], Training Loss: 21.6543, Validation Loss Current: 7.5541, Validation Loss AVG: 9.6260, lr: 0.010000000000000002
Epoch [15/50], Training Loss: 20.5740, Validation Loss Current: 7.0968, Validation Loss AVG: 9.4873, lr: 0.010000000000000002
Epoch [16/50], Training Loss: 19.8638, Validation Loss Current: 6.9523, Validation Loss AVG: 9.1077, lr: 0.010000000000000002
Epoch [17/50], Training Loss: 19.2718, Validation Loss Current: 7.5766, Validation Loss AVG: 9.6920, lr: 0.010000000000000002
Epoch [18/50], Training Loss: 18.9394, Validation Loss Current: 7.0925, Validation Loss AVG: 9.2167, lr: 0.010000000000000002
Epoch [19/50], Training Loss: 17.7202, Validation Loss Current: 7.3929, Validation Loss AVG: 10.0840, lr: 0.010000000000000002
Epoch [20/50], Training Loss: 16.3985, Validation Loss Current: 7.1792, Validation Loss AVG: 9.6488, lr: 0.0010000000000000002
Epoch [21/50], Training Loss: 16.2253, Validation Loss Current: 7.3593, Validation Loss AVG: 10.1067, lr: 0.0010000000000000002
Epoch [22/50], Training Loss: 16.7382, Validation Loss Current: 7.2998, Validation Loss AVG: 10.0267, lr: 0.0010000000000000002
Epoch [23/50], Training Loss: 16.5821, Validation Loss Current: 7.3277, Validation Loss AVG: 10.0964, lr: 0.0010000000000000002
Epoch [24/50], Training Loss: 16.1399, Validation Loss Current: 7.3470, Validation Loss AVG: 10.2203, lr: 0.0010000000000000002
Epoch [25/50], Training Loss: 15.8309, Validation Loss Current: 7.3820, Validation Loss AVG: 10.3195, lr: 0.0010000000000000002
Epoch [26/50], Training Loss: 16.0096, Validation Loss Current: 7.4376, Validation Loss AVG: 10.2761, lr: 0.00010000000000000003
Epoch [27/50], Training Loss: 16.3791, Validation Loss Current: 7.4934, Validation Loss AVG: 10.2463, lr: 0.00010000000000000003
Epoch [28/50], Training Loss: 15.7346, Validation Loss Current: 7.3699, Validation Loss AVG: 10.2065, lr: 0.00010000000000000003
Epoch [29/50], Training Loss: 15.9928, Validation Loss Current: 7.3345, Validation Loss AVG: 10.2159, lr: 0.00010000000000000003
Epoch [30/50], Training Loss: 15.8540, Validation Loss Current: 7.4122, Validation Loss AVG: 10.2733, lr: 0.00010000000000000003
Epoch [31/50], Training Loss: 16.4556, Validation Loss Current: 7.4536, Validation Loss AVG: 10.2471, lr: 0.00010000000000000003
Epoch [32/50], Training Loss: 16.2126, Validation Loss Current: 7.4720, Validation Loss AVG: 10.2241, lr: 1.0000000000000004e-05
Epoch [33/50], Training Loss: 15.8298, Validation Loss Current: 7.4071, Validation Loss AVG: 10.2491, lr: 1.0000000000000004e-05
Epoch [34/50], Training Loss: 16.5378, Validation Loss Current: 7.4577, Validation Loss AVG: 10.2533, lr: 1.0000000000000004e-05
Epoch [35/50], Training Loss: 16.3332, Validation Loss Current: 7.4525, Validation Loss AVG: 10.2456, lr: 1.0000000000000004e-05
Epoch [36/50], Training Loss: 16.0602, Validation Loss Current: 7.4187, Validation Loss AVG: 10.2744, lr: 1.0000000000000004e-05
Epoch [37/50], Training Loss: 15.5059, Validation Loss Current: 7.5021, Validation Loss AVG: 10.2767, lr: 1.0000000000000004e-05
Epoch [38/50], Training Loss: 16.1899, Validation Loss Current: 7.4484, Validation Loss AVG: 10.2655, lr: 1.0000000000000004e-06
Epoch [39/50], Training Loss: 15.3517, Validation Loss Current: 7.4582, Validation Loss AVG: 10.3100, lr: 1.0000000000000004e-06
Epoch [40/50], Training Loss: 15.6616, Validation Loss Current: 7.4238, Validation Loss AVG: 10.2507, lr: 1.0000000000000004e-06
Epoch [41/50], Training Loss: 15.3856, Validation Loss Current: 7.3582, Validation Loss AVG: 10.2487, lr: 1.0000000000000004e-06
Epoch [42/50], Training Loss: 15.1898, Validation Loss Current: 7.4544, Validation Loss AVG: 10.2634, lr: 1.0000000000000004e-06
Epoch [43/50], Training Loss: 15.6039, Validation Loss Current: 7.4482, Validation Loss AVG: 10.2808, lr: 1.0000000000000004e-06
 --- Early Stopped ---
Patch distance: 1 finished training. Best epoch: 13 Best val accuracy: [0.40789473684210525, 0.28289473684210525, 0.3996710526315789, 0.40789473684210525, 0.35855263157894735, 0.34210526315789475, 0.3717105263157895, 0.47039473684210525, 0.49506578947368424, 0.4917763157894737, 0.5148026315789473, 0.5049342105263158, 0.5164473684210527, 0.5016447368421053, 0.5098684210526315, 0.5279605263157895, 0.5, 0.5263157894736842, 0.5230263157894737, 0.5296052631578947, 0.5230263157894737, 0.5263157894736842, 0.5296052631578947, 0.5213815789473685, 0.5213815789473685, 0.5197368421052632, 0.5180921052631579, 0.5180921052631579, 0.5197368421052632, 0.5180921052631579, 0.5180921052631579, 0.5180921052631579, 0.5180921052631579, 0.5180921052631579, 0.5180921052631579, 0.5180921052631579, 0.5180921052631579, 0.5180921052631579, 0.5180921052631579, 0.5180921052631579, 0.5180921052631579, 0.5180921052631579, 0.5180921052631579] Best val loss: 6.950603246688843


----- Training alexnet with sequence: [0.8, 1] -----
Current group: 0.8
Epoch [1/75], Training Loss: 40.7202, Validation Loss Current: 10.1311, Validation Loss AVG: 10.1311, lr: 0.1
Epoch [2/75], Training Loss: 40.0751, Validation Loss Current: 10.0959, Validation Loss AVG: 10.0959, lr: 0.1
Epoch [3/75], Training Loss: 40.1842, Validation Loss Current: 10.2315, Validation Loss AVG: 10.2315, lr: 0.1
Epoch [4/75], Training Loss: 39.8420, Validation Loss Current: 10.1331, Validation Loss AVG: 10.1331, lr: 0.1
Epoch [5/75], Training Loss: 38.8165, Validation Loss Current: 9.4443, Validation Loss AVG: 9.4443, lr: 0.1
Epoch [6/75], Training Loss: 35.4470, Validation Loss Current: 10.1328, Validation Loss AVG: 10.1328, lr: 0.1
Epoch [7/75], Training Loss: 40.9229, Validation Loss Current: 9.5597, Validation Loss AVG: 9.5597, lr: 0.1
Epoch [8/75], Training Loss: 34.6714, Validation Loss Current: 10.3530, Validation Loss AVG: 10.3530, lr: 0.1
Epoch [9/75], Training Loss: 35.2231, Validation Loss Current: 9.8454, Validation Loss AVG: 9.8454, lr: 0.1
Epoch [10/75], Training Loss: 35.4441, Validation Loss Current: 9.2536, Validation Loss AVG: 9.2536, lr: 0.1
Epoch [11/75], Training Loss: 34.3242, Validation Loss Current: 9.8763, Validation Loss AVG: 9.8763, lr: 0.1
Epoch [12/75], Training Loss: 33.4005, Validation Loss Current: 9.9553, Validation Loss AVG: 9.9553, lr: 0.1
Epoch [13/75], Training Loss: 34.0029, Validation Loss Current: 10.4620, Validation Loss AVG: 10.4620, lr: 0.1
Epoch [14/75], Training Loss: 34.9221, Validation Loss Current: 9.1620, Validation Loss AVG: 9.1620, lr: 0.1
Epoch [15/75], Training Loss: 33.6909, Validation Loss Current: 10.1487, Validation Loss AVG: 10.1487, lr: 0.1
Epoch [16/75], Training Loss: 35.8994, Validation Loss Current: 9.5258, Validation Loss AVG: 9.5258, lr: 0.1
Epoch [17/75], Training Loss: 36.1429, Validation Loss Current: 9.6484, Validation Loss AVG: 9.6484, lr: 0.1
Epoch [18/75], Training Loss: 32.1609, Validation Loss Current: 12.5848, Validation Loss AVG: 12.5848, lr: 0.1
Epoch [19/75], Training Loss: 36.3948, Validation Loss Current: 9.9989, Validation Loss AVG: 9.9989, lr: 0.1
Epoch [20/75], Training Loss: 34.5702, Validation Loss Current: 9.1302, Validation Loss AVG: 9.1302, lr: 0.1
Epoch [21/75], Training Loss: 33.1036, Validation Loss Current: 10.1663, Validation Loss AVG: 10.1663, lr: 0.1
Epoch [22/75], Training Loss: 33.2322, Validation Loss Current: 11.7284, Validation Loss AVG: 11.7284, lr: 0.1
Epoch [23/75], Training Loss: 32.9663, Validation Loss Current: 10.0820, Validation Loss AVG: 10.0820, lr: 0.1
Epoch [24/75], Training Loss: 34.8730, Validation Loss Current: 9.7121, Validation Loss AVG: 9.7121, lr: 0.1
Epoch [25/75], Training Loss: 32.8716, Validation Loss Current: 9.3773, Validation Loss AVG: 9.3773, lr: 0.1
Epoch [26/75], Training Loss: 31.6110, Validation Loss Current: 9.6086, Validation Loss AVG: 9.6086, lr: 0.1
Epoch [27/75], Training Loss: 30.5334, Validation Loss Current: 9.0319, Validation Loss AVG: 9.0319, lr: 0.010000000000000002
Epoch [28/75], Training Loss: 28.1922, Validation Loss Current: 8.7737, Validation Loss AVG: 8.7737, lr: 0.010000000000000002
Epoch [29/75], Training Loss: 26.3407, Validation Loss Current: 8.8941, Validation Loss AVG: 8.8941, lr: 0.010000000000000002
Epoch [30/75], Training Loss: 27.0147, Validation Loss Current: 9.0618, Validation Loss AVG: 9.0618, lr: 0.010000000000000002
Epoch [31/75], Training Loss: 25.6774, Validation Loss Current: 9.5983, Validation Loss AVG: 9.5983, lr: 0.010000000000000002
Epoch [32/75], Training Loss: 25.7822, Validation Loss Current: 8.8939, Validation Loss AVG: 8.8939, lr: 0.010000000000000002
Epoch [33/75], Training Loss: 25.3591, Validation Loss Current: 8.9853, Validation Loss AVG: 8.9853, lr: 0.010000000000000002
Epoch [34/75], Training Loss: 24.9915, Validation Loss Current: 9.1396, Validation Loss AVG: 9.1396, lr: 0.010000000000000002
Epoch [35/75], Training Loss: 24.3801, Validation Loss Current: 8.9479, Validation Loss AVG: 8.9479, lr: 0.0010000000000000002
Epoch [36/75], Training Loss: 23.7738, Validation Loss Current: 9.0980, Validation Loss AVG: 9.0980, lr: 0.0010000000000000002
Epoch [37/75], Training Loss: 24.0692, Validation Loss Current: 9.0410, Validation Loss AVG: 9.0410, lr: 0.0010000000000000002
Epoch [38/75], Training Loss: 23.5204, Validation Loss Current: 9.0706, Validation Loss AVG: 9.0706, lr: 0.0010000000000000002
Epoch [39/75], Training Loss: 23.4881, Validation Loss Current: 9.1112, Validation Loss AVG: 9.1112, lr: 0.0010000000000000002
Epoch [40/75], Training Loss: 23.5244, Validation Loss Current: 9.0463, Validation Loss AVG: 9.0463, lr: 0.0010000000000000002
Epoch [41/75], Training Loss: 22.3985, Validation Loss Current: 9.0546, Validation Loss AVG: 9.0546, lr: 0.00010000000000000003
Epoch [42/75], Training Loss: 23.3782, Validation Loss Current: 9.0907, Validation Loss AVG: 9.0907, lr: 0.00010000000000000003
Epoch [43/75], Training Loss: 22.4268, Validation Loss Current: 9.0910, Validation Loss AVG: 9.0910, lr: 0.00010000000000000003
Epoch [44/75], Training Loss: 23.6424, Validation Loss Current: 9.0970, Validation Loss AVG: 9.0970, lr: 0.00010000000000000003
Epoch [45/75], Training Loss: 23.0321, Validation Loss Current: 9.0974, Validation Loss AVG: 9.0974, lr: 0.00010000000000000003
Epoch [46/75], Training Loss: 22.8216, Validation Loss Current: 9.0718, Validation Loss AVG: 9.0718, lr: 0.00010000000000000003
Epoch [47/75], Training Loss: 22.9543, Validation Loss Current: 9.1131, Validation Loss AVG: 9.1131, lr: 1.0000000000000004e-05
Epoch [48/75], Training Loss: 22.3320, Validation Loss Current: 9.1029, Validation Loss AVG: 9.1029, lr: 1.0000000000000004e-05
Epoch [49/75], Training Loss: 23.1397, Validation Loss Current: 9.1129, Validation Loss AVG: 9.1129, lr: 1.0000000000000004e-05
Epoch [50/75], Training Loss: 22.8404, Validation Loss Current: 9.0979, Validation Loss AVG: 9.0979, lr: 1.0000000000000004e-05
Epoch [51/75], Training Loss: 23.0649, Validation Loss Current: 9.0999, Validation Loss AVG: 9.0999, lr: 1.0000000000000004e-05
Epoch [52/75], Training Loss: 23.0147, Validation Loss Current: 9.0774, Validation Loss AVG: 9.0774, lr: 1.0000000000000004e-05
Epoch [53/75], Training Loss: 22.8409, Validation Loss Current: 9.0807, Validation Loss AVG: 9.0807, lr: 1.0000000000000004e-06
Epoch [54/75], Training Loss: 23.2096, Validation Loss Current: 9.1121, Validation Loss AVG: 9.1121, lr: 1.0000000000000004e-06
Epoch [55/75], Training Loss: 22.3219, Validation Loss Current: 9.0822, Validation Loss AVG: 9.0822, lr: 1.0000000000000004e-06
Epoch [56/75], Training Loss: 22.7183, Validation Loss Current: 9.0820, Validation Loss AVG: 9.0820, lr: 1.0000000000000004e-06
Epoch [57/75], Training Loss: 23.3092, Validation Loss Current: 9.0902, Validation Loss AVG: 9.0902, lr: 1.0000000000000004e-06
Epoch [58/75], Training Loss: 22.5060, Validation Loss Current: 9.0773, Validation Loss AVG: 9.0773, lr: 1.0000000000000004e-06
 --- Early Stopped ---
Patch distance: 0.8 finished training. Best epoch: 28 Best val accuracy: [0.2302631578947368, 0.2302631578947368, 0.18618421052631579, 0.2302631578947368, 0.3226973684210527, 0.3072368421052632, 0.2674342105263158, 0.3115131578947368, 0.28421052631578947, 0.325, 0.2690789473684211, 0.3125, 0.2756578947368421, 0.36381578947368426, 0.2651315789473684, 0.32203947368421054, 0.33157894736842103, 0.3368421052631579, 0.3016447368421053, 0.34605263157894733, 0.3394736842105263, 0.2569078947368421, 0.3338815789473684, 0.2878289473684211, 0.29769736842105265, 0.3190789473684211, 0.3651315789473684, 0.3585526315789474, 0.3674342105263158, 0.3513157894736842, 0.3769736842105263, 0.3832236842105263, 0.38190789473684206, 0.3921052631578948, 0.38618421052631574, 0.3898026315789474, 0.38585526315789476, 0.39144736842105265, 0.3871710526315789, 0.3848684210526316, 0.38848684210526313, 0.38815789473684215, 0.38782894736842105, 0.38717105263157897, 0.38717105263157897, 0.38684210526315793, 0.38684210526315793, 0.38651315789473684, 0.38651315789473684, 0.38750000000000007, 0.38750000000000007, 0.38717105263157897, 0.38717105263157897, 0.38684210526315793, 0.38750000000000007, 0.38717105263157897, 0.38750000000000007, 0.38782894736842105] Best val loss: 8.77374575138092


Loaded best state dict for [0.8]
Current group: 1
Epoch [1/75], Training Loss: 31.7968, Validation Loss Current: 8.0692, Validation Loss AVG: 8.9944, lr: 0.1
Epoch [2/75], Training Loss: 29.9037, Validation Loss Current: 9.6716, Validation Loss AVG: 10.6667, lr: 0.1
Epoch [3/75], Training Loss: 32.4733, Validation Loss Current: 8.3701, Validation Loss AVG: 9.3982, lr: 0.1
Epoch [4/75], Training Loss: 30.3561, Validation Loss Current: 8.7270, Validation Loss AVG: 9.7544, lr: 0.1
Epoch [5/75], Training Loss: 31.3738, Validation Loss Current: 8.6515, Validation Loss AVG: 9.3764, lr: 0.1
Epoch [6/75], Training Loss: 32.3751, Validation Loss Current: 8.7884, Validation Loss AVG: 9.5417, lr: 0.1
Epoch [7/75], Training Loss: 31.8077, Validation Loss Current: 9.1200, Validation Loss AVG: 9.8340, lr: 0.1
Epoch [8/75], Training Loss: 30.6436, Validation Loss Current: 7.8697, Validation Loss AVG: 8.8937, lr: 0.010000000000000002
Epoch [9/75], Training Loss: 27.1168, Validation Loss Current: 7.7097, Validation Loss AVG: 8.9238, lr: 0.010000000000000002
Epoch [10/75], Training Loss: 26.6176, Validation Loss Current: 7.6618, Validation Loss AVG: 8.7825, lr: 0.010000000000000002
Epoch [11/75], Training Loss: 25.6808, Validation Loss Current: 7.4240, Validation Loss AVG: 8.6826, lr: 0.010000000000000002
Epoch [12/75], Training Loss: 24.9966, Validation Loss Current: 7.5920, Validation Loss AVG: 8.7449, lr: 0.010000000000000002
Epoch [13/75], Training Loss: 27.0203, Validation Loss Current: 7.6015, Validation Loss AVG: 8.8834, lr: 0.010000000000000002
Epoch [14/75], Training Loss: 24.0210, Validation Loss Current: 7.4718, Validation Loss AVG: 8.7061, lr: 0.010000000000000002
Epoch [15/75], Training Loss: 25.5712, Validation Loss Current: 7.5521, Validation Loss AVG: 9.0118, lr: 0.010000000000000002
Epoch [16/75], Training Loss: 23.4690, Validation Loss Current: 7.6987, Validation Loss AVG: 9.3055, lr: 0.010000000000000002
Epoch [17/75], Training Loss: 23.3543, Validation Loss Current: 7.7493, Validation Loss AVG: 9.1493, lr: 0.010000000000000002
Epoch [18/75], Training Loss: 21.2516, Validation Loss Current: 7.5028, Validation Loss AVG: 8.8531, lr: 0.0010000000000000002
Epoch [19/75], Training Loss: 22.9727, Validation Loss Current: 7.5460, Validation Loss AVG: 8.9168, lr: 0.0010000000000000002
Epoch [20/75], Training Loss: 22.4357, Validation Loss Current: 7.4700, Validation Loss AVG: 8.8301, lr: 0.0010000000000000002
Epoch [21/75], Training Loss: 24.1260, Validation Loss Current: 7.5207, Validation Loss AVG: 8.9230, lr: 0.0010000000000000002
Epoch [22/75], Training Loss: 23.6862, Validation Loss Current: 7.4602, Validation Loss AVG: 8.9071, lr: 0.0010000000000000002
Epoch [23/75], Training Loss: 20.8099, Validation Loss Current: 7.5807, Validation Loss AVG: 8.9364, lr: 0.0010000000000000002
Epoch [24/75], Training Loss: 21.4417, Validation Loss Current: 7.5315, Validation Loss AVG: 8.9459, lr: 0.00010000000000000003
Epoch [25/75], Training Loss: 21.4690, Validation Loss Current: 7.5352, Validation Loss AVG: 8.9105, lr: 0.00010000000000000003
Epoch [26/75], Training Loss: 21.2949, Validation Loss Current: 7.5509, Validation Loss AVG: 8.9544, lr: 0.00010000000000000003
Epoch [27/75], Training Loss: 21.3203, Validation Loss Current: 7.6000, Validation Loss AVG: 8.9493, lr: 0.00010000000000000003
Epoch [28/75], Training Loss: 20.7103, Validation Loss Current: 7.5198, Validation Loss AVG: 8.9289, lr: 0.00010000000000000003
Epoch [29/75], Training Loss: 23.3029, Validation Loss Current: 7.5174, Validation Loss AVG: 8.9103, lr: 0.00010000000000000003
Epoch [30/75], Training Loss: 20.7110, Validation Loss Current: 7.5571, Validation Loss AVG: 8.9215, lr: 1.0000000000000004e-05
Epoch [31/75], Training Loss: 21.1181, Validation Loss Current: 7.6311, Validation Loss AVG: 8.9493, lr: 1.0000000000000004e-05
Epoch [32/75], Training Loss: 21.1947, Validation Loss Current: 7.5320, Validation Loss AVG: 8.9359, lr: 1.0000000000000004e-05
Epoch [33/75], Training Loss: 22.5296, Validation Loss Current: 7.4735, Validation Loss AVG: 8.9412, lr: 1.0000000000000004e-05
Epoch [34/75], Training Loss: 21.3275, Validation Loss Current: 7.5682, Validation Loss AVG: 8.9528, lr: 1.0000000000000004e-05
Epoch [35/75], Training Loss: 21.5492, Validation Loss Current: 7.5681, Validation Loss AVG: 8.9353, lr: 1.0000000000000004e-05
Epoch [36/75], Training Loss: 20.3768, Validation Loss Current: 7.5668, Validation Loss AVG: 8.9407, lr: 1.0000000000000004e-06
Epoch [37/75], Training Loss: 20.9983, Validation Loss Current: 7.5794, Validation Loss AVG: 8.9446, lr: 1.0000000000000004e-06
Epoch [38/75], Training Loss: 20.7966, Validation Loss Current: 7.5533, Validation Loss AVG: 8.9339, lr: 1.0000000000000004e-06
Epoch [39/75], Training Loss: 20.2634, Validation Loss Current: 7.5356, Validation Loss AVG: 8.9423, lr: 1.0000000000000004e-06
Epoch [40/75], Training Loss: 21.2091, Validation Loss Current: 7.5445, Validation Loss AVG: 8.9356, lr: 1.0000000000000004e-06
Epoch [41/75], Training Loss: 20.4412, Validation Loss Current: 7.5230, Validation Loss AVG: 8.9340, lr: 1.0000000000000004e-06
 --- Early Stopped ---
Patch distance: 1 finished training. Best epoch: 11 Best val accuracy: [0.41118421052631576, 0.29276315789473684, 0.4128289473684211, 0.41776315789473684, 0.3996710526315789, 0.34868421052631576, 0.3684210526315789, 0.4128289473684211, 0.4243421052631579, 0.43585526315789475, 0.4440789473684211, 0.4375, 0.43585526315789475, 0.4769736842105263, 0.46381578947368424, 0.44901315789473684, 0.47368421052631576, 0.4654605263157895, 0.4753289473684211, 0.47039473684210525, 0.46710526315789475, 0.4605263157894737, 0.4720394736842105, 0.4753289473684211, 0.4720394736842105, 0.46875, 0.4720394736842105, 0.4720394736842105, 0.47039473684210525, 0.47039473684210525, 0.47039473684210525, 0.47039473684210525, 0.47039473684210525, 0.47039473684210525, 0.4720394736842105, 0.4720394736842105, 0.4720394736842105, 0.4720394736842105, 0.4720394736842105, 0.4720394736842105, 0.4720394736842105] Best val loss: 7.423995137214661


----- Training alexnet with sequence: [1] -----
Current group: 1
Epoch [1/150], Training Loss: 40.5854, Validation Loss Current: 10.0784, Validation Loss AVG: 10.2940, lr: 0.1
Epoch [2/150], Training Loss: 39.7994, Validation Loss Current: 11.3058, Validation Loss AVG: 14.6154, lr: 0.1
Epoch [3/150], Training Loss: 40.0698, Validation Loss Current: 10.1502, Validation Loss AVG: 10.1628, lr: 0.1
Epoch [4/150], Training Loss: 39.7842, Validation Loss Current: 11.1525, Validation Loss AVG: 10.9233, lr: 0.1
Epoch [5/150], Training Loss: 38.7594, Validation Loss Current: 10.7641, Validation Loss AVG: 10.8125, lr: 0.1
Epoch [6/150], Training Loss: 39.7053, Validation Loss Current: 11.6787, Validation Loss AVG: 11.6067, lr: 0.1
Epoch [7/150], Training Loss: 37.9678, Validation Loss Current: 8.8511, Validation Loss AVG: 9.4151, lr: 0.1
Epoch [8/150], Training Loss: 36.2367, Validation Loss Current: 8.8587, Validation Loss AVG: 10.2088, lr: 0.1
Epoch [9/150], Training Loss: 34.6548, Validation Loss Current: 9.1312, Validation Loss AVG: 10.1698, lr: 0.1
Epoch [10/150], Training Loss: 33.9436, Validation Loss Current: 9.3364, Validation Loss AVG: 10.1899, lr: 0.1
Epoch [11/150], Training Loss: 36.8892, Validation Loss Current: 9.4128, Validation Loss AVG: 10.0558, lr: 0.1
Epoch [12/150], Training Loss: 33.6890, Validation Loss Current: 8.5781, Validation Loss AVG: 9.5997, lr: 0.1
Epoch [13/150], Training Loss: 35.2628, Validation Loss Current: 14.7237, Validation Loss AVG: 15.6516, lr: 0.1
Epoch [14/150], Training Loss: 37.5623, Validation Loss Current: 8.6019, Validation Loss AVG: 9.4353, lr: 0.1
Epoch [15/150], Training Loss: 32.2762, Validation Loss Current: 9.3703, Validation Loss AVG: 10.2069, lr: 0.1
Epoch [16/150], Training Loss: 34.2557, Validation Loss Current: 8.9685, Validation Loss AVG: 10.0918, lr: 0.1
Epoch [17/150], Training Loss: 31.2590, Validation Loss Current: 9.2682, Validation Loss AVG: 9.7752, lr: 0.1
Epoch [18/150], Training Loss: 33.3892, Validation Loss Current: 9.5863, Validation Loss AVG: 11.7348, lr: 0.1
Epoch [19/150], Training Loss: 30.3381, Validation Loss Current: 7.8510, Validation Loss AVG: 8.8160, lr: 0.010000000000000002
Epoch [20/150], Training Loss: 27.4516, Validation Loss Current: 7.5251, Validation Loss AVG: 8.9165, lr: 0.010000000000000002
Epoch [21/150], Training Loss: 26.2514, Validation Loss Current: 7.5227, Validation Loss AVG: 8.8133, lr: 0.010000000000000002
Epoch [22/150], Training Loss: 25.7327, Validation Loss Current: 7.4646, Validation Loss AVG: 8.7800, lr: 0.010000000000000002
Epoch [23/150], Training Loss: 24.6303, Validation Loss Current: 7.4589, Validation Loss AVG: 9.1534, lr: 0.010000000000000002
Epoch [24/150], Training Loss: 24.3174, Validation Loss Current: 7.4321, Validation Loss AVG: 9.5558, lr: 0.010000000000000002
Epoch [25/150], Training Loss: 25.0549, Validation Loss Current: 7.1747, Validation Loss AVG: 9.0687, lr: 0.010000000000000002
Epoch [26/150], Training Loss: 22.8385, Validation Loss Current: 7.2368, Validation Loss AVG: 9.2505, lr: 0.010000000000000002
Epoch [27/150], Training Loss: 22.4196, Validation Loss Current: 7.4243, Validation Loss AVG: 9.5843, lr: 0.010000000000000002
Epoch [28/150], Training Loss: 21.1546, Validation Loss Current: 7.2932, Validation Loss AVG: 9.2320, lr: 0.010000000000000002
Epoch [29/150], Training Loss: 21.7367, Validation Loss Current: 7.3624, Validation Loss AVG: 9.5452, lr: 0.010000000000000002
Epoch [30/150], Training Loss: 20.6388, Validation Loss Current: 7.5835, Validation Loss AVG: 9.4188, lr: 0.010000000000000002
Epoch [31/150], Training Loss: 20.8076, Validation Loss Current: 7.0805, Validation Loss AVG: 9.6286, lr: 0.010000000000000002
Epoch [32/150], Training Loss: 19.5633, Validation Loss Current: 7.0930, Validation Loss AVG: 9.4556, lr: 0.010000000000000002
Epoch [33/150], Training Loss: 18.7917, Validation Loss Current: 7.4054, Validation Loss AVG: 10.0917, lr: 0.010000000000000002
Epoch [34/150], Training Loss: 18.5377, Validation Loss Current: 7.1121, Validation Loss AVG: 10.2564, lr: 0.010000000000000002
Epoch [35/150], Training Loss: 18.3160, Validation Loss Current: 7.6772, Validation Loss AVG: 9.9835, lr: 0.010000000000000002
Epoch [36/150], Training Loss: 17.2003, Validation Loss Current: 7.3300, Validation Loss AVG: 10.1381, lr: 0.010000000000000002
Epoch [37/150], Training Loss: 16.1991, Validation Loss Current: 7.2836, Validation Loss AVG: 10.5056, lr: 0.010000000000000002
Epoch [38/150], Training Loss: 14.7140, Validation Loss Current: 7.0920, Validation Loss AVG: 10.3329, lr: 0.0010000000000000002
Epoch [39/150], Training Loss: 13.8341, Validation Loss Current: 7.2610, Validation Loss AVG: 10.6800, lr: 0.0010000000000000002
Epoch [40/150], Training Loss: 13.8890, Validation Loss Current: 7.4719, Validation Loss AVG: 10.7741, lr: 0.0010000000000000002
Epoch [41/150], Training Loss: 14.0961, Validation Loss Current: 7.3671, Validation Loss AVG: 11.0895, lr: 0.0010000000000000002
Epoch [42/150], Training Loss: 13.7505, Validation Loss Current: 7.4162, Validation Loss AVG: 11.3883, lr: 0.0010000000000000002
Epoch [43/150], Training Loss: 13.3025, Validation Loss Current: 7.3422, Validation Loss AVG: 11.1234, lr: 0.0010000000000000002
Epoch [44/150], Training Loss: 13.1282, Validation Loss Current: 7.3425, Validation Loss AVG: 11.0936, lr: 0.00010000000000000003
Epoch [45/150], Training Loss: 13.2653, Validation Loss Current: 7.3363, Validation Loss AVG: 11.0890, lr: 0.00010000000000000003
Epoch [46/150], Training Loss: 13.1180, Validation Loss Current: 7.3642, Validation Loss AVG: 11.1394, lr: 0.00010000000000000003
Epoch [47/150], Training Loss: 12.6555, Validation Loss Current: 7.4826, Validation Loss AVG: 11.2151, lr: 0.00010000000000000003
Epoch [48/150], Training Loss: 13.1623, Validation Loss Current: 7.4266, Validation Loss AVG: 11.2097, lr: 0.00010000000000000003
Epoch [49/150], Training Loss: 14.5576, Validation Loss Current: 7.4192, Validation Loss AVG: 11.1684, lr: 0.00010000000000000003
Epoch [50/150], Training Loss: 12.9778, Validation Loss Current: 7.4313, Validation Loss AVG: 11.1927, lr: 1.0000000000000004e-05
Epoch [51/150], Training Loss: 13.5481, Validation Loss Current: 7.4975, Validation Loss AVG: 11.2092, lr: 1.0000000000000004e-05
Epoch [52/150], Training Loss: 14.2438, Validation Loss Current: 7.3628, Validation Loss AVG: 11.1900, lr: 1.0000000000000004e-05
Epoch [53/150], Training Loss: 14.0728, Validation Loss Current: 7.5087, Validation Loss AVG: 11.2341, lr: 1.0000000000000004e-05
Epoch [54/150], Training Loss: 12.8414, Validation Loss Current: 7.4667, Validation Loss AVG: 11.1951, lr: 1.0000000000000004e-05
Epoch [55/150], Training Loss: 14.5858, Validation Loss Current: 7.4836, Validation Loss AVG: 11.2452, lr: 1.0000000000000004e-05
Epoch [56/150], Training Loss: 12.9519, Validation Loss Current: 7.4484, Validation Loss AVG: 11.2299, lr: 1.0000000000000004e-06
Epoch [57/150], Training Loss: 13.0583, Validation Loss Current: 7.4307, Validation Loss AVG: 11.2293, lr: 1.0000000000000004e-06
Epoch [58/150], Training Loss: 12.9983, Validation Loss Current: 7.4517, Validation Loss AVG: 11.2424, lr: 1.0000000000000004e-06
Epoch [59/150], Training Loss: 13.2096, Validation Loss Current: 7.4296, Validation Loss AVG: 11.2282, lr: 1.0000000000000004e-06
Epoch [60/150], Training Loss: 13.8643, Validation Loss Current: 7.5117, Validation Loss AVG: 11.2228, lr: 1.0000000000000004e-06
Epoch [61/150], Training Loss: 12.9584, Validation Loss Current: 7.4198, Validation Loss AVG: 11.2010, lr: 1.0000000000000004e-06
 --- Early Stopped ---
Patch distance: 1 finished training. Best epoch: 31 Best val accuracy: [0.21217105263157895, 0.23026315789473684, 0.23026315789473684, 0.19407894736842105, 0.23026315789473684, 0.16940789473684212, 0.3618421052631579, 0.33881578947368424, 0.37335526315789475, 0.3338815789473684, 0.2894736842105263, 0.38980263157894735, 0.2532894736842105, 0.3651315789473684, 0.2532894736842105, 0.3190789473684211, 0.3832236842105263, 0.2944078947368421, 0.4292763157894737, 0.4555921052631579, 0.4654605263157895, 0.4753289473684211, 0.4786184210526316, 0.48519736842105265, 0.48026315789473684, 0.47039473684210525, 0.48848684210526316, 0.4967105263157895, 0.5032894736842105, 0.5032894736842105, 0.5164473684210527, 0.5164473684210527, 0.5, 0.5230263157894737, 0.5263157894736842, 0.53125, 0.53125, 0.5493421052631579, 0.5592105263157895, 0.5394736842105263, 0.5608552631578947, 0.5608552631578947, 0.5476973684210527, 0.5493421052631579, 0.5493421052631579, 0.5526315789473685, 0.5542763157894737, 0.5526315789473685, 0.5575657894736842, 0.5542763157894737, 0.5542763157894737, 0.555921052631579, 0.555921052631579, 0.555921052631579, 0.555921052631579, 0.555921052631579, 0.555921052631579, 0.555921052631579, 0.555921052631579, 0.555921052631579, 0.555921052631579] Best val loss: 7.080525517463684


Fold: 3
----- Training alexnet with sequence: [0.2, 0.4, 0.6, 0.8, 1] -----
Current group: 0.2
Epoch [1/30], Training Loss: 40.2246, Validation Loss Current: 10.0538, Validation Loss AVG: 10.0538, lr: 0.1
Epoch [2/30], Training Loss: 39.6634, Validation Loss Current: 10.0451, Validation Loss AVG: 10.0451, lr: 0.1
Epoch [3/30], Training Loss: 39.9481, Validation Loss Current: 10.0440, Validation Loss AVG: 10.0440, lr: 0.1
Epoch [4/30], Training Loss: 39.9591, Validation Loss Current: 10.0683, Validation Loss AVG: 10.0683, lr: 0.1
Epoch [5/30], Training Loss: 40.3183, Validation Loss Current: 10.0592, Validation Loss AVG: 10.0592, lr: 0.1
Epoch [6/30], Training Loss: 40.2441, Validation Loss Current: 10.0015, Validation Loss AVG: 10.0015, lr: 0.1
Epoch [7/30], Training Loss: 39.6795, Validation Loss Current: 10.0938, Validation Loss AVG: 10.0938, lr: 0.1
Epoch [8/30], Training Loss: 39.2280, Validation Loss Current: 10.0462, Validation Loss AVG: 10.0462, lr: 0.1
Epoch [9/30], Training Loss: 37.3624, Validation Loss Current: 9.8454, Validation Loss AVG: 9.8454, lr: 0.1
Epoch [10/30], Training Loss: 36.7942, Validation Loss Current: 10.5416, Validation Loss AVG: 10.5416, lr: 0.1
Epoch [11/30], Training Loss: 37.3678, Validation Loss Current: 9.5187, Validation Loss AVG: 9.5187, lr: 0.1
Epoch [12/30], Training Loss: 35.4245, Validation Loss Current: 9.8473, Validation Loss AVG: 9.8473, lr: 0.1
Epoch [13/30], Training Loss: 36.3961, Validation Loss Current: 9.8339, Validation Loss AVG: 9.8339, lr: 0.1
Epoch [14/30], Training Loss: 34.7579, Validation Loss Current: 10.2864, Validation Loss AVG: 10.2864, lr: 0.1
Epoch [15/30], Training Loss: 34.8206, Validation Loss Current: 10.4726, Validation Loss AVG: 10.4726, lr: 0.1
Epoch [16/30], Training Loss: 35.3940, Validation Loss Current: 9.4533, Validation Loss AVG: 9.4533, lr: 0.1
Epoch [17/30], Training Loss: 33.3004, Validation Loss Current: 11.9238, Validation Loss AVG: 11.9238, lr: 0.1
Epoch [18/30], Training Loss: 40.2269, Validation Loss Current: 12.6415, Validation Loss AVG: 12.6415, lr: 0.1
Epoch [19/30], Training Loss: 37.4618, Validation Loss Current: 10.1483, Validation Loss AVG: 10.1483, lr: 0.1
Epoch [20/30], Training Loss: 35.1653, Validation Loss Current: 9.5422, Validation Loss AVG: 9.5422, lr: 0.1
Epoch [21/30], Training Loss: 34.5107, Validation Loss Current: 9.9541, Validation Loss AVG: 9.9541, lr: 0.1
Epoch [22/30], Training Loss: 32.6799, Validation Loss Current: 10.1458, Validation Loss AVG: 10.1458, lr: 0.1
Epoch [23/30], Training Loss: 31.6156, Validation Loss Current: 9.6162, Validation Loss AVG: 9.6162, lr: 0.010000000000000002
Epoch [24/30], Training Loss: 30.9777, Validation Loss Current: 9.5604, Validation Loss AVG: 9.5604, lr: 0.010000000000000002
Epoch [25/30], Training Loss: 30.0782, Validation Loss Current: 9.6226, Validation Loss AVG: 9.6226, lr: 0.010000000000000002
Epoch [26/30], Training Loss: 29.6070, Validation Loss Current: 9.9823, Validation Loss AVG: 9.9823, lr: 0.010000000000000002
Epoch [27/30], Training Loss: 28.3673, Validation Loss Current: 9.9666, Validation Loss AVG: 9.9666, lr: 0.010000000000000002
Epoch [28/30], Training Loss: 28.1329, Validation Loss Current: 10.1808, Validation Loss AVG: 10.1808, lr: 0.010000000000000002
Epoch [29/30], Training Loss: 27.7568, Validation Loss Current: 10.1319, Validation Loss AVG: 10.1319, lr: 0.0010000000000000002
Epoch [30/30], Training Loss: 26.6143, Validation Loss Current: 10.1991, Validation Loss AVG: 10.1991, lr: 0.0010000000000000002
Epoch [31/30], Training Loss: 26.8121, Validation Loss Current: 10.1365, Validation Loss AVG: 10.1365, lr: 0.0010000000000000002
Epoch [32/30], Training Loss: 26.1052, Validation Loss Current: 10.2025, Validation Loss AVG: 10.2025, lr: 0.0010000000000000002
Epoch [33/30], Training Loss: 26.7383, Validation Loss Current: 10.2484, Validation Loss AVG: 10.2484, lr: 0.0010000000000000002
Epoch [34/30], Training Loss: 26.7277, Validation Loss Current: 10.2976, Validation Loss AVG: 10.2976, lr: 0.0010000000000000002
Epoch [35/30], Training Loss: 26.8251, Validation Loss Current: 10.3006, Validation Loss AVG: 10.3006, lr: 0.00010000000000000003
Epoch [36/30], Training Loss: 25.8760, Validation Loss Current: 10.2776, Validation Loss AVG: 10.2776, lr: 0.00010000000000000003
Epoch [37/30], Training Loss: 25.9369, Validation Loss Current: 10.2786, Validation Loss AVG: 10.2786, lr: 0.00010000000000000003
Epoch [38/30], Training Loss: 26.7966, Validation Loss Current: 10.2777, Validation Loss AVG: 10.2777, lr: 0.00010000000000000003
Epoch [39/30], Training Loss: 26.0442, Validation Loss Current: 10.2984, Validation Loss AVG: 10.2984, lr: 0.00010000000000000003
Epoch [40/30], Training Loss: 26.6823, Validation Loss Current: 10.3045, Validation Loss AVG: 10.3045, lr: 0.00010000000000000003
Epoch [41/30], Training Loss: 27.1964, Validation Loss Current: 10.2795, Validation Loss AVG: 10.2795, lr: 1.0000000000000004e-05
Epoch [42/30], Training Loss: 26.0349, Validation Loss Current: 10.2857, Validation Loss AVG: 10.2857, lr: 1.0000000000000004e-05
Epoch [43/30], Training Loss: 26.4005, Validation Loss Current: 10.2946, Validation Loss AVG: 10.2946, lr: 1.0000000000000004e-05
Epoch [44/30], Training Loss: 27.1846, Validation Loss Current: 10.2784, Validation Loss AVG: 10.2784, lr: 1.0000000000000004e-05
Epoch [45/30], Training Loss: 27.1885, Validation Loss Current: 10.2851, Validation Loss AVG: 10.2851, lr: 1.0000000000000004e-05
Epoch [46/30], Training Loss: 25.7808, Validation Loss Current: 10.2991, Validation Loss AVG: 10.2991, lr: 1.0000000000000004e-05
 --- Early Stopped ---
Patch distance: 0.2 finished training. Best epoch: 16 Best val accuracy: [0.2351973684210526, 0.2351973684210526, 0.2351973684210526, 0.2351973684210526, 0.2351973684210526, 0.2351973684210526, 0.2375, 0.24210526315789474, 0.28421052631578947, 0.2375, 0.3157894736842105, 0.28388157894736843, 0.25756578947368425, 0.2625, 0.2026315789473684, 0.30888157894736845, 0.22664473684210526, 0.17861842105263157, 0.2375, 0.2667763157894737, 0.26809210526315785, 0.28618421052631576, 0.27730263157894736, 0.2875, 0.2713815789473684, 0.2480263157894737, 0.23585526315789473, 0.23651315789473687, 0.23552631578947372, 0.2351973684210526, 0.24046052631578946, 0.2375, 0.23355263157894735, 0.23059210526315793, 0.23190789473684212, 0.23355263157894735, 0.2335526315789474, 0.23322368421052633, 0.23322368421052633, 0.23223684210526319, 0.2325657894736842, 0.2325657894736842, 0.2325657894736842, 0.2325657894736842, 0.2325657894736842, 0.2325657894736842] Best val loss: 9.453303050994872


Loaded best state dict for [0.2]
Current group: 0.4
Epoch [1/30], Training Loss: 36.5515, Validation Loss Current: 9.9319, Validation Loss AVG: 9.9319, lr: 0.1
Epoch [2/30], Training Loss: 35.8660, Validation Loss Current: 9.0614, Validation Loss AVG: 9.0614, lr: 0.1
Epoch [3/30], Training Loss: 36.4399, Validation Loss Current: 9.2446, Validation Loss AVG: 9.2446, lr: 0.1
Epoch [4/30], Training Loss: 35.6356, Validation Loss Current: 9.4338, Validation Loss AVG: 9.4338, lr: 0.1
Epoch [5/30], Training Loss: 36.0567, Validation Loss Current: 9.2235, Validation Loss AVG: 9.2235, lr: 0.1
Epoch [6/30], Training Loss: 35.2332, Validation Loss Current: 13.0287, Validation Loss AVG: 13.0287, lr: 0.1
Epoch [7/30], Training Loss: 35.5099, Validation Loss Current: 9.2616, Validation Loss AVG: 9.2616, lr: 0.1
Epoch [8/30], Training Loss: 34.1612, Validation Loss Current: 9.9486, Validation Loss AVG: 9.9486, lr: 0.1
Epoch [9/30], Training Loss: 34.1237, Validation Loss Current: 8.9200, Validation Loss AVG: 8.9200, lr: 0.010000000000000002
Epoch [10/30], Training Loss: 30.3053, Validation Loss Current: 8.7658, Validation Loss AVG: 8.7658, lr: 0.010000000000000002
Epoch [11/30], Training Loss: 29.5662, Validation Loss Current: 8.5199, Validation Loss AVG: 8.5199, lr: 0.010000000000000002
Epoch [12/30], Training Loss: 27.7636, Validation Loss Current: 8.7191, Validation Loss AVG: 8.7191, lr: 0.010000000000000002
Epoch [13/30], Training Loss: 27.2269, Validation Loss Current: 8.6514, Validation Loss AVG: 8.6514, lr: 0.010000000000000002
Epoch [14/30], Training Loss: 26.0130, Validation Loss Current: 8.7308, Validation Loss AVG: 8.7308, lr: 0.010000000000000002
Epoch [15/30], Training Loss: 25.8589, Validation Loss Current: 8.5253, Validation Loss AVG: 8.5253, lr: 0.010000000000000002
Epoch [16/30], Training Loss: 24.9304, Validation Loss Current: 8.8066, Validation Loss AVG: 8.8066, lr: 0.010000000000000002
Epoch [17/30], Training Loss: 24.3389, Validation Loss Current: 8.3966, Validation Loss AVG: 8.3966, lr: 0.010000000000000002
Epoch [18/30], Training Loss: 24.6951, Validation Loss Current: 8.6055, Validation Loss AVG: 8.6055, lr: 0.010000000000000002
Epoch [19/30], Training Loss: 22.5457, Validation Loss Current: 8.7958, Validation Loss AVG: 8.7958, lr: 0.010000000000000002
Epoch [20/30], Training Loss: 23.2045, Validation Loss Current: 8.5517, Validation Loss AVG: 8.5517, lr: 0.010000000000000002
Epoch [21/30], Training Loss: 23.1066, Validation Loss Current: 8.8636, Validation Loss AVG: 8.8636, lr: 0.010000000000000002
Epoch [22/30], Training Loss: 21.8553, Validation Loss Current: 9.0180, Validation Loss AVG: 9.0180, lr: 0.010000000000000002
Epoch [23/30], Training Loss: 20.0604, Validation Loss Current: 9.6287, Validation Loss AVG: 9.6287, lr: 0.010000000000000002
Epoch [24/30], Training Loss: 21.4395, Validation Loss Current: 9.2455, Validation Loss AVG: 9.2455, lr: 0.0010000000000000002
Epoch [25/30], Training Loss: 18.1070, Validation Loss Current: 9.3226, Validation Loss AVG: 9.3226, lr: 0.0010000000000000002
Epoch [26/30], Training Loss: 18.0024, Validation Loss Current: 9.2704, Validation Loss AVG: 9.2704, lr: 0.0010000000000000002
Epoch [27/30], Training Loss: 18.2685, Validation Loss Current: 9.3980, Validation Loss AVG: 9.3980, lr: 0.0010000000000000002
Epoch [28/30], Training Loss: 17.5172, Validation Loss Current: 9.3026, Validation Loss AVG: 9.3026, lr: 0.0010000000000000002
Epoch [29/30], Training Loss: 18.6593, Validation Loss Current: 9.4198, Validation Loss AVG: 9.4198, lr: 0.0010000000000000002
Epoch [30/30], Training Loss: 16.9068, Validation Loss Current: 9.4173, Validation Loss AVG: 9.4173, lr: 0.00010000000000000003
Epoch [31/30], Training Loss: 16.8818, Validation Loss Current: 9.4581, Validation Loss AVG: 9.4581, lr: 0.00010000000000000003
Epoch [32/30], Training Loss: 17.5885, Validation Loss Current: 9.4634, Validation Loss AVG: 9.4634, lr: 0.00010000000000000003
Epoch [33/30], Training Loss: 17.8562, Validation Loss Current: 9.4362, Validation Loss AVG: 9.4362, lr: 0.00010000000000000003
Epoch [34/30], Training Loss: 17.1637, Validation Loss Current: 9.4640, Validation Loss AVG: 9.4640, lr: 0.00010000000000000003
Epoch [35/30], Training Loss: 17.0207, Validation Loss Current: 9.4866, Validation Loss AVG: 9.4866, lr: 0.00010000000000000003
Epoch [36/30], Training Loss: 17.4881, Validation Loss Current: 9.4559, Validation Loss AVG: 9.4559, lr: 1.0000000000000004e-05
Epoch [37/30], Training Loss: 16.9273, Validation Loss Current: 9.4604, Validation Loss AVG: 9.4604, lr: 1.0000000000000004e-05
Epoch [38/30], Training Loss: 19.3357, Validation Loss Current: 9.4802, Validation Loss AVG: 9.4802, lr: 1.0000000000000004e-05
Epoch [39/30], Training Loss: 16.9744, Validation Loss Current: 9.4642, Validation Loss AVG: 9.4642, lr: 1.0000000000000004e-05
Epoch [40/30], Training Loss: 19.4469, Validation Loss Current: 9.4985, Validation Loss AVG: 9.4985, lr: 1.0000000000000004e-05
Epoch [41/30], Training Loss: 17.7821, Validation Loss Current: 9.4468, Validation Loss AVG: 9.4468, lr: 1.0000000000000004e-05
Epoch [42/30], Training Loss: 17.0506, Validation Loss Current: 9.4689, Validation Loss AVG: 9.4689, lr: 1.0000000000000004e-06
Epoch [43/30], Training Loss: 17.2212, Validation Loss Current: 9.4733, Validation Loss AVG: 9.4733, lr: 1.0000000000000004e-06
Epoch [44/30], Training Loss: 17.8057, Validation Loss Current: 9.4622, Validation Loss AVG: 9.4622, lr: 1.0000000000000004e-06
Epoch [45/30], Training Loss: 17.1448, Validation Loss Current: 9.4682, Validation Loss AVG: 9.4682, lr: 1.0000000000000004e-06
Epoch [46/30], Training Loss: 17.4002, Validation Loss Current: 9.4787, Validation Loss AVG: 9.4787, lr: 1.0000000000000004e-06
Epoch [47/30], Training Loss: 16.8458, Validation Loss Current: 9.5020, Validation Loss AVG: 9.5020, lr: 1.0000000000000004e-06
 --- Early Stopped ---
Patch distance: 0.4 finished training. Best epoch: 17 Best val accuracy: [0.19210526315789472, 0.32401315789473684, 0.32598684210526313, 0.2865131578947368, 0.3279605263157895, 0.2641447368421052, 0.32730263157894735, 0.24078947368421053, 0.3542763157894737, 0.35526315789473684, 0.38190789473684217, 0.39111842105263156, 0.3828947368421053, 0.4009868421052632, 0.3963815789473685, 0.38848684210526313, 0.4115131578947368, 0.4082236842105263, 0.3973684210526316, 0.3996710526315789, 0.4, 0.39671052631578946, 0.40361842105263157, 0.4072368421052632, 0.40690789473684214, 0.4082236842105263, 0.41085526315789467, 0.41085526315789467, 0.41217105263157894, 0.41085526315789467, 0.41217105263157894, 0.4115131578947368, 0.4115131578947368, 0.4101973684210526, 0.4108552631578948, 0.41118421052631576, 0.41118421052631576, 0.4118421052631579, 0.4118421052631579, 0.41052631578947374, 0.41052631578947374, 0.41052631578947374, 0.41019736842105264, 0.41118421052631576, 0.41085526315789467, 0.41085526315789467, 0.41085526315789467] Best val loss: 8.39660930633545


Loaded best state dict for [0.2, 0.4]
Current group: 0.6
Epoch [1/30], Training Loss: 32.7851, Validation Loss Current: 9.0039, Validation Loss AVG: 9.0039, lr: 0.1
Epoch [2/30], Training Loss: 32.2285, Validation Loss Current: 10.5088, Validation Loss AVG: 10.5088, lr: 0.1
Epoch [3/30], Training Loss: 30.8531, Validation Loss Current: 9.1752, Validation Loss AVG: 9.1752, lr: 0.1
Epoch [4/30], Training Loss: 32.4772, Validation Loss Current: 9.8843, Validation Loss AVG: 9.8843, lr: 0.1
Epoch [5/30], Training Loss: 31.7123, Validation Loss Current: 9.5781, Validation Loss AVG: 9.5781, lr: 0.1
Epoch [6/30], Training Loss: 30.6688, Validation Loss Current: 8.8604, Validation Loss AVG: 8.8604, lr: 0.1
Epoch [7/30], Training Loss: 30.7773, Validation Loss Current: 9.7702, Validation Loss AVG: 9.7702, lr: 0.1
Epoch [8/30], Training Loss: 31.8325, Validation Loss Current: 10.7708, Validation Loss AVG: 10.7708, lr: 0.1
Epoch [9/30], Training Loss: 31.5031, Validation Loss Current: 8.7261, Validation Loss AVG: 8.7261, lr: 0.1
Epoch [10/30], Training Loss: 32.2729, Validation Loss Current: 8.5811, Validation Loss AVG: 8.5811, lr: 0.1
Epoch [11/30], Training Loss: 29.2762, Validation Loss Current: 10.3675, Validation Loss AVG: 10.3675, lr: 0.1
Epoch [12/30], Training Loss: 31.2746, Validation Loss Current: 9.0570, Validation Loss AVG: 9.0570, lr: 0.1
Epoch [13/30], Training Loss: 33.1499, Validation Loss Current: 9.0342, Validation Loss AVG: 9.0342, lr: 0.1
Epoch [14/30], Training Loss: 30.7348, Validation Loss Current: 13.3449, Validation Loss AVG: 13.3449, lr: 0.1
Epoch [15/30], Training Loss: 34.7260, Validation Loss Current: 9.5106, Validation Loss AVG: 9.5106, lr: 0.1
Epoch [16/30], Training Loss: 30.9957, Validation Loss Current: 9.2461, Validation Loss AVG: 9.2461, lr: 0.1
Epoch [17/30], Training Loss: 28.2484, Validation Loss Current: 8.1715, Validation Loss AVG: 8.1715, lr: 0.010000000000000002
Epoch [18/30], Training Loss: 24.1036, Validation Loss Current: 8.1714, Validation Loss AVG: 8.1714, lr: 0.010000000000000002
Epoch [19/30], Training Loss: 22.9600, Validation Loss Current: 8.2065, Validation Loss AVG: 8.2065, lr: 0.010000000000000002
Epoch [20/30], Training Loss: 23.2969, Validation Loss Current: 8.2764, Validation Loss AVG: 8.2764, lr: 0.010000000000000002
Epoch [21/30], Training Loss: 21.3286, Validation Loss Current: 8.2673, Validation Loss AVG: 8.2673, lr: 0.010000000000000002
Epoch [22/30], Training Loss: 21.8951, Validation Loss Current: 8.2242, Validation Loss AVG: 8.2242, lr: 0.010000000000000002
Epoch [23/30], Training Loss: 19.2983, Validation Loss Current: 8.4276, Validation Loss AVG: 8.4276, lr: 0.010000000000000002
Epoch [24/30], Training Loss: 18.0676, Validation Loss Current: 8.2781, Validation Loss AVG: 8.2781, lr: 0.0010000000000000002
Epoch [25/30], Training Loss: 18.8575, Validation Loss Current: 8.3741, Validation Loss AVG: 8.3741, lr: 0.0010000000000000002
Epoch [26/30], Training Loss: 17.8217, Validation Loss Current: 8.3580, Validation Loss AVG: 8.3580, lr: 0.0010000000000000002
Epoch [27/30], Training Loss: 17.2352, Validation Loss Current: 8.3686, Validation Loss AVG: 8.3686, lr: 0.0010000000000000002
Epoch [28/30], Training Loss: 17.4576, Validation Loss Current: 8.4075, Validation Loss AVG: 8.4075, lr: 0.0010000000000000002
Epoch [29/30], Training Loss: 16.7190, Validation Loss Current: 8.4466, Validation Loss AVG: 8.4466, lr: 0.0010000000000000002
Epoch [30/30], Training Loss: 17.3813, Validation Loss Current: 8.4526, Validation Loss AVG: 8.4526, lr: 0.00010000000000000003
Epoch [31/30], Training Loss: 17.6833, Validation Loss Current: 8.4721, Validation Loss AVG: 8.4721, lr: 0.00010000000000000003
Epoch [32/30], Training Loss: 17.6184, Validation Loss Current: 8.4529, Validation Loss AVG: 8.4529, lr: 0.00010000000000000003
Epoch [33/30], Training Loss: 17.8113, Validation Loss Current: 8.4544, Validation Loss AVG: 8.4544, lr: 0.00010000000000000003
Epoch [34/30], Training Loss: 16.8191, Validation Loss Current: 8.4445, Validation Loss AVG: 8.4445, lr: 0.00010000000000000003
Epoch [35/30], Training Loss: 17.2805, Validation Loss Current: 8.4569, Validation Loss AVG: 8.4569, lr: 0.00010000000000000003
Epoch [36/30], Training Loss: 16.9977, Validation Loss Current: 8.4471, Validation Loss AVG: 8.4471, lr: 1.0000000000000004e-05
Epoch [37/30], Training Loss: 16.9601, Validation Loss Current: 8.4417, Validation Loss AVG: 8.4417, lr: 1.0000000000000004e-05
Epoch [38/30], Training Loss: 17.5245, Validation Loss Current: 8.4497, Validation Loss AVG: 8.4497, lr: 1.0000000000000004e-05
Epoch [39/30], Training Loss: 16.5969, Validation Loss Current: 8.4513, Validation Loss AVG: 8.4513, lr: 1.0000000000000004e-05
Epoch [40/30], Training Loss: 16.8078, Validation Loss Current: 8.4625, Validation Loss AVG: 8.4625, lr: 1.0000000000000004e-05
Epoch [41/30], Training Loss: 17.5764, Validation Loss Current: 8.4532, Validation Loss AVG: 8.4532, lr: 1.0000000000000004e-05
Epoch [42/30], Training Loss: 16.8046, Validation Loss Current: 8.4205, Validation Loss AVG: 8.4205, lr: 1.0000000000000004e-06
Epoch [43/30], Training Loss: 17.7950, Validation Loss Current: 8.4631, Validation Loss AVG: 8.4631, lr: 1.0000000000000004e-06
Epoch [44/30], Training Loss: 16.6914, Validation Loss Current: 8.4711, Validation Loss AVG: 8.4711, lr: 1.0000000000000004e-06
Epoch [45/30], Training Loss: 17.0779, Validation Loss Current: 8.4577, Validation Loss AVG: 8.4577, lr: 1.0000000000000004e-06
Epoch [46/30], Training Loss: 17.3238, Validation Loss Current: 8.4539, Validation Loss AVG: 8.4539, lr: 1.0000000000000004e-06
Epoch [47/30], Training Loss: 17.7159, Validation Loss Current: 8.4214, Validation Loss AVG: 8.4214, lr: 1.0000000000000004e-06
Epoch [48/30], Training Loss: 17.7568, Validation Loss Current: 8.4341, Validation Loss AVG: 8.4341, lr: 1.0000000000000005e-07
 --- Early Stopped ---
Patch distance: 0.6 finished training. Best epoch: 18 Best val accuracy: [0.3457236842105263, 0.21282894736842106, 0.3598684210526316, 0.22697368421052633, 0.36809210526315794, 0.3286184210526316, 0.3226973684210526, 0.3401315789473684, 0.39210526315789473, 0.37894736842105264, 0.33092105263157895, 0.32500000000000007, 0.36644736842105263, 0.31085526315789475, 0.3338815789473684, 0.3796052631578947, 0.4286184210526316, 0.42368421052631583, 0.4302631578947368, 0.4302631578947368, 0.44177631578947363, 0.4391447368421053, 0.42927631578947373, 0.4565789473684211, 0.4529605263157895, 0.45197368421052636, 0.44703947368421054, 0.4496710526315789, 0.45230263157894735, 0.4532894736842105, 0.4519736842105263, 0.4509868421052632, 0.4516447368421053, 0.45230263157894746, 0.4532894736842105, 0.4532894736842105, 0.4532894736842105, 0.4532894736842105, 0.4532894736842105, 0.4532894736842105, 0.4532894736842105, 0.4532894736842105, 0.4532894736842105, 0.4532894736842105, 0.4532894736842105, 0.4532894736842105, 0.4532894736842105, 0.4532894736842105] Best val loss: 8.171403193473816


Loaded best state dict for [0.2, 0.4, 0.6]
Current group: 0.8
Epoch [1/30], Training Loss: 28.8799, Validation Loss Current: 10.4402, Validation Loss AVG: 10.4402, lr: 0.1
Epoch [2/30], Training Loss: 31.0388, Validation Loss Current: 8.8953, Validation Loss AVG: 8.8953, lr: 0.1
Epoch [3/30], Training Loss: 29.6132, Validation Loss Current: 8.4980, Validation Loss AVG: 8.4980, lr: 0.1
Epoch [4/30], Training Loss: 28.8361, Validation Loss Current: 8.9412, Validation Loss AVG: 8.9412, lr: 0.1
Epoch [5/30], Training Loss: 30.9759, Validation Loss Current: 8.5880, Validation Loss AVG: 8.5880, lr: 0.1
Epoch [6/30], Training Loss: 29.3144, Validation Loss Current: 8.7598, Validation Loss AVG: 8.7598, lr: 0.1
Epoch [7/30], Training Loss: 28.6266, Validation Loss Current: 12.6743, Validation Loss AVG: 12.6743, lr: 0.1
Epoch [8/30], Training Loss: 32.4881, Validation Loss Current: 9.2699, Validation Loss AVG: 9.2699, lr: 0.1
Epoch [9/30], Training Loss: 30.0861, Validation Loss Current: 8.5685, Validation Loss AVG: 8.5685, lr: 0.1
Epoch [10/30], Training Loss: 28.5890, Validation Loss Current: 8.7889, Validation Loss AVG: 8.7889, lr: 0.010000000000000002
Epoch [11/30], Training Loss: 25.1873, Validation Loss Current: 7.9355, Validation Loss AVG: 7.9355, lr: 0.010000000000000002
Epoch [12/30], Training Loss: 22.5414, Validation Loss Current: 7.9485, Validation Loss AVG: 7.9485, lr: 0.010000000000000002
Epoch [13/30], Training Loss: 21.9640, Validation Loss Current: 8.1449, Validation Loss AVG: 8.1449, lr: 0.010000000000000002
Epoch [14/30], Training Loss: 21.3524, Validation Loss Current: 8.2145, Validation Loss AVG: 8.2145, lr: 0.010000000000000002
Epoch [15/30], Training Loss: 21.2041, Validation Loss Current: 8.7893, Validation Loss AVG: 8.7893, lr: 0.010000000000000002
Epoch [16/30], Training Loss: 19.1700, Validation Loss Current: 8.4330, Validation Loss AVG: 8.4330, lr: 0.010000000000000002
Epoch [17/30], Training Loss: 18.1512, Validation Loss Current: 8.3960, Validation Loss AVG: 8.3960, lr: 0.010000000000000002
Epoch [18/30], Training Loss: 17.7591, Validation Loss Current: 8.4508, Validation Loss AVG: 8.4508, lr: 0.0010000000000000002
Epoch [19/30], Training Loss: 16.4852, Validation Loss Current: 8.5871, Validation Loss AVG: 8.5871, lr: 0.0010000000000000002
Epoch [20/30], Training Loss: 16.8059, Validation Loss Current: 8.6162, Validation Loss AVG: 8.6162, lr: 0.0010000000000000002
Epoch [21/30], Training Loss: 17.9523, Validation Loss Current: 8.6769, Validation Loss AVG: 8.6769, lr: 0.0010000000000000002
Epoch [22/30], Training Loss: 16.0006, Validation Loss Current: 8.7124, Validation Loss AVG: 8.7124, lr: 0.0010000000000000002
Epoch [23/30], Training Loss: 16.4836, Validation Loss Current: 8.7296, Validation Loss AVG: 8.7296, lr: 0.0010000000000000002
Epoch [24/30], Training Loss: 16.3302, Validation Loss Current: 8.7264, Validation Loss AVG: 8.7264, lr: 0.00010000000000000003
Epoch [25/30], Training Loss: 15.6236, Validation Loss Current: 8.7490, Validation Loss AVG: 8.7490, lr: 0.00010000000000000003
Epoch [26/30], Training Loss: 16.2765, Validation Loss Current: 8.7380, Validation Loss AVG: 8.7380, lr: 0.00010000000000000003
Epoch [27/30], Training Loss: 16.1323, Validation Loss Current: 8.7508, Validation Loss AVG: 8.7508, lr: 0.00010000000000000003
Epoch [28/30], Training Loss: 16.6672, Validation Loss Current: 8.7322, Validation Loss AVG: 8.7322, lr: 0.00010000000000000003
Epoch [29/30], Training Loss: 16.9585, Validation Loss Current: 8.7647, Validation Loss AVG: 8.7647, lr: 0.00010000000000000003
Epoch [30/30], Training Loss: 16.0003, Validation Loss Current: 8.7567, Validation Loss AVG: 8.7567, lr: 1.0000000000000004e-05
Epoch [31/30], Training Loss: 15.8714, Validation Loss Current: 8.7590, Validation Loss AVG: 8.7590, lr: 1.0000000000000004e-05
Epoch [32/30], Training Loss: 16.2851, Validation Loss Current: 8.7394, Validation Loss AVG: 8.7394, lr: 1.0000000000000004e-05
Epoch [33/30], Training Loss: 16.8845, Validation Loss Current: 8.7583, Validation Loss AVG: 8.7583, lr: 1.0000000000000004e-05
Epoch [34/30], Training Loss: 16.6903, Validation Loss Current: 8.7599, Validation Loss AVG: 8.7599, lr: 1.0000000000000004e-05
Epoch [35/30], Training Loss: 15.9689, Validation Loss Current: 8.7042, Validation Loss AVG: 8.7042, lr: 1.0000000000000004e-05
Epoch [36/30], Training Loss: 15.9421, Validation Loss Current: 8.7490, Validation Loss AVG: 8.7490, lr: 1.0000000000000004e-06
Epoch [37/30], Training Loss: 16.0179, Validation Loss Current: 8.7285, Validation Loss AVG: 8.7285, lr: 1.0000000000000004e-06
Epoch [38/30], Training Loss: 15.8583, Validation Loss Current: 8.7348, Validation Loss AVG: 8.7348, lr: 1.0000000000000004e-06
Epoch [39/30], Training Loss: 16.1261, Validation Loss Current: 8.7562, Validation Loss AVG: 8.7562, lr: 1.0000000000000004e-06
Epoch [40/30], Training Loss: 16.6506, Validation Loss Current: 8.7868, Validation Loss AVG: 8.7868, lr: 1.0000000000000004e-06
Epoch [41/30], Training Loss: 15.7785, Validation Loss Current: 8.7583, Validation Loss AVG: 8.7583, lr: 1.0000000000000004e-06
 --- Early Stopped ---
Patch distance: 0.8 finished training. Best epoch: 11 Best val accuracy: [0.2151315789473684, 0.3506578947368421, 0.3861842105263158, 0.35657894736842105, 0.3805921052631579, 0.3628289473684211, 0.24868421052631579, 0.3710526315789474, 0.3759868421052631, 0.3993421052631579, 0.4444078947368421, 0.4355263157894737, 0.4427631578947368, 0.44046052631578947, 0.4269736842105264, 0.44309210526315795, 0.4384868421052632, 0.4424342105263158, 0.4394736842105263, 0.4414473684210526, 0.4342105263157895, 0.43782894736842104, 0.43980263157894733, 0.4394736842105263, 0.4391447368421053, 0.44177631578947363, 0.43980263157894733, 0.44046052631578947, 0.43980263157894733, 0.44013157894736843, 0.43980263157894733, 0.44013157894736843, 0.43980263157894733, 0.43980263157894733, 0.4394736842105263, 0.4391447368421052, 0.4391447368421052, 0.4391447368421052, 0.4391447368421052, 0.4391447368421052, 0.4394736842105263] Best val loss: 7.935469341278076


Loaded best state dict for [0.2, 0.4, 0.6, 0.8]
Current group: 1
Epoch [1/30], Training Loss: 29.4360, Validation Loss Current: 8.7695, Validation Loss AVG: 10.4156, lr: 0.1
Epoch [2/30], Training Loss: 30.6673, Validation Loss Current: 7.7486, Validation Loss AVG: 9.9037, lr: 0.1
Epoch [3/30], Training Loss: 29.1196, Validation Loss Current: 7.6303, Validation Loss AVG: 8.9868, lr: 0.1
Epoch [4/30], Training Loss: 31.4145, Validation Loss Current: 8.7446, Validation Loss AVG: 10.6052, lr: 0.1
Epoch [5/30], Training Loss: 29.8064, Validation Loss Current: 8.2805, Validation Loss AVG: 9.1304, lr: 0.1
Epoch [6/30], Training Loss: 32.2646, Validation Loss Current: 7.9210, Validation Loss AVG: 9.2218, lr: 0.1
Epoch [7/30], Training Loss: 28.1448, Validation Loss Current: 7.3955, Validation Loss AVG: 9.0661, lr: 0.1
Epoch [8/30], Training Loss: 27.2007, Validation Loss Current: 7.8209, Validation Loss AVG: 9.1328, lr: 0.1
Epoch [9/30], Training Loss: 28.0296, Validation Loss Current: 7.8494, Validation Loss AVG: 8.6744, lr: 0.1
Epoch [10/30], Training Loss: 28.6642, Validation Loss Current: 7.4421, Validation Loss AVG: 9.0151, lr: 0.1
Epoch [11/30], Training Loss: 28.0067, Validation Loss Current: 7.1953, Validation Loss AVG: 9.4519, lr: 0.1
Epoch [12/30], Training Loss: 26.6786, Validation Loss Current: 8.8243, Validation Loss AVG: 10.3514, lr: 0.1
Epoch [13/30], Training Loss: 30.8260, Validation Loss Current: 8.4076, Validation Loss AVG: 10.3644, lr: 0.1
Epoch [14/30], Training Loss: 29.1086, Validation Loss Current: 7.3873, Validation Loss AVG: 8.7913, lr: 0.1
Epoch [15/30], Training Loss: 28.1631, Validation Loss Current: 7.9140, Validation Loss AVG: 9.7567, lr: 0.1
Epoch [16/30], Training Loss: 29.3348, Validation Loss Current: 8.1469, Validation Loss AVG: 12.1575, lr: 0.1
Epoch [17/30], Training Loss: 28.6952, Validation Loss Current: 7.8971, Validation Loss AVG: 10.0168, lr: 0.1
Epoch [18/30], Training Loss: 26.4304, Validation Loss Current: 6.9138, Validation Loss AVG: 8.6211, lr: 0.010000000000000002
Epoch [19/30], Training Loss: 22.5809, Validation Loss Current: 6.8675, Validation Loss AVG: 8.6091, lr: 0.010000000000000002
Epoch [20/30], Training Loss: 21.6779, Validation Loss Current: 6.7555, Validation Loss AVG: 9.2300, lr: 0.010000000000000002
Epoch [21/30], Training Loss: 19.9566, Validation Loss Current: 6.8990, Validation Loss AVG: 8.9326, lr: 0.010000000000000002
Epoch [22/30], Training Loss: 19.1744, Validation Loss Current: 6.9713, Validation Loss AVG: 9.6398, lr: 0.010000000000000002
Epoch [23/30], Training Loss: 19.0385, Validation Loss Current: 7.1600, Validation Loss AVG: 9.5046, lr: 0.010000000000000002
Epoch [24/30], Training Loss: 18.4062, Validation Loss Current: 7.0454, Validation Loss AVG: 9.3770, lr: 0.010000000000000002
Epoch [25/30], Training Loss: 17.1933, Validation Loss Current: 7.4605, Validation Loss AVG: 10.0467, lr: 0.010000000000000002
Epoch [26/30], Training Loss: 15.3476, Validation Loss Current: 7.6211, Validation Loss AVG: 9.9862, lr: 0.010000000000000002
Epoch [27/30], Training Loss: 15.0606, Validation Loss Current: 7.6461, Validation Loss AVG: 10.2541, lr: 0.0010000000000000002
Epoch [28/30], Training Loss: 13.8962, Validation Loss Current: 7.4609, Validation Loss AVG: 10.0811, lr: 0.0010000000000000002
Epoch [29/30], Training Loss: 13.9609, Validation Loss Current: 7.6318, Validation Loss AVG: 10.2019, lr: 0.0010000000000000002
Epoch [30/30], Training Loss: 13.8865, Validation Loss Current: 7.5775, Validation Loss AVG: 10.1567, lr: 0.0010000000000000002
Epoch [31/30], Training Loss: 13.6451, Validation Loss Current: 7.6439, Validation Loss AVG: 10.2932, lr: 0.0010000000000000002
Epoch [32/30], Training Loss: 14.3019, Validation Loss Current: 7.6152, Validation Loss AVG: 10.3451, lr: 0.0010000000000000002
Epoch [33/30], Training Loss: 13.5843, Validation Loss Current: 7.6883, Validation Loss AVG: 10.3830, lr: 0.00010000000000000003
Epoch [34/30], Training Loss: 13.2767, Validation Loss Current: 7.5904, Validation Loss AVG: 10.3519, lr: 0.00010000000000000003
Epoch [35/30], Training Loss: 13.0070, Validation Loss Current: 7.5518, Validation Loss AVG: 10.3953, lr: 0.00010000000000000003
Epoch [36/30], Training Loss: 13.1144, Validation Loss Current: 7.6261, Validation Loss AVG: 10.4194, lr: 0.00010000000000000003
Epoch [37/30], Training Loss: 14.2781, Validation Loss Current: 7.6365, Validation Loss AVG: 10.4339, lr: 0.00010000000000000003
Epoch [38/30], Training Loss: 13.3181, Validation Loss Current: 7.6683, Validation Loss AVG: 10.4070, lr: 0.00010000000000000003
Epoch [39/30], Training Loss: 13.2700, Validation Loss Current: 7.6613, Validation Loss AVG: 10.4199, lr: 1.0000000000000004e-05
Epoch [40/30], Training Loss: 13.4361, Validation Loss Current: 7.6156, Validation Loss AVG: 10.3409, lr: 1.0000000000000004e-05
Epoch [41/30], Training Loss: 13.5926, Validation Loss Current: 7.6775, Validation Loss AVG: 10.4061, lr: 1.0000000000000004e-05
Epoch [42/30], Training Loss: 13.1232, Validation Loss Current: 7.6981, Validation Loss AVG: 10.3710, lr: 1.0000000000000004e-05
Epoch [43/30], Training Loss: 14.5317, Validation Loss Current: 7.6825, Validation Loss AVG: 10.4160, lr: 1.0000000000000004e-05
Epoch [44/30], Training Loss: 13.1331, Validation Loss Current: 7.6771, Validation Loss AVG: 10.4297, lr: 1.0000000000000004e-05
Epoch [45/30], Training Loss: 13.1348, Validation Loss Current: 7.6023, Validation Loss AVG: 10.3907, lr: 1.0000000000000004e-06
Epoch [46/30], Training Loss: 13.6236, Validation Loss Current: 7.7836, Validation Loss AVG: 10.4257, lr: 1.0000000000000004e-06
Epoch [47/30], Training Loss: 13.7761, Validation Loss Current: 7.7171, Validation Loss AVG: 10.4080, lr: 1.0000000000000004e-06
Epoch [48/30], Training Loss: 13.3574, Validation Loss Current: 7.6605, Validation Loss AVG: 10.4215, lr: 1.0000000000000004e-06
Epoch [49/30], Training Loss: 13.2967, Validation Loss Current: 7.6712, Validation Loss AVG: 10.4018, lr: 1.0000000000000004e-06
Epoch [50/30], Training Loss: 14.4683, Validation Loss Current: 7.7260, Validation Loss AVG: 10.4099, lr: 1.0000000000000004e-06
 --- Early Stopped ---
Patch distance: 1 finished training. Best epoch: 20 Best val accuracy: [0.42105263157894735, 0.4440789473684211, 0.4588815789473684, 0.3832236842105263, 0.3980263157894737, 0.46875, 0.46381578947368424, 0.43585526315789475, 0.4720394736842105, 0.4621710526315789, 0.4934210526315789, 0.4309210526315789, 0.40789473684210525, 0.4868421052631579, 0.4128289473684211, 0.4144736842105263, 0.4621710526315789, 0.48848684210526316, 0.49835526315789475, 0.5230263157894737, 0.5, 0.5115131578947368, 0.5016447368421053, 0.5197368421052632, 0.5115131578947368, 0.5, 0.5180921052631579, 0.5082236842105263, 0.5082236842105263, 0.5032894736842105, 0.5148026315789473, 0.5131578947368421, 0.5131578947368421, 0.5148026315789473, 0.5148026315789473, 0.5115131578947368, 0.5131578947368421, 0.5131578947368421, 0.5131578947368421, 0.5131578947368421, 0.5131578947368421, 0.5131578947368421, 0.5131578947368421, 0.5131578947368421, 0.5131578947368421, 0.5131578947368421, 0.5131578947368421, 0.5131578947368421, 0.5131578947368421, 0.5131578947368421] Best val loss: 6.75550901889801


----- Training alexnet with sequence: [0.4, 0.6, 0.8, 1] -----
Current group: 0.4
Epoch [1/38], Training Loss: 40.8295, Validation Loss Current: 10.0429, Validation Loss AVG: 10.0429, lr: 0.1
Epoch [2/38], Training Loss: 40.6116, Validation Loss Current: 10.0629, Validation Loss AVG: 10.0629, lr: 0.1
Epoch [3/38], Training Loss: 40.2188, Validation Loss Current: 10.0304, Validation Loss AVG: 10.0304, lr: 0.1
Epoch [4/38], Training Loss: 40.3001, Validation Loss Current: 10.0506, Validation Loss AVG: 10.0506, lr: 0.1
Epoch [5/38], Training Loss: 39.8546, Validation Loss Current: 12.4778, Validation Loss AVG: 12.4778, lr: 0.1
Epoch [6/38], Training Loss: 41.2124, Validation Loss Current: 10.0422, Validation Loss AVG: 10.0422, lr: 0.1
Epoch [7/38], Training Loss: 39.7355, Validation Loss Current: 10.4787, Validation Loss AVG: 10.4787, lr: 0.1
Epoch [8/38], Training Loss: 39.8368, Validation Loss Current: 10.0980, Validation Loss AVG: 10.0980, lr: 0.1
Epoch [9/38], Training Loss: 40.2868, Validation Loss Current: 9.9769, Validation Loss AVG: 9.9769, lr: 0.1
Epoch [10/38], Training Loss: 38.4665, Validation Loss Current: 9.6460, Validation Loss AVG: 9.6460, lr: 0.1
Epoch [11/38], Training Loss: 37.1413, Validation Loss Current: 11.9247, Validation Loss AVG: 11.9247, lr: 0.1
Epoch [12/38], Training Loss: 40.3058, Validation Loss Current: 9.7967, Validation Loss AVG: 9.7967, lr: 0.1
Epoch [13/38], Training Loss: 37.1672, Validation Loss Current: 9.6813, Validation Loss AVG: 9.6813, lr: 0.1
Epoch [14/38], Training Loss: 36.3856, Validation Loss Current: 9.4032, Validation Loss AVG: 9.4032, lr: 0.1
Epoch [15/38], Training Loss: 35.3818, Validation Loss Current: 9.7618, Validation Loss AVG: 9.7618, lr: 0.1
Epoch [16/38], Training Loss: 37.4890, Validation Loss Current: 9.8279, Validation Loss AVG: 9.8279, lr: 0.1
Epoch [17/38], Training Loss: 35.6473, Validation Loss Current: 10.6361, Validation Loss AVG: 10.6361, lr: 0.1
Epoch [18/38], Training Loss: 34.7732, Validation Loss Current: 10.5717, Validation Loss AVG: 10.5717, lr: 0.1
Epoch [19/38], Training Loss: 35.0339, Validation Loss Current: 9.7620, Validation Loss AVG: 9.7620, lr: 0.1
Epoch [20/38], Training Loss: 33.8097, Validation Loss Current: 13.9300, Validation Loss AVG: 13.9300, lr: 0.1
Epoch [21/38], Training Loss: 37.5797, Validation Loss Current: 9.1982, Validation Loss AVG: 9.1982, lr: 0.010000000000000002
Epoch [22/38], Training Loss: 31.9922, Validation Loss Current: 9.0081, Validation Loss AVG: 9.0081, lr: 0.010000000000000002
Epoch [23/38], Training Loss: 31.1703, Validation Loss Current: 8.9362, Validation Loss AVG: 8.9362, lr: 0.010000000000000002
Epoch [24/38], Training Loss: 30.1847, Validation Loss Current: 8.9507, Validation Loss AVG: 8.9507, lr: 0.010000000000000002
Epoch [25/38], Training Loss: 30.8656, Validation Loss Current: 8.8862, Validation Loss AVG: 8.8862, lr: 0.010000000000000002
Epoch [26/38], Training Loss: 29.3059, Validation Loss Current: 9.0115, Validation Loss AVG: 9.0115, lr: 0.010000000000000002
Epoch [27/38], Training Loss: 28.7327, Validation Loss Current: 8.9575, Validation Loss AVG: 8.9575, lr: 0.010000000000000002
Epoch [28/38], Training Loss: 28.3253, Validation Loss Current: 8.9237, Validation Loss AVG: 8.9237, lr: 0.010000000000000002
Epoch [29/38], Training Loss: 28.3499, Validation Loss Current: 8.9753, Validation Loss AVG: 8.9753, lr: 0.010000000000000002
Epoch [30/38], Training Loss: 27.6325, Validation Loss Current: 9.1269, Validation Loss AVG: 9.1269, lr: 0.010000000000000002
Epoch [31/38], Training Loss: 27.3488, Validation Loss Current: 8.8067, Validation Loss AVG: 8.8067, lr: 0.010000000000000002
Epoch [32/38], Training Loss: 26.8785, Validation Loss Current: 8.8251, Validation Loss AVG: 8.8251, lr: 0.010000000000000002
Epoch [33/38], Training Loss: 26.4890, Validation Loss Current: 9.0544, Validation Loss AVG: 9.0544, lr: 0.010000000000000002
Epoch [34/38], Training Loss: 25.9066, Validation Loss Current: 8.8851, Validation Loss AVG: 8.8851, lr: 0.010000000000000002
Epoch [35/38], Training Loss: 25.7782, Validation Loss Current: 8.8621, Validation Loss AVG: 8.8621, lr: 0.010000000000000002
Epoch [36/38], Training Loss: 26.0678, Validation Loss Current: 9.0884, Validation Loss AVG: 9.0884, lr: 0.010000000000000002
Epoch [37/38], Training Loss: 26.1054, Validation Loss Current: 9.1551, Validation Loss AVG: 9.1551, lr: 0.010000000000000002
Epoch [38/38], Training Loss: 25.9823, Validation Loss Current: 9.1491, Validation Loss AVG: 9.1491, lr: 0.0010000000000000002
Epoch [39/38], Training Loss: 25.2321, Validation Loss Current: 9.1149, Validation Loss AVG: 9.1149, lr: 0.0010000000000000002
Epoch [40/38], Training Loss: 23.3962, Validation Loss Current: 9.1078, Validation Loss AVG: 9.1078, lr: 0.0010000000000000002
Epoch [41/38], Training Loss: 23.8498, Validation Loss Current: 9.2143, Validation Loss AVG: 9.2143, lr: 0.0010000000000000002
Epoch [42/38], Training Loss: 24.2108, Validation Loss Current: 9.1591, Validation Loss AVG: 9.1591, lr: 0.0010000000000000002
Epoch [43/38], Training Loss: 22.9462, Validation Loss Current: 9.1352, Validation Loss AVG: 9.1352, lr: 0.0010000000000000002
Epoch [44/38], Training Loss: 26.1843, Validation Loss Current: 9.1509, Validation Loss AVG: 9.1509, lr: 0.00010000000000000003
Epoch [45/38], Training Loss: 23.5293, Validation Loss Current: 9.1496, Validation Loss AVG: 9.1496, lr: 0.00010000000000000003
Epoch [46/38], Training Loss: 25.4427, Validation Loss Current: 9.1648, Validation Loss AVG: 9.1648, lr: 0.00010000000000000003
Epoch [47/38], Training Loss: 23.3331, Validation Loss Current: 9.1809, Validation Loss AVG: 9.1809, lr: 0.00010000000000000003
Epoch [48/38], Training Loss: 23.0517, Validation Loss Current: 9.1571, Validation Loss AVG: 9.1571, lr: 0.00010000000000000003
Epoch [49/38], Training Loss: 22.7612, Validation Loss Current: 9.2011, Validation Loss AVG: 9.2011, lr: 0.00010000000000000003
Epoch [50/38], Training Loss: 24.2692, Validation Loss Current: 9.1829, Validation Loss AVG: 9.1829, lr: 1.0000000000000004e-05
Epoch [51/38], Training Loss: 22.5310, Validation Loss Current: 9.2110, Validation Loss AVG: 9.2110, lr: 1.0000000000000004e-05
Epoch [52/38], Training Loss: 23.7961, Validation Loss Current: 9.1854, Validation Loss AVG: 9.1854, lr: 1.0000000000000004e-05
Epoch [53/38], Training Loss: 25.1685, Validation Loss Current: 9.1832, Validation Loss AVG: 9.1832, lr: 1.0000000000000004e-05
Epoch [54/38], Training Loss: 23.9399, Validation Loss Current: 9.1803, Validation Loss AVG: 9.1803, lr: 1.0000000000000004e-05
Epoch [55/38], Training Loss: 23.7032, Validation Loss Current: 9.2051, Validation Loss AVG: 9.2051, lr: 1.0000000000000004e-05
Epoch [56/38], Training Loss: 24.2069, Validation Loss Current: 9.1961, Validation Loss AVG: 9.1961, lr: 1.0000000000000004e-06
Epoch [57/38], Training Loss: 23.3452, Validation Loss Current: 9.1810, Validation Loss AVG: 9.1810, lr: 1.0000000000000004e-06
Epoch [58/38], Training Loss: 23.7523, Validation Loss Current: 9.1917, Validation Loss AVG: 9.1917, lr: 1.0000000000000004e-06
Epoch [59/38], Training Loss: 24.5612, Validation Loss Current: 9.2165, Validation Loss AVG: 9.2165, lr: 1.0000000000000004e-06
Epoch [60/38], Training Loss: 23.1408, Validation Loss Current: 9.1839, Validation Loss AVG: 9.1839, lr: 1.0000000000000004e-06
Epoch [61/38], Training Loss: 23.7146, Validation Loss Current: 9.2229, Validation Loss AVG: 9.2229, lr: 1.0000000000000004e-06
 --- Early Stopped ---
Patch distance: 0.4 finished training. Best epoch: 31 Best val accuracy: [0.2351973684210526, 0.2351973684210526, 0.2351973684210526, 0.2351973684210526, 0.20197368421052633, 0.2351973684210526, 0.1513157894736842, 0.2351973684210526, 0.2582236842105263, 0.28125, 0.26776315789473687, 0.2667763157894737, 0.2911184210526316, 0.3243421052631579, 0.30657894736842106, 0.2894736842105264, 0.23782894736842106, 0.30164473684210524, 0.2993421052631579, 0.2976973684210526, 0.3194078947368421, 0.34111842105263157, 0.35328947368421054, 0.3355263157894737, 0.34506578947368427, 0.34243421052631573, 0.3480263157894737, 0.33782894736842106, 0.34539473684210525, 0.35263157894736835, 0.36019736842105265, 0.36184210526315785, 0.3536184210526316, 0.3703947368421052, 0.34967105263157894, 0.36085526315789473, 0.35460526315789476, 0.3493421052631579, 0.34967105263157894, 0.3486842105263158, 0.3463815789473684, 0.35197368421052627, 0.3503289473684211, 0.35065789473684206, 0.3483552631578947, 0.3486842105263158, 0.34835526315789467, 0.3483552631578947, 0.34769736842105264, 0.3473684210526316, 0.34769736842105264, 0.3473684210526316, 0.3470394736842105, 0.34638157894736843, 0.34638157894736843, 0.34638157894736843, 0.34638157894736843, 0.34638157894736843, 0.34638157894736843, 0.34638157894736843, 0.34638157894736843] Best val loss: 8.806717085838319


Loaded best state dict for [0.4]
Current group: 0.6
Epoch [1/38], Training Loss: 32.6235, Validation Loss Current: 18.8798, Validation Loss AVG: 18.8798, lr: 0.1
Epoch [2/38], Training Loss: 35.7295, Validation Loss Current: 10.6715, Validation Loss AVG: 10.6715, lr: 0.1
Epoch [3/38], Training Loss: 36.1380, Validation Loss Current: 11.4940, Validation Loss AVG: 11.4940, lr: 0.1
Epoch [4/38], Training Loss: 37.8697, Validation Loss Current: 9.7403, Validation Loss AVG: 9.7403, lr: 0.1
Epoch [5/38], Training Loss: 35.2917, Validation Loss Current: 9.9532, Validation Loss AVG: 9.9532, lr: 0.1
Epoch [6/38], Training Loss: 34.7408, Validation Loss Current: 9.2699, Validation Loss AVG: 9.2699, lr: 0.1
Epoch [7/38], Training Loss: 35.7185, Validation Loss Current: 10.2150, Validation Loss AVG: 10.2150, lr: 0.1
Epoch [8/38], Training Loss: 33.8666, Validation Loss Current: 9.3727, Validation Loss AVG: 9.3727, lr: 0.1
Epoch [9/38], Training Loss: 32.5576, Validation Loss Current: 10.2890, Validation Loss AVG: 10.2890, lr: 0.1
Epoch [10/38], Training Loss: 32.2191, Validation Loss Current: 10.0543, Validation Loss AVG: 10.0543, lr: 0.1
Epoch [11/38], Training Loss: 34.8322, Validation Loss Current: 9.2936, Validation Loss AVG: 9.2936, lr: 0.1
Epoch [12/38], Training Loss: 32.8139, Validation Loss Current: 10.2862, Validation Loss AVG: 10.2862, lr: 0.1
Epoch [13/38], Training Loss: 31.0884, Validation Loss Current: 9.1805, Validation Loss AVG: 9.1805, lr: 0.010000000000000002
Epoch [14/38], Training Loss: 28.6301, Validation Loss Current: 9.1107, Validation Loss AVG: 9.1107, lr: 0.010000000000000002
Epoch [15/38], Training Loss: 27.5543, Validation Loss Current: 9.4819, Validation Loss AVG: 9.4819, lr: 0.010000000000000002
Epoch [16/38], Training Loss: 27.1247, Validation Loss Current: 9.2513, Validation Loss AVG: 9.2513, lr: 0.010000000000000002
Epoch [17/38], Training Loss: 26.7440, Validation Loss Current: 9.7400, Validation Loss AVG: 9.7400, lr: 0.010000000000000002
Epoch [18/38], Training Loss: 25.2692, Validation Loss Current: 9.4418, Validation Loss AVG: 9.4418, lr: 0.010000000000000002
Epoch [19/38], Training Loss: 24.3090, Validation Loss Current: 9.0852, Validation Loss AVG: 9.0852, lr: 0.010000000000000002
Epoch [20/38], Training Loss: 23.6504, Validation Loss Current: 9.7126, Validation Loss AVG: 9.7126, lr: 0.010000000000000002
Epoch [21/38], Training Loss: 22.7894, Validation Loss Current: 9.6275, Validation Loss AVG: 9.6275, lr: 0.010000000000000002
Epoch [22/38], Training Loss: 23.1445, Validation Loss Current: 9.2869, Validation Loss AVG: 9.2869, lr: 0.010000000000000002
Epoch [23/38], Training Loss: 23.0205, Validation Loss Current: 10.1255, Validation Loss AVG: 10.1255, lr: 0.010000000000000002
Epoch [24/38], Training Loss: 23.5915, Validation Loss Current: 9.9778, Validation Loss AVG: 9.9778, lr: 0.010000000000000002
Epoch [25/38], Training Loss: 21.6105, Validation Loss Current: 9.5148, Validation Loss AVG: 9.5148, lr: 0.010000000000000002
Epoch [26/38], Training Loss: 19.6035, Validation Loss Current: 9.7321, Validation Loss AVG: 9.7321, lr: 0.0010000000000000002
Epoch [27/38], Training Loss: 19.6918, Validation Loss Current: 9.9228, Validation Loss AVG: 9.9228, lr: 0.0010000000000000002
Epoch [28/38], Training Loss: 19.6305, Validation Loss Current: 9.9423, Validation Loss AVG: 9.9423, lr: 0.0010000000000000002
Epoch [29/38], Training Loss: 18.7207, Validation Loss Current: 10.1972, Validation Loss AVG: 10.1972, lr: 0.0010000000000000002
Epoch [30/38], Training Loss: 18.9987, Validation Loss Current: 10.2104, Validation Loss AVG: 10.2104, lr: 0.0010000000000000002
Epoch [31/38], Training Loss: 18.5133, Validation Loss Current: 10.2429, Validation Loss AVG: 10.2429, lr: 0.0010000000000000002
Epoch [32/38], Training Loss: 18.4605, Validation Loss Current: 10.2287, Validation Loss AVG: 10.2287, lr: 0.00010000000000000003
Epoch [33/38], Training Loss: 20.3402, Validation Loss Current: 10.2333, Validation Loss AVG: 10.2333, lr: 0.00010000000000000003
Epoch [34/38], Training Loss: 18.3204, Validation Loss Current: 10.2303, Validation Loss AVG: 10.2303, lr: 0.00010000000000000003
Epoch [35/38], Training Loss: 18.1672, Validation Loss Current: 10.2204, Validation Loss AVG: 10.2204, lr: 0.00010000000000000003
Epoch [36/38], Training Loss: 18.2224, Validation Loss Current: 10.2215, Validation Loss AVG: 10.2215, lr: 0.00010000000000000003
Epoch [37/38], Training Loss: 17.9592, Validation Loss Current: 10.1967, Validation Loss AVG: 10.1967, lr: 0.00010000000000000003
Epoch [38/38], Training Loss: 18.1566, Validation Loss Current: 10.2155, Validation Loss AVG: 10.2155, lr: 1.0000000000000004e-05
Epoch [39/38], Training Loss: 20.1885, Validation Loss Current: 10.2108, Validation Loss AVG: 10.2108, lr: 1.0000000000000004e-05
Epoch [40/38], Training Loss: 18.0425, Validation Loss Current: 10.1678, Validation Loss AVG: 10.1678, lr: 1.0000000000000004e-05
Epoch [41/38], Training Loss: 19.3081, Validation Loss Current: 10.1831, Validation Loss AVG: 10.1831, lr: 1.0000000000000004e-05
Epoch [42/38], Training Loss: 18.2856, Validation Loss Current: 10.1920, Validation Loss AVG: 10.1920, lr: 1.0000000000000004e-05
Epoch [43/38], Training Loss: 17.8806, Validation Loss Current: 10.2393, Validation Loss AVG: 10.2393, lr: 1.0000000000000004e-05
Epoch [44/38], Training Loss: 18.7097, Validation Loss Current: 10.2032, Validation Loss AVG: 10.2032, lr: 1.0000000000000004e-06
Epoch [45/38], Training Loss: 18.3727, Validation Loss Current: 10.1873, Validation Loss AVG: 10.1873, lr: 1.0000000000000004e-06
Epoch [46/38], Training Loss: 18.9371, Validation Loss Current: 10.1948, Validation Loss AVG: 10.1948, lr: 1.0000000000000004e-06
Epoch [47/38], Training Loss: 17.8211, Validation Loss Current: 10.2199, Validation Loss AVG: 10.2199, lr: 1.0000000000000004e-06
Epoch [48/38], Training Loss: 17.7943, Validation Loss Current: 10.2030, Validation Loss AVG: 10.2030, lr: 1.0000000000000004e-06
Epoch [49/38], Training Loss: 18.4731, Validation Loss Current: 10.1799, Validation Loss AVG: 10.1799, lr: 1.0000000000000004e-06
 --- Early Stopped ---
Patch distance: 0.6 finished training. Best epoch: 19 Best val accuracy: [0.24769736842105267, 0.30328947368421055, 0.268421052631579, 0.32203947368421054, 0.275, 0.34342105263157896, 0.3078947368421053, 0.32532894736842105, 0.3125, 0.33026315789473687, 0.3236842105263158, 0.3319078947368421, 0.3684210526315789, 0.3707236842105263, 0.3615131578947368, 0.3733552631578948, 0.36217105263157895, 0.3740131578947369, 0.38223684210526315, 0.3838815789473684, 0.38618421052631574, 0.3851973684210527, 0.3868421052631579, 0.3848684210526316, 0.38585526315789476, 0.3990131578947368, 0.40328947368421053, 0.4046052631578947, 0.4016447368421052, 0.4023026315789474, 0.39934210526315794, 0.40230263157894736, 0.4046052631578947, 0.40427631578947365, 0.4036184210526315, 0.40394736842105267, 0.40427631578947365, 0.40427631578947365, 0.40427631578947365, 0.40394736842105256, 0.40427631578947365, 0.40427631578947365, 0.4046052631578947, 0.40427631578947365, 0.40427631578947365, 0.40427631578947365, 0.40427631578947365, 0.40427631578947365, 0.40427631578947365] Best val loss: 9.085190749168396


Loaded best state dict for [0.4, 0.6]
Current group: 0.8
Epoch [1/38], Training Loss: 31.9297, Validation Loss Current: 9.4354, Validation Loss AVG: 9.4354, lr: 0.1
Epoch [2/38], Training Loss: 31.0792, Validation Loss Current: 9.3565, Validation Loss AVG: 9.3565, lr: 0.1
Epoch [3/38], Training Loss: 31.4708, Validation Loss Current: 12.5145, Validation Loss AVG: 12.5145, lr: 0.1
Epoch [4/38], Training Loss: 32.7686, Validation Loss Current: 9.7588, Validation Loss AVG: 9.7588, lr: 0.1
Epoch [5/38], Training Loss: 30.9206, Validation Loss Current: 9.6498, Validation Loss AVG: 9.6498, lr: 0.1
Epoch [6/38], Training Loss: 30.2216, Validation Loss Current: 10.1171, Validation Loss AVG: 10.1171, lr: 0.1
Epoch [7/38], Training Loss: 32.8666, Validation Loss Current: 8.8577, Validation Loss AVG: 8.8577, lr: 0.1
Epoch [8/38], Training Loss: 28.9707, Validation Loss Current: 9.9376, Validation Loss AVG: 9.9376, lr: 0.1
Epoch [9/38], Training Loss: 30.7675, Validation Loss Current: 9.7827, Validation Loss AVG: 9.7827, lr: 0.1
Epoch [10/38], Training Loss: 30.0101, Validation Loss Current: 9.0817, Validation Loss AVG: 9.0817, lr: 0.1
Epoch [11/38], Training Loss: 28.8131, Validation Loss Current: 9.6675, Validation Loss AVG: 9.6675, lr: 0.1
Epoch [12/38], Training Loss: 30.8208, Validation Loss Current: 9.1620, Validation Loss AVG: 9.1620, lr: 0.1
Epoch [13/38], Training Loss: 28.7645, Validation Loss Current: 9.4971, Validation Loss AVG: 9.4971, lr: 0.1
Epoch [14/38], Training Loss: 28.4765, Validation Loss Current: 8.9084, Validation Loss AVG: 8.9084, lr: 0.010000000000000002
Epoch [15/38], Training Loss: 25.4554, Validation Loss Current: 8.8053, Validation Loss AVG: 8.8053, lr: 0.010000000000000002
Epoch [16/38], Training Loss: 25.3947, Validation Loss Current: 8.8230, Validation Loss AVG: 8.8230, lr: 0.010000000000000002
Epoch [17/38], Training Loss: 22.9895, Validation Loss Current: 8.8103, Validation Loss AVG: 8.8103, lr: 0.010000000000000002
Epoch [18/38], Training Loss: 21.3238, Validation Loss Current: 8.9939, Validation Loss AVG: 8.9939, lr: 0.010000000000000002
Epoch [19/38], Training Loss: 21.0188, Validation Loss Current: 9.4954, Validation Loss AVG: 9.4954, lr: 0.010000000000000002
Epoch [20/38], Training Loss: 20.6373, Validation Loss Current: 9.4444, Validation Loss AVG: 9.4444, lr: 0.010000000000000002
Epoch [21/38], Training Loss: 19.0295, Validation Loss Current: 9.5014, Validation Loss AVG: 9.5014, lr: 0.010000000000000002
Epoch [22/38], Training Loss: 18.6924, Validation Loss Current: 9.4415, Validation Loss AVG: 9.4415, lr: 0.0010000000000000002
Epoch [23/38], Training Loss: 18.9950, Validation Loss Current: 9.3401, Validation Loss AVG: 9.3401, lr: 0.0010000000000000002
Epoch [24/38], Training Loss: 17.9793, Validation Loss Current: 9.4188, Validation Loss AVG: 9.4188, lr: 0.0010000000000000002
Epoch [25/38], Training Loss: 18.5105, Validation Loss Current: 9.4466, Validation Loss AVG: 9.4466, lr: 0.0010000000000000002
Epoch [26/38], Training Loss: 17.4020, Validation Loss Current: 9.4895, Validation Loss AVG: 9.4895, lr: 0.0010000000000000002
Epoch [27/38], Training Loss: 18.2275, Validation Loss Current: 9.5316, Validation Loss AVG: 9.5316, lr: 0.0010000000000000002
Epoch [28/38], Training Loss: 17.3209, Validation Loss Current: 9.4937, Validation Loss AVG: 9.4937, lr: 0.00010000000000000003
Epoch [29/38], Training Loss: 17.4255, Validation Loss Current: 9.5283, Validation Loss AVG: 9.5283, lr: 0.00010000000000000003
Epoch [30/38], Training Loss: 17.6342, Validation Loss Current: 9.5393, Validation Loss AVG: 9.5393, lr: 0.00010000000000000003
Epoch [31/38], Training Loss: 17.0993, Validation Loss Current: 9.5420, Validation Loss AVG: 9.5420, lr: 0.00010000000000000003
Epoch [32/38], Training Loss: 17.8560, Validation Loss Current: 9.5070, Validation Loss AVG: 9.5070, lr: 0.00010000000000000003
Epoch [33/38], Training Loss: 17.7491, Validation Loss Current: 9.5069, Validation Loss AVG: 9.5069, lr: 0.00010000000000000003
Epoch [34/38], Training Loss: 17.0221, Validation Loss Current: 9.4905, Validation Loss AVG: 9.4905, lr: 1.0000000000000004e-05
Epoch [35/38], Training Loss: 16.9595, Validation Loss Current: 9.5119, Validation Loss AVG: 9.5119, lr: 1.0000000000000004e-05
Epoch [36/38], Training Loss: 17.4509, Validation Loss Current: 9.5266, Validation Loss AVG: 9.5266, lr: 1.0000000000000004e-05
Epoch [37/38], Training Loss: 17.8892, Validation Loss Current: 9.5203, Validation Loss AVG: 9.5203, lr: 1.0000000000000004e-05
Epoch [38/38], Training Loss: 17.2816, Validation Loss Current: 9.4961, Validation Loss AVG: 9.4961, lr: 1.0000000000000004e-05
Epoch [39/38], Training Loss: 18.3670, Validation Loss Current: 9.5011, Validation Loss AVG: 9.5011, lr: 1.0000000000000004e-05
Epoch [40/38], Training Loss: 17.2524, Validation Loss Current: 9.5066, Validation Loss AVG: 9.5066, lr: 1.0000000000000004e-06
Epoch [41/38], Training Loss: 17.3465, Validation Loss Current: 9.5117, Validation Loss AVG: 9.5117, lr: 1.0000000000000004e-06
Epoch [42/38], Training Loss: 17.1643, Validation Loss Current: 9.4953, Validation Loss AVG: 9.4953, lr: 1.0000000000000004e-06
Epoch [43/38], Training Loss: 17.9837, Validation Loss Current: 9.5245, Validation Loss AVG: 9.5245, lr: 1.0000000000000004e-06
Epoch [44/38], Training Loss: 18.8008, Validation Loss Current: 9.5222, Validation Loss AVG: 9.5222, lr: 1.0000000000000004e-06
Epoch [45/38], Training Loss: 17.2158, Validation Loss Current: 9.5402, Validation Loss AVG: 9.5402, lr: 1.0000000000000004e-06
 --- Early Stopped ---
Patch distance: 0.8 finished training. Best epoch: 15 Best val accuracy: [0.2654605263157895, 0.3282894736842105, 0.3358552631578947, 0.35789473684210527, 0.3180921052631579, 0.25855263157894737, 0.38125, 0.3371710526315789, 0.3404605263157895, 0.3263157894736842, 0.3125, 0.37335526315789475, 0.26085526315789476, 0.3786184210526316, 0.3792763157894737, 0.3805921052631579, 0.3740131578947369, 0.3730263157894737, 0.3700657894736842, 0.3773026315789474, 0.381578947368421, 0.3766447368421053, 0.3799342105263158, 0.3802631578947368, 0.3799342105263158, 0.3782894736842105, 0.38092105263157894, 0.37927631578947374, 0.37927631578947374, 0.3789473684210526, 0.38026315789473686, 0.3805921052631579, 0.38026315789473686, 0.38026315789473686, 0.38026315789473686, 0.38026315789473686, 0.38026315789473686, 0.38026315789473686, 0.38026315789473686, 0.38026315789473686, 0.38026315789473686, 0.38026315789473686, 0.38026315789473686, 0.38026315789473686, 0.38026315789473686] Best val loss: 8.805348539352417


Loaded best state dict for [0.4, 0.6, 0.8]
Current group: 1
Epoch [1/38], Training Loss: 28.7420, Validation Loss Current: 7.9480, Validation Loss AVG: 9.1266, lr: 0.1
Epoch [2/38], Training Loss: 31.1862, Validation Loss Current: 8.0171, Validation Loss AVG: 9.0621, lr: 0.1
Epoch [3/38], Training Loss: 30.4742, Validation Loss Current: 8.0739, Validation Loss AVG: 9.6185, lr: 0.1
Epoch [4/38], Training Loss: 30.7509, Validation Loss Current: 9.5845, Validation Loss AVG: 9.7477, lr: 0.1
Epoch [5/38], Training Loss: 30.5650, Validation Loss Current: 10.4785, Validation Loss AVG: 11.1513, lr: 0.1
Epoch [6/38], Training Loss: 33.8150, Validation Loss Current: 8.6330, Validation Loss AVG: 11.1897, lr: 0.1
Epoch [7/38], Training Loss: 31.3623, Validation Loss Current: 7.9725, Validation Loss AVG: 9.0487, lr: 0.1
Epoch [8/38], Training Loss: 29.2195, Validation Loss Current: 7.3267, Validation Loss AVG: 8.8152, lr: 0.010000000000000002
Epoch [9/38], Training Loss: 26.9712, Validation Loss Current: 7.1916, Validation Loss AVG: 8.8376, lr: 0.010000000000000002
Epoch [10/38], Training Loss: 25.3844, Validation Loss Current: 7.0472, Validation Loss AVG: 8.7044, lr: 0.010000000000000002
Epoch [11/38], Training Loss: 24.8976, Validation Loss Current: 7.0364, Validation Loss AVG: 8.7284, lr: 0.010000000000000002
Epoch [12/38], Training Loss: 24.0733, Validation Loss Current: 7.0755, Validation Loss AVG: 9.0130, lr: 0.010000000000000002
Epoch [13/38], Training Loss: 23.2925, Validation Loss Current: 7.2888, Validation Loss AVG: 8.8738, lr: 0.010000000000000002
Epoch [14/38], Training Loss: 23.8151, Validation Loss Current: 7.0050, Validation Loss AVG: 9.2415, lr: 0.010000000000000002
Epoch [15/38], Training Loss: 22.6208, Validation Loss Current: 7.0498, Validation Loss AVG: 9.1915, lr: 0.010000000000000002
Epoch [16/38], Training Loss: 21.8566, Validation Loss Current: 6.9109, Validation Loss AVG: 9.3158, lr: 0.010000000000000002
Epoch [17/38], Training Loss: 21.3186, Validation Loss Current: 7.0958, Validation Loss AVG: 9.7672, lr: 0.010000000000000002
Epoch [18/38], Training Loss: 22.0485, Validation Loss Current: 7.1933, Validation Loss AVG: 9.4480, lr: 0.010000000000000002
Epoch [19/38], Training Loss: 20.3013, Validation Loss Current: 7.4209, Validation Loss AVG: 10.0087, lr: 0.010000000000000002
Epoch [20/38], Training Loss: 18.0767, Validation Loss Current: 7.3402, Validation Loss AVG: 9.8474, lr: 0.010000000000000002
Epoch [21/38], Training Loss: 17.6186, Validation Loss Current: 7.5160, Validation Loss AVG: 10.4050, lr: 0.010000000000000002
Epoch [22/38], Training Loss: 17.7618, Validation Loss Current: 7.6291, Validation Loss AVG: 9.9301, lr: 0.010000000000000002
Epoch [23/38], Training Loss: 15.7128, Validation Loss Current: 7.5543, Validation Loss AVG: 10.0145, lr: 0.0010000000000000002
Epoch [24/38], Training Loss: 15.8739, Validation Loss Current: 7.6301, Validation Loss AVG: 10.2608, lr: 0.0010000000000000002
Epoch [25/38], Training Loss: 16.5307, Validation Loss Current: 7.6047, Validation Loss AVG: 10.2568, lr: 0.0010000000000000002
Epoch [26/38], Training Loss: 15.2129, Validation Loss Current: 7.7039, Validation Loss AVG: 10.3544, lr: 0.0010000000000000002
Epoch [27/38], Training Loss: 15.6725, Validation Loss Current: 7.6764, Validation Loss AVG: 10.3544, lr: 0.0010000000000000002
Epoch [28/38], Training Loss: 15.0723, Validation Loss Current: 7.8002, Validation Loss AVG: 10.3872, lr: 0.0010000000000000002
Epoch [29/38], Training Loss: 14.5435, Validation Loss Current: 7.7551, Validation Loss AVG: 10.3849, lr: 0.00010000000000000003
Epoch [30/38], Training Loss: 15.4589, Validation Loss Current: 7.7390, Validation Loss AVG: 10.4023, lr: 0.00010000000000000003
Epoch [31/38], Training Loss: 14.7380, Validation Loss Current: 7.7697, Validation Loss AVG: 10.4127, lr: 0.00010000000000000003
Epoch [32/38], Training Loss: 16.0321, Validation Loss Current: 7.8004, Validation Loss AVG: 10.4212, lr: 0.00010000000000000003
Epoch [33/38], Training Loss: 14.4747, Validation Loss Current: 7.7576, Validation Loss AVG: 10.4072, lr: 0.00010000000000000003
Epoch [34/38], Training Loss: 14.7808, Validation Loss Current: 7.6846, Validation Loss AVG: 10.4115, lr: 0.00010000000000000003
Epoch [35/38], Training Loss: 16.0429, Validation Loss Current: 7.6545, Validation Loss AVG: 10.4387, lr: 1.0000000000000004e-05
Epoch [36/38], Training Loss: 14.7131, Validation Loss Current: 7.6803, Validation Loss AVG: 10.4385, lr: 1.0000000000000004e-05
Epoch [37/38], Training Loss: 14.7413, Validation Loss Current: 7.7920, Validation Loss AVG: 10.4311, lr: 1.0000000000000004e-05
Epoch [38/38], Training Loss: 15.1280, Validation Loss Current: 7.7564, Validation Loss AVG: 10.4391, lr: 1.0000000000000004e-05
Epoch [39/38], Training Loss: 14.7861, Validation Loss Current: 7.6874, Validation Loss AVG: 10.4345, lr: 1.0000000000000004e-05
Epoch [40/38], Training Loss: 16.1533, Validation Loss Current: 7.7485, Validation Loss AVG: 10.4309, lr: 1.0000000000000004e-05
Epoch [41/38], Training Loss: 15.3093, Validation Loss Current: 7.7038, Validation Loss AVG: 10.4154, lr: 1.0000000000000004e-06
Epoch [42/38], Training Loss: 15.8407, Validation Loss Current: 7.7547, Validation Loss AVG: 10.4814, lr: 1.0000000000000004e-06
Epoch [43/38], Training Loss: 15.8720, Validation Loss Current: 7.8162, Validation Loss AVG: 10.4736, lr: 1.0000000000000004e-06
Epoch [44/38], Training Loss: 15.3006, Validation Loss Current: 7.7651, Validation Loss AVG: 10.4365, lr: 1.0000000000000004e-06
Epoch [45/38], Training Loss: 14.9844, Validation Loss Current: 7.7411, Validation Loss AVG: 10.4250, lr: 1.0000000000000004e-06
Epoch [46/38], Training Loss: 14.7248, Validation Loss Current: 7.7416, Validation Loss AVG: 10.4465, lr: 1.0000000000000004e-06
 --- Early Stopped ---
Patch distance: 1 finished training. Best epoch: 16 Best val accuracy: [0.4309210526315789, 0.4029605263157895, 0.40131578947368424, 0.3519736842105263, 0.4029605263157895, 0.41118421052631576, 0.43585526315789475, 0.46381578947368424, 0.48026315789473684, 0.47368421052631576, 0.4917763157894737, 0.4917763157894737, 0.5, 0.5032894736842105, 0.5049342105263158, 0.5148026315789473, 0.5131578947368421, 0.5049342105263158, 0.49506578947368424, 0.5115131578947368, 0.5049342105263158, 0.5016447368421053, 0.5, 0.5, 0.5049342105263158, 0.4967105263157895, 0.5016447368421053, 0.5016447368421053, 0.5, 0.5098684210526315, 0.5098684210526315, 0.5082236842105263, 0.5032894736842105, 0.5098684210526315, 0.5082236842105263, 0.5082236842105263, 0.5082236842105263, 0.5082236842105263, 0.5082236842105263, 0.5082236842105263, 0.506578947368421, 0.506578947368421, 0.506578947368421, 0.5082236842105263, 0.5082236842105263, 0.5082236842105263] Best val loss: 6.910939574241638


----- Training alexnet with sequence: [0.6, 0.8, 1] -----
Current group: 0.6
Epoch [1/50], Training Loss: 40.1815, Validation Loss Current: 10.1180, Validation Loss AVG: 10.1180, lr: 0.1
Epoch [2/50], Training Loss: 40.3114, Validation Loss Current: 10.0366, Validation Loss AVG: 10.0366, lr: 0.1
Epoch [3/50], Training Loss: 40.6551, Validation Loss Current: 10.0355, Validation Loss AVG: 10.0355, lr: 0.1
Epoch [4/50], Training Loss: 40.1210, Validation Loss Current: 10.0317, Validation Loss AVG: 10.0317, lr: 0.1
Epoch [5/50], Training Loss: 39.9516, Validation Loss Current: 10.0861, Validation Loss AVG: 10.0861, lr: 0.1
Epoch [6/50], Training Loss: 39.5994, Validation Loss Current: 9.9125, Validation Loss AVG: 9.9125, lr: 0.1
Epoch [7/50], Training Loss: 38.1495, Validation Loss Current: 9.3814, Validation Loss AVG: 9.3814, lr: 0.1
Epoch [8/50], Training Loss: 38.0850, Validation Loss Current: 9.6555, Validation Loss AVG: 9.6555, lr: 0.1
Epoch [9/50], Training Loss: 37.8503, Validation Loss Current: 9.3111, Validation Loss AVG: 9.3111, lr: 0.1
Epoch [10/50], Training Loss: 35.1149, Validation Loss Current: 9.4142, Validation Loss AVG: 9.4142, lr: 0.1
Epoch [11/50], Training Loss: 34.7616, Validation Loss Current: 9.0968, Validation Loss AVG: 9.0968, lr: 0.1
Epoch [12/50], Training Loss: 33.7237, Validation Loss Current: 10.6327, Validation Loss AVG: 10.6327, lr: 0.1
Epoch [13/50], Training Loss: 34.7520, Validation Loss Current: 9.8354, Validation Loss AVG: 9.8354, lr: 0.1
Epoch [14/50], Training Loss: 35.5547, Validation Loss Current: 9.5488, Validation Loss AVG: 9.5488, lr: 0.1
Epoch [15/50], Training Loss: 36.3448, Validation Loss Current: 9.5137, Validation Loss AVG: 9.5137, lr: 0.1
Epoch [16/50], Training Loss: 34.0293, Validation Loss Current: 11.6224, Validation Loss AVG: 11.6224, lr: 0.1
Epoch [17/50], Training Loss: 33.4836, Validation Loss Current: 12.9556, Validation Loss AVG: 12.9556, lr: 0.1
Epoch [18/50], Training Loss: 35.2933, Validation Loss Current: 8.8399, Validation Loss AVG: 8.8399, lr: 0.010000000000000002
Epoch [19/50], Training Loss: 29.7596, Validation Loss Current: 8.5849, Validation Loss AVG: 8.5849, lr: 0.010000000000000002
Epoch [20/50], Training Loss: 28.5186, Validation Loss Current: 8.3721, Validation Loss AVG: 8.3721, lr: 0.010000000000000002
Epoch [21/50], Training Loss: 28.2193, Validation Loss Current: 8.2834, Validation Loss AVG: 8.2834, lr: 0.010000000000000002
Epoch [22/50], Training Loss: 26.7112, Validation Loss Current: 8.3650, Validation Loss AVG: 8.3650, lr: 0.010000000000000002
Epoch [23/50], Training Loss: 26.3764, Validation Loss Current: 8.6992, Validation Loss AVG: 8.6992, lr: 0.010000000000000002
Epoch [24/50], Training Loss: 26.3923, Validation Loss Current: 8.5237, Validation Loss AVG: 8.5237, lr: 0.010000000000000002
Epoch [25/50], Training Loss: 24.6280, Validation Loss Current: 8.3009, Validation Loss AVG: 8.3009, lr: 0.010000000000000002
Epoch [26/50], Training Loss: 25.9745, Validation Loss Current: 8.3854, Validation Loss AVG: 8.3854, lr: 0.010000000000000002
Epoch [27/50], Training Loss: 25.9137, Validation Loss Current: 8.4639, Validation Loss AVG: 8.4639, lr: 0.010000000000000002
Epoch [28/50], Training Loss: 24.0545, Validation Loss Current: 8.2947, Validation Loss AVG: 8.2947, lr: 0.0010000000000000002
Epoch [29/50], Training Loss: 23.4078, Validation Loss Current: 8.3568, Validation Loss AVG: 8.3568, lr: 0.0010000000000000002
Epoch [30/50], Training Loss: 21.9629, Validation Loss Current: 8.4233, Validation Loss AVG: 8.4233, lr: 0.0010000000000000002
Epoch [31/50], Training Loss: 23.5526, Validation Loss Current: 8.4008, Validation Loss AVG: 8.4008, lr: 0.0010000000000000002
Epoch [32/50], Training Loss: 22.4023, Validation Loss Current: 8.3492, Validation Loss AVG: 8.3492, lr: 0.0010000000000000002
Epoch [33/50], Training Loss: 21.7571, Validation Loss Current: 8.4356, Validation Loss AVG: 8.4356, lr: 0.0010000000000000002
Epoch [34/50], Training Loss: 21.9879, Validation Loss Current: 8.4130, Validation Loss AVG: 8.4130, lr: 0.00010000000000000003
Epoch [35/50], Training Loss: 21.5530, Validation Loss Current: 8.4212, Validation Loss AVG: 8.4212, lr: 0.00010000000000000003
Epoch [36/50], Training Loss: 21.2709, Validation Loss Current: 8.3850, Validation Loss AVG: 8.3850, lr: 0.00010000000000000003
Epoch [37/50], Training Loss: 21.3352, Validation Loss Current: 8.4188, Validation Loss AVG: 8.4188, lr: 0.00010000000000000003
Epoch [38/50], Training Loss: 22.6764, Validation Loss Current: 8.4243, Validation Loss AVG: 8.4243, lr: 0.00010000000000000003
Epoch [39/50], Training Loss: 21.4312, Validation Loss Current: 8.4019, Validation Loss AVG: 8.4019, lr: 0.00010000000000000003
Epoch [40/50], Training Loss: 22.0608, Validation Loss Current: 8.3928, Validation Loss AVG: 8.3928, lr: 1.0000000000000004e-05
Epoch [41/50], Training Loss: 22.5236, Validation Loss Current: 8.4091, Validation Loss AVG: 8.4091, lr: 1.0000000000000004e-05
Epoch [42/50], Training Loss: 22.5228, Validation Loss Current: 8.4354, Validation Loss AVG: 8.4354, lr: 1.0000000000000004e-05
Epoch [43/50], Training Loss: 21.7789, Validation Loss Current: 8.4194, Validation Loss AVG: 8.4194, lr: 1.0000000000000004e-05
Epoch [44/50], Training Loss: 21.4150, Validation Loss Current: 8.4230, Validation Loss AVG: 8.4230, lr: 1.0000000000000004e-05
Epoch [45/50], Training Loss: 22.0911, Validation Loss Current: 8.3979, Validation Loss AVG: 8.3979, lr: 1.0000000000000004e-05
Epoch [46/50], Training Loss: 21.6729, Validation Loss Current: 8.4041, Validation Loss AVG: 8.4041, lr: 1.0000000000000004e-06
Epoch [47/50], Training Loss: 21.5991, Validation Loss Current: 8.4116, Validation Loss AVG: 8.4116, lr: 1.0000000000000004e-06
Epoch [48/50], Training Loss: 22.6799, Validation Loss Current: 8.4194, Validation Loss AVG: 8.4194, lr: 1.0000000000000004e-06
Epoch [49/50], Training Loss: 22.3405, Validation Loss Current: 8.3885, Validation Loss AVG: 8.3885, lr: 1.0000000000000004e-06
Epoch [50/50], Training Loss: 21.9397, Validation Loss Current: 8.4166, Validation Loss AVG: 8.4166, lr: 1.0000000000000004e-06
Epoch [51/50], Training Loss: 22.2552, Validation Loss Current: 8.3930, Validation Loss AVG: 8.3930, lr: 1.0000000000000004e-06
 --- Early Stopped ---
Patch distance: 0.6 finished training. Best epoch: 21 Best val accuracy: [0.2351973684210526, 0.2351973684210526, 0.2351973684210526, 0.2351973684210526, 0.2351973684210526, 0.26348684210526313, 0.31743421052631576, 0.2690789473684211, 0.3263157894736842, 0.3473684210526316, 0.3549342105263158, 0.23914473684210527, 0.3213815789473684, 0.2917763157894737, 0.3388157894736842, 0.2845394736842105, 0.3078947368421052, 0.35690789473684215, 0.37763157894736843, 0.3888157894736842, 0.3907894736842105, 0.38125000000000003, 0.38421052631578945, 0.3703947368421052, 0.39572368421052634, 0.3944078947368421, 0.3875, 0.40032894736842106, 0.4, 0.39440789473684207, 0.3953947368421053, 0.40230263157894736, 0.39473684210526316, 0.3940789473684211, 0.393421052631579, 0.39671052631578946, 0.39539473684210524, 0.3986842105263158, 0.39671052631578946, 0.3973684210526316, 0.3963815789473684, 0.39703947368421055, 0.39703947368421055, 0.39671052631578946, 0.3960526315789473, 0.3963815789473684, 0.3963815789473684, 0.3963815789473684, 0.3963815789473684, 0.3960526315789473, 0.3963815789473684] Best val loss: 8.283354759216309


Loaded best state dict for [0.6]
Current group: 0.8
Epoch [1/50], Training Loss: 29.9277, Validation Loss Current: 14.1446, Validation Loss AVG: 14.1446, lr: 0.1
Epoch [2/50], Training Loss: 34.7039, Validation Loss Current: 11.3309, Validation Loss AVG: 11.3309, lr: 0.1
Epoch [3/50], Training Loss: 33.7602, Validation Loss Current: 9.0113, Validation Loss AVG: 9.0113, lr: 0.1
Epoch [4/50], Training Loss: 33.5161, Validation Loss Current: 11.3302, Validation Loss AVG: 11.3302, lr: 0.1
Epoch [5/50], Training Loss: 34.2405, Validation Loss Current: 12.1726, Validation Loss AVG: 12.1726, lr: 0.1
Epoch [6/50], Training Loss: 34.1980, Validation Loss Current: 9.8954, Validation Loss AVG: 9.8954, lr: 0.1
Epoch [7/50], Training Loss: 34.7972, Validation Loss Current: 9.1730, Validation Loss AVG: 9.1730, lr: 0.1
Epoch [8/50], Training Loss: 33.3915, Validation Loss Current: 12.6917, Validation Loss AVG: 12.6917, lr: 0.1
Epoch [9/50], Training Loss: 32.8139, Validation Loss Current: 12.8013, Validation Loss AVG: 12.8013, lr: 0.1
Epoch [10/50], Training Loss: 33.9349, Validation Loss Current: 8.8056, Validation Loss AVG: 8.8056, lr: 0.010000000000000002
Epoch [11/50], Training Loss: 28.5833, Validation Loss Current: 8.4093, Validation Loss AVG: 8.4093, lr: 0.010000000000000002
Epoch [12/50], Training Loss: 26.9250, Validation Loss Current: 8.4964, Validation Loss AVG: 8.4964, lr: 0.010000000000000002
Epoch [13/50], Training Loss: 26.3360, Validation Loss Current: 8.7058, Validation Loss AVG: 8.7058, lr: 0.010000000000000002
Epoch [14/50], Training Loss: 25.6421, Validation Loss Current: 8.3361, Validation Loss AVG: 8.3361, lr: 0.010000000000000002
Epoch [15/50], Training Loss: 27.0905, Validation Loss Current: 8.5578, Validation Loss AVG: 8.5578, lr: 0.010000000000000002
Epoch [16/50], Training Loss: 23.6736, Validation Loss Current: 8.2929, Validation Loss AVG: 8.2929, lr: 0.010000000000000002
Epoch [17/50], Training Loss: 23.5847, Validation Loss Current: 8.4554, Validation Loss AVG: 8.4554, lr: 0.010000000000000002
Epoch [18/50], Training Loss: 23.8469, Validation Loss Current: 8.4582, Validation Loss AVG: 8.4582, lr: 0.010000000000000002
Epoch [19/50], Training Loss: 22.5246, Validation Loss Current: 8.8918, Validation Loss AVG: 8.8918, lr: 0.010000000000000002
Epoch [20/50], Training Loss: 21.9262, Validation Loss Current: 8.8448, Validation Loss AVG: 8.8448, lr: 0.010000000000000002
Epoch [21/50], Training Loss: 21.3670, Validation Loss Current: 8.6846, Validation Loss AVG: 8.6846, lr: 0.010000000000000002
Epoch [22/50], Training Loss: 20.5000, Validation Loss Current: 9.3299, Validation Loss AVG: 9.3299, lr: 0.010000000000000002
Epoch [23/50], Training Loss: 20.8716, Validation Loss Current: 9.1883, Validation Loss AVG: 9.1883, lr: 0.0010000000000000002
Epoch [24/50], Training Loss: 19.2936, Validation Loss Current: 9.1236, Validation Loss AVG: 9.1236, lr: 0.0010000000000000002
Epoch [25/50], Training Loss: 20.9370, Validation Loss Current: 9.2017, Validation Loss AVG: 9.2017, lr: 0.0010000000000000002
Epoch [26/50], Training Loss: 18.5492, Validation Loss Current: 9.4212, Validation Loss AVG: 9.4212, lr: 0.0010000000000000002
Epoch [27/50], Training Loss: 19.4771, Validation Loss Current: 9.2491, Validation Loss AVG: 9.2491, lr: 0.0010000000000000002
Epoch [28/50], Training Loss: 18.6487, Validation Loss Current: 9.4978, Validation Loss AVG: 9.4978, lr: 0.0010000000000000002
Epoch [29/50], Training Loss: 19.2761, Validation Loss Current: 9.4503, Validation Loss AVG: 9.4503, lr: 0.00010000000000000003
Epoch [30/50], Training Loss: 19.7309, Validation Loss Current: 9.4140, Validation Loss AVG: 9.4140, lr: 0.00010000000000000003
Epoch [31/50], Training Loss: 18.8690, Validation Loss Current: 9.4119, Validation Loss AVG: 9.4119, lr: 0.00010000000000000003
Epoch [32/50], Training Loss: 18.3261, Validation Loss Current: 9.4345, Validation Loss AVG: 9.4345, lr: 0.00010000000000000003
Epoch [33/50], Training Loss: 18.5104, Validation Loss Current: 9.3778, Validation Loss AVG: 9.3778, lr: 0.00010000000000000003
Epoch [34/50], Training Loss: 18.7195, Validation Loss Current: 9.3907, Validation Loss AVG: 9.3907, lr: 0.00010000000000000003
Epoch [35/50], Training Loss: 18.1378, Validation Loss Current: 9.4253, Validation Loss AVG: 9.4253, lr: 1.0000000000000004e-05
Epoch [36/50], Training Loss: 18.6864, Validation Loss Current: 9.3909, Validation Loss AVG: 9.3909, lr: 1.0000000000000004e-05
Epoch [37/50], Training Loss: 18.7841, Validation Loss Current: 9.4497, Validation Loss AVG: 9.4497, lr: 1.0000000000000004e-05
Epoch [38/50], Training Loss: 19.1824, Validation Loss Current: 9.4224, Validation Loss AVG: 9.4224, lr: 1.0000000000000004e-05
Epoch [39/50], Training Loss: 18.5074, Validation Loss Current: 9.4259, Validation Loss AVG: 9.4259, lr: 1.0000000000000004e-05
Epoch [40/50], Training Loss: 19.3274, Validation Loss Current: 9.4376, Validation Loss AVG: 9.4376, lr: 1.0000000000000004e-05
Epoch [41/50], Training Loss: 18.8763, Validation Loss Current: 9.4413, Validation Loss AVG: 9.4413, lr: 1.0000000000000004e-06
Epoch [42/50], Training Loss: 18.5443, Validation Loss Current: 9.4430, Validation Loss AVG: 9.4430, lr: 1.0000000000000004e-06
Epoch [43/50], Training Loss: 19.6540, Validation Loss Current: 9.4318, Validation Loss AVG: 9.4318, lr: 1.0000000000000004e-06
Epoch [44/50], Training Loss: 18.1982, Validation Loss Current: 9.4490, Validation Loss AVG: 9.4490, lr: 1.0000000000000004e-06
Epoch [45/50], Training Loss: 17.8606, Validation Loss Current: 9.4238, Validation Loss AVG: 9.4238, lr: 1.0000000000000004e-06
Epoch [46/50], Training Loss: 19.2877, Validation Loss Current: 9.4306, Validation Loss AVG: 9.4306, lr: 1.0000000000000004e-06
 --- Early Stopped ---
Patch distance: 0.8 finished training. Best epoch: 16 Best val accuracy: [0.1963815789473684, 0.28519736842105264, 0.33980263157894736, 0.26151315789473684, 0.1680921052631579, 0.31282894736842104, 0.31644736842105264, 0.26414473684210527, 0.24177631578947373, 0.3651315789473685, 0.3901315789473684, 0.39013157894736844, 0.3799342105263158, 0.3963815789473685, 0.3960526315789473, 0.39868421052631586, 0.3986842105263158, 0.4072368421052632, 0.4023026315789474, 0.41217105263157894, 0.41217105263157894, 0.4072368421052632, 0.4128289473684211, 0.41381578947368425, 0.4134868421052632, 0.41217105263157894, 0.41578947368421054, 0.40855263157894733, 0.4095394736842105, 0.41052631578947374, 0.4098684210526316, 0.41052631578947374, 0.41019736842105264, 0.41085526315789467, 0.41052631578947363, 0.41019736842105264, 0.41052631578947363, 0.41019736842105264, 0.41052631578947363, 0.41019736842105264, 0.41019736842105264, 0.41019736842105264, 0.41019736842105264, 0.41019736842105264, 0.41019736842105264, 0.41019736842105264] Best val loss: 8.292916846275329


Loaded best state dict for [0.6, 0.8]
Current group: 1
Epoch [1/50], Training Loss: 30.6403, Validation Loss Current: 8.4069, Validation Loss AVG: 9.4470, lr: 0.1
Epoch [2/50], Training Loss: 32.0845, Validation Loss Current: 14.1983, Validation Loss AVG: 23.8513, lr: 0.1
Epoch [3/50], Training Loss: 35.2484, Validation Loss Current: 8.5622, Validation Loss AVG: 10.2242, lr: 0.1
Epoch [4/50], Training Loss: 32.9771, Validation Loss Current: 18.5694, Validation Loss AVG: 26.3501, lr: 0.1
Epoch [5/50], Training Loss: 35.2110, Validation Loss Current: 8.7327, Validation Loss AVG: 9.9047, lr: 0.1
Epoch [6/50], Training Loss: 32.2133, Validation Loss Current: 9.5556, Validation Loss AVG: 10.6669, lr: 0.1
Epoch [7/50], Training Loss: 32.3175, Validation Loss Current: 10.1487, Validation Loss AVG: 11.4663, lr: 0.1
Epoch [8/50], Training Loss: 31.9109, Validation Loss Current: 7.5871, Validation Loss AVG: 9.4527, lr: 0.010000000000000002
Epoch [9/50], Training Loss: 27.7894, Validation Loss Current: 7.1042, Validation Loss AVG: 9.0076, lr: 0.010000000000000002
Epoch [10/50], Training Loss: 25.8661, Validation Loss Current: 6.9495, Validation Loss AVG: 8.5480, lr: 0.010000000000000002
Epoch [11/50], Training Loss: 25.1348, Validation Loss Current: 7.0191, Validation Loss AVG: 8.6274, lr: 0.010000000000000002
Epoch [12/50], Training Loss: 25.3393, Validation Loss Current: 6.9381, Validation Loss AVG: 8.8652, lr: 0.010000000000000002
Epoch [13/50], Training Loss: 23.0398, Validation Loss Current: 6.7698, Validation Loss AVG: 8.7164, lr: 0.010000000000000002
Epoch [14/50], Training Loss: 23.5452, Validation Loss Current: 6.9056, Validation Loss AVG: 8.5978, lr: 0.010000000000000002
Epoch [15/50], Training Loss: 22.1713, Validation Loss Current: 6.8582, Validation Loss AVG: 8.9008, lr: 0.010000000000000002
Epoch [16/50], Training Loss: 22.7210, Validation Loss Current: 7.1297, Validation Loss AVG: 9.3630, lr: 0.010000000000000002
Epoch [17/50], Training Loss: 22.6366, Validation Loss Current: 6.9342, Validation Loss AVG: 9.1994, lr: 0.010000000000000002
Epoch [18/50], Training Loss: 20.8178, Validation Loss Current: 7.0435, Validation Loss AVG: 9.0213, lr: 0.010000000000000002
Epoch [19/50], Training Loss: 22.9820, Validation Loss Current: 7.3084, Validation Loss AVG: 9.6249, lr: 0.010000000000000002
Epoch [20/50], Training Loss: 18.9820, Validation Loss Current: 6.9034, Validation Loss AVG: 9.5544, lr: 0.0010000000000000002
Epoch [21/50], Training Loss: 18.5818, Validation Loss Current: 6.9306, Validation Loss AVG: 9.5203, lr: 0.0010000000000000002
Epoch [22/50], Training Loss: 19.0763, Validation Loss Current: 6.9918, Validation Loss AVG: 9.6189, lr: 0.0010000000000000002
Epoch [23/50], Training Loss: 18.4202, Validation Loss Current: 6.9896, Validation Loss AVG: 9.7596, lr: 0.0010000000000000002
Epoch [24/50], Training Loss: 17.7418, Validation Loss Current: 7.0558, Validation Loss AVG: 9.6619, lr: 0.0010000000000000002
Epoch [25/50], Training Loss: 17.6090, Validation Loss Current: 7.0769, Validation Loss AVG: 9.7955, lr: 0.0010000000000000002
Epoch [26/50], Training Loss: 18.8339, Validation Loss Current: 7.0184, Validation Loss AVG: 9.8177, lr: 0.00010000000000000003
Epoch [27/50], Training Loss: 17.9152, Validation Loss Current: 7.0293, Validation Loss AVG: 9.8086, lr: 0.00010000000000000003
Epoch [28/50], Training Loss: 17.6473, Validation Loss Current: 7.0029, Validation Loss AVG: 9.8029, lr: 0.00010000000000000003
Epoch [29/50], Training Loss: 17.2155, Validation Loss Current: 6.9843, Validation Loss AVG: 9.7935, lr: 0.00010000000000000003
Epoch [30/50], Training Loss: 17.3667, Validation Loss Current: 7.0103, Validation Loss AVG: 9.7947, lr: 0.00010000000000000003
Epoch [31/50], Training Loss: 17.2841, Validation Loss Current: 7.0697, Validation Loss AVG: 9.7883, lr: 0.00010000000000000003
Epoch [32/50], Training Loss: 20.4185, Validation Loss Current: 6.9830, Validation Loss AVG: 9.7633, lr: 1.0000000000000004e-05
Epoch [33/50], Training Loss: 17.7536, Validation Loss Current: 7.0162, Validation Loss AVG: 9.7616, lr: 1.0000000000000004e-05
Epoch [34/50], Training Loss: 17.7945, Validation Loss Current: 7.0185, Validation Loss AVG: 9.7723, lr: 1.0000000000000004e-05
Epoch [35/50], Training Loss: 18.2881, Validation Loss Current: 7.0734, Validation Loss AVG: 9.7690, lr: 1.0000000000000004e-05
Epoch [36/50], Training Loss: 17.7491, Validation Loss Current: 7.0702, Validation Loss AVG: 9.7892, lr: 1.0000000000000004e-05
Epoch [37/50], Training Loss: 17.4793, Validation Loss Current: 7.0043, Validation Loss AVG: 9.7932, lr: 1.0000000000000004e-05
Epoch [38/50], Training Loss: 19.0806, Validation Loss Current: 7.0344, Validation Loss AVG: 9.7852, lr: 1.0000000000000004e-06
Epoch [39/50], Training Loss: 17.6518, Validation Loss Current: 7.0556, Validation Loss AVG: 9.7960, lr: 1.0000000000000004e-06
Epoch [40/50], Training Loss: 17.4744, Validation Loss Current: 7.0195, Validation Loss AVG: 9.7966, lr: 1.0000000000000004e-06
Epoch [41/50], Training Loss: 17.3501, Validation Loss Current: 7.0387, Validation Loss AVG: 9.8022, lr: 1.0000000000000004e-06
Epoch [42/50], Training Loss: 17.3778, Validation Loss Current: 7.0344, Validation Loss AVG: 9.8061, lr: 1.0000000000000004e-06
Epoch [43/50], Training Loss: 17.4918, Validation Loss Current: 7.0488, Validation Loss AVG: 9.7892, lr: 1.0000000000000004e-06
 --- Early Stopped ---
Patch distance: 1 finished training. Best epoch: 13 Best val accuracy: [0.3980263157894737, 0.26480263157894735, 0.38651315789473684, 0.3092105263157895, 0.3881578947368421, 0.3207236842105263, 0.28125, 0.43256578947368424, 0.48026315789473684, 0.4967105263157895, 0.5, 0.49835526315789475, 0.5164473684210527, 0.5098684210526315, 0.5, 0.5, 0.5016447368421053, 0.5032894736842105, 0.48355263157894735, 0.5098684210526315, 0.5213815789473685, 0.5148026315789473, 0.5164473684210527, 0.5148026315789473, 0.5131578947368421, 0.5131578947368421, 0.5131578947368421, 0.5131578947368421, 0.5098684210526315, 0.5098684210526315, 0.5098684210526315, 0.5098684210526315, 0.5098684210526315, 0.5098684210526315, 0.5098684210526315, 0.5098684210526315, 0.5098684210526315, 0.5098684210526315, 0.5098684210526315, 0.5098684210526315, 0.5098684210526315, 0.5098684210526315, 0.5098684210526315] Best val loss: 6.769819259643555


----- Training alexnet with sequence: [0.8, 1] -----
Current group: 0.8
Epoch [1/75], Training Loss: 40.9414, Validation Loss Current: 10.0467, Validation Loss AVG: 10.0467, lr: 0.1
Epoch [2/75], Training Loss: 40.3557, Validation Loss Current: 10.0383, Validation Loss AVG: 10.0383, lr: 0.1
Epoch [3/75], Training Loss: 40.6742, Validation Loss Current: 10.0530, Validation Loss AVG: 10.0530, lr: 0.1
Epoch [4/75], Training Loss: 39.7759, Validation Loss Current: 10.2036, Validation Loss AVG: 10.2036, lr: 0.1
Epoch [5/75], Training Loss: 39.1504, Validation Loss Current: 9.8216, Validation Loss AVG: 9.8216, lr: 0.1
Epoch [6/75], Training Loss: 37.1667, Validation Loss Current: 9.9268, Validation Loss AVG: 9.9268, lr: 0.1
Epoch [7/75], Training Loss: 35.7632, Validation Loss Current: 10.0557, Validation Loss AVG: 10.0557, lr: 0.1
Epoch [8/75], Training Loss: 35.4946, Validation Loss Current: 9.9112, Validation Loss AVG: 9.9112, lr: 0.1
Epoch [9/75], Training Loss: 35.0484, Validation Loss Current: 10.0107, Validation Loss AVG: 10.0107, lr: 0.1
Epoch [10/75], Training Loss: 36.5786, Validation Loss Current: 9.0428, Validation Loss AVG: 9.0428, lr: 0.1
Epoch [11/75], Training Loss: 32.4410, Validation Loss Current: 9.1177, Validation Loss AVG: 9.1177, lr: 0.1
Epoch [12/75], Training Loss: 32.7906, Validation Loss Current: 11.4638, Validation Loss AVG: 11.4638, lr: 0.1
Epoch [13/75], Training Loss: 34.3480, Validation Loss Current: 12.8496, Validation Loss AVG: 12.8496, lr: 0.1
Epoch [14/75], Training Loss: 33.4157, Validation Loss Current: 9.4338, Validation Loss AVG: 9.4338, lr: 0.1
Epoch [15/75], Training Loss: 35.9911, Validation Loss Current: 9.8995, Validation Loss AVG: 9.8995, lr: 0.1
Epoch [16/75], Training Loss: 35.4378, Validation Loss Current: 9.0770, Validation Loss AVG: 9.0770, lr: 0.1
Epoch [17/75], Training Loss: 30.9405, Validation Loss Current: 8.4415, Validation Loss AVG: 8.4415, lr: 0.010000000000000002
Epoch [18/75], Training Loss: 27.8405, Validation Loss Current: 8.4537, Validation Loss AVG: 8.4537, lr: 0.010000000000000002
Epoch [19/75], Training Loss: 26.0827, Validation Loss Current: 8.3209, Validation Loss AVG: 8.3209, lr: 0.010000000000000002
Epoch [20/75], Training Loss: 26.7327, Validation Loss Current: 8.3701, Validation Loss AVG: 8.3701, lr: 0.010000000000000002
Epoch [21/75], Training Loss: 24.9936, Validation Loss Current: 8.2137, Validation Loss AVG: 8.2137, lr: 0.010000000000000002
Epoch [22/75], Training Loss: 24.5674, Validation Loss Current: 8.3445, Validation Loss AVG: 8.3445, lr: 0.010000000000000002
Epoch [23/75], Training Loss: 24.8217, Validation Loss Current: 8.4467, Validation Loss AVG: 8.4467, lr: 0.010000000000000002
Epoch [24/75], Training Loss: 23.8817, Validation Loss Current: 8.1406, Validation Loss AVG: 8.1406, lr: 0.010000000000000002
Epoch [25/75], Training Loss: 23.3799, Validation Loss Current: 9.1114, Validation Loss AVG: 9.1114, lr: 0.010000000000000002
Epoch [26/75], Training Loss: 23.0191, Validation Loss Current: 8.6104, Validation Loss AVG: 8.6104, lr: 0.010000000000000002
Epoch [27/75], Training Loss: 23.2031, Validation Loss Current: 8.6893, Validation Loss AVG: 8.6893, lr: 0.010000000000000002
Epoch [28/75], Training Loss: 22.2500, Validation Loss Current: 9.0296, Validation Loss AVG: 9.0296, lr: 0.010000000000000002
Epoch [29/75], Training Loss: 21.9213, Validation Loss Current: 8.7373, Validation Loss AVG: 8.7373, lr: 0.010000000000000002
Epoch [30/75], Training Loss: 21.8769, Validation Loss Current: 8.7096, Validation Loss AVG: 8.7096, lr: 0.010000000000000002
Epoch [31/75], Training Loss: 20.7113, Validation Loss Current: 8.6879, Validation Loss AVG: 8.6879, lr: 0.0010000000000000002
Epoch [32/75], Training Loss: 20.9639, Validation Loss Current: 8.8278, Validation Loss AVG: 8.8278, lr: 0.0010000000000000002
Epoch [33/75], Training Loss: 19.7306, Validation Loss Current: 8.7224, Validation Loss AVG: 8.7224, lr: 0.0010000000000000002
Epoch [34/75], Training Loss: 19.7200, Validation Loss Current: 8.6720, Validation Loss AVG: 8.6720, lr: 0.0010000000000000002
Epoch [35/75], Training Loss: 18.9748, Validation Loss Current: 8.7502, Validation Loss AVG: 8.7502, lr: 0.0010000000000000002
Epoch [36/75], Training Loss: 19.2066, Validation Loss Current: 8.8344, Validation Loss AVG: 8.8344, lr: 0.0010000000000000002
Epoch [37/75], Training Loss: 19.5925, Validation Loss Current: 8.8589, Validation Loss AVG: 8.8589, lr: 0.00010000000000000003
Epoch [38/75], Training Loss: 19.1398, Validation Loss Current: 8.8640, Validation Loss AVG: 8.8640, lr: 0.00010000000000000003
Epoch [39/75], Training Loss: 20.0779, Validation Loss Current: 8.8270, Validation Loss AVG: 8.8270, lr: 0.00010000000000000003
Epoch [40/75], Training Loss: 19.3368, Validation Loss Current: 8.8402, Validation Loss AVG: 8.8402, lr: 0.00010000000000000003
Epoch [41/75], Training Loss: 20.1836, Validation Loss Current: 8.8243, Validation Loss AVG: 8.8243, lr: 0.00010000000000000003
Epoch [42/75], Training Loss: 19.3403, Validation Loss Current: 8.8547, Validation Loss AVG: 8.8547, lr: 0.00010000000000000003
Epoch [43/75], Training Loss: 20.3219, Validation Loss Current: 8.8167, Validation Loss AVG: 8.8167, lr: 1.0000000000000004e-05
Epoch [44/75], Training Loss: 18.8877, Validation Loss Current: 8.8337, Validation Loss AVG: 8.8337, lr: 1.0000000000000004e-05
Epoch [45/75], Training Loss: 19.7783, Validation Loss Current: 8.8317, Validation Loss AVG: 8.8317, lr: 1.0000000000000004e-05
Epoch [46/75], Training Loss: 18.6456, Validation Loss Current: 8.8569, Validation Loss AVG: 8.8569, lr: 1.0000000000000004e-05
Epoch [47/75], Training Loss: 19.5212, Validation Loss Current: 8.8303, Validation Loss AVG: 8.8303, lr: 1.0000000000000004e-05
Epoch [48/75], Training Loss: 19.5700, Validation Loss Current: 8.8357, Validation Loss AVG: 8.8357, lr: 1.0000000000000004e-05
Epoch [49/75], Training Loss: 19.0858, Validation Loss Current: 8.8292, Validation Loss AVG: 8.8292, lr: 1.0000000000000004e-06
Epoch [50/75], Training Loss: 19.4169, Validation Loss Current: 8.8083, Validation Loss AVG: 8.8083, lr: 1.0000000000000004e-06
Epoch [51/75], Training Loss: 19.6267, Validation Loss Current: 8.8325, Validation Loss AVG: 8.8325, lr: 1.0000000000000004e-06
Epoch [52/75], Training Loss: 19.3385, Validation Loss Current: 8.8246, Validation Loss AVG: 8.8246, lr: 1.0000000000000004e-06
Epoch [53/75], Training Loss: 21.1731, Validation Loss Current: 8.8274, Validation Loss AVG: 8.8274, lr: 1.0000000000000004e-06
Epoch [54/75], Training Loss: 18.7136, Validation Loss Current: 8.8036, Validation Loss AVG: 8.8036, lr: 1.0000000000000004e-06
 --- Early Stopped ---
Patch distance: 0.8 finished training. Best epoch: 24 Best val accuracy: [0.2351973684210526, 0.2351973684210526, 0.2351973684210526, 0.2351973684210526, 0.26151315789473684, 0.29210526315789476, 0.21875, 0.30328947368421055, 0.3078947368421053, 0.3200657894736842, 0.33092105263157895, 0.22467105263157894, 0.3154605263157895, 0.34638157894736843, 0.2967105263157895, 0.31085526315789475, 0.3888157894736842, 0.3759868421052632, 0.39572368421052634, 0.39276315789473687, 0.39375, 0.3894736842105263, 0.40328947368421053, 0.41217105263157894, 0.3697368421052632, 0.3740131578947369, 0.37138157894736845, 0.3694078947368421, 0.4141447368421053, 0.39375, 0.4161184210526316, 0.4134868421052631, 0.4240131578947368, 0.42434210526315785, 0.42269736842105265, 0.4233552631578947, 0.4200657894736842, 0.41940789473684204, 0.4213815789473684, 0.4217105263157895, 0.4200657894736842, 0.4203947368421053, 0.4207236842105263, 0.4213815789473684, 0.4203947368421052, 0.42105263157894735, 0.42105263157894735, 0.4213815789473684, 0.4213815789473684, 0.4213815789473684, 0.4213815789473684, 0.4213815789473684, 0.4217105263157895, 0.4217105263157895] Best val loss: 8.14057960510254


Loaded best state dict for [0.8]
Current group: 1
Epoch [1/75], Training Loss: 29.4658, Validation Loss Current: 13.0476, Validation Loss AVG: 22.6037, lr: 0.1
Epoch [2/75], Training Loss: 38.2305, Validation Loss Current: 8.6313, Validation Loss AVG: 9.5811, lr: 0.1
Epoch [3/75], Training Loss: 32.2412, Validation Loss Current: 11.0776, Validation Loss AVG: 14.2886, lr: 0.1
Epoch [4/75], Training Loss: 32.4088, Validation Loss Current: 9.5035, Validation Loss AVG: 9.7691, lr: 0.1
Epoch [5/75], Training Loss: 34.3254, Validation Loss Current: 9.2136, Validation Loss AVG: 10.2287, lr: 0.1
Epoch [6/75], Training Loss: 31.9924, Validation Loss Current: 7.6731, Validation Loss AVG: 8.9207, lr: 0.1
Epoch [7/75], Training Loss: 31.0814, Validation Loss Current: 7.8020, Validation Loss AVG: 11.2013, lr: 0.1
Epoch [8/75], Training Loss: 32.5336, Validation Loss Current: 11.2332, Validation Loss AVG: 17.4726, lr: 0.1
Epoch [9/75], Training Loss: 35.3218, Validation Loss Current: 9.6337, Validation Loss AVG: 10.0805, lr: 0.1
Epoch [10/75], Training Loss: 34.5123, Validation Loss Current: 9.7724, Validation Loss AVG: 12.5707, lr: 0.1
Epoch [11/75], Training Loss: 32.0626, Validation Loss Current: 16.1121, Validation Loss AVG: 27.5641, lr: 0.1
Epoch [12/75], Training Loss: 36.1111, Validation Loss Current: 8.8039, Validation Loss AVG: 9.4245, lr: 0.1
Epoch [13/75], Training Loss: 31.6879, Validation Loss Current: 7.8191, Validation Loss AVG: 8.9955, lr: 0.010000000000000002
Epoch [14/75], Training Loss: 29.8411, Validation Loss Current: 7.5026, Validation Loss AVG: 8.9726, lr: 0.010000000000000002
Epoch [15/75], Training Loss: 27.2665, Validation Loss Current: 7.2065, Validation Loss AVG: 8.6768, lr: 0.010000000000000002
Epoch [16/75], Training Loss: 25.8427, Validation Loss Current: 7.1722, Validation Loss AVG: 8.8381, lr: 0.010000000000000002
Epoch [17/75], Training Loss: 24.1423, Validation Loss Current: 7.1707, Validation Loss AVG: 8.8416, lr: 0.010000000000000002
Epoch [18/75], Training Loss: 24.2087, Validation Loss Current: 6.9817, Validation Loss AVG: 9.1624, lr: 0.010000000000000002
Epoch [19/75], Training Loss: 23.1287, Validation Loss Current: 7.0866, Validation Loss AVG: 8.8320, lr: 0.010000000000000002
Epoch [20/75], Training Loss: 22.4359, Validation Loss Current: 6.9602, Validation Loss AVG: 9.6520, lr: 0.010000000000000002
Epoch [21/75], Training Loss: 21.7151, Validation Loss Current: 7.2555, Validation Loss AVG: 9.3164, lr: 0.010000000000000002
Epoch [22/75], Training Loss: 21.6788, Validation Loss Current: 6.9346, Validation Loss AVG: 9.1732, lr: 0.010000000000000002
Epoch [23/75], Training Loss: 21.1932, Validation Loss Current: 6.9659, Validation Loss AVG: 9.2669, lr: 0.010000000000000002
Epoch [24/75], Training Loss: 19.9688, Validation Loss Current: 6.9936, Validation Loss AVG: 10.0579, lr: 0.010000000000000002
Epoch [25/75], Training Loss: 20.1949, Validation Loss Current: 7.0263, Validation Loss AVG: 9.5151, lr: 0.010000000000000002
Epoch [26/75], Training Loss: 19.3288, Validation Loss Current: 7.1652, Validation Loss AVG: 9.3037, lr: 0.010000000000000002
Epoch [27/75], Training Loss: 18.6073, Validation Loss Current: 7.1732, Validation Loss AVG: 10.1978, lr: 0.010000000000000002
Epoch [28/75], Training Loss: 18.0293, Validation Loss Current: 7.3281, Validation Loss AVG: 10.1834, lr: 0.010000000000000002
Epoch [29/75], Training Loss: 17.2124, Validation Loss Current: 7.2153, Validation Loss AVG: 9.5343, lr: 0.0010000000000000002
Epoch [30/75], Training Loss: 16.7744, Validation Loss Current: 7.2494, Validation Loss AVG: 9.6254, lr: 0.0010000000000000002
Epoch [31/75], Training Loss: 16.9821, Validation Loss Current: 7.2576, Validation Loss AVG: 9.6341, lr: 0.0010000000000000002
Epoch [32/75], Training Loss: 16.3569, Validation Loss Current: 7.1977, Validation Loss AVG: 9.6906, lr: 0.0010000000000000002
Epoch [33/75], Training Loss: 15.8985, Validation Loss Current: 7.3503, Validation Loss AVG: 9.8401, lr: 0.0010000000000000002
Epoch [34/75], Training Loss: 16.0221, Validation Loss Current: 7.3274, Validation Loss AVG: 9.7882, lr: 0.0010000000000000002
Epoch [35/75], Training Loss: 15.7264, Validation Loss Current: 7.3660, Validation Loss AVG: 9.7919, lr: 0.00010000000000000003
Epoch [36/75], Training Loss: 16.3090, Validation Loss Current: 7.2914, Validation Loss AVG: 9.7885, lr: 0.00010000000000000003
Epoch [37/75], Training Loss: 16.3882, Validation Loss Current: 7.3457, Validation Loss AVG: 9.8319, lr: 0.00010000000000000003
Epoch [38/75], Training Loss: 16.1131, Validation Loss Current: 7.3327, Validation Loss AVG: 9.8268, lr: 0.00010000000000000003
Epoch [39/75], Training Loss: 15.7449, Validation Loss Current: 7.3374, Validation Loss AVG: 9.8353, lr: 0.00010000000000000003
Epoch [40/75], Training Loss: 15.6047, Validation Loss Current: 7.3212, Validation Loss AVG: 9.8338, lr: 0.00010000000000000003
Epoch [41/75], Training Loss: 15.6348, Validation Loss Current: 7.3676, Validation Loss AVG: 9.7986, lr: 1.0000000000000004e-05
Epoch [42/75], Training Loss: 15.5751, Validation Loss Current: 7.3644, Validation Loss AVG: 9.8102, lr: 1.0000000000000004e-05
Epoch [43/75], Training Loss: 15.5602, Validation Loss Current: 7.2947, Validation Loss AVG: 9.8557, lr: 1.0000000000000004e-05
Epoch [44/75], Training Loss: 15.7135, Validation Loss Current: 7.4407, Validation Loss AVG: 9.8509, lr: 1.0000000000000004e-05
Epoch [45/75], Training Loss: 16.2201, Validation Loss Current: 7.3711, Validation Loss AVG: 9.8360, lr: 1.0000000000000004e-05
Epoch [46/75], Training Loss: 16.0218, Validation Loss Current: 7.3104, Validation Loss AVG: 9.8246, lr: 1.0000000000000004e-05
Epoch [47/75], Training Loss: 16.0389, Validation Loss Current: 7.3866, Validation Loss AVG: 9.8369, lr: 1.0000000000000004e-06
Epoch [48/75], Training Loss: 16.1106, Validation Loss Current: 7.3946, Validation Loss AVG: 9.8302, lr: 1.0000000000000004e-06
Epoch [49/75], Training Loss: 15.7398, Validation Loss Current: 7.4730, Validation Loss AVG: 9.8655, lr: 1.0000000000000004e-06
Epoch [50/75], Training Loss: 17.2822, Validation Loss Current: 7.3534, Validation Loss AVG: 9.8510, lr: 1.0000000000000004e-06
Epoch [51/75], Training Loss: 15.9427, Validation Loss Current: 7.3213, Validation Loss AVG: 9.8244, lr: 1.0000000000000004e-06
Epoch [52/75], Training Loss: 15.5272, Validation Loss Current: 7.3730, Validation Loss AVG: 9.8653, lr: 1.0000000000000004e-06
 --- Early Stopped ---
Patch distance: 1 finished training. Best epoch: 22 Best val accuracy: [0.32730263157894735, 0.37006578947368424, 0.3157894736842105, 0.3996710526315789, 0.3190789473684211, 0.43585526315789475, 0.4457236842105263, 0.3059210526315789, 0.2680921052631579, 0.3223684210526316, 0.38651315789473684, 0.37006578947368424, 0.4309210526315789, 0.45394736842105265, 0.4786184210526316, 0.4786184210526316, 0.49506578947368424, 0.506578947368421, 0.5032894736842105, 0.4934210526315789, 0.5082236842105263, 0.5115131578947368, 0.5148026315789473, 0.506578947368421, 0.5148026315789473, 0.5115131578947368, 0.506578947368421, 0.5180921052631579, 0.5213815789473685, 0.5180921052631579, 0.5148026315789473, 0.5197368421052632, 0.5230263157894737, 0.5164473684210527, 0.5197368421052632, 0.5164473684210527, 0.5148026315789473, 0.5180921052631579, 0.5197368421052632, 0.5197368421052632, 0.5197368421052632, 0.5197368421052632, 0.5197368421052632, 0.5197368421052632, 0.5197368421052632, 0.5197368421052632, 0.5197368421052632, 0.5197368421052632, 0.5213815789473685, 0.5197368421052632, 0.5213815789473685, 0.5213815789473685] Best val loss: 6.934636831283569


----- Training alexnet with sequence: [1] -----
Current group: 1
Epoch [1/150], Training Loss: 40.1739, Validation Loss Current: 10.0071, Validation Loss AVG: 10.0977, lr: 0.1
Epoch [2/150], Training Loss: 40.1820, Validation Loss Current: 10.0514, Validation Loss AVG: 10.0554, lr: 0.1
Epoch [3/150], Training Loss: 39.9847, Validation Loss Current: 11.3703, Validation Loss AVG: 12.9472, lr: 0.1
Epoch [4/150], Training Loss: 40.1464, Validation Loss Current: 10.0575, Validation Loss AVG: 10.0894, lr: 0.1
Epoch [5/150], Training Loss: 38.8761, Validation Loss Current: 19.2290, Validation Loss AVG: 27.3999, lr: 0.1
Epoch [6/150], Training Loss: 42.5987, Validation Loss Current: 9.5409, Validation Loss AVG: 9.6767, lr: 0.1
Epoch [7/150], Training Loss: 39.8584, Validation Loss Current: 10.5548, Validation Loss AVG: 11.1742, lr: 0.1
Epoch [8/150], Training Loss: 38.3125, Validation Loss Current: 9.2144, Validation Loss AVG: 9.8525, lr: 0.1
Epoch [9/150], Training Loss: 38.6437, Validation Loss Current: 9.3092, Validation Loss AVG: 9.9097, lr: 0.1
Epoch [10/150], Training Loss: 36.6045, Validation Loss Current: 8.6855, Validation Loss AVG: 9.0858, lr: 0.1
Epoch [11/150], Training Loss: 37.2591, Validation Loss Current: 10.2846, Validation Loss AVG: 12.4433, lr: 0.1
Epoch [12/150], Training Loss: 35.7203, Validation Loss Current: 8.4688, Validation Loss AVG: 9.5378, lr: 0.1
Epoch [13/150], Training Loss: 36.4262, Validation Loss Current: 8.8737, Validation Loss AVG: 9.4526, lr: 0.1
Epoch [14/150], Training Loss: 35.6009, Validation Loss Current: 11.0672, Validation Loss AVG: 11.2509, lr: 0.1
Epoch [15/150], Training Loss: 35.1574, Validation Loss Current: 8.5775, Validation Loss AVG: 9.6019, lr: 0.1
Epoch [16/150], Training Loss: 32.4245, Validation Loss Current: 11.0491, Validation Loss AVG: 12.0070, lr: 0.1
Epoch [17/150], Training Loss: 36.5972, Validation Loss Current: 8.9255, Validation Loss AVG: 9.2602, lr: 0.1
Epoch [18/150], Training Loss: 35.3629, Validation Loss Current: 9.4840, Validation Loss AVG: 10.4335, lr: 0.1
Epoch [19/150], Training Loss: 32.9460, Validation Loss Current: 7.8649, Validation Loss AVG: 8.8799, lr: 0.010000000000000002
Epoch [20/150], Training Loss: 31.8111, Validation Loss Current: 7.6875, Validation Loss AVG: 8.7593, lr: 0.010000000000000002
Epoch [21/150], Training Loss: 31.8298, Validation Loss Current: 7.4314, Validation Loss AVG: 8.5687, lr: 0.010000000000000002
Epoch [22/150], Training Loss: 29.1808, Validation Loss Current: 7.3545, Validation Loss AVG: 8.5428, lr: 0.010000000000000002
Epoch [23/150], Training Loss: 27.7457, Validation Loss Current: 7.1137, Validation Loss AVG: 8.3241, lr: 0.010000000000000002
Epoch [24/150], Training Loss: 28.4146, Validation Loss Current: 7.1202, Validation Loss AVG: 8.4700, lr: 0.010000000000000002
Epoch [25/150], Training Loss: 26.2858, Validation Loss Current: 7.5693, Validation Loss AVG: 8.8707, lr: 0.010000000000000002
Epoch [26/150], Training Loss: 28.4448, Validation Loss Current: 7.0762, Validation Loss AVG: 8.3888, lr: 0.010000000000000002
Epoch [27/150], Training Loss: 25.5146, Validation Loss Current: 6.9328, Validation Loss AVG: 8.4002, lr: 0.010000000000000002
Epoch [28/150], Training Loss: 24.6796, Validation Loss Current: 6.8622, Validation Loss AVG: 8.3709, lr: 0.010000000000000002
Epoch [29/150], Training Loss: 25.5496, Validation Loss Current: 7.0135, Validation Loss AVG: 9.3699, lr: 0.010000000000000002
Epoch [30/150], Training Loss: 24.4824, Validation Loss Current: 7.0118, Validation Loss AVG: 8.9353, lr: 0.010000000000000002
Epoch [31/150], Training Loss: 24.9853, Validation Loss Current: 6.8812, Validation Loss AVG: 8.4713, lr: 0.010000000000000002
Epoch [32/150], Training Loss: 23.9641, Validation Loss Current: 7.4777, Validation Loss AVG: 9.5455, lr: 0.010000000000000002
Epoch [33/150], Training Loss: 24.5787, Validation Loss Current: 6.6401, Validation Loss AVG: 8.8336, lr: 0.010000000000000002
Epoch [34/150], Training Loss: 23.1206, Validation Loss Current: 6.7153, Validation Loss AVG: 8.6696, lr: 0.010000000000000002
Epoch [35/150], Training Loss: 22.2957, Validation Loss Current: 6.7053, Validation Loss AVG: 8.8233, lr: 0.010000000000000002
Epoch [36/150], Training Loss: 23.5841, Validation Loss Current: 6.6022, Validation Loss AVG: 9.1281, lr: 0.010000000000000002
Epoch [37/150], Training Loss: 22.3111, Validation Loss Current: 6.7960, Validation Loss AVG: 8.8938, lr: 0.010000000000000002
Epoch [38/150], Training Loss: 21.7549, Validation Loss Current: 6.7172, Validation Loss AVG: 8.8851, lr: 0.010000000000000002
Epoch [39/150], Training Loss: 22.0753, Validation Loss Current: 7.3438, Validation Loss AVG: 10.4668, lr: 0.010000000000000002
Epoch [40/150], Training Loss: 21.8252, Validation Loss Current: 7.1158, Validation Loss AVG: 10.4978, lr: 0.010000000000000002
Epoch [41/150], Training Loss: 20.5888, Validation Loss Current: 6.6671, Validation Loss AVG: 9.7630, lr: 0.010000000000000002
Epoch [42/150], Training Loss: 21.1362, Validation Loss Current: 7.1504, Validation Loss AVG: 10.6301, lr: 0.010000000000000002
Epoch [43/150], Training Loss: 19.3347, Validation Loss Current: 6.6193, Validation Loss AVG: 9.8285, lr: 0.0010000000000000002
Epoch [44/150], Training Loss: 18.9034, Validation Loss Current: 6.7583, Validation Loss AVG: 9.8702, lr: 0.0010000000000000002
Epoch [45/150], Training Loss: 20.1750, Validation Loss Current: 6.6939, Validation Loss AVG: 9.9240, lr: 0.0010000000000000002
Epoch [46/150], Training Loss: 18.2560, Validation Loss Current: 6.7417, Validation Loss AVG: 9.9047, lr: 0.0010000000000000002
Epoch [47/150], Training Loss: 18.2193, Validation Loss Current: 6.8635, Validation Loss AVG: 10.0049, lr: 0.0010000000000000002
Epoch [48/150], Training Loss: 17.8156, Validation Loss Current: 6.8156, Validation Loss AVG: 10.2064, lr: 0.0010000000000000002
Epoch [49/150], Training Loss: 17.6639, Validation Loss Current: 6.8176, Validation Loss AVG: 10.0756, lr: 0.00010000000000000003
Epoch [50/150], Training Loss: 17.5438, Validation Loss Current: 6.7543, Validation Loss AVG: 10.0384, lr: 0.00010000000000000003
Epoch [51/150], Training Loss: 17.1702, Validation Loss Current: 6.7573, Validation Loss AVG: 10.0435, lr: 0.00010000000000000003
Epoch [52/150], Training Loss: 17.3700, Validation Loss Current: 6.7855, Validation Loss AVG: 10.0463, lr: 0.00010000000000000003
Epoch [53/150], Training Loss: 17.4013, Validation Loss Current: 6.8089, Validation Loss AVG: 10.1459, lr: 0.00010000000000000003
Epoch [54/150], Training Loss: 17.3294, Validation Loss Current: 6.7478, Validation Loss AVG: 10.1208, lr: 0.00010000000000000003
Epoch [55/150], Training Loss: 16.9940, Validation Loss Current: 6.7824, Validation Loss AVG: 10.1100, lr: 1.0000000000000004e-05
Epoch [56/150], Training Loss: 17.7314, Validation Loss Current: 6.7988, Validation Loss AVG: 10.1258, lr: 1.0000000000000004e-05
Epoch [57/150], Training Loss: 17.2773, Validation Loss Current: 6.7407, Validation Loss AVG: 10.1375, lr: 1.0000000000000004e-05
Epoch [58/150], Training Loss: 17.3344, Validation Loss Current: 6.8102, Validation Loss AVG: 10.1562, lr: 1.0000000000000004e-05
Epoch [59/150], Training Loss: 17.7868, Validation Loss Current: 6.7949, Validation Loss AVG: 10.1372, lr: 1.0000000000000004e-05
Epoch [60/150], Training Loss: 17.1888, Validation Loss Current: 6.8197, Validation Loss AVG: 10.1030, lr: 1.0000000000000004e-05
Epoch [61/150], Training Loss: 18.4549, Validation Loss Current: 6.7884, Validation Loss AVG: 10.1153, lr: 1.0000000000000004e-06
Epoch [62/150], Training Loss: 17.0970, Validation Loss Current: 6.8202, Validation Loss AVG: 10.0968, lr: 1.0000000000000004e-06
Epoch [63/150], Training Loss: 18.2037, Validation Loss Current: 6.7848, Validation Loss AVG: 10.1102, lr: 1.0000000000000004e-06
Epoch [64/150], Training Loss: 17.1783, Validation Loss Current: 6.7668, Validation Loss AVG: 10.1163, lr: 1.0000000000000004e-06
Epoch [65/150], Training Loss: 17.1134, Validation Loss Current: 6.7648, Validation Loss AVG: 10.1126, lr: 1.0000000000000004e-06
Epoch [66/150], Training Loss: 18.6615, Validation Loss Current: 6.8293, Validation Loss AVG: 10.1457, lr: 1.0000000000000004e-06
 --- Early Stopped ---
Patch distance: 1 finished training. Best epoch: 36 Best val accuracy: [0.23519736842105263, 0.23519736842105263, 0.1513157894736842, 0.23519736842105263, 0.14802631578947367, 0.2845394736842105, 0.23519736842105263, 0.3338815789473684, 0.2779605263157895, 0.3569078947368421, 0.2746710526315789, 0.3963815789473684, 0.3355263157894737, 0.2532894736842105, 0.3651315789473684, 0.3125, 0.3519736842105263, 0.32894736842105265, 0.4506578947368421, 0.44901315789473684, 0.46381578947368424, 0.47368421052631576, 0.4901315789473684, 0.4753289473684211, 0.4605263157894737, 0.4769736842105263, 0.5049342105263158, 0.4967105263157895, 0.47368421052631576, 0.5082236842105263, 0.506578947368421, 0.44901315789473684, 0.5263157894736842, 0.5082236842105263, 0.4967105263157895, 0.4901315789473684, 0.5164473684210527, 0.506578947368421, 0.4934210526315789, 0.5032894736842105, 0.5263157894736842, 0.5032894736842105, 0.5361842105263158, 0.5411184210526315, 0.5296052631578947, 0.5394736842105263, 0.5296052631578947, 0.5411184210526315, 0.537828947368421, 0.5394736842105263, 0.5394736842105263, 0.5394736842105263, 0.5361842105263158, 0.5345394736842105, 0.5328947368421053, 0.5328947368421053, 0.5328947368421053, 0.5328947368421053, 0.5328947368421053, 0.5328947368421053, 0.5328947368421053, 0.5328947368421053, 0.5328947368421053, 0.5328947368421053, 0.5328947368421053, 0.5328947368421053] Best val loss: 6.602163076400757


Fold: 4
----- Training alexnet with sequence: [0.2, 0.4, 0.6, 0.8, 1] -----
Current group: 0.2
Epoch [1/30], Training Loss: 40.5802, Validation Loss Current: 9.9682, Validation Loss AVG: 9.9682, lr: 0.1
Epoch [2/30], Training Loss: 39.9597, Validation Loss Current: 9.9840, Validation Loss AVG: 9.9840, lr: 0.1
Epoch [3/30], Training Loss: 40.0984, Validation Loss Current: 9.9825, Validation Loss AVG: 9.9825, lr: 0.1
Epoch [4/30], Training Loss: 40.2763, Validation Loss Current: 9.9929, Validation Loss AVG: 9.9929, lr: 0.1
Epoch [5/30], Training Loss: 40.2430, Validation Loss Current: 10.0479, Validation Loss AVG: 10.0479, lr: 0.1
Epoch [6/30], Training Loss: 40.7332, Validation Loss Current: 9.9582, Validation Loss AVG: 9.9582, lr: 0.1
Epoch [7/30], Training Loss: 40.3405, Validation Loss Current: 9.9787, Validation Loss AVG: 9.9787, lr: 0.1
Epoch [8/30], Training Loss: 40.5861, Validation Loss Current: 9.9910, Validation Loss AVG: 9.9910, lr: 0.1
Epoch [9/30], Training Loss: 40.4592, Validation Loss Current: 10.0005, Validation Loss AVG: 10.0005, lr: 0.1
Epoch [10/30], Training Loss: 40.1780, Validation Loss Current: 9.9957, Validation Loss AVG: 9.9957, lr: 0.1
Epoch [11/30], Training Loss: 40.1185, Validation Loss Current: 9.9747, Validation Loss AVG: 9.9747, lr: 0.1
Epoch [12/30], Training Loss: 39.7097, Validation Loss Current: 9.9465, Validation Loss AVG: 9.9465, lr: 0.1
Epoch [13/30], Training Loss: 40.2319, Validation Loss Current: 10.5073, Validation Loss AVG: 10.5073, lr: 0.1
Epoch [14/30], Training Loss: 39.4704, Validation Loss Current: 9.9817, Validation Loss AVG: 9.9817, lr: 0.1
Epoch [15/30], Training Loss: 40.4115, Validation Loss Current: 9.8724, Validation Loss AVG: 9.8724, lr: 0.1
Epoch [16/30], Training Loss: 39.9614, Validation Loss Current: 9.6693, Validation Loss AVG: 9.6693, lr: 0.1
Epoch [17/30], Training Loss: 39.7716, Validation Loss Current: 10.4126, Validation Loss AVG: 10.4126, lr: 0.1
Epoch [18/30], Training Loss: 40.2457, Validation Loss Current: 9.9126, Validation Loss AVG: 9.9126, lr: 0.1
Epoch [19/30], Training Loss: 39.8907, Validation Loss Current: 10.2192, Validation Loss AVG: 10.2192, lr: 0.1
Epoch [20/30], Training Loss: 39.4915, Validation Loss Current: 12.1149, Validation Loss AVG: 12.1149, lr: 0.1
Epoch [21/30], Training Loss: 38.9091, Validation Loss Current: 15.7700, Validation Loss AVG: 15.7700, lr: 0.1
Epoch [22/30], Training Loss: 40.5328, Validation Loss Current: 9.6652, Validation Loss AVG: 9.6652, lr: 0.1
Epoch [23/30], Training Loss: 36.3271, Validation Loss Current: 10.2484, Validation Loss AVG: 10.2484, lr: 0.1
Epoch [24/30], Training Loss: 38.5429, Validation Loss Current: 11.2068, Validation Loss AVG: 11.2068, lr: 0.1
Epoch [25/30], Training Loss: 37.4935, Validation Loss Current: 9.9769, Validation Loss AVG: 9.9769, lr: 0.1
Epoch [26/30], Training Loss: 36.9249, Validation Loss Current: 9.4357, Validation Loss AVG: 9.4357, lr: 0.1
Epoch [27/30], Training Loss: 35.7067, Validation Loss Current: 10.7285, Validation Loss AVG: 10.7285, lr: 0.1
Epoch [28/30], Training Loss: 37.5125, Validation Loss Current: 9.4737, Validation Loss AVG: 9.4737, lr: 0.1
Epoch [29/30], Training Loss: 36.1332, Validation Loss Current: 9.8316, Validation Loss AVG: 9.8316, lr: 0.1
Epoch [30/30], Training Loss: 36.7336, Validation Loss Current: 9.9061, Validation Loss AVG: 9.9061, lr: 0.1
Epoch [31/30], Training Loss: 35.7610, Validation Loss Current: 9.3329, Validation Loss AVG: 9.3329, lr: 0.1
Epoch [32/30], Training Loss: 35.0617, Validation Loss Current: 10.7823, Validation Loss AVG: 10.7823, lr: 0.1
Epoch [33/30], Training Loss: 35.6817, Validation Loss Current: 9.8374, Validation Loss AVG: 9.8374, lr: 0.1
Epoch [34/30], Training Loss: 35.0489, Validation Loss Current: 9.8708, Validation Loss AVG: 9.8708, lr: 0.1
Epoch [35/30], Training Loss: 34.3328, Validation Loss Current: 9.5981, Validation Loss AVG: 9.5981, lr: 0.1
Epoch [36/30], Training Loss: 33.7514, Validation Loss Current: 9.7163, Validation Loss AVG: 9.7163, lr: 0.1
Epoch [37/30], Training Loss: 33.5098, Validation Loss Current: 9.7525, Validation Loss AVG: 9.7525, lr: 0.1
Epoch [38/30], Training Loss: 32.9824, Validation Loss Current: 9.1315, Validation Loss AVG: 9.1315, lr: 0.010000000000000002
Epoch [39/30], Training Loss: 31.0197, Validation Loss Current: 9.3105, Validation Loss AVG: 9.3105, lr: 0.010000000000000002
Epoch [40/30], Training Loss: 30.8718, Validation Loss Current: 9.4811, Validation Loss AVG: 9.4811, lr: 0.010000000000000002
Epoch [41/30], Training Loss: 29.8903, Validation Loss Current: 9.7393, Validation Loss AVG: 9.7393, lr: 0.010000000000000002
Epoch [42/30], Training Loss: 29.8078, Validation Loss Current: 9.5543, Validation Loss AVG: 9.5543, lr: 0.010000000000000002
Epoch [43/30], Training Loss: 28.9183, Validation Loss Current: 9.7839, Validation Loss AVG: 9.7839, lr: 0.010000000000000002
Epoch [44/30], Training Loss: 28.1223, Validation Loss Current: 9.7975, Validation Loss AVG: 9.7975, lr: 0.010000000000000002
Epoch [45/30], Training Loss: 28.3662, Validation Loss Current: 9.7307, Validation Loss AVG: 9.7307, lr: 0.0010000000000000002
Epoch [46/30], Training Loss: 28.7571, Validation Loss Current: 9.7952, Validation Loss AVG: 9.7952, lr: 0.0010000000000000002
Epoch [47/30], Training Loss: 28.3190, Validation Loss Current: 9.7831, Validation Loss AVG: 9.7831, lr: 0.0010000000000000002
Epoch [48/30], Training Loss: 27.6186, Validation Loss Current: 9.8472, Validation Loss AVG: 9.8472, lr: 0.0010000000000000002
Epoch [49/30], Training Loss: 26.8459, Validation Loss Current: 9.8560, Validation Loss AVG: 9.8560, lr: 0.0010000000000000002
Epoch [50/30], Training Loss: 27.5639, Validation Loss Current: 9.8651, Validation Loss AVG: 9.8651, lr: 0.0010000000000000002
Epoch [51/30], Training Loss: 26.5785, Validation Loss Current: 9.8884, Validation Loss AVG: 9.8884, lr: 0.00010000000000000003
Epoch [52/30], Training Loss: 27.4669, Validation Loss Current: 9.8866, Validation Loss AVG: 9.8866, lr: 0.00010000000000000003
Epoch [53/30], Training Loss: 27.1789, Validation Loss Current: 9.8849, Validation Loss AVG: 9.8849, lr: 0.00010000000000000003
Epoch [54/30], Training Loss: 27.3437, Validation Loss Current: 9.8895, Validation Loss AVG: 9.8895, lr: 0.00010000000000000003
Epoch [55/30], Training Loss: 28.1608, Validation Loss Current: 9.8778, Validation Loss AVG: 9.8778, lr: 0.00010000000000000003
Epoch [56/30], Training Loss: 26.5981, Validation Loss Current: 9.8869, Validation Loss AVG: 9.8869, lr: 0.00010000000000000003
Epoch [57/30], Training Loss: 27.2378, Validation Loss Current: 9.8789, Validation Loss AVG: 9.8789, lr: 1.0000000000000004e-05
Epoch [58/30], Training Loss: 27.6121, Validation Loss Current: 9.8836, Validation Loss AVG: 9.8836, lr: 1.0000000000000004e-05
Epoch [59/30], Training Loss: 27.1654, Validation Loss Current: 9.8807, Validation Loss AVG: 9.8807, lr: 1.0000000000000004e-05
Epoch [60/30], Training Loss: 27.8033, Validation Loss Current: 9.8947, Validation Loss AVG: 9.8947, lr: 1.0000000000000004e-05
Epoch [61/30], Training Loss: 27.4834, Validation Loss Current: 9.8885, Validation Loss AVG: 9.8885, lr: 1.0000000000000004e-05
Epoch [62/30], Training Loss: 28.0117, Validation Loss Current: 9.8734, Validation Loss AVG: 9.8734, lr: 1.0000000000000004e-05
Epoch [63/30], Training Loss: 27.6239, Validation Loss Current: 9.8913, Validation Loss AVG: 9.8913, lr: 1.0000000000000004e-06
Epoch [64/30], Training Loss: 26.6037, Validation Loss Current: 9.8848, Validation Loss AVG: 9.8848, lr: 1.0000000000000004e-06
Epoch [65/30], Training Loss: 27.3817, Validation Loss Current: 9.9121, Validation Loss AVG: 9.9121, lr: 1.0000000000000004e-06
Epoch [66/30], Training Loss: 27.9295, Validation Loss Current: 9.8852, Validation Loss AVG: 9.8852, lr: 1.0000000000000004e-06
Epoch [67/30], Training Loss: 27.2751, Validation Loss Current: 9.8679, Validation Loss AVG: 9.8679, lr: 1.0000000000000004e-06
Epoch [68/30], Training Loss: 26.6737, Validation Loss Current: 9.8823, Validation Loss AVG: 9.8823, lr: 1.0000000000000004e-06
 --- Early Stopped ---
Patch distance: 0.2 finished training. Best epoch: 38 Best val accuracy: [0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.19013157894736843, 0.2631578947368421, 0.2960526315789474, 0.2802631578947369, 0.27960526315789475, 0.29078947368421054, 0.19934210526315793, 0.14736842105263157, 0.29934210526315785, 0.2957236842105263, 0.2723684210526316, 0.12467105263157893, 0.23651315789473681, 0.306578947368421, 0.23750000000000004, 0.29638157894736844, 0.2483552631578947, 0.22730263157894734, 0.30559210526315794, 0.2138157894736842, 0.19407894736842107, 0.18125, 0.24605263157894736, 0.28848684210526315, 0.29210526315789476, 0.32467105263157897, 0.2825657894736842, 0.2654605263157895, 0.26414473684210527, 0.26184210526315793, 0.27039473684210524, 0.26118421052631585, 0.2542763157894737, 0.2549342105263158, 0.2569078947368421, 0.25296052631578947, 0.25197368421052635, 0.25625, 0.25592105263157894, 0.2536184210526316, 0.25394736842105264, 0.25394736842105264, 0.25394736842105264, 0.25296052631578947, 0.25263157894736843, 0.25296052631578947, 0.25230263157894733, 0.2532894736842105, 0.2532894736842105, 0.25296052631578947, 0.25296052631578947, 0.25296052631578947, 0.25296052631578947, 0.25296052631578947, 0.2532894736842105, 0.2536184210526316] Best val loss: 9.1315096616745


Loaded best state dict for [0.2]
Current group: 0.4
Epoch [1/30], Training Loss: 35.5487, Validation Loss Current: 10.9578, Validation Loss AVG: 10.9578, lr: 0.1
Epoch [2/30], Training Loss: 34.9945, Validation Loss Current: 8.9977, Validation Loss AVG: 8.9977, lr: 0.1
Epoch [3/30], Training Loss: 35.4321, Validation Loss Current: 9.7930, Validation Loss AVG: 9.7930, lr: 0.1
Epoch [4/30], Training Loss: 33.9451, Validation Loss Current: 9.8274, Validation Loss AVG: 9.8274, lr: 0.1
Epoch [5/30], Training Loss: 32.6570, Validation Loss Current: 9.5838, Validation Loss AVG: 9.5838, lr: 0.1
Epoch [6/30], Training Loss: 33.5922, Validation Loss Current: 10.9604, Validation Loss AVG: 10.9604, lr: 0.1
Epoch [7/30], Training Loss: 33.0669, Validation Loss Current: 11.5823, Validation Loss AVG: 11.5823, lr: 0.1
Epoch [8/30], Training Loss: 33.2729, Validation Loss Current: 9.7677, Validation Loss AVG: 9.7677, lr: 0.1
Epoch [9/30], Training Loss: 32.8677, Validation Loss Current: 8.6895, Validation Loss AVG: 8.6895, lr: 0.010000000000000002
Epoch [10/30], Training Loss: 28.7844, Validation Loss Current: 8.6960, Validation Loss AVG: 8.6960, lr: 0.010000000000000002
Epoch [11/30], Training Loss: 27.4853, Validation Loss Current: 8.6337, Validation Loss AVG: 8.6337, lr: 0.010000000000000002
Epoch [12/30], Training Loss: 26.6226, Validation Loss Current: 8.8381, Validation Loss AVG: 8.8381, lr: 0.010000000000000002
Epoch [13/30], Training Loss: 26.1364, Validation Loss Current: 8.7454, Validation Loss AVG: 8.7454, lr: 0.010000000000000002
Epoch [14/30], Training Loss: 26.4936, Validation Loss Current: 8.7901, Validation Loss AVG: 8.7901, lr: 0.010000000000000002
Epoch [15/30], Training Loss: 25.1433, Validation Loss Current: 8.8830, Validation Loss AVG: 8.8830, lr: 0.010000000000000002
Epoch [16/30], Training Loss: 25.3387, Validation Loss Current: 9.0121, Validation Loss AVG: 9.0121, lr: 0.010000000000000002
Epoch [17/30], Training Loss: 24.2462, Validation Loss Current: 9.3769, Validation Loss AVG: 9.3769, lr: 0.010000000000000002
Epoch [18/30], Training Loss: 23.4342, Validation Loss Current: 9.0710, Validation Loss AVG: 9.0710, lr: 0.0010000000000000002
Epoch [19/30], Training Loss: 22.6008, Validation Loss Current: 9.1133, Validation Loss AVG: 9.1133, lr: 0.0010000000000000002
Epoch [20/30], Training Loss: 21.6727, Validation Loss Current: 9.0853, Validation Loss AVG: 9.0853, lr: 0.0010000000000000002
Epoch [21/30], Training Loss: 22.5458, Validation Loss Current: 9.1499, Validation Loss AVG: 9.1499, lr: 0.0010000000000000002
Epoch [22/30], Training Loss: 21.6159, Validation Loss Current: 9.2269, Validation Loss AVG: 9.2269, lr: 0.0010000000000000002
Epoch [23/30], Training Loss: 22.9461, Validation Loss Current: 9.3244, Validation Loss AVG: 9.3244, lr: 0.0010000000000000002
Epoch [24/30], Training Loss: 21.8856, Validation Loss Current: 9.3163, Validation Loss AVG: 9.3163, lr: 0.00010000000000000003
Epoch [25/30], Training Loss: 21.4950, Validation Loss Current: 9.3444, Validation Loss AVG: 9.3444, lr: 0.00010000000000000003
Epoch [26/30], Training Loss: 21.6266, Validation Loss Current: 9.3141, Validation Loss AVG: 9.3141, lr: 0.00010000000000000003
Epoch [27/30], Training Loss: 21.8753, Validation Loss Current: 9.3343, Validation Loss AVG: 9.3343, lr: 0.00010000000000000003
Epoch [28/30], Training Loss: 21.3203, Validation Loss Current: 9.3337, Validation Loss AVG: 9.3337, lr: 0.00010000000000000003
Epoch [29/30], Training Loss: 21.6394, Validation Loss Current: 9.3324, Validation Loss AVG: 9.3324, lr: 0.00010000000000000003
Epoch [30/30], Training Loss: 20.9834, Validation Loss Current: 9.3287, Validation Loss AVG: 9.3287, lr: 1.0000000000000004e-05
Epoch [31/30], Training Loss: 21.4521, Validation Loss Current: 9.3483, Validation Loss AVG: 9.3483, lr: 1.0000000000000004e-05
Epoch [32/30], Training Loss: 21.1715, Validation Loss Current: 9.3062, Validation Loss AVG: 9.3062, lr: 1.0000000000000004e-05
Epoch [33/30], Training Loss: 21.0154, Validation Loss Current: 9.3224, Validation Loss AVG: 9.3224, lr: 1.0000000000000004e-05
Epoch [34/30], Training Loss: 21.0266, Validation Loss Current: 9.3253, Validation Loss AVG: 9.3253, lr: 1.0000000000000004e-05
Epoch [35/30], Training Loss: 20.7322, Validation Loss Current: 9.3612, Validation Loss AVG: 9.3612, lr: 1.0000000000000004e-05
Epoch [36/30], Training Loss: 21.3362, Validation Loss Current: 9.3460, Validation Loss AVG: 9.3460, lr: 1.0000000000000004e-06
Epoch [37/30], Training Loss: 21.9430, Validation Loss Current: 9.3456, Validation Loss AVG: 9.3456, lr: 1.0000000000000004e-06
Epoch [38/30], Training Loss: 22.8073, Validation Loss Current: 9.3603, Validation Loss AVG: 9.3603, lr: 1.0000000000000004e-06
Epoch [39/30], Training Loss: 21.6174, Validation Loss Current: 9.3124, Validation Loss AVG: 9.3124, lr: 1.0000000000000004e-06
Epoch [40/30], Training Loss: 21.0667, Validation Loss Current: 9.3027, Validation Loss AVG: 9.3027, lr: 1.0000000000000004e-06
Epoch [41/30], Training Loss: 22.4965, Validation Loss Current: 9.3192, Validation Loss AVG: 9.3192, lr: 1.0000000000000004e-06
 --- Early Stopped ---
Patch distance: 0.4 finished training. Best epoch: 11 Best val accuracy: [0.3177631578947368, 0.3503289473684211, 0.34342105263157896, 0.29539473684210527, 0.35592105263157897, 0.2536184210526316, 0.3092105263157895, 0.22335526315789472, 0.36677631578947373, 0.35460526315789476, 0.3483552631578947, 0.33782894736842106, 0.3453947368421053, 0.34407894736842104, 0.36710526315789477, 0.35460526315789476, 0.3391447368421052, 0.3605263157894737, 0.3615131578947368, 0.3651315789473684, 0.362171052631579, 0.36019736842105265, 0.3559210526315789, 0.35723684210526313, 0.35460526315789476, 0.3569078947368421, 0.35690789473684215, 0.35657894736842105, 0.3559210526315789, 0.35657894736842105, 0.35690789473684215, 0.35657894736842105, 0.35657894736842105, 0.35690789473684215, 0.35690789473684215, 0.35690789473684215, 0.35690789473684215, 0.35690789473684215, 0.35690789473684215, 0.35690789473684215, 0.35690789473684215] Best val loss: 8.633694076538086


Loaded best state dict for [0.2, 0.4]
Current group: 0.6
Epoch [1/30], Training Loss: 31.3145, Validation Loss Current: 9.1148, Validation Loss AVG: 9.1148, lr: 0.1
Epoch [2/30], Training Loss: 31.1329, Validation Loss Current: 9.2326, Validation Loss AVG: 9.2326, lr: 0.1
Epoch [3/30], Training Loss: 29.8362, Validation Loss Current: 9.9348, Validation Loss AVG: 9.9348, lr: 0.1
Epoch [4/30], Training Loss: 30.5333, Validation Loss Current: 8.8147, Validation Loss AVG: 8.8147, lr: 0.1
Epoch [5/30], Training Loss: 29.8763, Validation Loss Current: 9.0940, Validation Loss AVG: 9.0940, lr: 0.1
Epoch [6/30], Training Loss: 29.9142, Validation Loss Current: 9.7558, Validation Loss AVG: 9.7558, lr: 0.1
Epoch [7/30], Training Loss: 29.5575, Validation Loss Current: 9.3393, Validation Loss AVG: 9.3393, lr: 0.1
Epoch [8/30], Training Loss: 28.3846, Validation Loss Current: 9.2987, Validation Loss AVG: 9.2987, lr: 0.1
Epoch [9/30], Training Loss: 29.7897, Validation Loss Current: 8.9215, Validation Loss AVG: 8.9215, lr: 0.1
Epoch [10/30], Training Loss: 28.7808, Validation Loss Current: 9.3959, Validation Loss AVG: 9.3959, lr: 0.1
Epoch [11/30], Training Loss: 27.7661, Validation Loss Current: 8.7558, Validation Loss AVG: 8.7558, lr: 0.010000000000000002
Epoch [12/30], Training Loss: 25.2093, Validation Loss Current: 9.0111, Validation Loss AVG: 9.0111, lr: 0.010000000000000002
Epoch [13/30], Training Loss: 22.2211, Validation Loss Current: 9.2112, Validation Loss AVG: 9.2112, lr: 0.010000000000000002
Epoch [14/30], Training Loss: 22.7797, Validation Loss Current: 9.4667, Validation Loss AVG: 9.4667, lr: 0.010000000000000002
Epoch [15/30], Training Loss: 19.8617, Validation Loss Current: 9.4944, Validation Loss AVG: 9.4944, lr: 0.010000000000000002
Epoch [16/30], Training Loss: 19.2681, Validation Loss Current: 9.9353, Validation Loss AVG: 9.9353, lr: 0.010000000000000002
Epoch [17/30], Training Loss: 17.6370, Validation Loss Current: 10.7139, Validation Loss AVG: 10.7139, lr: 0.010000000000000002
Epoch [18/30], Training Loss: 18.1179, Validation Loss Current: 10.6125, Validation Loss AVG: 10.6125, lr: 0.0010000000000000002
Epoch [19/30], Training Loss: 16.4255, Validation Loss Current: 10.7648, Validation Loss AVG: 10.7648, lr: 0.0010000000000000002
Epoch [20/30], Training Loss: 16.0654, Validation Loss Current: 10.9545, Validation Loss AVG: 10.9545, lr: 0.0010000000000000002
Epoch [21/30], Training Loss: 16.9212, Validation Loss Current: 11.0051, Validation Loss AVG: 11.0051, lr: 0.0010000000000000002
Epoch [22/30], Training Loss: 16.5630, Validation Loss Current: 11.0748, Validation Loss AVG: 11.0748, lr: 0.0010000000000000002
Epoch [23/30], Training Loss: 15.5981, Validation Loss Current: 11.1963, Validation Loss AVG: 11.1963, lr: 0.0010000000000000002
Epoch [24/30], Training Loss: 15.5166, Validation Loss Current: 11.2257, Validation Loss AVG: 11.2257, lr: 0.00010000000000000003
Epoch [25/30], Training Loss: 16.5435, Validation Loss Current: 11.2286, Validation Loss AVG: 11.2286, lr: 0.00010000000000000003
Epoch [26/30], Training Loss: 16.0056, Validation Loss Current: 11.2572, Validation Loss AVG: 11.2572, lr: 0.00010000000000000003
Epoch [27/30], Training Loss: 15.5974, Validation Loss Current: 11.1935, Validation Loss AVG: 11.1935, lr: 0.00010000000000000003
Epoch [28/30], Training Loss: 15.5315, Validation Loss Current: 11.2529, Validation Loss AVG: 11.2529, lr: 0.00010000000000000003
Epoch [29/30], Training Loss: 16.4966, Validation Loss Current: 11.2552, Validation Loss AVG: 11.2552, lr: 0.00010000000000000003
Epoch [30/30], Training Loss: 16.0761, Validation Loss Current: 11.2818, Validation Loss AVG: 11.2818, lr: 1.0000000000000004e-05
Epoch [31/30], Training Loss: 16.2994, Validation Loss Current: 11.2470, Validation Loss AVG: 11.2470, lr: 1.0000000000000004e-05
Epoch [32/30], Training Loss: 15.4127, Validation Loss Current: 11.2346, Validation Loss AVG: 11.2346, lr: 1.0000000000000004e-05
Epoch [33/30], Training Loss: 15.6514, Validation Loss Current: 11.2506, Validation Loss AVG: 11.2506, lr: 1.0000000000000004e-05
Epoch [34/30], Training Loss: 16.9034, Validation Loss Current: 11.2759, Validation Loss AVG: 11.2759, lr: 1.0000000000000004e-05
Epoch [35/30], Training Loss: 15.9689, Validation Loss Current: 11.2684, Validation Loss AVG: 11.2684, lr: 1.0000000000000004e-05
Epoch [36/30], Training Loss: 15.3285, Validation Loss Current: 11.2969, Validation Loss AVG: 11.2969, lr: 1.0000000000000004e-06
Epoch [37/30], Training Loss: 15.4290, Validation Loss Current: 11.2695, Validation Loss AVG: 11.2695, lr: 1.0000000000000004e-06
Epoch [38/30], Training Loss: 15.8131, Validation Loss Current: 11.2692, Validation Loss AVG: 11.2692, lr: 1.0000000000000004e-06
Epoch [39/30], Training Loss: 16.1077, Validation Loss Current: 11.2639, Validation Loss AVG: 11.2639, lr: 1.0000000000000004e-06
Epoch [40/30], Training Loss: 15.6711, Validation Loss Current: 11.2433, Validation Loss AVG: 11.2433, lr: 1.0000000000000004e-06
Epoch [41/30], Training Loss: 16.1020, Validation Loss Current: 11.2614, Validation Loss AVG: 11.2614, lr: 1.0000000000000004e-06
 --- Early Stopped ---
Patch distance: 0.6 finished training. Best epoch: 11 Best val accuracy: [0.3315789473684211, 0.38125, 0.3493421052631579, 0.3717105263157895, 0.3055921052631579, 0.2832236842105263, 0.40032894736842106, 0.2555921052631579, 0.3851973684210527, 0.39506578947368426, 0.4151315789473685, 0.40197368421052626, 0.3773026315789474, 0.42631578947368426, 0.42894736842105263, 0.41447368421052627, 0.4072368421052632, 0.41480263157894737, 0.41743421052631574, 0.4125, 0.4164473684210527, 0.4108552631578948, 0.4164473684210527, 0.4151315789473684, 0.41414473684210523, 0.41480263157894737, 0.4151315789473684, 0.41480263157894737, 0.41414473684210523, 0.41414473684210523, 0.41414473684210523, 0.41447368421052627, 0.41447368421052627, 0.41447368421052627, 0.41447368421052627, 0.41447368421052627, 0.41447368421052627, 0.41447368421052627, 0.41447368421052627, 0.41447368421052627, 0.41447368421052627] Best val loss: 8.755825543403626


Loaded best state dict for [0.2, 0.4, 0.6]
Current group: 0.8
Epoch [1/30], Training Loss: 29.3756, Validation Loss Current: 9.5468, Validation Loss AVG: 9.5468, lr: 0.1
Epoch [2/30], Training Loss: 30.5046, Validation Loss Current: 12.0587, Validation Loss AVG: 12.0587, lr: 0.1
Epoch [3/30], Training Loss: 32.4225, Validation Loss Current: 8.5178, Validation Loss AVG: 8.5178, lr: 0.1
Epoch [4/30], Training Loss: 31.0209, Validation Loss Current: 8.4669, Validation Loss AVG: 8.4669, lr: 0.1
Epoch [5/30], Training Loss: 29.0379, Validation Loss Current: 9.0433, Validation Loss AVG: 9.0433, lr: 0.1
Epoch [6/30], Training Loss: 29.9652, Validation Loss Current: 10.8041, Validation Loss AVG: 10.8041, lr: 0.1
Epoch [7/30], Training Loss: 29.7321, Validation Loss Current: 10.4531, Validation Loss AVG: 10.4531, lr: 0.1
Epoch [8/30], Training Loss: 30.2467, Validation Loss Current: 10.2167, Validation Loss AVG: 10.2167, lr: 0.1
Epoch [9/30], Training Loss: 29.3584, Validation Loss Current: 9.3221, Validation Loss AVG: 9.3221, lr: 0.1
Epoch [10/30], Training Loss: 31.0142, Validation Loss Current: 8.9679, Validation Loss AVG: 8.9679, lr: 0.1
Epoch [11/30], Training Loss: 28.2109, Validation Loss Current: 8.9845, Validation Loss AVG: 8.9845, lr: 0.010000000000000002
Epoch [12/30], Training Loss: 24.8318, Validation Loss Current: 8.5814, Validation Loss AVG: 8.5814, lr: 0.010000000000000002
Epoch [13/30], Training Loss: 24.4334, Validation Loss Current: 8.6915, Validation Loss AVG: 8.6915, lr: 0.010000000000000002
Epoch [14/30], Training Loss: 23.3281, Validation Loss Current: 8.8759, Validation Loss AVG: 8.8759, lr: 0.010000000000000002
Epoch [15/30], Training Loss: 22.2120, Validation Loss Current: 8.7888, Validation Loss AVG: 8.7888, lr: 0.010000000000000002
Epoch [16/30], Training Loss: 20.3010, Validation Loss Current: 9.0562, Validation Loss AVG: 9.0562, lr: 0.010000000000000002
Epoch [17/30], Training Loss: 20.7761, Validation Loss Current: 9.2285, Validation Loss AVG: 9.2285, lr: 0.0010000000000000002
Epoch [18/30], Training Loss: 18.9531, Validation Loss Current: 9.2263, Validation Loss AVG: 9.2263, lr: 0.0010000000000000002
Epoch [19/30], Training Loss: 18.7074, Validation Loss Current: 9.3032, Validation Loss AVG: 9.3032, lr: 0.0010000000000000002
Epoch [20/30], Training Loss: 18.1814, Validation Loss Current: 9.3608, Validation Loss AVG: 9.3608, lr: 0.0010000000000000002
Epoch [21/30], Training Loss: 19.1180, Validation Loss Current: 9.3573, Validation Loss AVG: 9.3573, lr: 0.0010000000000000002
Epoch [22/30], Training Loss: 17.5456, Validation Loss Current: 9.3852, Validation Loss AVG: 9.3852, lr: 0.0010000000000000002
Epoch [23/30], Training Loss: 18.5966, Validation Loss Current: 9.4139, Validation Loss AVG: 9.4139, lr: 0.00010000000000000003
Epoch [24/30], Training Loss: 17.3347, Validation Loss Current: 9.3872, Validation Loss AVG: 9.3872, lr: 0.00010000000000000003
Epoch [25/30], Training Loss: 18.1888, Validation Loss Current: 9.3886, Validation Loss AVG: 9.3886, lr: 0.00010000000000000003
Epoch [26/30], Training Loss: 17.8789, Validation Loss Current: 9.4040, Validation Loss AVG: 9.4040, lr: 0.00010000000000000003
Epoch [27/30], Training Loss: 17.9464, Validation Loss Current: 9.4104, Validation Loss AVG: 9.4104, lr: 0.00010000000000000003
Epoch [28/30], Training Loss: 17.7911, Validation Loss Current: 9.4432, Validation Loss AVG: 9.4432, lr: 0.00010000000000000003
Epoch [29/30], Training Loss: 18.1663, Validation Loss Current: 9.4368, Validation Loss AVG: 9.4368, lr: 1.0000000000000004e-05
Epoch [30/30], Training Loss: 18.0383, Validation Loss Current: 9.4372, Validation Loss AVG: 9.4372, lr: 1.0000000000000004e-05
Epoch [31/30], Training Loss: 18.6335, Validation Loss Current: 9.4316, Validation Loss AVG: 9.4316, lr: 1.0000000000000004e-05
Epoch [32/30], Training Loss: 17.7701, Validation Loss Current: 9.4186, Validation Loss AVG: 9.4186, lr: 1.0000000000000004e-05
Epoch [33/30], Training Loss: 17.6195, Validation Loss Current: 9.4152, Validation Loss AVG: 9.4152, lr: 1.0000000000000004e-05
Epoch [34/30], Training Loss: 17.8908, Validation Loss Current: 9.4257, Validation Loss AVG: 9.4257, lr: 1.0000000000000004e-05
 --- Early Stopped ---
Patch distance: 0.8 finished training. Best epoch: 4 Best val accuracy: [0.3539473684210526, 0.36743421052631575, 0.35328947368421054, 0.36973684210526314, 0.3467105263157894, 0.31118421052631573, 0.36217105263157895, 0.35493421052631574, 0.325, 0.36644736842105263, 0.3710526315789474, 0.39769736842105263, 0.38782894736842105, 0.3960526315789473, 0.40921052631578947, 0.39769736842105263, 0.40230263157894736, 0.4029605263157895, 0.4046052631578947, 0.4072368421052632, 0.40559210526315786, 0.40625, 0.40690789473684214, 0.4052631578947368, 0.40559210526315786, 0.4052631578947368, 0.4046052631578947, 0.40493421052631573, 0.40493421052631573, 0.40493421052631573, 0.40493421052631573, 0.40493421052631573, 0.4052631578947368, 0.4052631578947368] Best val loss: 8.466850996017456


Loaded best state dict for [0.2, 0.4, 0.6, 0.8]
Current group: 1
Epoch [1/30], Training Loss: 28.6982, Validation Loss Current: 7.8216, Validation Loss AVG: 10.0201, lr: 0.1
Epoch [2/30], Training Loss: 28.5881, Validation Loss Current: 8.1001, Validation Loss AVG: 11.3897, lr: 0.1
Epoch [3/30], Training Loss: 29.5297, Validation Loss Current: 8.0614, Validation Loss AVG: 8.9708, lr: 0.1
Epoch [4/30], Training Loss: 28.4985, Validation Loss Current: 9.5172, Validation Loss AVG: 11.7447, lr: 0.1
Epoch [5/30], Training Loss: 31.1469, Validation Loss Current: 7.9596, Validation Loss AVG: 8.9813, lr: 0.1
Epoch [6/30], Training Loss: 29.3711, Validation Loss Current: 7.7281, Validation Loss AVG: 11.2437, lr: 0.1
Epoch [7/30], Training Loss: 30.0600, Validation Loss Current: 10.0744, Validation Loss AVG: 10.8685, lr: 0.1
Epoch [8/30], Training Loss: 30.9241, Validation Loss Current: 7.8131, Validation Loss AVG: 9.5525, lr: 0.1
Epoch [9/30], Training Loss: 30.3245, Validation Loss Current: 7.4087, Validation Loss AVG: 9.1208, lr: 0.1
Epoch [10/30], Training Loss: 27.9023, Validation Loss Current: 9.4655, Validation Loss AVG: 11.6826, lr: 0.1
Epoch [11/30], Training Loss: 30.3482, Validation Loss Current: 8.4179, Validation Loss AVG: 9.4551, lr: 0.1
Epoch [12/30], Training Loss: 28.0328, Validation Loss Current: 8.9968, Validation Loss AVG: 11.6843, lr: 0.1
Epoch [13/30], Training Loss: 30.4310, Validation Loss Current: 15.5532, Validation Loss AVG: 14.5216, lr: 0.1
Epoch [14/30], Training Loss: 33.5688, Validation Loss Current: 8.5376, Validation Loss AVG: 10.6306, lr: 0.1
Epoch [15/30], Training Loss: 29.6248, Validation Loss Current: 7.1242, Validation Loss AVG: 9.3027, lr: 0.1
Epoch [16/30], Training Loss: 29.5130, Validation Loss Current: 8.3957, Validation Loss AVG: 10.2660, lr: 0.1
Epoch [17/30], Training Loss: 29.3810, Validation Loss Current: 7.5127, Validation Loss AVG: 9.0226, lr: 0.1
Epoch [18/30], Training Loss: 28.3360, Validation Loss Current: 7.6472, Validation Loss AVG: 9.1897, lr: 0.1
Epoch [19/30], Training Loss: 30.0467, Validation Loss Current: 8.6244, Validation Loss AVG: 9.9336, lr: 0.1
Epoch [20/30], Training Loss: 30.8204, Validation Loss Current: 7.6413, Validation Loss AVG: 9.7330, lr: 0.1
Epoch [21/30], Training Loss: 32.5081, Validation Loss Current: 7.3198, Validation Loss AVG: 10.0143, lr: 0.1
Epoch [22/30], Training Loss: 25.6819, Validation Loss Current: 6.8327, Validation Loss AVG: 9.4362, lr: 0.010000000000000002
Epoch [23/30], Training Loss: 23.7540, Validation Loss Current: 6.8546, Validation Loss AVG: 10.7358, lr: 0.010000000000000002
Epoch [24/30], Training Loss: 23.1143, Validation Loss Current: 6.5631, Validation Loss AVG: 9.5914, lr: 0.010000000000000002
Epoch [25/30], Training Loss: 21.6599, Validation Loss Current: 6.5607, Validation Loss AVG: 11.1595, lr: 0.010000000000000002
Epoch [26/30], Training Loss: 21.5519, Validation Loss Current: 6.5743, Validation Loss AVG: 11.6472, lr: 0.010000000000000002
Epoch [27/30], Training Loss: 19.0579, Validation Loss Current: 6.6340, Validation Loss AVG: 11.6044, lr: 0.010000000000000002
Epoch [28/30], Training Loss: 20.0088, Validation Loss Current: 6.6322, Validation Loss AVG: 11.8811, lr: 0.010000000000000002
Epoch [29/30], Training Loss: 18.2320, Validation Loss Current: 6.7318, Validation Loss AVG: 12.0819, lr: 0.010000000000000002
Epoch [30/30], Training Loss: 16.5882, Validation Loss Current: 6.8795, Validation Loss AVG: 13.8048, lr: 0.010000000000000002
Epoch [31/30], Training Loss: 15.9094, Validation Loss Current: 6.8841, Validation Loss AVG: 13.1799, lr: 0.010000000000000002
Epoch [32/30], Training Loss: 15.0271, Validation Loss Current: 6.9040, Validation Loss AVG: 13.4144, lr: 0.0010000000000000002
Epoch [33/30], Training Loss: 15.9361, Validation Loss Current: 6.9704, Validation Loss AVG: 13.8675, lr: 0.0010000000000000002
Epoch [34/30], Training Loss: 13.9086, Validation Loss Current: 7.0381, Validation Loss AVG: 14.0752, lr: 0.0010000000000000002
Epoch [35/30], Training Loss: 14.2844, Validation Loss Current: 7.0926, Validation Loss AVG: 14.1961, lr: 0.0010000000000000002
Epoch [36/30], Training Loss: 15.9104, Validation Loss Current: 7.1729, Validation Loss AVG: 14.3634, lr: 0.0010000000000000002
Epoch [37/30], Training Loss: 13.7755, Validation Loss Current: 7.1906, Validation Loss AVG: 14.4340, lr: 0.0010000000000000002
Epoch [38/30], Training Loss: 14.3092, Validation Loss Current: 7.1408, Validation Loss AVG: 14.3749, lr: 0.00010000000000000003
Epoch [39/30], Training Loss: 13.8429, Validation Loss Current: 7.2903, Validation Loss AVG: 14.4593, lr: 0.00010000000000000003
Epoch [40/30], Training Loss: 13.7840, Validation Loss Current: 7.2186, Validation Loss AVG: 14.4388, lr: 0.00010000000000000003
Epoch [41/30], Training Loss: 13.6958, Validation Loss Current: 7.2244, Validation Loss AVG: 14.5254, lr: 0.00010000000000000003
Epoch [42/30], Training Loss: 14.0083, Validation Loss Current: 7.1920, Validation Loss AVG: 14.4946, lr: 0.00010000000000000003
Epoch [43/30], Training Loss: 14.7902, Validation Loss Current: 7.2387, Validation Loss AVG: 14.4930, lr: 0.00010000000000000003
Epoch [44/30], Training Loss: 13.6031, Validation Loss Current: 7.1894, Validation Loss AVG: 14.4739, lr: 1.0000000000000004e-05
Epoch [45/30], Training Loss: 14.8640, Validation Loss Current: 7.2083, Validation Loss AVG: 14.4675, lr: 1.0000000000000004e-05
Epoch [46/30], Training Loss: 13.4325, Validation Loss Current: 7.2502, Validation Loss AVG: 14.5138, lr: 1.0000000000000004e-05
Epoch [47/30], Training Loss: 13.6772, Validation Loss Current: 7.2249, Validation Loss AVG: 14.4728, lr: 1.0000000000000004e-05
Epoch [48/30], Training Loss: 13.7786, Validation Loss Current: 7.2242, Validation Loss AVG: 14.4464, lr: 1.0000000000000004e-05
Epoch [49/30], Training Loss: 14.9861, Validation Loss Current: 7.2164, Validation Loss AVG: 14.4784, lr: 1.0000000000000004e-05
Epoch [50/30], Training Loss: 15.1915, Validation Loss Current: 7.1410, Validation Loss AVG: 14.4835, lr: 1.0000000000000004e-06
Epoch [51/30], Training Loss: 13.3064, Validation Loss Current: 7.1678, Validation Loss AVG: 14.4480, lr: 1.0000000000000004e-06
Epoch [52/30], Training Loss: 13.4301, Validation Loss Current: 7.1978, Validation Loss AVG: 14.4680, lr: 1.0000000000000004e-06
Epoch [53/30], Training Loss: 13.6244, Validation Loss Current: 7.2502, Validation Loss AVG: 14.4792, lr: 1.0000000000000004e-06
Epoch [54/30], Training Loss: 13.5912, Validation Loss Current: 7.2670, Validation Loss AVG: 14.5057, lr: 1.0000000000000004e-06
Epoch [55/30], Training Loss: 14.2446, Validation Loss Current: 7.1870, Validation Loss AVG: 14.4791, lr: 1.0000000000000004e-06
 --- Early Stopped ---
Patch distance: 1 finished training. Best epoch: 25 Best val accuracy: [0.4342105263157895, 0.4621710526315789, 0.4276315789473684, 0.3404605263157895, 0.42105263157894735, 0.4375, 0.4407894736842105, 0.46381578947368424, 0.4621710526315789, 0.3881578947368421, 0.45230263157894735, 0.4144736842105263, 0.37993421052631576, 0.3404605263157895, 0.4769736842105263, 0.4243421052631579, 0.4967105263157895, 0.4473684210526316, 0.4654605263157895, 0.4605263157894737, 0.49506578947368424, 0.5296052631578947, 0.5115131578947368, 0.5460526315789473, 0.5345394736842105, 0.5493421052631579, 0.53125, 0.5476973684210527, 0.5394736842105263, 0.5542763157894737, 0.5460526315789473, 0.5509868421052632, 0.5476973684210527, 0.5526315789473685, 0.5444078947368421, 0.5460526315789473, 0.5427631578947368, 0.5460526315789473, 0.5476973684210527, 0.5460526315789473, 0.5427631578947368, 0.5444078947368421, 0.5444078947368421, 0.5444078947368421, 0.5444078947368421, 0.5444078947368421, 0.5444078947368421, 0.5444078947368421, 0.5444078947368421, 0.5444078947368421, 0.5444078947368421, 0.5444078947368421, 0.5444078947368421, 0.5444078947368421, 0.5444078947368421] Best val loss: 6.560689806938171


----- Training alexnet with sequence: [0.4, 0.6, 0.8, 1] -----
Current group: 0.4
Epoch [1/38], Training Loss: 40.3078, Validation Loss Current: 10.0285, Validation Loss AVG: 10.0285, lr: 0.1
Epoch [2/38], Training Loss: 39.9349, Validation Loss Current: 9.9553, Validation Loss AVG: 9.9553, lr: 0.1
Epoch [3/38], Training Loss: 40.1662, Validation Loss Current: 9.9931, Validation Loss AVG: 9.9931, lr: 0.1
Epoch [4/38], Training Loss: 40.2250, Validation Loss Current: 9.8996, Validation Loss AVG: 9.8996, lr: 0.1
Epoch [5/38], Training Loss: 39.4766, Validation Loss Current: 10.0192, Validation Loss AVG: 10.0192, lr: 0.1
Epoch [6/38], Training Loss: 38.4311, Validation Loss Current: 9.3968, Validation Loss AVG: 9.3968, lr: 0.1
Epoch [7/38], Training Loss: 38.4988, Validation Loss Current: 12.1417, Validation Loss AVG: 12.1417, lr: 0.1
Epoch [8/38], Training Loss: 40.4453, Validation Loss Current: 19.5625, Validation Loss AVG: 19.5625, lr: 0.1
Epoch [9/38], Training Loss: 41.0987, Validation Loss Current: 9.8346, Validation Loss AVG: 9.8346, lr: 0.1
Epoch [10/38], Training Loss: 36.9535, Validation Loss Current: 13.2610, Validation Loss AVG: 13.2610, lr: 0.1
Epoch [11/38], Training Loss: 37.5878, Validation Loss Current: 9.0943, Validation Loss AVG: 9.0943, lr: 0.1
Epoch [12/38], Training Loss: 36.2896, Validation Loss Current: 9.7115, Validation Loss AVG: 9.7115, lr: 0.1
Epoch [13/38], Training Loss: 36.0282, Validation Loss Current: 9.9420, Validation Loss AVG: 9.9420, lr: 0.1
Epoch [14/38], Training Loss: 40.0483, Validation Loss Current: 9.4740, Validation Loss AVG: 9.4740, lr: 0.1
Epoch [15/38], Training Loss: 35.1016, Validation Loss Current: 10.1185, Validation Loss AVG: 10.1185, lr: 0.1
Epoch [16/38], Training Loss: 35.5376, Validation Loss Current: 9.3490, Validation Loss AVG: 9.3490, lr: 0.1
Epoch [17/38], Training Loss: 35.1058, Validation Loss Current: 9.0213, Validation Loss AVG: 9.0213, lr: 0.1
Epoch [18/38], Training Loss: 35.2219, Validation Loss Current: 9.0181, Validation Loss AVG: 9.0181, lr: 0.1
Epoch [19/38], Training Loss: 34.6580, Validation Loss Current: 9.0004, Validation Loss AVG: 9.0004, lr: 0.1
Epoch [20/38], Training Loss: 33.4876, Validation Loss Current: 9.2292, Validation Loss AVG: 9.2292, lr: 0.1
Epoch [21/38], Training Loss: 34.1792, Validation Loss Current: 10.2438, Validation Loss AVG: 10.2438, lr: 0.1
Epoch [22/38], Training Loss: 35.2452, Validation Loss Current: 9.3499, Validation Loss AVG: 9.3499, lr: 0.1
Epoch [23/38], Training Loss: 35.1730, Validation Loss Current: 9.2690, Validation Loss AVG: 9.2690, lr: 0.1
Epoch [24/38], Training Loss: 32.7983, Validation Loss Current: 11.8203, Validation Loss AVG: 11.8203, lr: 0.1
Epoch [25/38], Training Loss: 34.4014, Validation Loss Current: 10.0556, Validation Loss AVG: 10.0556, lr: 0.1
Epoch [26/38], Training Loss: 33.7065, Validation Loss Current: 8.6609, Validation Loss AVG: 8.6609, lr: 0.010000000000000002
Epoch [27/38], Training Loss: 29.3905, Validation Loss Current: 8.7032, Validation Loss AVG: 8.7032, lr: 0.010000000000000002
Epoch [28/38], Training Loss: 29.6097, Validation Loss Current: 8.4687, Validation Loss AVG: 8.4687, lr: 0.010000000000000002
Epoch [29/38], Training Loss: 28.6859, Validation Loss Current: 8.6241, Validation Loss AVG: 8.6241, lr: 0.010000000000000002
Epoch [30/38], Training Loss: 27.5205, Validation Loss Current: 8.6175, Validation Loss AVG: 8.6175, lr: 0.010000000000000002
Epoch [31/38], Training Loss: 27.6466, Validation Loss Current: 8.5386, Validation Loss AVG: 8.5386, lr: 0.010000000000000002
Epoch [32/38], Training Loss: 26.2093, Validation Loss Current: 8.5024, Validation Loss AVG: 8.5024, lr: 0.010000000000000002
Epoch [33/38], Training Loss: 26.2649, Validation Loss Current: 8.7947, Validation Loss AVG: 8.7947, lr: 0.010000000000000002
Epoch [34/38], Training Loss: 24.9891, Validation Loss Current: 9.0470, Validation Loss AVG: 9.0470, lr: 0.010000000000000002
Epoch [35/38], Training Loss: 23.7035, Validation Loss Current: 8.7776, Validation Loss AVG: 8.7776, lr: 0.0010000000000000002
Epoch [36/38], Training Loss: 24.0280, Validation Loss Current: 8.8024, Validation Loss AVG: 8.8024, lr: 0.0010000000000000002
Epoch [37/38], Training Loss: 24.4822, Validation Loss Current: 8.8940, Validation Loss AVG: 8.8940, lr: 0.0010000000000000002
Epoch [38/38], Training Loss: 23.2952, Validation Loss Current: 8.8265, Validation Loss AVG: 8.8265, lr: 0.0010000000000000002
Epoch [39/38], Training Loss: 23.9243, Validation Loss Current: 8.9359, Validation Loss AVG: 8.9359, lr: 0.0010000000000000002
Epoch [40/38], Training Loss: 23.3181, Validation Loss Current: 8.8821, Validation Loss AVG: 8.8821, lr: 0.0010000000000000002
Epoch [41/38], Training Loss: 23.1568, Validation Loss Current: 8.8784, Validation Loss AVG: 8.8784, lr: 0.00010000000000000003
Epoch [42/38], Training Loss: 23.6415, Validation Loss Current: 8.9009, Validation Loss AVG: 8.9009, lr: 0.00010000000000000003
Epoch [43/38], Training Loss: 23.0603, Validation Loss Current: 8.9069, Validation Loss AVG: 8.9069, lr: 0.00010000000000000003
Epoch [44/38], Training Loss: 22.9838, Validation Loss Current: 8.9174, Validation Loss AVG: 8.9174, lr: 0.00010000000000000003
Epoch [45/38], Training Loss: 23.5608, Validation Loss Current: 8.8966, Validation Loss AVG: 8.8966, lr: 0.00010000000000000003
Epoch [46/38], Training Loss: 23.6143, Validation Loss Current: 8.8798, Validation Loss AVG: 8.8798, lr: 0.00010000000000000003
Epoch [47/38], Training Loss: 23.5610, Validation Loss Current: 8.9109, Validation Loss AVG: 8.9109, lr: 1.0000000000000004e-05
Epoch [48/38], Training Loss: 22.8371, Validation Loss Current: 8.8986, Validation Loss AVG: 8.8986, lr: 1.0000000000000004e-05
Epoch [49/38], Training Loss: 22.7545, Validation Loss Current: 8.9137, Validation Loss AVG: 8.9137, lr: 1.0000000000000004e-05
Epoch [50/38], Training Loss: 23.6035, Validation Loss Current: 8.9038, Validation Loss AVG: 8.9038, lr: 1.0000000000000004e-05
Epoch [51/38], Training Loss: 22.9880, Validation Loss Current: 8.9187, Validation Loss AVG: 8.9187, lr: 1.0000000000000004e-05
Epoch [52/38], Training Loss: 23.7955, Validation Loss Current: 8.9038, Validation Loss AVG: 8.9038, lr: 1.0000000000000004e-05
Epoch [53/38], Training Loss: 24.1073, Validation Loss Current: 8.9297, Validation Loss AVG: 8.9297, lr: 1.0000000000000004e-06
Epoch [54/38], Training Loss: 24.3588, Validation Loss Current: 8.9268, Validation Loss AVG: 8.9268, lr: 1.0000000000000004e-06
Epoch [55/38], Training Loss: 23.2834, Validation Loss Current: 8.8992, Validation Loss AVG: 8.8992, lr: 1.0000000000000004e-06
Epoch [56/38], Training Loss: 23.8484, Validation Loss Current: 8.9238, Validation Loss AVG: 8.9238, lr: 1.0000000000000004e-06
Epoch [57/38], Training Loss: 23.5990, Validation Loss Current: 8.9193, Validation Loss AVG: 8.9193, lr: 1.0000000000000004e-06
Epoch [58/38], Training Loss: 22.9050, Validation Loss Current: 8.9327, Validation Loss AVG: 8.9327, lr: 1.0000000000000004e-06
 --- Early Stopped ---
Patch distance: 0.4 finished training. Best epoch: 28 Best val accuracy: [0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.3269736842105263, 0.15164473684210525, 0.16085526315789472, 0.3470394736842105, 0.2773026315789474, 0.3266447368421052, 0.28322368421052635, 0.3019736842105263, 0.30625, 0.26875, 0.30986842105263157, 0.3473684210526316, 0.3338815789473685, 0.34078947368421053, 0.28388157894736843, 0.3305921052631579, 0.3338815789473684, 0.34638157894736843, 0.30164473684210524, 0.34638157894736843, 0.35230263157894737, 0.3605263157894737, 0.37434210526315786, 0.3743421052631579, 0.3526315789473684, 0.38355263157894737, 0.3851973684210526, 0.3628289473684211, 0.36217105263157895, 0.3717105263157895, 0.3697368421052632, 0.3657894736842105, 0.36875, 0.36743421052631586, 0.37138157894736845, 0.36973684210526314, 0.3680921052631579, 0.368421052631579, 0.36875, 0.3684210526315789, 0.3684210526315789, 0.3680921052631579, 0.3680921052631579, 0.36743421052631586, 0.3677631578947368, 0.3677631578947368, 0.3680921052631579, 0.3680921052631579, 0.3680921052631579, 0.3680921052631579, 0.3680921052631579, 0.3680921052631579, 0.3680921052631579] Best val loss: 8.468698287010193


Loaded best state dict for [0.4]
Current group: 0.6
Epoch [1/38], Training Loss: 32.0638, Validation Loss Current: 9.0532, Validation Loss AVG: 9.0532, lr: 0.1
Epoch [2/38], Training Loss: 33.3007, Validation Loss Current: 9.1983, Validation Loss AVG: 9.1983, lr: 0.1
Epoch [3/38], Training Loss: 32.7885, Validation Loss Current: 9.5796, Validation Loss AVG: 9.5796, lr: 0.1
Epoch [4/38], Training Loss: 33.3291, Validation Loss Current: 8.9527, Validation Loss AVG: 8.9527, lr: 0.1
Epoch [5/38], Training Loss: 31.8407, Validation Loss Current: 10.1054, Validation Loss AVG: 10.1054, lr: 0.1
Epoch [6/38], Training Loss: 31.8377, Validation Loss Current: 10.1034, Validation Loss AVG: 10.1034, lr: 0.1
Epoch [7/38], Training Loss: 32.8977, Validation Loss Current: 8.7386, Validation Loss AVG: 8.7386, lr: 0.1
Epoch [8/38], Training Loss: 31.7550, Validation Loss Current: 8.6443, Validation Loss AVG: 8.6443, lr: 0.1
Epoch [9/38], Training Loss: 32.2625, Validation Loss Current: 8.9230, Validation Loss AVG: 8.9230, lr: 0.1
Epoch [10/38], Training Loss: 32.2489, Validation Loss Current: 9.9703, Validation Loss AVG: 9.9703, lr: 0.1
Epoch [11/38], Training Loss: 34.2984, Validation Loss Current: 9.0744, Validation Loss AVG: 9.0744, lr: 0.1
Epoch [12/38], Training Loss: 31.7443, Validation Loss Current: 9.0266, Validation Loss AVG: 9.0266, lr: 0.1
Epoch [13/38], Training Loss: 31.1268, Validation Loss Current: 9.3666, Validation Loss AVG: 9.3666, lr: 0.1
Epoch [14/38], Training Loss: 31.7118, Validation Loss Current: 8.5845, Validation Loss AVG: 8.5845, lr: 0.1
Epoch [15/38], Training Loss: 32.2565, Validation Loss Current: 8.9823, Validation Loss AVG: 8.9823, lr: 0.1
Epoch [16/38], Training Loss: 31.6883, Validation Loss Current: 9.2169, Validation Loss AVG: 9.2169, lr: 0.1
Epoch [17/38], Training Loss: 33.5924, Validation Loss Current: 9.1914, Validation Loss AVG: 9.1914, lr: 0.1
Epoch [18/38], Training Loss: 32.6509, Validation Loss Current: 9.0970, Validation Loss AVG: 9.0970, lr: 0.1
Epoch [19/38], Training Loss: 30.0102, Validation Loss Current: 9.5444, Validation Loss AVG: 9.5444, lr: 0.1
Epoch [20/38], Training Loss: 31.6011, Validation Loss Current: 8.6504, Validation Loss AVG: 8.6504, lr: 0.1
Epoch [21/38], Training Loss: 28.9273, Validation Loss Current: 8.1799, Validation Loss AVG: 8.1799, lr: 0.010000000000000002
Epoch [22/38], Training Loss: 25.5288, Validation Loss Current: 8.4396, Validation Loss AVG: 8.4396, lr: 0.010000000000000002
Epoch [23/38], Training Loss: 24.9769, Validation Loss Current: 8.3535, Validation Loss AVG: 8.3535, lr: 0.010000000000000002
Epoch [24/38], Training Loss: 23.6706, Validation Loss Current: 8.3638, Validation Loss AVG: 8.3638, lr: 0.010000000000000002
Epoch [25/38], Training Loss: 23.5751, Validation Loss Current: 8.5260, Validation Loss AVG: 8.5260, lr: 0.010000000000000002
Epoch [26/38], Training Loss: 23.0469, Validation Loss Current: 8.4030, Validation Loss AVG: 8.4030, lr: 0.010000000000000002
Epoch [27/38], Training Loss: 23.5778, Validation Loss Current: 8.5195, Validation Loss AVG: 8.5195, lr: 0.010000000000000002
Epoch [28/38], Training Loss: 21.4275, Validation Loss Current: 8.4575, Validation Loss AVG: 8.4575, lr: 0.0010000000000000002
Epoch [29/38], Training Loss: 21.3457, Validation Loss Current: 8.5101, Validation Loss AVG: 8.5101, lr: 0.0010000000000000002
Epoch [30/38], Training Loss: 21.1311, Validation Loss Current: 8.5584, Validation Loss AVG: 8.5584, lr: 0.0010000000000000002
Epoch [31/38], Training Loss: 21.1917, Validation Loss Current: 8.5467, Validation Loss AVG: 8.5467, lr: 0.0010000000000000002
Epoch [32/38], Training Loss: 20.3605, Validation Loss Current: 8.5592, Validation Loss AVG: 8.5592, lr: 0.0010000000000000002
Epoch [33/38], Training Loss: 20.9018, Validation Loss Current: 8.6227, Validation Loss AVG: 8.6227, lr: 0.0010000000000000002
Epoch [34/38], Training Loss: 20.6255, Validation Loss Current: 8.6074, Validation Loss AVG: 8.6074, lr: 0.00010000000000000003
Epoch [35/38], Training Loss: 20.7428, Validation Loss Current: 8.6108, Validation Loss AVG: 8.6108, lr: 0.00010000000000000003
Epoch [36/38], Training Loss: 19.9089, Validation Loss Current: 8.6092, Validation Loss AVG: 8.6092, lr: 0.00010000000000000003
Epoch [37/38], Training Loss: 19.8675, Validation Loss Current: 8.6210, Validation Loss AVG: 8.6210, lr: 0.00010000000000000003
Epoch [38/38], Training Loss: 19.7645, Validation Loss Current: 8.6045, Validation Loss AVG: 8.6045, lr: 0.00010000000000000003
Epoch [39/38], Training Loss: 19.8272, Validation Loss Current: 8.5885, Validation Loss AVG: 8.5885, lr: 0.00010000000000000003
Epoch [40/38], Training Loss: 22.3274, Validation Loss Current: 8.6181, Validation Loss AVG: 8.6181, lr: 1.0000000000000004e-05
Epoch [41/38], Training Loss: 21.4684, Validation Loss Current: 8.6187, Validation Loss AVG: 8.6187, lr: 1.0000000000000004e-05
Epoch [42/38], Training Loss: 19.9336, Validation Loss Current: 8.6226, Validation Loss AVG: 8.6226, lr: 1.0000000000000004e-05
Epoch [43/38], Training Loss: 20.2188, Validation Loss Current: 8.6421, Validation Loss AVG: 8.6421, lr: 1.0000000000000004e-05
Epoch [44/38], Training Loss: 22.1961, Validation Loss Current: 8.6208, Validation Loss AVG: 8.6208, lr: 1.0000000000000004e-05
Epoch [45/38], Training Loss: 20.6532, Validation Loss Current: 8.6247, Validation Loss AVG: 8.6247, lr: 1.0000000000000004e-05
Epoch [46/38], Training Loss: 21.1552, Validation Loss Current: 8.6024, Validation Loss AVG: 8.6024, lr: 1.0000000000000004e-06
Epoch [47/38], Training Loss: 19.8883, Validation Loss Current: 8.6110, Validation Loss AVG: 8.6110, lr: 1.0000000000000004e-06
Epoch [48/38], Training Loss: 20.1797, Validation Loss Current: 8.6304, Validation Loss AVG: 8.6304, lr: 1.0000000000000004e-06
Epoch [49/38], Training Loss: 19.9078, Validation Loss Current: 8.5967, Validation Loss AVG: 8.5967, lr: 1.0000000000000004e-06
Epoch [50/38], Training Loss: 22.3721, Validation Loss Current: 8.6095, Validation Loss AVG: 8.6095, lr: 1.0000000000000004e-06
Epoch [51/38], Training Loss: 20.2508, Validation Loss Current: 8.5927, Validation Loss AVG: 8.5927, lr: 1.0000000000000004e-06
 --- Early Stopped ---
Patch distance: 0.6 finished training. Best epoch: 21 Best val accuracy: [0.3059210526315789, 0.38355263157894737, 0.33125, 0.3575657894736842, 0.33125, 0.29934210526315785, 0.34309210526315786, 0.3608552631578948, 0.3917763157894737, 0.3078947368421053, 0.3226973684210527, 0.3476973684210526, 0.3595394736842105, 0.3713815789473684, 0.3194078947368421, 0.3460526315789474, 0.3559210526315789, 0.33453947368421055, 0.36348684210526316, 0.35328947368421054, 0.3973684210526316, 0.3796052631578947, 0.38223684210526315, 0.38256578947368425, 0.36973684210526314, 0.39440789473684207, 0.3717105263157895, 0.3881578947368421, 0.3894736842105263, 0.38585526315789476, 0.3832236842105263, 0.38289473684210523, 0.38355263157894737, 0.3832236842105263, 0.3825657894736842, 0.3838815789473684, 0.38289473684210523, 0.3828947368421053, 0.3828947368421053, 0.3828947368421053, 0.38355263157894737, 0.38256578947368425, 0.38289473684210523, 0.38289473684210523, 0.38322368421052627, 0.38322368421052627, 0.38322368421052627, 0.38322368421052627, 0.3828947368421053, 0.3828947368421053, 0.38322368421052627] Best val loss: 8.179890370368957


Loaded best state dict for [0.4, 0.6]
Current group: 0.8
Epoch [1/38], Training Loss: 30.8819, Validation Loss Current: 9.7744, Validation Loss AVG: 9.7744, lr: 0.1
Epoch [2/38], Training Loss: 31.7506, Validation Loss Current: 11.0554, Validation Loss AVG: 11.0554, lr: 0.1
Epoch [3/38], Training Loss: 31.2695, Validation Loss Current: 9.4531, Validation Loss AVG: 9.4531, lr: 0.1
Epoch [4/38], Training Loss: 30.9982, Validation Loss Current: 8.9762, Validation Loss AVG: 8.9762, lr: 0.1
Epoch [5/38], Training Loss: 31.3661, Validation Loss Current: 8.7829, Validation Loss AVG: 8.7829, lr: 0.1
Epoch [6/38], Training Loss: 29.3308, Validation Loss Current: 9.1845, Validation Loss AVG: 9.1845, lr: 0.1
Epoch [7/38], Training Loss: 30.6217, Validation Loss Current: 9.5298, Validation Loss AVG: 9.5298, lr: 0.1
Epoch [8/38], Training Loss: 35.4001, Validation Loss Current: 9.0819, Validation Loss AVG: 9.0819, lr: 0.1
Epoch [9/38], Training Loss: 30.6467, Validation Loss Current: 10.3830, Validation Loss AVG: 10.3830, lr: 0.1
Epoch [10/38], Training Loss: 30.0631, Validation Loss Current: 9.3212, Validation Loss AVG: 9.3212, lr: 0.1
Epoch [11/38], Training Loss: 31.9193, Validation Loss Current: 9.1514, Validation Loss AVG: 9.1514, lr: 0.1
Epoch [12/38], Training Loss: 28.9106, Validation Loss Current: 8.6586, Validation Loss AVG: 8.6586, lr: 0.010000000000000002
Epoch [13/38], Training Loss: 27.1296, Validation Loss Current: 9.0131, Validation Loss AVG: 9.0131, lr: 0.010000000000000002
Epoch [14/38], Training Loss: 24.4408, Validation Loss Current: 8.8992, Validation Loss AVG: 8.8992, lr: 0.010000000000000002
Epoch [15/38], Training Loss: 24.5257, Validation Loss Current: 8.8446, Validation Loss AVG: 8.8446, lr: 0.010000000000000002
Epoch [16/38], Training Loss: 23.9249, Validation Loss Current: 9.0452, Validation Loss AVG: 9.0452, lr: 0.010000000000000002
Epoch [17/38], Training Loss: 24.1882, Validation Loss Current: 8.9426, Validation Loss AVG: 8.9426, lr: 0.010000000000000002
Epoch [18/38], Training Loss: 22.8121, Validation Loss Current: 9.2403, Validation Loss AVG: 9.2403, lr: 0.010000000000000002
Epoch [19/38], Training Loss: 22.1439, Validation Loss Current: 9.2225, Validation Loss AVG: 9.2225, lr: 0.0010000000000000002
Epoch [20/38], Training Loss: 21.1587, Validation Loss Current: 9.2286, Validation Loss AVG: 9.2286, lr: 0.0010000000000000002
Epoch [21/38], Training Loss: 20.6360, Validation Loss Current: 9.2351, Validation Loss AVG: 9.2351, lr: 0.0010000000000000002
Epoch [22/38], Training Loss: 22.9184, Validation Loss Current: 9.2665, Validation Loss AVG: 9.2665, lr: 0.0010000000000000002
Epoch [23/38], Training Loss: 21.1888, Validation Loss Current: 9.2524, Validation Loss AVG: 9.2524, lr: 0.0010000000000000002
Epoch [24/38], Training Loss: 21.2190, Validation Loss Current: 9.2739, Validation Loss AVG: 9.2739, lr: 0.0010000000000000002
Epoch [25/38], Training Loss: 22.2339, Validation Loss Current: 9.2745, Validation Loss AVG: 9.2745, lr: 0.00010000000000000003
Epoch [26/38], Training Loss: 20.3814, Validation Loss Current: 9.2584, Validation Loss AVG: 9.2584, lr: 0.00010000000000000003
Epoch [27/38], Training Loss: 20.5384, Validation Loss Current: 9.3016, Validation Loss AVG: 9.3016, lr: 0.00010000000000000003
Epoch [28/38], Training Loss: 20.0916, Validation Loss Current: 9.2857, Validation Loss AVG: 9.2857, lr: 0.00010000000000000003
Epoch [29/38], Training Loss: 20.1063, Validation Loss Current: 9.3065, Validation Loss AVG: 9.3065, lr: 0.00010000000000000003
Epoch [30/38], Training Loss: 20.6752, Validation Loss Current: 9.3186, Validation Loss AVG: 9.3186, lr: 0.00010000000000000003
Epoch [31/38], Training Loss: 20.0731, Validation Loss Current: 9.3357, Validation Loss AVG: 9.3357, lr: 1.0000000000000004e-05
Epoch [32/38], Training Loss: 21.0540, Validation Loss Current: 9.2997, Validation Loss AVG: 9.2997, lr: 1.0000000000000004e-05
Epoch [33/38], Training Loss: 20.1826, Validation Loss Current: 9.3117, Validation Loss AVG: 9.3117, lr: 1.0000000000000004e-05
Epoch [34/38], Training Loss: 19.8165, Validation Loss Current: 9.2820, Validation Loss AVG: 9.2820, lr: 1.0000000000000004e-05
Epoch [35/38], Training Loss: 20.5670, Validation Loss Current: 9.3115, Validation Loss AVG: 9.3115, lr: 1.0000000000000004e-05
Epoch [36/38], Training Loss: 20.7396, Validation Loss Current: 9.3068, Validation Loss AVG: 9.3068, lr: 1.0000000000000004e-05
Epoch [37/38], Training Loss: 21.0261, Validation Loss Current: 9.3202, Validation Loss AVG: 9.3202, lr: 1.0000000000000004e-06
Epoch [38/38], Training Loss: 20.9214, Validation Loss Current: 9.2911, Validation Loss AVG: 9.2911, lr: 1.0000000000000004e-06
Epoch [39/38], Training Loss: 21.2774, Validation Loss Current: 9.3144, Validation Loss AVG: 9.3144, lr: 1.0000000000000004e-06
Epoch [40/38], Training Loss: 20.3842, Validation Loss Current: 9.2954, Validation Loss AVG: 9.2954, lr: 1.0000000000000004e-06
Epoch [41/38], Training Loss: 20.6460, Validation Loss Current: 9.3495, Validation Loss AVG: 9.3495, lr: 1.0000000000000004e-06
Epoch [42/38], Training Loss: 20.5904, Validation Loss Current: 9.3209, Validation Loss AVG: 9.3209, lr: 1.0000000000000004e-06
 --- Early Stopped ---
Patch distance: 0.8 finished training. Best epoch: 12 Best val accuracy: [0.3269736842105263, 0.34111842105263157, 0.3631578947368421, 0.3398026315789474, 0.3394736842105263, 0.31776315789473686, 0.31118421052631573, 0.32203947368421054, 0.31480263157894733, 0.31118421052631573, 0.36546052631578946, 0.36151315789473687, 0.37532894736842104, 0.3595394736842105, 0.3674342105263158, 0.35131578947368425, 0.37236842105263157, 0.36217105263157895, 0.36447368421052634, 0.35921052631578954, 0.3638157894736842, 0.3625, 0.3618421052631579, 0.36578947368421055, 0.3648026315789473, 0.36546052631578946, 0.3651315789473685, 0.36447368421052634, 0.3648026315789474, 0.3654605263157895, 0.36447368421052634, 0.36414473684210524, 0.36414473684210524, 0.36447368421052634, 0.3648026315789474, 0.3648026315789473, 0.36447368421052634, 0.36447368421052634, 0.36447368421052634, 0.3648026315789474, 0.36447368421052634, 0.3648026315789474] Best val loss: 8.658629393577575


Loaded best state dict for [0.4, 0.6, 0.8]
Current group: 1
Epoch [1/38], Training Loss: 31.0786, Validation Loss Current: 9.8396, Validation Loss AVG: 12.1887, lr: 0.1
Epoch [2/38], Training Loss: 35.7978, Validation Loss Current: 8.6255, Validation Loss AVG: 9.3247, lr: 0.1
Epoch [3/38], Training Loss: 30.3704, Validation Loss Current: 8.6377, Validation Loss AVG: 11.3374, lr: 0.1
Epoch [4/38], Training Loss: 33.1174, Validation Loss Current: 8.8127, Validation Loss AVG: 9.6298, lr: 0.1
Epoch [5/38], Training Loss: 30.2900, Validation Loss Current: 7.6816, Validation Loss AVG: 9.7020, lr: 0.1
Epoch [6/38], Training Loss: 30.9640, Validation Loss Current: 9.3867, Validation Loss AVG: 11.3075, lr: 0.1
Epoch [7/38], Training Loss: 35.3705, Validation Loss Current: 8.3245, Validation Loss AVG: 9.2236, lr: 0.1
Epoch [8/38], Training Loss: 32.9115, Validation Loss Current: 8.0693, Validation Loss AVG: 9.8170, lr: 0.1
Epoch [9/38], Training Loss: 31.6687, Validation Loss Current: 8.9213, Validation Loss AVG: 9.7697, lr: 0.1
Epoch [10/38], Training Loss: 31.6249, Validation Loss Current: 8.0404, Validation Loss AVG: 9.0862, lr: 0.1
Epoch [11/38], Training Loss: 31.8359, Validation Loss Current: 8.2732, Validation Loss AVG: 10.1096, lr: 0.1
Epoch [12/38], Training Loss: 29.8776, Validation Loss Current: 7.4218, Validation Loss AVG: 9.3792, lr: 0.010000000000000002
Epoch [13/38], Training Loss: 26.7024, Validation Loss Current: 7.1230, Validation Loss AVG: 9.0406, lr: 0.010000000000000002
Epoch [14/38], Training Loss: 28.8324, Validation Loss Current: 7.0003, Validation Loss AVG: 9.6740, lr: 0.010000000000000002
Epoch [15/38], Training Loss: 24.5278, Validation Loss Current: 6.9538, Validation Loss AVG: 9.7422, lr: 0.010000000000000002
Epoch [16/38], Training Loss: 24.8235, Validation Loss Current: 6.9473, Validation Loss AVG: 10.0474, lr: 0.010000000000000002
Epoch [17/38], Training Loss: 23.1028, Validation Loss Current: 6.8639, Validation Loss AVG: 10.9345, lr: 0.010000000000000002
Epoch [18/38], Training Loss: 22.7417, Validation Loss Current: 6.8834, Validation Loss AVG: 10.3481, lr: 0.010000000000000002
Epoch [19/38], Training Loss: 21.7478, Validation Loss Current: 6.9616, Validation Loss AVG: 10.5503, lr: 0.010000000000000002
Epoch [20/38], Training Loss: 21.6033, Validation Loss Current: 6.9106, Validation Loss AVG: 11.0844, lr: 0.010000000000000002
Epoch [21/38], Training Loss: 20.1905, Validation Loss Current: 6.9917, Validation Loss AVG: 11.5197, lr: 0.010000000000000002
Epoch [22/38], Training Loss: 19.9334, Validation Loss Current: 7.3804, Validation Loss AVG: 13.4002, lr: 0.010000000000000002
Epoch [23/38], Training Loss: 19.2669, Validation Loss Current: 7.2377, Validation Loss AVG: 11.2517, lr: 0.010000000000000002
Epoch [24/38], Training Loss: 17.2290, Validation Loss Current: 7.2262, Validation Loss AVG: 11.9893, lr: 0.0010000000000000002
Epoch [25/38], Training Loss: 17.5675, Validation Loss Current: 7.1825, Validation Loss AVG: 12.0195, lr: 0.0010000000000000002
Epoch [26/38], Training Loss: 16.9033, Validation Loss Current: 7.2666, Validation Loss AVG: 12.1505, lr: 0.0010000000000000002
Epoch [27/38], Training Loss: 18.9087, Validation Loss Current: 7.2340, Validation Loss AVG: 12.1158, lr: 0.0010000000000000002
Epoch [28/38], Training Loss: 17.0283, Validation Loss Current: 7.2880, Validation Loss AVG: 12.3662, lr: 0.0010000000000000002
Epoch [29/38], Training Loss: 17.7048, Validation Loss Current: 7.3292, Validation Loss AVG: 12.3810, lr: 0.0010000000000000002
Epoch [30/38], Training Loss: 16.1987, Validation Loss Current: 7.3231, Validation Loss AVG: 12.4128, lr: 0.00010000000000000003
Epoch [31/38], Training Loss: 16.1382, Validation Loss Current: 7.3462, Validation Loss AVG: 12.4144, lr: 0.00010000000000000003
Epoch [32/38], Training Loss: 16.8796, Validation Loss Current: 7.3019, Validation Loss AVG: 12.3684, lr: 0.00010000000000000003
Epoch [33/38], Training Loss: 16.5715, Validation Loss Current: 7.3370, Validation Loss AVG: 12.3841, lr: 0.00010000000000000003
Epoch [34/38], Training Loss: 17.1206, Validation Loss Current: 7.2711, Validation Loss AVG: 12.3684, lr: 0.00010000000000000003
Epoch [35/38], Training Loss: 16.9090, Validation Loss Current: 7.3151, Validation Loss AVG: 12.4192, lr: 0.00010000000000000003
Epoch [36/38], Training Loss: 16.6759, Validation Loss Current: 7.3236, Validation Loss AVG: 12.3966, lr: 1.0000000000000004e-05
Epoch [37/38], Training Loss: 16.9575, Validation Loss Current: 7.3197, Validation Loss AVG: 12.3846, lr: 1.0000000000000004e-05
Epoch [38/38], Training Loss: 17.4569, Validation Loss Current: 7.3508, Validation Loss AVG: 12.3934, lr: 1.0000000000000004e-05
Epoch [39/38], Training Loss: 16.6146, Validation Loss Current: 7.2612, Validation Loss AVG: 12.3919, lr: 1.0000000000000004e-05
Epoch [40/38], Training Loss: 16.2565, Validation Loss Current: 7.2716, Validation Loss AVG: 12.3313, lr: 1.0000000000000004e-05
Epoch [41/38], Training Loss: 16.9305, Validation Loss Current: 7.3635, Validation Loss AVG: 12.3691, lr: 1.0000000000000004e-05
Epoch [42/38], Training Loss: 15.9728, Validation Loss Current: 7.3005, Validation Loss AVG: 12.3398, lr: 1.0000000000000004e-06
Epoch [43/38], Training Loss: 16.6936, Validation Loss Current: 7.3609, Validation Loss AVG: 12.4054, lr: 1.0000000000000004e-06
Epoch [44/38], Training Loss: 17.2503, Validation Loss Current: 7.3405, Validation Loss AVG: 12.4495, lr: 1.0000000000000004e-06
Epoch [45/38], Training Loss: 17.7364, Validation Loss Current: 7.3595, Validation Loss AVG: 12.3794, lr: 1.0000000000000004e-06
Epoch [46/38], Training Loss: 16.7186, Validation Loss Current: 7.3295, Validation Loss AVG: 12.4401, lr: 1.0000000000000004e-06
Epoch [47/38], Training Loss: 18.8996, Validation Loss Current: 7.2831, Validation Loss AVG: 12.3702, lr: 1.0000000000000004e-06
 --- Early Stopped ---
Patch distance: 1 finished training. Best epoch: 17 Best val accuracy: [0.37006578947368424, 0.39473684210526316, 0.4095394736842105, 0.35855263157894735, 0.4440789473684211, 0.3305921052631579, 0.37664473684210525, 0.4457236842105263, 0.38980263157894735, 0.3996710526315789, 0.4128289473684211, 0.46710526315789475, 0.4786184210526316, 0.4819078947368421, 0.4934210526315789, 0.4917763157894737, 0.5, 0.4934210526315789, 0.5049342105263158, 0.5016447368421053, 0.49835526315789475, 0.5016447368421053, 0.5213815789473685, 0.5148026315789473, 0.5263157894736842, 0.5213815789473685, 0.5230263157894737, 0.524671052631579, 0.5230263157894737, 0.524671052631579, 0.5230263157894737, 0.5230263157894737, 0.5213815789473685, 0.5213815789473685, 0.5213815789473685, 0.5213815789473685, 0.5213815789473685, 0.5213815789473685, 0.5213815789473685, 0.5213815789473685, 0.5213815789473685, 0.5213815789473685, 0.5213815789473685, 0.5213815789473685, 0.5213815789473685, 0.5213815789473685, 0.5213815789473685] Best val loss: 6.863900184631348


----- Training alexnet with sequence: [0.6, 0.8, 1] -----
Current group: 0.6
Epoch [1/50], Training Loss: 40.9224, Validation Loss Current: 10.0437, Validation Loss AVG: 10.0437, lr: 0.1
Epoch [2/50], Training Loss: 40.3721, Validation Loss Current: 9.9442, Validation Loss AVG: 9.9442, lr: 0.1
Epoch [3/50], Training Loss: 40.6987, Validation Loss Current: 9.9593, Validation Loss AVG: 9.9593, lr: 0.1
Epoch [4/50], Training Loss: 40.5750, Validation Loss Current: 9.9742, Validation Loss AVG: 9.9742, lr: 0.1
Epoch [5/50], Training Loss: 40.1417, Validation Loss Current: 10.0172, Validation Loss AVG: 10.0172, lr: 0.1
Epoch [6/50], Training Loss: 39.9375, Validation Loss Current: 9.9722, Validation Loss AVG: 9.9722, lr: 0.1
Epoch [7/50], Training Loss: 40.6266, Validation Loss Current: 9.9826, Validation Loss AVG: 9.9826, lr: 0.1
Epoch [8/50], Training Loss: 40.1989, Validation Loss Current: 9.9534, Validation Loss AVG: 9.9534, lr: 0.1
Epoch [9/50], Training Loss: 40.0100, Validation Loss Current: 9.7691, Validation Loss AVG: 9.7691, lr: 0.010000000000000002
Epoch [10/50], Training Loss: 38.4396, Validation Loss Current: 9.6873, Validation Loss AVG: 9.6873, lr: 0.010000000000000002
Epoch [11/50], Training Loss: 36.3599, Validation Loss Current: 9.1854, Validation Loss AVG: 9.1854, lr: 0.010000000000000002
Epoch [12/50], Training Loss: 36.5017, Validation Loss Current: 8.9885, Validation Loss AVG: 8.9885, lr: 0.010000000000000002
Epoch [13/50], Training Loss: 34.9489, Validation Loss Current: 10.0871, Validation Loss AVG: 10.0871, lr: 0.010000000000000002
Epoch [14/50], Training Loss: 36.5763, Validation Loss Current: 8.6582, Validation Loss AVG: 8.6582, lr: 0.010000000000000002
Epoch [15/50], Training Loss: 33.6414, Validation Loss Current: 8.8603, Validation Loss AVG: 8.8603, lr: 0.010000000000000002
Epoch [16/50], Training Loss: 33.3299, Validation Loss Current: 8.5277, Validation Loss AVG: 8.5277, lr: 0.010000000000000002
Epoch [17/50], Training Loss: 32.2426, Validation Loss Current: 8.8282, Validation Loss AVG: 8.8282, lr: 0.010000000000000002
Epoch [18/50], Training Loss: 31.4292, Validation Loss Current: 8.2272, Validation Loss AVG: 8.2272, lr: 0.010000000000000002
Epoch [19/50], Training Loss: 31.6248, Validation Loss Current: 9.4005, Validation Loss AVG: 9.4005, lr: 0.010000000000000002
Epoch [20/50], Training Loss: 32.1068, Validation Loss Current: 8.7688, Validation Loss AVG: 8.7688, lr: 0.010000000000000002
Epoch [21/50], Training Loss: 31.1474, Validation Loss Current: 8.2006, Validation Loss AVG: 8.2006, lr: 0.010000000000000002
Epoch [22/50], Training Loss: 28.8162, Validation Loss Current: 8.1869, Validation Loss AVG: 8.1869, lr: 0.010000000000000002
Epoch [23/50], Training Loss: 29.2604, Validation Loss Current: 8.7194, Validation Loss AVG: 8.7194, lr: 0.010000000000000002
Epoch [24/50], Training Loss: 28.3063, Validation Loss Current: 9.0603, Validation Loss AVG: 9.0603, lr: 0.010000000000000002
Epoch [25/50], Training Loss: 29.2441, Validation Loss Current: 8.0897, Validation Loss AVG: 8.0897, lr: 0.010000000000000002
Epoch [26/50], Training Loss: 27.8169, Validation Loss Current: 8.1593, Validation Loss AVG: 8.1593, lr: 0.010000000000000002
Epoch [27/50], Training Loss: 25.9952, Validation Loss Current: 9.1209, Validation Loss AVG: 9.1209, lr: 0.010000000000000002
Epoch [28/50], Training Loss: 30.0406, Validation Loss Current: 8.7808, Validation Loss AVG: 8.7808, lr: 0.010000000000000002
Epoch [29/50], Training Loss: 28.5941, Validation Loss Current: 8.7390, Validation Loss AVG: 8.7390, lr: 0.010000000000000002
Epoch [30/50], Training Loss: 25.6293, Validation Loss Current: 9.0731, Validation Loss AVG: 9.0731, lr: 0.010000000000000002
Epoch [31/50], Training Loss: 25.4055, Validation Loss Current: 8.7833, Validation Loss AVG: 8.7833, lr: 0.010000000000000002
Epoch [32/50], Training Loss: 24.3623, Validation Loss Current: 8.0461, Validation Loss AVG: 8.0461, lr: 0.0010000000000000002
Epoch [33/50], Training Loss: 23.6500, Validation Loss Current: 8.2133, Validation Loss AVG: 8.2133, lr: 0.0010000000000000002
Epoch [34/50], Training Loss: 23.0442, Validation Loss Current: 8.1904, Validation Loss AVG: 8.1904, lr: 0.0010000000000000002
Epoch [35/50], Training Loss: 24.4116, Validation Loss Current: 8.1066, Validation Loss AVG: 8.1066, lr: 0.0010000000000000002
Epoch [36/50], Training Loss: 22.7355, Validation Loss Current: 8.4277, Validation Loss AVG: 8.4277, lr: 0.0010000000000000002
Epoch [37/50], Training Loss: 22.1732, Validation Loss Current: 8.4372, Validation Loss AVG: 8.4372, lr: 0.0010000000000000002
Epoch [38/50], Training Loss: 21.9726, Validation Loss Current: 8.4321, Validation Loss AVG: 8.4321, lr: 0.0010000000000000002
Epoch [39/50], Training Loss: 22.5549, Validation Loss Current: 8.2868, Validation Loss AVG: 8.2868, lr: 0.00010000000000000003
Epoch [40/50], Training Loss: 22.4954, Validation Loss Current: 8.2392, Validation Loss AVG: 8.2392, lr: 0.00010000000000000003
Epoch [41/50], Training Loss: 21.5673, Validation Loss Current: 8.2296, Validation Loss AVG: 8.2296, lr: 0.00010000000000000003
Epoch [42/50], Training Loss: 21.2706, Validation Loss Current: 8.2784, Validation Loss AVG: 8.2784, lr: 0.00010000000000000003
Epoch [43/50], Training Loss: 22.5347, Validation Loss Current: 8.2263, Validation Loss AVG: 8.2263, lr: 0.00010000000000000003
Epoch [44/50], Training Loss: 21.7203, Validation Loss Current: 8.2751, Validation Loss AVG: 8.2751, lr: 0.00010000000000000003
Epoch [45/50], Training Loss: 21.6571, Validation Loss Current: 8.2974, Validation Loss AVG: 8.2974, lr: 1.0000000000000004e-05
Epoch [46/50], Training Loss: 21.5415, Validation Loss Current: 8.2667, Validation Loss AVG: 8.2667, lr: 1.0000000000000004e-05
Epoch [47/50], Training Loss: 22.2745, Validation Loss Current: 8.2751, Validation Loss AVG: 8.2751, lr: 1.0000000000000004e-05
Epoch [48/50], Training Loss: 21.0977, Validation Loss Current: 8.2676, Validation Loss AVG: 8.2676, lr: 1.0000000000000004e-05
Epoch [49/50], Training Loss: 21.7094, Validation Loss Current: 8.2307, Validation Loss AVG: 8.2307, lr: 1.0000000000000004e-05
Epoch [50/50], Training Loss: 23.5018, Validation Loss Current: 8.2886, Validation Loss AVG: 8.2886, lr: 1.0000000000000004e-05
Epoch [51/50], Training Loss: 22.5427, Validation Loss Current: 8.2801, Validation Loss AVG: 8.2801, lr: 1.0000000000000004e-06
Epoch [52/50], Training Loss: 21.4226, Validation Loss Current: 8.2795, Validation Loss AVG: 8.2795, lr: 1.0000000000000004e-06
Epoch [53/50], Training Loss: 21.7610, Validation Loss Current: 8.2719, Validation Loss AVG: 8.2719, lr: 1.0000000000000004e-06
Epoch [54/50], Training Loss: 22.0577, Validation Loss Current: 8.2909, Validation Loss AVG: 8.2909, lr: 1.0000000000000004e-06
Epoch [55/50], Training Loss: 21.0028, Validation Loss Current: 8.2831, Validation Loss AVG: 8.2831, lr: 1.0000000000000004e-06
Epoch [56/50], Training Loss: 22.3073, Validation Loss Current: 8.2463, Validation Loss AVG: 8.2463, lr: 1.0000000000000004e-06
Epoch [57/50], Training Loss: 21.9148, Validation Loss Current: 8.2627, Validation Loss AVG: 8.2627, lr: 1.0000000000000005e-07
Epoch [58/50], Training Loss: 22.6682, Validation Loss Current: 8.2925, Validation Loss AVG: 8.2925, lr: 1.0000000000000005e-07
Epoch [59/50], Training Loss: 24.2388, Validation Loss Current: 8.2698, Validation Loss AVG: 8.2698, lr: 1.0000000000000005e-07
Epoch [60/50], Training Loss: 21.4450, Validation Loss Current: 8.2625, Validation Loss AVG: 8.2625, lr: 1.0000000000000005e-07
Epoch [61/50], Training Loss: 23.8526, Validation Loss Current: 8.2863, Validation Loss AVG: 8.2863, lr: 1.0000000000000005e-07
Epoch [62/50], Training Loss: 23.6571, Validation Loss Current: 8.2612, Validation Loss AVG: 8.2612, lr: 1.0000000000000005e-07
 --- Early Stopped ---
Patch distance: 0.6 finished training. Best epoch: 32 Best val accuracy: [0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2375, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.31282894736842104, 0.28125, 0.3338815789473684, 0.35625, 0.24802631578947368, 0.38190789473684206, 0.35625000000000007, 0.37467105263157896, 0.35559210526315793, 0.39440789473684207, 0.3026315789473685, 0.356578947368421, 0.39703947368421055, 0.4026315789473684, 0.37467105263157896, 0.3542763157894736, 0.3973684210526316, 0.4266447368421053, 0.38223684210526315, 0.40625, 0.3960526315789473, 0.3894736842105263, 0.4009868421052632, 0.42927631578947373, 0.4315789473684212, 0.43881578947368427, 0.44210526315789467, 0.4292763157894736, 0.43684210526315786, 0.4338815789473684, 0.4427631578947368, 0.44046052631578947, 0.4430921052631579, 0.4391447368421053, 0.4427631578947368, 0.43684210526315786, 0.43815789473684214, 0.4375, 0.4384868421052632, 0.43881578947368427, 0.43980263157894744, 0.43881578947368427, 0.43881578947368427, 0.43881578947368427, 0.43881578947368427, 0.4391447368421053, 0.43947368421052635, 0.43980263157894744, 0.43980263157894744, 0.43947368421052635, 0.43947368421052635, 0.43947368421052635, 0.43947368421052635, 0.43947368421052635] Best val loss: 8.046120119094848


Loaded best state dict for [0.6]
Current group: 0.8
Epoch [1/50], Training Loss: 39.7991, Validation Loss Current: 16.0359, Validation Loss AVG: 16.0359, lr: 0.1
Epoch [2/50], Training Loss: 38.6580, Validation Loss Current: 14.3338, Validation Loss AVG: 14.3338, lr: 0.1
Epoch [3/50], Training Loss: 38.1566, Validation Loss Current: 9.4639, Validation Loss AVG: 9.4639, lr: 0.1
Epoch [4/50], Training Loss: 35.1727, Validation Loss Current: 10.1526, Validation Loss AVG: 10.1526, lr: 0.1
Epoch [5/50], Training Loss: 34.5689, Validation Loss Current: 13.8824, Validation Loss AVG: 13.8824, lr: 0.1
Epoch [6/50], Training Loss: 34.9767, Validation Loss Current: 10.6449, Validation Loss AVG: 10.6449, lr: 0.1
Epoch [7/50], Training Loss: 38.7084, Validation Loss Current: 10.1034, Validation Loss AVG: 10.1034, lr: 0.1
Epoch [8/50], Training Loss: 34.6480, Validation Loss Current: 10.3063, Validation Loss AVG: 10.3063, lr: 0.1
Epoch [9/50], Training Loss: 33.9593, Validation Loss Current: 12.0824, Validation Loss AVG: 12.0824, lr: 0.1
Epoch [10/50], Training Loss: 34.4722, Validation Loss Current: 8.8887, Validation Loss AVG: 8.8887, lr: 0.010000000000000002
Epoch [11/50], Training Loss: 29.3060, Validation Loss Current: 8.5592, Validation Loss AVG: 8.5592, lr: 0.010000000000000002
Epoch [12/50], Training Loss: 27.9810, Validation Loss Current: 8.5567, Validation Loss AVG: 8.5567, lr: 0.010000000000000002
Epoch [13/50], Training Loss: 26.3548, Validation Loss Current: 8.5977, Validation Loss AVG: 8.5977, lr: 0.010000000000000002
Epoch [14/50], Training Loss: 27.7233, Validation Loss Current: 8.4979, Validation Loss AVG: 8.4979, lr: 0.010000000000000002
Epoch [15/50], Training Loss: 25.8676, Validation Loss Current: 8.3956, Validation Loss AVG: 8.3956, lr: 0.010000000000000002
Epoch [16/50], Training Loss: 24.7261, Validation Loss Current: 8.8634, Validation Loss AVG: 8.8634, lr: 0.010000000000000002
Epoch [17/50], Training Loss: 25.9039, Validation Loss Current: 8.3404, Validation Loss AVG: 8.3404, lr: 0.010000000000000002
Epoch [18/50], Training Loss: 25.9478, Validation Loss Current: 8.5883, Validation Loss AVG: 8.5883, lr: 0.010000000000000002
Epoch [19/50], Training Loss: 25.0545, Validation Loss Current: 8.3354, Validation Loss AVG: 8.3354, lr: 0.010000000000000002
Epoch [20/50], Training Loss: 22.5512, Validation Loss Current: 8.3597, Validation Loss AVG: 8.3597, lr: 0.010000000000000002
Epoch [21/50], Training Loss: 23.8377, Validation Loss Current: 8.3163, Validation Loss AVG: 8.3163, lr: 0.010000000000000002
Epoch [22/50], Training Loss: 23.1829, Validation Loss Current: 8.2292, Validation Loss AVG: 8.2292, lr: 0.010000000000000002
Epoch [23/50], Training Loss: 21.8854, Validation Loss Current: 9.1875, Validation Loss AVG: 9.1875, lr: 0.010000000000000002
Epoch [24/50], Training Loss: 22.5025, Validation Loss Current: 8.0438, Validation Loss AVG: 8.0438, lr: 0.010000000000000002
Epoch [25/50], Training Loss: 21.0423, Validation Loss Current: 9.4395, Validation Loss AVG: 9.4395, lr: 0.010000000000000002
Epoch [26/50], Training Loss: 21.3163, Validation Loss Current: 9.6889, Validation Loss AVG: 9.6889, lr: 0.010000000000000002
Epoch [27/50], Training Loss: 21.7506, Validation Loss Current: 9.2431, Validation Loss AVG: 9.2431, lr: 0.010000000000000002
Epoch [28/50], Training Loss: 21.6345, Validation Loss Current: 8.5661, Validation Loss AVG: 8.5661, lr: 0.010000000000000002
Epoch [29/50], Training Loss: 19.9125, Validation Loss Current: 9.9263, Validation Loss AVG: 9.9263, lr: 0.010000000000000002
Epoch [30/50], Training Loss: 19.0437, Validation Loss Current: 9.0051, Validation Loss AVG: 9.0051, lr: 0.010000000000000002
Epoch [31/50], Training Loss: 19.7306, Validation Loss Current: 9.0364, Validation Loss AVG: 9.0364, lr: 0.0010000000000000002
Epoch [32/50], Training Loss: 17.1024, Validation Loss Current: 9.2579, Validation Loss AVG: 9.2579, lr: 0.0010000000000000002
Epoch [33/50], Training Loss: 17.2046, Validation Loss Current: 9.6203, Validation Loss AVG: 9.6203, lr: 0.0010000000000000002
Epoch [34/50], Training Loss: 16.8867, Validation Loss Current: 9.5487, Validation Loss AVG: 9.5487, lr: 0.0010000000000000002
Epoch [35/50], Training Loss: 16.5680, Validation Loss Current: 9.5801, Validation Loss AVG: 9.5801, lr: 0.0010000000000000002
Epoch [36/50], Training Loss: 16.5295, Validation Loss Current: 9.4632, Validation Loss AVG: 9.4632, lr: 0.0010000000000000002
Epoch [37/50], Training Loss: 16.1287, Validation Loss Current: 9.4985, Validation Loss AVG: 9.4985, lr: 0.00010000000000000003
Epoch [38/50], Training Loss: 16.5044, Validation Loss Current: 9.5340, Validation Loss AVG: 9.5340, lr: 0.00010000000000000003
Epoch [39/50], Training Loss: 16.3685, Validation Loss Current: 9.5053, Validation Loss AVG: 9.5053, lr: 0.00010000000000000003
Epoch [40/50], Training Loss: 17.4470, Validation Loss Current: 9.5402, Validation Loss AVG: 9.5402, lr: 0.00010000000000000003
Epoch [41/50], Training Loss: 16.6770, Validation Loss Current: 9.5896, Validation Loss AVG: 9.5896, lr: 0.00010000000000000003
Epoch [42/50], Training Loss: 16.1672, Validation Loss Current: 9.6040, Validation Loss AVG: 9.6040, lr: 0.00010000000000000003
Epoch [43/50], Training Loss: 16.6081, Validation Loss Current: 9.6050, Validation Loss AVG: 9.6050, lr: 1.0000000000000004e-05
Epoch [44/50], Training Loss: 17.0206, Validation Loss Current: 9.6599, Validation Loss AVG: 9.6599, lr: 1.0000000000000004e-05
Epoch [45/50], Training Loss: 17.3985, Validation Loss Current: 9.5992, Validation Loss AVG: 9.5992, lr: 1.0000000000000004e-05
Epoch [46/50], Training Loss: 17.2550, Validation Loss Current: 9.5718, Validation Loss AVG: 9.5718, lr: 1.0000000000000004e-05
Epoch [47/50], Training Loss: 16.6868, Validation Loss Current: 9.5821, Validation Loss AVG: 9.5821, lr: 1.0000000000000004e-05
Epoch [48/50], Training Loss: 16.7548, Validation Loss Current: 9.5666, Validation Loss AVG: 9.5666, lr: 1.0000000000000004e-05
Epoch [49/50], Training Loss: 18.3636, Validation Loss Current: 9.5885, Validation Loss AVG: 9.5885, lr: 1.0000000000000004e-06
Epoch [50/50], Training Loss: 18.3586, Validation Loss Current: 9.5852, Validation Loss AVG: 9.5852, lr: 1.0000000000000004e-06
Epoch [51/50], Training Loss: 17.0804, Validation Loss Current: 9.6209, Validation Loss AVG: 9.6209, lr: 1.0000000000000004e-06
Epoch [52/50], Training Loss: 16.3632, Validation Loss Current: 9.5995, Validation Loss AVG: 9.5995, lr: 1.0000000000000004e-06
Epoch [53/50], Training Loss: 16.2815, Validation Loss Current: 9.6242, Validation Loss AVG: 9.6242, lr: 1.0000000000000004e-06
Epoch [54/50], Training Loss: 16.3196, Validation Loss Current: 9.5756, Validation Loss AVG: 9.5756, lr: 1.0000000000000004e-06
 --- Early Stopped ---
Patch distance: 0.8 finished training. Best epoch: 24 Best val accuracy: [0.12631578947368421, 0.30394736842105263, 0.2986842105263158, 0.3230263157894737, 0.3299342105263158, 0.29407894736842105, 0.14769736842105263, 0.22269736842105264, 0.28322368421052635, 0.3759868421052632, 0.4, 0.37467105263157896, 0.37894736842105264, 0.37894736842105264, 0.4095394736842105, 0.38552631578947366, 0.4240131578947369, 0.4302631578947368, 0.41710526315789476, 0.4118421052631579, 0.4256578947368421, 0.4236842105263158, 0.4009868421052631, 0.43815789473684214, 0.39901315789473685, 0.3848684210526316, 0.39473684210526316, 0.42894736842105263, 0.39342105263157895, 0.43157894736842106, 0.4233552631578948, 0.4200657894736842, 0.4131578947368421, 0.4151315789473684, 0.4131578947368421, 0.4233552631578947, 0.41907894736842105, 0.4180921052631579, 0.4167763157894737, 0.4200657894736842, 0.4197368421052632, 0.4197368421052632, 0.4197368421052632, 0.4197368421052632, 0.41875, 0.41940789473684215, 0.4184210526315789, 0.4184210526315789, 0.4184210526315789, 0.4180921052631579, 0.4180921052631579, 0.4180921052631579, 0.4180921052631579, 0.4180921052631579] Best val loss: 8.043772506713868


Loaded best state dict for [0.6, 0.8]
Current group: 1
Epoch [1/50], Training Loss: 30.1128, Validation Loss Current: 8.7382, Validation Loss AVG: 9.5228, lr: 0.1
Epoch [2/50], Training Loss: 34.1025, Validation Loss Current: 10.1830, Validation Loss AVG: 10.6029, lr: 0.1
Epoch [3/50], Training Loss: 35.2041, Validation Loss Current: 8.5486, Validation Loss AVG: 9.3257, lr: 0.1
Epoch [4/50], Training Loss: 32.8496, Validation Loss Current: 11.2787, Validation Loss AVG: 11.9743, lr: 0.1
Epoch [5/50], Training Loss: 33.7359, Validation Loss Current: 9.2319, Validation Loss AVG: 10.3655, lr: 0.1
Epoch [6/50], Training Loss: 34.6737, Validation Loss Current: 10.4730, Validation Loss AVG: 11.4390, lr: 0.1
Epoch [7/50], Training Loss: 35.8141, Validation Loss Current: 8.1917, Validation Loss AVG: 9.8431, lr: 0.1
Epoch [8/50], Training Loss: 32.1513, Validation Loss Current: 11.0255, Validation Loss AVG: 12.8067, lr: 0.1
Epoch [9/50], Training Loss: 35.4578, Validation Loss Current: 9.0550, Validation Loss AVG: 9.5557, lr: 0.1
Epoch [10/50], Training Loss: 32.2363, Validation Loss Current: 14.2004, Validation Loss AVG: 22.9254, lr: 0.1
Epoch [11/50], Training Loss: 33.2443, Validation Loss Current: 7.7743, Validation Loss AVG: 9.7002, lr: 0.1
Epoch [12/50], Training Loss: 31.9318, Validation Loss Current: 10.1062, Validation Loss AVG: 10.3045, lr: 0.1
Epoch [13/50], Training Loss: 33.7896, Validation Loss Current: 8.6469, Validation Loss AVG: 11.0956, lr: 0.1
Epoch [14/50], Training Loss: 30.9791, Validation Loss Current: 8.4576, Validation Loss AVG: 10.5710, lr: 0.1
Epoch [15/50], Training Loss: 31.7398, Validation Loss Current: 7.8231, Validation Loss AVG: 9.2999, lr: 0.1
Epoch [16/50], Training Loss: 31.7901, Validation Loss Current: 9.7575, Validation Loss AVG: 10.0005, lr: 0.1
Epoch [17/50], Training Loss: 31.6656, Validation Loss Current: 8.2888, Validation Loss AVG: 10.2457, lr: 0.1
Epoch [18/50], Training Loss: 30.8871, Validation Loss Current: 7.0880, Validation Loss AVG: 9.0768, lr: 0.010000000000000002
Epoch [19/50], Training Loss: 26.1707, Validation Loss Current: 6.7912, Validation Loss AVG: 8.2218, lr: 0.010000000000000002
Epoch [20/50], Training Loss: 25.7440, Validation Loss Current: 6.6955, Validation Loss AVG: 8.1998, lr: 0.010000000000000002
Epoch [21/50], Training Loss: 25.0659, Validation Loss Current: 6.7756, Validation Loss AVG: 8.5092, lr: 0.010000000000000002
Epoch [22/50], Training Loss: 25.5150, Validation Loss Current: 6.7764, Validation Loss AVG: 9.0408, lr: 0.010000000000000002
Epoch [23/50], Training Loss: 23.9400, Validation Loss Current: 6.5860, Validation Loss AVG: 8.7588, lr: 0.010000000000000002
Epoch [24/50], Training Loss: 21.8678, Validation Loss Current: 6.6755, Validation Loss AVG: 8.6686, lr: 0.010000000000000002
Epoch [25/50], Training Loss: 25.1375, Validation Loss Current: 6.5450, Validation Loss AVG: 10.2045, lr: 0.010000000000000002
Epoch [26/50], Training Loss: 21.6145, Validation Loss Current: 6.5186, Validation Loss AVG: 9.1069, lr: 0.010000000000000002
Epoch [27/50], Training Loss: 19.8545, Validation Loss Current: 6.5935, Validation Loss AVG: 8.8920, lr: 0.010000000000000002
Epoch [28/50], Training Loss: 19.6411, Validation Loss Current: 6.9440, Validation Loss AVG: 11.5390, lr: 0.010000000000000002
Epoch [29/50], Training Loss: 19.2569, Validation Loss Current: 6.9073, Validation Loss AVG: 12.9721, lr: 0.010000000000000002
Epoch [30/50], Training Loss: 18.3702, Validation Loss Current: 6.9171, Validation Loss AVG: 11.1185, lr: 0.010000000000000002
Epoch [31/50], Training Loss: 18.4146, Validation Loss Current: 6.6595, Validation Loss AVG: 9.9312, lr: 0.010000000000000002
Epoch [32/50], Training Loss: 17.5571, Validation Loss Current: 6.9669, Validation Loss AVG: 12.5006, lr: 0.010000000000000002
Epoch [33/50], Training Loss: 16.8273, Validation Loss Current: 6.9237, Validation Loss AVG: 11.5523, lr: 0.0010000000000000002
Epoch [34/50], Training Loss: 16.6138, Validation Loss Current: 6.7794, Validation Loss AVG: 11.0943, lr: 0.0010000000000000002
Epoch [35/50], Training Loss: 15.3381, Validation Loss Current: 6.8947, Validation Loss AVG: 11.5826, lr: 0.0010000000000000002
Epoch [36/50], Training Loss: 20.1122, Validation Loss Current: 7.0015, Validation Loss AVG: 11.7382, lr: 0.0010000000000000002
Epoch [37/50], Training Loss: 15.2287, Validation Loss Current: 6.9499, Validation Loss AVG: 12.0948, lr: 0.0010000000000000002
Epoch [38/50], Training Loss: 16.2066, Validation Loss Current: 6.9666, Validation Loss AVG: 12.1047, lr: 0.0010000000000000002
Epoch [39/50], Training Loss: 14.6479, Validation Loss Current: 7.0047, Validation Loss AVG: 12.0285, lr: 0.00010000000000000003
Epoch [40/50], Training Loss: 14.9391, Validation Loss Current: 7.0563, Validation Loss AVG: 12.0795, lr: 0.00010000000000000003
Epoch [41/50], Training Loss: 15.0079, Validation Loss Current: 6.9836, Validation Loss AVG: 12.0680, lr: 0.00010000000000000003
Epoch [42/50], Training Loss: 14.8004, Validation Loss Current: 6.9807, Validation Loss AVG: 12.1358, lr: 0.00010000000000000003
Epoch [43/50], Training Loss: 15.4459, Validation Loss Current: 6.9937, Validation Loss AVG: 12.1582, lr: 0.00010000000000000003
Epoch [44/50], Training Loss: 15.2738, Validation Loss Current: 7.0460, Validation Loss AVG: 12.1104, lr: 0.00010000000000000003
Epoch [45/50], Training Loss: 14.4723, Validation Loss Current: 6.9291, Validation Loss AVG: 12.0889, lr: 1.0000000000000004e-05
Epoch [46/50], Training Loss: 14.3764, Validation Loss Current: 6.9653, Validation Loss AVG: 12.0948, lr: 1.0000000000000004e-05
Epoch [47/50], Training Loss: 15.3349, Validation Loss Current: 7.0109, Validation Loss AVG: 12.1674, lr: 1.0000000000000004e-05
Epoch [48/50], Training Loss: 15.5883, Validation Loss Current: 6.9343, Validation Loss AVG: 12.1004, lr: 1.0000000000000004e-05
Epoch [49/50], Training Loss: 14.4436, Validation Loss Current: 7.0044, Validation Loss AVG: 12.1216, lr: 1.0000000000000004e-05
Epoch [50/50], Training Loss: 15.1594, Validation Loss Current: 6.9951, Validation Loss AVG: 12.1593, lr: 1.0000000000000004e-05
Epoch [51/50], Training Loss: 14.7072, Validation Loss Current: 6.9649, Validation Loss AVG: 12.1422, lr: 1.0000000000000004e-06
Epoch [52/50], Training Loss: 16.4690, Validation Loss Current: 7.0428, Validation Loss AVG: 12.1519, lr: 1.0000000000000004e-06
Epoch [53/50], Training Loss: 15.3935, Validation Loss Current: 7.0261, Validation Loss AVG: 12.1432, lr: 1.0000000000000004e-06
Epoch [54/50], Training Loss: 14.3614, Validation Loss Current: 6.9944, Validation Loss AVG: 12.1267, lr: 1.0000000000000004e-06
Epoch [55/50], Training Loss: 15.3415, Validation Loss Current: 7.0583, Validation Loss AVG: 12.1572, lr: 1.0000000000000004e-06
Epoch [56/50], Training Loss: 14.8502, Validation Loss Current: 6.9873, Validation Loss AVG: 12.1302, lr: 1.0000000000000004e-06
 --- Early Stopped ---
Patch distance: 1 finished training. Best epoch: 26 Best val accuracy: [0.36019736842105265, 0.3404605263157895, 0.36348684210526316, 0.2713815789473684, 0.3651315789473684, 0.3338815789473684, 0.43914473684210525, 0.3355263157894737, 0.3207236842105263, 0.3355263157894737, 0.4276315789473684, 0.37006578947368424, 0.4194078947368421, 0.4506578947368421, 0.42105263157894735, 0.3963815789473684, 0.4128289473684211, 0.4769736842105263, 0.5164473684210527, 0.5082236842105263, 0.5263157894736842, 0.4967105263157895, 0.5476973684210527, 0.53125, 0.5296052631578947, 0.5361842105263158, 0.5279605263157895, 0.5197368421052632, 0.5164473684210527, 0.537828947368421, 0.5296052631578947, 0.5148026315789473, 0.5328947368421053, 0.5263157894736842, 0.5296052631578947, 0.53125, 0.5279605263157895, 0.5296052631578947, 0.5296052631578947, 0.53125, 0.5328947368421053, 0.5279605263157895, 0.53125, 0.5328947368421053, 0.5296052631578947, 0.5296052631578947, 0.5296052631578947, 0.5296052631578947, 0.5296052631578947, 0.5296052631578947, 0.5296052631578947, 0.5296052631578947, 0.5296052631578947, 0.5296052631578947, 0.5296052631578947, 0.5296052631578947] Best val loss: 6.51863706111908


----- Training alexnet with sequence: [0.8, 1] -----
Current group: 0.8
Epoch [1/75], Training Loss: 40.8459, Validation Loss Current: 9.9743, Validation Loss AVG: 9.9743, lr: 0.1
Epoch [2/75], Training Loss: 40.7171, Validation Loss Current: 9.9684, Validation Loss AVG: 9.9684, lr: 0.1
Epoch [3/75], Training Loss: 40.3848, Validation Loss Current: 10.1887, Validation Loss AVG: 10.1887, lr: 0.1
Epoch [4/75], Training Loss: 39.5680, Validation Loss Current: 9.9611, Validation Loss AVG: 9.9611, lr: 0.1
Epoch [5/75], Training Loss: 39.6437, Validation Loss Current: 9.7254, Validation Loss AVG: 9.7254, lr: 0.1
Epoch [6/75], Training Loss: 39.0907, Validation Loss Current: 13.8208, Validation Loss AVG: 13.8208, lr: 0.1
Epoch [7/75], Training Loss: 37.5076, Validation Loss Current: 9.1710, Validation Loss AVG: 9.1710, lr: 0.1
Epoch [8/75], Training Loss: 34.4383, Validation Loss Current: 9.5969, Validation Loss AVG: 9.5969, lr: 0.1
Epoch [9/75], Training Loss: 37.9144, Validation Loss Current: 10.3436, Validation Loss AVG: 10.3436, lr: 0.1
Epoch [10/75], Training Loss: 36.0762, Validation Loss Current: 10.0721, Validation Loss AVG: 10.0721, lr: 0.1
Epoch [11/75], Training Loss: 34.2486, Validation Loss Current: 9.1357, Validation Loss AVG: 9.1357, lr: 0.1
Epoch [12/75], Training Loss: 33.1309, Validation Loss Current: 18.1841, Validation Loss AVG: 18.1841, lr: 0.1
Epoch [13/75], Training Loss: 37.4667, Validation Loss Current: 13.6293, Validation Loss AVG: 13.6293, lr: 0.1
Epoch [14/75], Training Loss: 36.6746, Validation Loss Current: 10.0059, Validation Loss AVG: 10.0059, lr: 0.1
Epoch [15/75], Training Loss: 37.0067, Validation Loss Current: 9.6928, Validation Loss AVG: 9.6928, lr: 0.1
Epoch [16/75], Training Loss: 34.2407, Validation Loss Current: 10.1656, Validation Loss AVG: 10.1656, lr: 0.1
Epoch [17/75], Training Loss: 34.4420, Validation Loss Current: 9.2627, Validation Loss AVG: 9.2627, lr: 0.1
Epoch [18/75], Training Loss: 30.2205, Validation Loss Current: 8.3456, Validation Loss AVG: 8.3456, lr: 0.010000000000000002
Epoch [19/75], Training Loss: 29.1378, Validation Loss Current: 8.4170, Validation Loss AVG: 8.4170, lr: 0.010000000000000002
Epoch [20/75], Training Loss: 28.3876, Validation Loss Current: 8.3987, Validation Loss AVG: 8.3987, lr: 0.010000000000000002
Epoch [21/75], Training Loss: 27.0359, Validation Loss Current: 8.2270, Validation Loss AVG: 8.2270, lr: 0.010000000000000002
Epoch [22/75], Training Loss: 25.1633, Validation Loss Current: 8.2937, Validation Loss AVG: 8.2937, lr: 0.010000000000000002
Epoch [23/75], Training Loss: 24.7863, Validation Loss Current: 8.5162, Validation Loss AVG: 8.5162, lr: 0.010000000000000002
Epoch [24/75], Training Loss: 26.3870, Validation Loss Current: 8.3201, Validation Loss AVG: 8.3201, lr: 0.010000000000000002
Epoch [25/75], Training Loss: 23.2296, Validation Loss Current: 8.4392, Validation Loss AVG: 8.4392, lr: 0.010000000000000002
Epoch [26/75], Training Loss: 23.2680, Validation Loss Current: 8.8140, Validation Loss AVG: 8.8140, lr: 0.010000000000000002
Epoch [27/75], Training Loss: 22.1437, Validation Loss Current: 8.4816, Validation Loss AVG: 8.4816, lr: 0.010000000000000002
Epoch [28/75], Training Loss: 21.7727, Validation Loss Current: 8.3261, Validation Loss AVG: 8.3261, lr: 0.0010000000000000002
Epoch [29/75], Training Loss: 21.3010, Validation Loss Current: 8.4228, Validation Loss AVG: 8.4228, lr: 0.0010000000000000002
Epoch [30/75], Training Loss: 20.3578, Validation Loss Current: 8.4167, Validation Loss AVG: 8.4167, lr: 0.0010000000000000002
Epoch [31/75], Training Loss: 20.1046, Validation Loss Current: 8.3648, Validation Loss AVG: 8.3648, lr: 0.0010000000000000002
Epoch [32/75], Training Loss: 22.0292, Validation Loss Current: 8.5451, Validation Loss AVG: 8.5451, lr: 0.0010000000000000002
Epoch [33/75], Training Loss: 21.4747, Validation Loss Current: 8.5929, Validation Loss AVG: 8.5929, lr: 0.0010000000000000002
Epoch [34/75], Training Loss: 19.4099, Validation Loss Current: 8.5248, Validation Loss AVG: 8.5248, lr: 0.00010000000000000003
Epoch [35/75], Training Loss: 20.2010, Validation Loss Current: 8.5113, Validation Loss AVG: 8.5113, lr: 0.00010000000000000003
Epoch [36/75], Training Loss: 19.3536, Validation Loss Current: 8.4948, Validation Loss AVG: 8.4948, lr: 0.00010000000000000003
Epoch [37/75], Training Loss: 19.9781, Validation Loss Current: 8.5316, Validation Loss AVG: 8.5316, lr: 0.00010000000000000003
Epoch [38/75], Training Loss: 19.7868, Validation Loss Current: 8.5251, Validation Loss AVG: 8.5251, lr: 0.00010000000000000003
Epoch [39/75], Training Loss: 19.8268, Validation Loss Current: 8.5247, Validation Loss AVG: 8.5247, lr: 0.00010000000000000003
Epoch [40/75], Training Loss: 19.9111, Validation Loss Current: 8.5111, Validation Loss AVG: 8.5111, lr: 1.0000000000000004e-05
Epoch [41/75], Training Loss: 19.9908, Validation Loss Current: 8.5231, Validation Loss AVG: 8.5231, lr: 1.0000000000000004e-05
Epoch [42/75], Training Loss: 19.9636, Validation Loss Current: 8.5185, Validation Loss AVG: 8.5185, lr: 1.0000000000000004e-05
Epoch [43/75], Training Loss: 19.3986, Validation Loss Current: 8.5449, Validation Loss AVG: 8.5449, lr: 1.0000000000000004e-05
Epoch [44/75], Training Loss: 20.8545, Validation Loss Current: 8.5361, Validation Loss AVG: 8.5361, lr: 1.0000000000000004e-05
Epoch [45/75], Training Loss: 19.9662, Validation Loss Current: 8.5462, Validation Loss AVG: 8.5462, lr: 1.0000000000000004e-05
Epoch [46/75], Training Loss: 19.8700, Validation Loss Current: 8.5362, Validation Loss AVG: 8.5362, lr: 1.0000000000000004e-06
Epoch [47/75], Training Loss: 19.1200, Validation Loss Current: 8.5335, Validation Loss AVG: 8.5335, lr: 1.0000000000000004e-06
Epoch [48/75], Training Loss: 19.4719, Validation Loss Current: 8.5507, Validation Loss AVG: 8.5507, lr: 1.0000000000000004e-06
Epoch [49/75], Training Loss: 19.4756, Validation Loss Current: 8.5448, Validation Loss AVG: 8.5448, lr: 1.0000000000000004e-06
Epoch [50/75], Training Loss: 20.5881, Validation Loss Current: 8.5335, Validation Loss AVG: 8.5335, lr: 1.0000000000000004e-06
Epoch [51/75], Training Loss: 20.5776, Validation Loss Current: 8.5332, Validation Loss AVG: 8.5332, lr: 1.0000000000000004e-06
 --- Early Stopped ---
Patch distance: 0.8 finished training. Best epoch: 21 Best val accuracy: [0.2631578947368421, 0.2631578947368421, 0.17401315789473684, 0.2552631578947368, 0.2717105263157895, 0.20592105263157895, 0.3286184210526316, 0.33651315789473685, 0.2625, 0.2934210526315789, 0.3101973684210526, 0.28059210526315786, 0.22828947368421054, 0.23815789473684207, 0.3259868421052631, 0.2644736842105263, 0.31513157894736843, 0.3868421052631579, 0.39769736842105263, 0.3759868421052631, 0.40559210526315786, 0.4029605263157895, 0.40230263157894736, 0.3967105263157895, 0.41019736842105264, 0.4164473684210527, 0.4200657894736842, 0.41743421052631585, 0.41052631578947374, 0.41578947368421054, 0.4180921052631579, 0.4131578947368421, 0.41546052631578945, 0.4151315789473684, 0.41546052631578945, 0.4144736842105264, 0.4151315789473684, 0.4128289473684211, 0.4134868421052632, 0.4134868421052632, 0.4131578947368421, 0.4134868421052632, 0.4131578947368421, 0.4131578947368421, 0.4134868421052632, 0.4134868421052632, 0.4134868421052632, 0.4134868421052632, 0.4134868421052632, 0.4134868421052632, 0.4134868421052632] Best val loss: 8.226983737945556


Loaded best state dict for [0.8]
Current group: 1
Epoch [1/75], Training Loss: 31.1879, Validation Loss Current: 7.8123, Validation Loss AVG: 8.6989, lr: 0.1
Epoch [2/75], Training Loss: 33.3152, Validation Loss Current: 8.6522, Validation Loss AVG: 9.4309, lr: 0.1
Epoch [3/75], Training Loss: 32.5991, Validation Loss Current: 8.9200, Validation Loss AVG: 12.5337, lr: 0.1
Epoch [4/75], Training Loss: 33.2240, Validation Loss Current: 8.4539, Validation Loss AVG: 10.9724, lr: 0.1
Epoch [5/75], Training Loss: 31.7586, Validation Loss Current: 7.9839, Validation Loss AVG: 9.0307, lr: 0.1
Epoch [6/75], Training Loss: 31.4230, Validation Loss Current: 8.9165, Validation Loss AVG: 11.6394, lr: 0.1
Epoch [7/75], Training Loss: 31.8718, Validation Loss Current: 12.4067, Validation Loss AVG: 14.0555, lr: 0.1
Epoch [8/75], Training Loss: 32.8030, Validation Loss Current: 7.3987, Validation Loss AVG: 8.9564, lr: 0.010000000000000002
Epoch [9/75], Training Loss: 28.5236, Validation Loss Current: 7.1549, Validation Loss AVG: 9.5510, lr: 0.010000000000000002
Epoch [10/75], Training Loss: 26.8387, Validation Loss Current: 6.8855, Validation Loss AVG: 9.1584, lr: 0.010000000000000002
Epoch [11/75], Training Loss: 25.3996, Validation Loss Current: 6.6210, Validation Loss AVG: 9.2115, lr: 0.010000000000000002
Epoch [12/75], Training Loss: 24.7429, Validation Loss Current: 6.5382, Validation Loss AVG: 9.2157, lr: 0.010000000000000002
Epoch [13/75], Training Loss: 24.0712, Validation Loss Current: 6.7016, Validation Loss AVG: 9.8278, lr: 0.010000000000000002
Epoch [14/75], Training Loss: 22.5846, Validation Loss Current: 6.6456, Validation Loss AVG: 9.9547, lr: 0.010000000000000002
Epoch [15/75], Training Loss: 23.4685, Validation Loss Current: 6.6341, Validation Loss AVG: 9.6216, lr: 0.010000000000000002
Epoch [16/75], Training Loss: 20.7164, Validation Loss Current: 6.8731, Validation Loss AVG: 10.2438, lr: 0.010000000000000002
Epoch [17/75], Training Loss: 20.5053, Validation Loss Current: 6.6226, Validation Loss AVG: 10.0558, lr: 0.010000000000000002
Epoch [18/75], Training Loss: 21.1764, Validation Loss Current: 7.1460, Validation Loss AVG: 10.3512, lr: 0.010000000000000002
Epoch [19/75], Training Loss: 19.3528, Validation Loss Current: 6.5007, Validation Loss AVG: 9.6710, lr: 0.0010000000000000002
Epoch [20/75], Training Loss: 20.3324, Validation Loss Current: 6.5532, Validation Loss AVG: 10.0913, lr: 0.0010000000000000002
Epoch [21/75], Training Loss: 20.9645, Validation Loss Current: 6.5741, Validation Loss AVG: 10.0273, lr: 0.0010000000000000002
Epoch [22/75], Training Loss: 19.6912, Validation Loss Current: 6.5682, Validation Loss AVG: 10.2314, lr: 0.0010000000000000002
Epoch [23/75], Training Loss: 19.8425, Validation Loss Current: 6.6498, Validation Loss AVG: 10.5618, lr: 0.0010000000000000002
Epoch [24/75], Training Loss: 17.8331, Validation Loss Current: 6.5922, Validation Loss AVG: 10.4112, lr: 0.0010000000000000002
Epoch [25/75], Training Loss: 17.7052, Validation Loss Current: 6.5826, Validation Loss AVG: 10.4979, lr: 0.0010000000000000002
Epoch [26/75], Training Loss: 18.8055, Validation Loss Current: 6.5918, Validation Loss AVG: 10.4746, lr: 0.00010000000000000003
Epoch [27/75], Training Loss: 18.5689, Validation Loss Current: 6.7115, Validation Loss AVG: 10.4883, lr: 0.00010000000000000003
Epoch [28/75], Training Loss: 18.2299, Validation Loss Current: 6.6303, Validation Loss AVG: 10.4426, lr: 0.00010000000000000003
Epoch [29/75], Training Loss: 18.1580, Validation Loss Current: 6.6005, Validation Loss AVG: 10.4906, lr: 0.00010000000000000003
Epoch [30/75], Training Loss: 17.9941, Validation Loss Current: 6.6071, Validation Loss AVG: 10.4801, lr: 0.00010000000000000003
Epoch [31/75], Training Loss: 18.2184, Validation Loss Current: 6.5836, Validation Loss AVG: 10.4676, lr: 0.00010000000000000003
Epoch [32/75], Training Loss: 17.9687, Validation Loss Current: 6.6370, Validation Loss AVG: 10.4511, lr: 1.0000000000000004e-05
Epoch [33/75], Training Loss: 18.3779, Validation Loss Current: 6.6368, Validation Loss AVG: 10.4939, lr: 1.0000000000000004e-05
Epoch [34/75], Training Loss: 18.0563, Validation Loss Current: 6.5768, Validation Loss AVG: 10.4539, lr: 1.0000000000000004e-05
Epoch [35/75], Training Loss: 17.5354, Validation Loss Current: 6.6435, Validation Loss AVG: 10.4103, lr: 1.0000000000000004e-05
Epoch [36/75], Training Loss: 18.5760, Validation Loss Current: 6.6207, Validation Loss AVG: 10.4631, lr: 1.0000000000000004e-05
Epoch [37/75], Training Loss: 18.4526, Validation Loss Current: 6.5605, Validation Loss AVG: 10.4474, lr: 1.0000000000000004e-05
Epoch [38/75], Training Loss: 19.1032, Validation Loss Current: 6.6551, Validation Loss AVG: 10.4309, lr: 1.0000000000000004e-06
Epoch [39/75], Training Loss: 17.8079, Validation Loss Current: 6.6297, Validation Loss AVG: 10.4510, lr: 1.0000000000000004e-06
Epoch [40/75], Training Loss: 17.9552, Validation Loss Current: 6.6340, Validation Loss AVG: 10.4659, lr: 1.0000000000000004e-06
Epoch [41/75], Training Loss: 17.5217, Validation Loss Current: 6.6042, Validation Loss AVG: 10.4565, lr: 1.0000000000000004e-06
Epoch [42/75], Training Loss: 18.4777, Validation Loss Current: 6.5858, Validation Loss AVG: 10.4779, lr: 1.0000000000000004e-06
Epoch [43/75], Training Loss: 17.6790, Validation Loss Current: 6.5532, Validation Loss AVG: 10.4606, lr: 1.0000000000000004e-06
Epoch [44/75], Training Loss: 18.5097, Validation Loss Current: 6.6059, Validation Loss AVG: 10.4513, lr: 1.0000000000000005e-07
Epoch [45/75], Training Loss: 19.2609, Validation Loss Current: 6.6141, Validation Loss AVG: 10.4362, lr: 1.0000000000000005e-07
Epoch [46/75], Training Loss: 18.1258, Validation Loss Current: 6.6004, Validation Loss AVG: 10.4599, lr: 1.0000000000000005e-07
Epoch [47/75], Training Loss: 17.7497, Validation Loss Current: 6.6203, Validation Loss AVG: 10.5022, lr: 1.0000000000000005e-07
Epoch [48/75], Training Loss: 18.2232, Validation Loss Current: 6.6152, Validation Loss AVG: 10.4615, lr: 1.0000000000000005e-07
Epoch [49/75], Training Loss: 18.5094, Validation Loss Current: 6.6339, Validation Loss AVG: 10.4650, lr: 1.0000000000000005e-07
 --- Early Stopped ---
Patch distance: 1 finished training. Best epoch: 19 Best val accuracy: [0.40789473684210525, 0.35526315789473684, 0.33881578947368424, 0.4144736842105263, 0.42269736842105265, 0.3848684210526316, 0.3371710526315789, 0.49506578947368424, 0.4769736842105263, 0.5213815789473685, 0.5131578947368421, 0.53125, 0.5328947368421053, 0.5394736842105263, 0.5328947368421053, 0.5394736842105263, 0.5526315789473685, 0.524671052631579, 0.5509868421052632, 0.5575657894736842, 0.5575657894736842, 0.5592105263157895, 0.5625, 0.5625, 0.5608552631578947, 0.5657894736842105, 0.5641447368421053, 0.569078947368421, 0.5657894736842105, 0.5641447368421053, 0.5674342105263158, 0.5674342105263158, 0.5674342105263158, 0.5674342105263158, 0.5657894736842105, 0.5657894736842105, 0.5657894736842105, 0.5657894736842105, 0.5657894736842105, 0.5657894736842105, 0.5657894736842105, 0.5657894736842105, 0.5657894736842105, 0.5657894736842105, 0.5657894736842105, 0.5657894736842105, 0.5657894736842105, 0.5657894736842105, 0.5657894736842105] Best val loss: 6.50067675113678


----- Training alexnet with sequence: [1] -----
Current group: 1
Epoch [1/150], Training Loss: 41.1247, Validation Loss Current: 9.9577, Validation Loss AVG: 9.9636, lr: 0.1
Epoch [2/150], Training Loss: 40.4259, Validation Loss Current: 9.9155, Validation Loss AVG: 9.9465, lr: 0.1
Epoch [3/150], Training Loss: 40.2426, Validation Loss Current: 15.8125, Validation Loss AVG: 14.4979, lr: 0.1
Epoch [4/150], Training Loss: 42.0903, Validation Loss Current: 9.9187, Validation Loss AVG: 9.9513, lr: 0.1
Epoch [5/150], Training Loss: 39.7293, Validation Loss Current: 10.7265, Validation Loss AVG: 12.7758, lr: 0.1
Epoch [6/150], Training Loss: 41.9464, Validation Loss Current: 23.3887, Validation Loss AVG: 22.0103, lr: 0.1
Epoch [7/150], Training Loss: 44.8672, Validation Loss Current: 9.8542, Validation Loss AVG: 9.9363, lr: 0.1
Epoch [8/150], Training Loss: 39.4612, Validation Loss Current: 9.3072, Validation Loss AVG: 10.3140, lr: 0.1
Epoch [9/150], Training Loss: 36.5767, Validation Loss Current: 8.6759, Validation Loss AVG: 9.3607, lr: 0.1
Epoch [10/150], Training Loss: 34.8195, Validation Loss Current: 13.0210, Validation Loss AVG: 11.9707, lr: 0.1
Epoch [11/150], Training Loss: 35.9303, Validation Loss Current: 14.5226, Validation Loss AVG: 15.0660, lr: 0.1
Epoch [12/150], Training Loss: 38.8210, Validation Loss Current: 9.0658, Validation Loss AVG: 9.4781, lr: 0.1
Epoch [13/150], Training Loss: 34.6948, Validation Loss Current: 8.1742, Validation Loss AVG: 8.8827, lr: 0.1
Epoch [14/150], Training Loss: 32.5458, Validation Loss Current: 9.6176, Validation Loss AVG: 11.2264, lr: 0.1
Epoch [15/150], Training Loss: 35.4051, Validation Loss Current: 8.9194, Validation Loss AVG: 9.7474, lr: 0.1
Epoch [16/150], Training Loss: 34.5105, Validation Loss Current: 8.7880, Validation Loss AVG: 9.6098, lr: 0.1
Epoch [17/150], Training Loss: 36.4122, Validation Loss Current: 10.9261, Validation Loss AVG: 11.8168, lr: 0.1
Epoch [18/150], Training Loss: 35.8928, Validation Loss Current: 8.5773, Validation Loss AVG: 9.3429, lr: 0.1
Epoch [19/150], Training Loss: 36.5200, Validation Loss Current: 9.8736, Validation Loss AVG: 11.3735, lr: 0.1
Epoch [20/150], Training Loss: 33.3436, Validation Loss Current: 7.7303, Validation Loss AVG: 8.6942, lr: 0.010000000000000002
Epoch [21/150], Training Loss: 30.7292, Validation Loss Current: 7.1997, Validation Loss AVG: 8.9020, lr: 0.010000000000000002
Epoch [22/150], Training Loss: 28.5115, Validation Loss Current: 7.0437, Validation Loss AVG: 8.4734, lr: 0.010000000000000002
Epoch [23/150], Training Loss: 27.5389, Validation Loss Current: 7.0186, Validation Loss AVG: 8.6925, lr: 0.010000000000000002
Epoch [24/150], Training Loss: 27.8441, Validation Loss Current: 7.0594, Validation Loss AVG: 9.2469, lr: 0.010000000000000002
Epoch [25/150], Training Loss: 27.6903, Validation Loss Current: 6.8212, Validation Loss AVG: 8.3949, lr: 0.010000000000000002
Epoch [26/150], Training Loss: 27.1511, Validation Loss Current: 6.7648, Validation Loss AVG: 8.2535, lr: 0.010000000000000002
Epoch [27/150], Training Loss: 25.9434, Validation Loss Current: 6.7478, Validation Loss AVG: 8.3681, lr: 0.010000000000000002
Epoch [28/150], Training Loss: 26.3172, Validation Loss Current: 6.7233, Validation Loss AVG: 8.2324, lr: 0.010000000000000002
Epoch [29/150], Training Loss: 25.9129, Validation Loss Current: 6.7024, Validation Loss AVG: 8.3203, lr: 0.010000000000000002
Epoch [30/150], Training Loss: 25.3828, Validation Loss Current: 6.6125, Validation Loss AVG: 8.7450, lr: 0.010000000000000002
Epoch [31/150], Training Loss: 24.8507, Validation Loss Current: 6.7229, Validation Loss AVG: 9.3202, lr: 0.010000000000000002
Epoch [32/150], Training Loss: 23.6698, Validation Loss Current: 6.8448, Validation Loss AVG: 9.0298, lr: 0.010000000000000002
Epoch [33/150], Training Loss: 25.0705, Validation Loss Current: 6.9569, Validation Loss AVG: 8.7280, lr: 0.010000000000000002
Epoch [34/150], Training Loss: 24.2010, Validation Loss Current: 6.6163, Validation Loss AVG: 8.7343, lr: 0.010000000000000002
Epoch [35/150], Training Loss: 23.6710, Validation Loss Current: 6.7894, Validation Loss AVG: 9.2763, lr: 0.010000000000000002
Epoch [36/150], Training Loss: 22.6299, Validation Loss Current: 6.5402, Validation Loss AVG: 8.8654, lr: 0.010000000000000002
Epoch [37/150], Training Loss: 21.1980, Validation Loss Current: 6.4284, Validation Loss AVG: 10.1412, lr: 0.010000000000000002
Epoch [38/150], Training Loss: 22.0249, Validation Loss Current: 6.6161, Validation Loss AVG: 9.0219, lr: 0.010000000000000002
Epoch [39/150], Training Loss: 20.4635, Validation Loss Current: 6.9607, Validation Loss AVG: 11.3503, lr: 0.010000000000000002
Epoch [40/150], Training Loss: 20.6469, Validation Loss Current: 6.7580, Validation Loss AVG: 11.4796, lr: 0.010000000000000002
Epoch [41/150], Training Loss: 20.7915, Validation Loss Current: 6.7103, Validation Loss AVG: 10.3165, lr: 0.010000000000000002
Epoch [42/150], Training Loss: 19.6176, Validation Loss Current: 6.3999, Validation Loss AVG: 9.7332, lr: 0.010000000000000002
Epoch [43/150], Training Loss: 18.5331, Validation Loss Current: 6.5243, Validation Loss AVG: 9.7712, lr: 0.010000000000000002
Epoch [44/150], Training Loss: 18.8903, Validation Loss Current: 6.5190, Validation Loss AVG: 9.8828, lr: 0.010000000000000002
Epoch [45/150], Training Loss: 19.7344, Validation Loss Current: 7.4806, Validation Loss AVG: 9.8042, lr: 0.010000000000000002
Epoch [46/150], Training Loss: 18.8542, Validation Loss Current: 7.0685, Validation Loss AVG: 12.4451, lr: 0.010000000000000002
Epoch [47/150], Training Loss: 17.9721, Validation Loss Current: 7.1136, Validation Loss AVG: 10.1449, lr: 0.010000000000000002
Epoch [48/150], Training Loss: 16.7789, Validation Loss Current: 7.0720, Validation Loss AVG: 12.5499, lr: 0.010000000000000002
Epoch [49/150], Training Loss: 16.1936, Validation Loss Current: 6.6180, Validation Loss AVG: 11.1631, lr: 0.0010000000000000002
Epoch [50/150], Training Loss: 15.9749, Validation Loss Current: 6.6093, Validation Loss AVG: 11.2150, lr: 0.0010000000000000002
Epoch [51/150], Training Loss: 14.5505, Validation Loss Current: 6.7704, Validation Loss AVG: 11.9234, lr: 0.0010000000000000002
Epoch [52/150], Training Loss: 15.4055, Validation Loss Current: 6.7612, Validation Loss AVG: 11.2379, lr: 0.0010000000000000002
Epoch [53/150], Training Loss: 14.9443, Validation Loss Current: 6.6977, Validation Loss AVG: 11.0575, lr: 0.0010000000000000002
Epoch [54/150], Training Loss: 14.5156, Validation Loss Current: 6.8088, Validation Loss AVG: 10.8930, lr: 0.0010000000000000002
Epoch [55/150], Training Loss: 14.8099, Validation Loss Current: 6.7435, Validation Loss AVG: 10.9806, lr: 0.00010000000000000003
Epoch [56/150], Training Loss: 15.0808, Validation Loss Current: 6.7592, Validation Loss AVG: 11.1994, lr: 0.00010000000000000003
Epoch [57/150], Training Loss: 13.9879, Validation Loss Current: 6.8380, Validation Loss AVG: 11.1508, lr: 0.00010000000000000003
Epoch [58/150], Training Loss: 14.0712, Validation Loss Current: 6.7605, Validation Loss AVG: 11.1572, lr: 0.00010000000000000003
Epoch [59/150], Training Loss: 14.4905, Validation Loss Current: 6.7812, Validation Loss AVG: 11.2014, lr: 0.00010000000000000003
Epoch [60/150], Training Loss: 15.3464, Validation Loss Current: 6.7981, Validation Loss AVG: 11.1932, lr: 0.00010000000000000003
Epoch [61/150], Training Loss: 13.6884, Validation Loss Current: 6.7842, Validation Loss AVG: 11.1577, lr: 1.0000000000000004e-05
Epoch [62/150], Training Loss: 15.0950, Validation Loss Current: 6.7509, Validation Loss AVG: 11.1852, lr: 1.0000000000000004e-05
Epoch [63/150], Training Loss: 14.6539, Validation Loss Current: 6.8392, Validation Loss AVG: 11.1513, lr: 1.0000000000000004e-05
Epoch [64/150], Training Loss: 13.8568, Validation Loss Current: 6.7672, Validation Loss AVG: 11.1495, lr: 1.0000000000000004e-05
Epoch [65/150], Training Loss: 14.2790, Validation Loss Current: 6.7769, Validation Loss AVG: 11.1807, lr: 1.0000000000000004e-05
Epoch [66/150], Training Loss: 13.8210, Validation Loss Current: 6.8146, Validation Loss AVG: 11.1937, lr: 1.0000000000000004e-05
Epoch [67/150], Training Loss: 13.8973, Validation Loss Current: 6.7284, Validation Loss AVG: 11.1244, lr: 1.0000000000000004e-06
Epoch [68/150], Training Loss: 13.4920, Validation Loss Current: 6.8191, Validation Loss AVG: 11.1545, lr: 1.0000000000000004e-06
Epoch [69/150], Training Loss: 14.3671, Validation Loss Current: 6.7772, Validation Loss AVG: 11.1766, lr: 1.0000000000000004e-06
Epoch [70/150], Training Loss: 14.5441, Validation Loss Current: 6.8048, Validation Loss AVG: 11.1580, lr: 1.0000000000000004e-06
Epoch [71/150], Training Loss: 14.5485, Validation Loss Current: 6.7807, Validation Loss AVG: 11.1699, lr: 1.0000000000000004e-06
Epoch [72/150], Training Loss: 13.6374, Validation Loss Current: 6.7988, Validation Loss AVG: 11.1542, lr: 1.0000000000000004e-06
 --- Early Stopped ---
Patch distance: 1 finished training. Best epoch: 42 Best val accuracy: [0.2631578947368421, 0.2631578947368421, 0.10855263157894737, 0.2878289473684211, 0.18092105263157895, 0.1611842105263158, 0.2631578947368421, 0.3338815789473684, 0.3996710526315789, 0.26480263157894735, 0.28618421052631576, 0.3519736842105263, 0.375, 0.2713815789473684, 0.31085526315789475, 0.32730263157894735, 0.23684210526315788, 0.3667763157894737, 0.3042763157894737, 0.4473684210526316, 0.46710526315789475, 0.4868421052631579, 0.49835526315789475, 0.49835526315789475, 0.5049342105263158, 0.5032894736842105, 0.5213815789473685, 0.5032894736842105, 0.5049342105263158, 0.5016447368421053, 0.5098684210526315, 0.5148026315789473, 0.5115131578947368, 0.5345394736842105, 0.5148026315789473, 0.5180921052631579, 0.5493421052631579, 0.5542763157894737, 0.5148026315789473, 0.5197368421052632, 0.5411184210526315, 0.5674342105263158, 0.5476973684210527, 0.5444078947368421, 0.53125, 0.5197368421052632, 0.5493421052631579, 0.5361842105263158, 0.5575657894736842, 0.5625, 0.5575657894736842, 0.5723684210526315, 0.5625, 0.5756578947368421, 0.569078947368421, 0.5674342105263158, 0.5657894736842105, 0.569078947368421, 0.5674342105263158, 0.569078947368421, 0.5674342105263158, 0.5674342105263158, 0.5674342105263158, 0.5674342105263158, 0.5674342105263158, 0.5674342105263158, 0.5674342105263158, 0.5674342105263158, 0.5674342105263158, 0.5674342105263158, 0.5674342105263158, 0.5674342105263158] Best val loss: 6.39986526966095


-------------------- All training done --------------------


 --- Evaluating ---
Fold: 0
---- Testing model trained on sequence: [0.2, 0.4, 0.6, 0.8, 1] ----
Test set distance: 0.2 Top 1 Accuracy: 0.19098548510313215
Test set distance: 0.4 Top 1 Accuracy: 0.26585179526355995
Test set distance: 0.6 Top 1 Accuracy: 0.4110007639419404
Test set distance: 0.8 Top 1 Accuracy: 0.5278838808250573
Test set distance: 1 Top 1 Accuracy: 0.5416348357524828
---- Testing model trained on sequence: [0.4, 0.6, 0.8, 1] ----
Test set distance: 0.2 Top 1 Accuracy: 0.1841100076394194
Test set distance: 0.4 Top 1 Accuracy: 0.23071046600458364
Test set distance: 0.6 Top 1 Accuracy: 0.36134453781512604
Test set distance: 0.8 Top 1 Accuracy: 0.494270435446906
Test set distance: 1 Top 1 Accuracy: 0.5286478227654698
---- Testing model trained on sequence: [0.6, 0.8, 1] ----
Test set distance: 0.2 Top 1 Accuracy: 0.20626432391138275
Test set distance: 0.4 Top 1 Accuracy: 0.23529411764705882
Test set distance: 0.6 Top 1 Accuracy: 0.37433155080213903
Test set distance: 0.8 Top 1 Accuracy: 0.4774637127578304
Test set distance: 1 Top 1 Accuracy: 0.5042016806722689
---- Testing model trained on sequence: [0.8, 1] ----
Test set distance: 0.2 Top 1 Accuracy: 0.17188693659281895
Test set distance: 0.4 Top 1 Accuracy: 0.22841864018334607
Test set distance: 0.6 Top 1 Accuracy: 0.3025210084033613
Test set distance: 0.8 Top 1 Accuracy: 0.4912146676852559
Test set distance: 1 Top 1 Accuracy: 0.5553857906799083
---- Testing model trained on sequence: [1] ----
Test set distance: 0.2 Top 1 Accuracy: 0.22307104660045837
Test set distance: 0.4 Top 1 Accuracy: 0.22459893048128343
Test set distance: 0.6 Top 1 Accuracy: 0.3521772345301757
Test set distance: 0.8 Top 1 Accuracy: 0.46829640947288004
Test set distance: 1 Top 1 Accuracy: 0.5423987776928954
Fold: 1
---- Testing model trained on sequence: [0.2, 0.4, 0.6, 0.8, 1] ----
Test set distance: 0.2 Top 1 Accuracy: 0.17494270435446907
Test set distance: 0.4 Top 1 Accuracy: 0.19938884644766997
Test set distance: 0.6 Top 1 Accuracy: 0.3705118411000764
Test set distance: 0.8 Top 1 Accuracy: 0.5103132161955691
Test set distance: 1 Top 1 Accuracy: 0.5485103132161956
---- Testing model trained on sequence: [0.4, 0.6, 0.8, 1] ----
Test set distance: 0.2 Top 1 Accuracy: 0.1894576012223071
Test set distance: 0.4 Top 1 Accuracy: 0.26814362108479756
Test set distance: 0.6 Top 1 Accuracy: 0.33231474407944994
Test set distance: 0.8 Top 1 Accuracy: 0.4216959511077158
Test set distance: 1 Top 1 Accuracy: 0.49961802902979374
---- Testing model trained on sequence: [0.6, 0.8, 1] ----
Test set distance: 0.2 Top 1 Accuracy: 0.12223071046600459
Test set distance: 0.4 Top 1 Accuracy: 0.18792971734148206
Test set distance: 0.6 Top 1 Accuracy: 0.36669213139801377
Test set distance: 0.8 Top 1 Accuracy: 0.4782276546982429
Test set distance: 1 Top 1 Accuracy: 0.5355233002291826
---- Testing model trained on sequence: [0.8, 1] ----
Test set distance: 0.2 Top 1 Accuracy: 0.17570664629488159
Test set distance: 0.4 Top 1 Accuracy: 0.26585179526355995
Test set distance: 0.6 Top 1 Accuracy: 0.4155844155844156
Test set distance: 0.8 Top 1 Accuracy: 0.5240641711229946
Test set distance: 1 Top 1 Accuracy: 0.5508021390374331
---- Testing model trained on sequence: [1] ----
Test set distance: 0.2 Top 1 Accuracy: 0.21390374331550802
Test set distance: 0.4 Top 1 Accuracy: 0.25362872421695953
Test set distance: 0.6 Top 1 Accuracy: 0.3552330022918258
Test set distance: 0.8 Top 1 Accuracy: 0.44461420932009166
Test set distance: 1 Top 1 Accuracy: 0.5194805194805194
Fold: 2
---- Testing model trained on sequence: [0.2, 0.4, 0.6, 0.8, 1] ----
Test set distance: 0.2 Top 1 Accuracy: 0.1650114591291062
Test set distance: 0.4 Top 1 Accuracy: 0.1970970206264324
Test set distance: 0.6 Top 1 Accuracy: 0.3430099312452254
Test set distance: 0.8 Top 1 Accuracy: 0.5210084033613446
Test set distance: 1 Top 1 Accuracy: 0.5622612681436211
---- Testing model trained on sequence: [0.4, 0.6, 0.8, 1] ----
Test set distance: 0.2 Top 1 Accuracy: 0.1650114591291062
Test set distance: 0.4 Top 1 Accuracy: 0.24216959511077157
Test set distance: 0.6 Top 1 Accuracy: 0.37203972498090143
Test set distance: 0.8 Top 1 Accuracy: 0.48892284186401835
Test set distance: 1 Top 1 Accuracy: 0.5148968678380443
---- Testing model trained on sequence: [0.6, 0.8, 1] ----
Test set distance: 0.2 Top 1 Accuracy: 0.19938884644766997
Test set distance: 0.4 Top 1 Accuracy: 0.33460656990068754
Test set distance: 0.6 Top 1 Accuracy: 0.4247517188693659
Test set distance: 0.8 Top 1 Accuracy: 0.4988540870893812
Test set distance: 1 Top 1 Accuracy: 0.5324675324675324
---- Testing model trained on sequence: [0.8, 1] ----
Test set distance: 0.2 Top 1 Accuracy: 0.23911382734912145
Test set distance: 0.4 Top 1 Accuracy: 0.3460656990068755
Test set distance: 0.6 Top 1 Accuracy: 0.43162719633307867
Test set distance: 0.8 Top 1 Accuracy: 0.48968678380443087
Test set distance: 1 Top 1 Accuracy: 0.5042016806722689
---- Testing model trained on sequence: [1] ----
Test set distance: 0.2 Top 1 Accuracy: 0.20168067226890757
Test set distance: 0.4 Top 1 Accuracy: 0.27807486631016043
Test set distance: 0.6 Top 1 Accuracy: 0.4270435446906035
Test set distance: 0.8 Top 1 Accuracy: 0.5164247517188694
Test set distance: 1 Top 1 Accuracy: 0.5530939648586708
Fold: 3
---- Testing model trained on sequence: [0.2, 0.4, 0.6, 0.8, 1] ----
Test set distance: 0.2 Top 1 Accuracy: 0.24598930481283424
Test set distance: 0.4 Top 1 Accuracy: 0.29488158899923606
Test set distance: 0.6 Top 1 Accuracy: 0.43162719633307867
Test set distance: 0.8 Top 1 Accuracy: 0.5225362872421696
Test set distance: 1 Top 1 Accuracy: 0.5454545454545454
---- Testing model trained on sequence: [0.4, 0.6, 0.8, 1] ----
Test set distance: 0.2 Top 1 Accuracy: 0.1917494270435447
Test set distance: 0.4 Top 1 Accuracy: 0.2734912146676853
Test set distance: 0.6 Top 1 Accuracy: 0.34912146676852557
Test set distance: 0.8 Top 1 Accuracy: 0.4369747899159664
Test set distance: 1 Top 1 Accuracy: 0.4881588999236058
---- Testing model trained on sequence: [0.6, 0.8, 1] ----
Test set distance: 0.2 Top 1 Accuracy: 0.21466768525592056
Test set distance: 0.4 Top 1 Accuracy: 0.2826585179526356
Test set distance: 0.6 Top 1 Accuracy: 0.4025974025974026
Test set distance: 0.8 Top 1 Accuracy: 0.5042016806722689
Test set distance: 1 Top 1 Accuracy: 0.5385790679908327
---- Testing model trained on sequence: [0.8, 1] ----
Test set distance: 0.2 Top 1 Accuracy: 0.22307104660045837
Test set distance: 0.4 Top 1 Accuracy: 0.2757830404889228
Test set distance: 0.6 Top 1 Accuracy: 0.3949579831932773
Test set distance: 0.8 Top 1 Accuracy: 0.5003819709702063
Test set distance: 1 Top 1 Accuracy: 0.5362872421695951
---- Testing model trained on sequence: [1] ----
Test set distance: 0.2 Top 1 Accuracy: 0.20855614973262032
Test set distance: 0.4 Top 1 Accuracy: 0.25668449197860965
Test set distance: 0.6 Top 1 Accuracy: 0.37662337662337664
Test set distance: 0.8 Top 1 Accuracy: 0.48357524828113063
Test set distance: 1 Top 1 Accuracy: 0.5294117647058824
Fold: 4
---- Testing model trained on sequence: [0.2, 0.4, 0.6, 0.8, 1] ----
Test set distance: 0.2 Top 1 Accuracy: 0.15126050420168066
Test set distance: 0.4 Top 1 Accuracy: 0.20320855614973263
Test set distance: 0.6 Top 1 Accuracy: 0.3292589763177998
Test set distance: 0.8 Top 1 Accuracy: 0.439266615737204
Test set distance: 1 Top 1 Accuracy: 0.5202444614209321
---- Testing model trained on sequence: [0.4, 0.6, 0.8, 1] ----
Test set distance: 0.2 Top 1 Accuracy: 0.14514896867838045
Test set distance: 0.4 Top 1 Accuracy: 0.2131398013750955
Test set distance: 0.6 Top 1 Accuracy: 0.3269671504965623
Test set distance: 0.8 Top 1 Accuracy: 0.4820473644003056
Test set distance: 1 Top 1 Accuracy: 0.5095492742551566
---- Testing model trained on sequence: [0.6, 0.8, 1] ----
Test set distance: 0.2 Top 1 Accuracy: 0.2047364400305577
Test set distance: 0.4 Top 1 Accuracy: 0.24293353705118412
Test set distance: 0.6 Top 1 Accuracy: 0.36210847975553856
Test set distance: 0.8 Top 1 Accuracy: 0.48433919022154315
Test set distance: 1 Top 1 Accuracy: 0.5286478227654698
---- Testing model trained on sequence: [0.8, 1] ----
Test set distance: 0.2 Top 1 Accuracy: 0.18869365928189458
Test set distance: 0.4 Top 1 Accuracy: 0.26508785332314744
Test set distance: 0.6 Top 1 Accuracy: 0.3819709702062643
Test set distance: 0.8 Top 1 Accuracy: 0.5034377387318564
Test set distance: 1 Top 1 Accuracy: 0.5477463712757831
---- Testing model trained on sequence: [1] ----
Test set distance: 0.2 Top 1 Accuracy: 0.15584415584415584
Test set distance: 0.4 Top 1 Accuracy: 0.24675324675324675
Test set distance: 0.6 Top 1 Accuracy: 0.3949579831932773
Test set distance: 0.8 Top 1 Accuracy: 0.5187165775401069
Test set distance: 1 Top 1 Accuracy: 0.5500381970970206
------------------------------ End ------------------------------








