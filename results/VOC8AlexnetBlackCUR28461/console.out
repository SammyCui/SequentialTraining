Loading openmpi/cuda/64/3.1.4
  Loading requirement: hpcx/2.4.0
Loading pytorch-py36-cuda10.1-gcc/1.5.0
  Loading requirement: python36 ml-pythondeps-py36-cuda10.1-gcc/3.3.0
    openblas/dynamic/0.2.20 cudnn7.6-cuda10.1/7.6.5.32 hdf5_18/1.8.20
    nccl2-cuda10.1-gcc/2.7.8
Run:  0
 # ------------------ Running pipeline on as_is color run_0 -------------------- #
cuda:0
 ------ Pipeline with following parameters ------
training_root_path :  /u/erdos/students/xcui32/cnslab/datasets/VOC2012/VOC2012_filtered/train
val_root_path :  /u/erdos/students/xcui32/cnslab/datasets/VOC2012/VOC2012_filtered/val
test_root_path :  /u/erdos/students/xcui32/cnslab/datasets/VOC2012/VOC2012_filtered/test
dataset_name :  VOC
target_distances :  [0.2, 0.8, 0.4, 0.6, 1]
training_mode :  as_is
n_distances :  None
training_size :  None
background :  color
size :  (150, 150)
cls_to_use :  ['aeroplane', 'bicycle', 'bird', 'boat', 'car', 'cat', 'train', 'tvmonitor']
batch_size :  128
val_size :  1
epochs :  400
resize_method :  long
n_folds :  5
num_workers :  16
model_name :  alexnet
device :  cuda:0
random_seed :  40
result_dirpath :  /u/erdos/students/xcui32/cnslab/results/VOC8AlexnetBlackCUR28461
save_checkpoints :  False
save_progress_checkpoints :  False
verbose :  0
 ---  Loading datasets ---
 ---  Running  ---
Parameters: --------------------
{'scheduler_kwargs': {'mode': 'min', 'factor': 0.1, 'patience': 5}, 'optim_kwargs': {'lr': 0.001, 'momentum': 0.9}, 'max_norm': None, 'val_target': 'current', 'patience': 30, 'early_stopping': True, 'scheduler_object': None, 'optimizer_object': <class 'torch.optim.sgd.SGD'>, 'criterion_object': <class 'torch.nn.modules.loss.CrossEntropyLoss'>, 'self': <pipelineCV2.RunModel object at 0x2aac786f7d68>}
--------------------
Fold: 0
----- Training alexnet with sequence: [0.2, 0.8, 0.4, 0.6, 1] -----
Current group: 0.2
Epoch [1/80], Training Loss: 41.5859, Validation Loss Current: 10.3803, Validation Loss AVG: 10.3803, lr: 0.001
Epoch [2/80], Training Loss: 41.4123, Validation Loss Current: 10.3455, Validation Loss AVG: 10.3455, lr: 0.001
Epoch [3/80], Training Loss: 41.2621, Validation Loss Current: 10.3005, Validation Loss AVG: 10.3005, lr: 0.001
Epoch [4/80], Training Loss: 41.1044, Validation Loss Current: 10.2515, Validation Loss AVG: 10.2515, lr: 0.001
Epoch [5/80], Training Loss: 40.8568, Validation Loss Current: 10.1878, Validation Loss AVG: 10.1878, lr: 0.001
Epoch [6/80], Training Loss: 40.4564, Validation Loss Current: 10.0985, Validation Loss AVG: 10.0985, lr: 0.001
Epoch [7/80], Training Loss: 39.4344, Validation Loss Current: 10.0252, Validation Loss AVG: 10.0252, lr: 0.001
Epoch [8/80], Training Loss: 40.3079, Validation Loss Current: 10.0405, Validation Loss AVG: 10.0405, lr: 0.001
Epoch [9/80], Training Loss: 40.1215, Validation Loss Current: 10.0544, Validation Loss AVG: 10.0544, lr: 0.001
Epoch [10/80], Training Loss: 40.0576, Validation Loss Current: 10.0247, Validation Loss AVG: 10.0247, lr: 0.001
Epoch [11/80], Training Loss: 39.8804, Validation Loss Current: 10.0251, Validation Loss AVG: 10.0251, lr: 0.001
Epoch [12/80], Training Loss: 40.0447, Validation Loss Current: 10.0243, Validation Loss AVG: 10.0243, lr: 0.001
Epoch [13/80], Training Loss: 40.3733, Validation Loss Current: 10.0509, Validation Loss AVG: 10.0509, lr: 0.001
Epoch [14/80], Training Loss: 40.0191, Validation Loss Current: 10.0461, Validation Loss AVG: 10.0461, lr: 0.001
Epoch [15/80], Training Loss: 40.4524, Validation Loss Current: 10.0448, Validation Loss AVG: 10.0448, lr: 0.001
Epoch [16/80], Training Loss: 40.1688, Validation Loss Current: 10.0571, Validation Loss AVG: 10.0571, lr: 0.001
Epoch [17/80], Training Loss: 39.7631, Validation Loss Current: 10.0290, Validation Loss AVG: 10.0290, lr: 0.001
Epoch [18/80], Training Loss: 40.0739, Validation Loss Current: 10.0300, Validation Loss AVG: 10.0300, lr: 0.001
Epoch [19/80], Training Loss: 39.8039, Validation Loss Current: 10.0428, Validation Loss AVG: 10.0428, lr: 0.001
Epoch [20/80], Training Loss: 39.6498, Validation Loss Current: 10.0334, Validation Loss AVG: 10.0334, lr: 0.001
Epoch [21/80], Training Loss: 40.0548, Validation Loss Current: 10.0345, Validation Loss AVG: 10.0345, lr: 0.001
Epoch [22/80], Training Loss: 39.7932, Validation Loss Current: 10.0429, Validation Loss AVG: 10.0429, lr: 0.001
Epoch [23/80], Training Loss: 40.3267, Validation Loss Current: 10.0390, Validation Loss AVG: 10.0390, lr: 0.001
Epoch [24/80], Training Loss: 39.9657, Validation Loss Current: 10.0567, Validation Loss AVG: 10.0567, lr: 0.001
Epoch [25/80], Training Loss: 40.3155, Validation Loss Current: 10.0369, Validation Loss AVG: 10.0369, lr: 0.001
Epoch [26/80], Training Loss: 39.7823, Validation Loss Current: 10.0562, Validation Loss AVG: 10.0562, lr: 0.001
Epoch [27/80], Training Loss: 40.6336, Validation Loss Current: 10.0453, Validation Loss AVG: 10.0453, lr: 0.001
Epoch [28/80], Training Loss: 39.8731, Validation Loss Current: 10.0830, Validation Loss AVG: 10.0830, lr: 0.001
Epoch [29/80], Training Loss: 40.0115, Validation Loss Current: 10.0101, Validation Loss AVG: 10.0101, lr: 0.001
Epoch [30/80], Training Loss: 39.9359, Validation Loss Current: 10.0455, Validation Loss AVG: 10.0455, lr: 0.001
Epoch [31/80], Training Loss: 39.9827, Validation Loss Current: 10.0449, Validation Loss AVG: 10.0449, lr: 0.001
Epoch [32/80], Training Loss: 39.9715, Validation Loss Current: 10.0533, Validation Loss AVG: 10.0533, lr: 0.001
Epoch [33/80], Training Loss: 39.7530, Validation Loss Current: 10.0424, Validation Loss AVG: 10.0424, lr: 0.001
Epoch [34/80], Training Loss: 39.9265, Validation Loss Current: 10.0267, Validation Loss AVG: 10.0267, lr: 0.001
Epoch [35/80], Training Loss: 39.9229, Validation Loss Current: 10.0252, Validation Loss AVG: 10.0252, lr: 0.001
Epoch [36/80], Training Loss: 39.7770, Validation Loss Current: 10.0363, Validation Loss AVG: 10.0363, lr: 0.001
Epoch [37/80], Training Loss: 40.2450, Validation Loss Current: 10.0553, Validation Loss AVG: 10.0553, lr: 0.001
Epoch [38/80], Training Loss: 40.1321, Validation Loss Current: 10.0519, Validation Loss AVG: 10.0519, lr: 0.001
Epoch [39/80], Training Loss: 40.0930, Validation Loss Current: 10.0262, Validation Loss AVG: 10.0262, lr: 0.001
Epoch [40/80], Training Loss: 39.5211, Validation Loss Current: 10.0347, Validation Loss AVG: 10.0347, lr: 0.001
Epoch [41/80], Training Loss: 39.5627, Validation Loss Current: 10.0335, Validation Loss AVG: 10.0335, lr: 0.001
Epoch [42/80], Training Loss: 39.5266, Validation Loss Current: 10.0152, Validation Loss AVG: 10.0152, lr: 0.001
Epoch [43/80], Training Loss: 39.7583, Validation Loss Current: 10.0546, Validation Loss AVG: 10.0546, lr: 0.001
Epoch [44/80], Training Loss: 39.9866, Validation Loss Current: 10.0201, Validation Loss AVG: 10.0201, lr: 0.001
Epoch [45/80], Training Loss: 39.0475, Validation Loss Current: 10.0208, Validation Loss AVG: 10.0208, lr: 0.001
Epoch [46/80], Training Loss: 39.9575, Validation Loss Current: 10.0885, Validation Loss AVG: 10.0885, lr: 0.001
Epoch [47/80], Training Loss: 39.8300, Validation Loss Current: 10.0134, Validation Loss AVG: 10.0134, lr: 0.001
Epoch [48/80], Training Loss: 39.4517, Validation Loss Current: 10.0024, Validation Loss AVG: 10.0024, lr: 0.001
Epoch [49/80], Training Loss: 39.5839, Validation Loss Current: 10.0478, Validation Loss AVG: 10.0478, lr: 0.001
Epoch [50/80], Training Loss: 40.0891, Validation Loss Current: 10.0641, Validation Loss AVG: 10.0641, lr: 0.001
Epoch [51/80], Training Loss: 39.8722, Validation Loss Current: 10.0240, Validation Loss AVG: 10.0240, lr: 0.001
Epoch [52/80], Training Loss: 39.7015, Validation Loss Current: 9.9999, Validation Loss AVG: 9.9999, lr: 0.001
Epoch [53/80], Training Loss: 38.9856, Validation Loss Current: 9.9878, Validation Loss AVG: 9.9878, lr: 0.001
Epoch [54/80], Training Loss: 39.9007, Validation Loss Current: 10.0411, Validation Loss AVG: 10.0411, lr: 0.001
Epoch [55/80], Training Loss: 39.3401, Validation Loss Current: 10.0075, Validation Loss AVG: 10.0075, lr: 0.001
Epoch [56/80], Training Loss: 39.4757, Validation Loss Current: 10.0583, Validation Loss AVG: 10.0583, lr: 0.001
Epoch [57/80], Training Loss: 39.5410, Validation Loss Current: 9.9992, Validation Loss AVG: 9.9992, lr: 0.001
Epoch [58/80], Training Loss: 39.6320, Validation Loss Current: 10.0220, Validation Loss AVG: 10.0220, lr: 0.001
Epoch [59/80], Training Loss: 39.1523, Validation Loss Current: 9.9769, Validation Loss AVG: 9.9769, lr: 0.001
Epoch [60/80], Training Loss: 39.4906, Validation Loss Current: 10.0085, Validation Loss AVG: 10.0085, lr: 0.001
Epoch [61/80], Training Loss: 39.7776, Validation Loss Current: 10.0347, Validation Loss AVG: 10.0347, lr: 0.001
Epoch [62/80], Training Loss: 39.2638, Validation Loss Current: 9.9257, Validation Loss AVG: 9.9257, lr: 0.001
Epoch [63/80], Training Loss: 39.1224, Validation Loss Current: 9.9545, Validation Loss AVG: 9.9545, lr: 0.001
Epoch [64/80], Training Loss: 39.4054, Validation Loss Current: 9.9743, Validation Loss AVG: 9.9743, lr: 0.001
Epoch [65/80], Training Loss: 38.5978, Validation Loss Current: 9.9351, Validation Loss AVG: 9.9351, lr: 0.001
Epoch [66/80], Training Loss: 38.4370, Validation Loss Current: 9.8792, Validation Loss AVG: 9.8792, lr: 0.001
Epoch [67/80], Training Loss: 39.3122, Validation Loss Current: 9.8784, Validation Loss AVG: 9.8784, lr: 0.001
Epoch [68/80], Training Loss: 39.4592, Validation Loss Current: 9.9196, Validation Loss AVG: 9.9196, lr: 0.001
Epoch [69/80], Training Loss: 39.2755, Validation Loss Current: 9.8851, Validation Loss AVG: 9.8851, lr: 0.001
Epoch [70/80], Training Loss: 38.7132, Validation Loss Current: 9.8192, Validation Loss AVG: 9.8192, lr: 0.001
Epoch [71/80], Training Loss: 38.7560, Validation Loss Current: 9.9476, Validation Loss AVG: 9.9476, lr: 0.001
Epoch [72/80], Training Loss: 38.4992, Validation Loss Current: 9.9334, Validation Loss AVG: 9.9334, lr: 0.001
Epoch [73/80], Training Loss: 38.5862, Validation Loss Current: 9.9031, Validation Loss AVG: 9.9031, lr: 0.001
Epoch [74/80], Training Loss: 38.5777, Validation Loss Current: 9.8421, Validation Loss AVG: 9.8421, lr: 0.001
Epoch [75/80], Training Loss: 38.4422, Validation Loss Current: 9.8032, Validation Loss AVG: 9.8032, lr: 0.001
Epoch [76/80], Training Loss: 37.7796, Validation Loss Current: 9.7698, Validation Loss AVG: 9.7698, lr: 0.001
Epoch [77/80], Training Loss: 38.3484, Validation Loss Current: 9.7287, Validation Loss AVG: 9.7287, lr: 0.001
Epoch [78/80], Training Loss: 38.0388, Validation Loss Current: 9.8147, Validation Loss AVG: 9.8147, lr: 0.001
Epoch [79/80], Training Loss: 37.6859, Validation Loss Current: 9.7446, Validation Loss AVG: 9.7446, lr: 0.001
Epoch [80/80], Training Loss: 38.2278, Validation Loss Current: 9.7126, Validation Loss AVG: 9.7126, lr: 0.001
Patch distance: 0.2 finished training. Best epoch: 80 Best val accuracy: [0.11611842105263157, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22532894736842107, 0.22598684210526315, 0.23651315789473681, 0.25953947368421054, 0.27401315789473685, 0.28125, 0.28651315789473686, 0.2848684210526316, 0.2667763157894737, 0.2782894736842106, 0.28157894736842104] Best val loss: 9.712583923339844


Current group: 0.8
Epoch [1/80], Training Loss: 38.5954, Validation Loss Current: 9.6171, Validation Loss AVG: 9.6171, lr: 0.001
Epoch [2/80], Training Loss: 38.4229, Validation Loss Current: 9.5719, Validation Loss AVG: 9.5719, lr: 0.001
Epoch [3/80], Training Loss: 38.0152, Validation Loss Current: 9.5030, Validation Loss AVG: 9.5030, lr: 0.001
Epoch [4/80], Training Loss: 37.5204, Validation Loss Current: 9.5151, Validation Loss AVG: 9.5151, lr: 0.001
Epoch [5/80], Training Loss: 37.1977, Validation Loss Current: 9.4872, Validation Loss AVG: 9.4872, lr: 0.001
Epoch [6/80], Training Loss: 37.0496, Validation Loss Current: 9.5704, Validation Loss AVG: 9.5704, lr: 0.001
Epoch [7/80], Training Loss: 36.7352, Validation Loss Current: 9.4991, Validation Loss AVG: 9.4991, lr: 0.001
Epoch [8/80], Training Loss: 36.6953, Validation Loss Current: 9.3540, Validation Loss AVG: 9.3540, lr: 0.001
Epoch [9/80], Training Loss: 36.4790, Validation Loss Current: 9.2972, Validation Loss AVG: 9.2972, lr: 0.001
Epoch [10/80], Training Loss: 36.9408, Validation Loss Current: 9.5849, Validation Loss AVG: 9.5849, lr: 0.001
Epoch [11/80], Training Loss: 41.1471, Validation Loss Current: 10.3592, Validation Loss AVG: 10.3592, lr: 0.001
Epoch [12/80], Training Loss: 41.0589, Validation Loss Current: 10.2975, Validation Loss AVG: 10.2975, lr: 0.001
Epoch [13/80], Training Loss: 40.6596, Validation Loss Current: 10.2139, Validation Loss AVG: 10.2139, lr: 0.001
Epoch [14/80], Training Loss: 40.4492, Validation Loss Current: 10.1751, Validation Loss AVG: 10.1751, lr: 0.001
Epoch [15/80], Training Loss: 40.6053, Validation Loss Current: 10.1604, Validation Loss AVG: 10.1604, lr: 0.001
Epoch [16/80], Training Loss: 40.2126, Validation Loss Current: 10.1186, Validation Loss AVG: 10.1186, lr: 0.001
Epoch [17/80], Training Loss: 40.2766, Validation Loss Current: 10.0976, Validation Loss AVG: 10.0976, lr: 0.001
Epoch [18/80], Training Loss: 40.0014, Validation Loss Current: 10.0711, Validation Loss AVG: 10.0711, lr: 0.001
Epoch [19/80], Training Loss: 39.8950, Validation Loss Current: 10.0373, Validation Loss AVG: 10.0373, lr: 0.001
Epoch [20/80], Training Loss: 39.9440, Validation Loss Current: 9.9925, Validation Loss AVG: 9.9925, lr: 0.001
Epoch [21/80], Training Loss: 40.0094, Validation Loss Current: 9.9339, Validation Loss AVG: 9.9339, lr: 0.001
Epoch [22/80], Training Loss: 39.1113, Validation Loss Current: 9.8563, Validation Loss AVG: 9.8563, lr: 0.001
Epoch [23/80], Training Loss: 39.5483, Validation Loss Current: 9.7916, Validation Loss AVG: 9.7916, lr: 0.001
Epoch [24/80], Training Loss: 38.8569, Validation Loss Current: 9.7305, Validation Loss AVG: 9.7305, lr: 0.001
Epoch [25/80], Training Loss: 38.4650, Validation Loss Current: 9.6120, Validation Loss AVG: 9.6120, lr: 0.001
Epoch [26/80], Training Loss: 37.9737, Validation Loss Current: 9.5233, Validation Loss AVG: 9.5233, lr: 0.001
Epoch [27/80], Training Loss: 37.3227, Validation Loss Current: 9.4412, Validation Loss AVG: 9.4412, lr: 0.001
Epoch [28/80], Training Loss: 36.6962, Validation Loss Current: 9.6203, Validation Loss AVG: 9.6203, lr: 0.001
Epoch [29/80], Training Loss: 37.5088, Validation Loss Current: 9.2418, Validation Loss AVG: 9.2418, lr: 0.001
Epoch [30/80], Training Loss: 36.2844, Validation Loss Current: 9.1537, Validation Loss AVG: 9.1537, lr: 0.001
Epoch [31/80], Training Loss: 35.8124, Validation Loss Current: 9.3868, Validation Loss AVG: 9.3868, lr: 0.001
Epoch [32/80], Training Loss: 35.2231, Validation Loss Current: 9.9143, Validation Loss AVG: 9.9143, lr: 0.001
Epoch [33/80], Training Loss: 35.7372, Validation Loss Current: 9.0393, Validation Loss AVG: 9.0393, lr: 0.001
Epoch [34/80], Training Loss: 34.5538, Validation Loss Current: 8.9344, Validation Loss AVG: 8.9344, lr: 0.001
Epoch [35/80], Training Loss: 33.1415, Validation Loss Current: 8.9969, Validation Loss AVG: 8.9969, lr: 0.001
Epoch [36/80], Training Loss: 33.4742, Validation Loss Current: 8.9550, Validation Loss AVG: 8.9550, lr: 0.001
Epoch [37/80], Training Loss: 33.1375, Validation Loss Current: 9.8301, Validation Loss AVG: 9.8301, lr: 0.001
Epoch [38/80], Training Loss: 34.6083, Validation Loss Current: 8.9334, Validation Loss AVG: 8.9334, lr: 0.001
Epoch [39/80], Training Loss: 33.2400, Validation Loss Current: 9.4130, Validation Loss AVG: 9.4130, lr: 0.001
Epoch [40/80], Training Loss: 34.4414, Validation Loss Current: 9.2534, Validation Loss AVG: 9.2534, lr: 0.001
Epoch [41/80], Training Loss: 33.6692, Validation Loss Current: 8.7885, Validation Loss AVG: 8.7885, lr: 0.001
Epoch [42/80], Training Loss: 32.5435, Validation Loss Current: 8.7668, Validation Loss AVG: 8.7668, lr: 0.001
Epoch [43/80], Training Loss: 31.9045, Validation Loss Current: 8.6878, Validation Loss AVG: 8.6878, lr: 0.001
Epoch [44/80], Training Loss: 32.3335, Validation Loss Current: 8.7200, Validation Loss AVG: 8.7200, lr: 0.001
Epoch [45/80], Training Loss: 32.8402, Validation Loss Current: 8.6927, Validation Loss AVG: 8.6927, lr: 0.001
Epoch [46/80], Training Loss: 32.7791, Validation Loss Current: 9.9668, Validation Loss AVG: 9.9668, lr: 0.001
Epoch [47/80], Training Loss: 32.6883, Validation Loss Current: 8.5674, Validation Loss AVG: 8.5674, lr: 0.001
Epoch [48/80], Training Loss: 30.4342, Validation Loss Current: 8.6335, Validation Loss AVG: 8.6335, lr: 0.001
Epoch [49/80], Training Loss: 30.9162, Validation Loss Current: 8.5845, Validation Loss AVG: 8.5845, lr: 0.001
Epoch [50/80], Training Loss: 30.0179, Validation Loss Current: 8.6759, Validation Loss AVG: 8.6759, lr: 0.001
Epoch [51/80], Training Loss: 31.7531, Validation Loss Current: 8.5099, Validation Loss AVG: 8.5099, lr: 0.001
Epoch [52/80], Training Loss: 29.1752, Validation Loss Current: 8.7280, Validation Loss AVG: 8.7280, lr: 0.001
Epoch [53/80], Training Loss: 30.4239, Validation Loss Current: 8.5885, Validation Loss AVG: 8.5885, lr: 0.001
Epoch [54/80], Training Loss: 31.0379, Validation Loss Current: 8.7656, Validation Loss AVG: 8.7656, lr: 0.001
Epoch [55/80], Training Loss: 29.2977, Validation Loss Current: 9.6242, Validation Loss AVG: 9.6242, lr: 0.001
Epoch [56/80], Training Loss: 29.3431, Validation Loss Current: 9.2069, Validation Loss AVG: 9.2069, lr: 0.001
Epoch [57/80], Training Loss: 29.6730, Validation Loss Current: 9.3149, Validation Loss AVG: 9.3149, lr: 0.001
Epoch [58/80], Training Loss: 30.2719, Validation Loss Current: 8.3640, Validation Loss AVG: 8.3640, lr: 0.001
Epoch [59/80], Training Loss: 29.9230, Validation Loss Current: 8.1826, Validation Loss AVG: 8.1826, lr: 0.001
Epoch [60/80], Training Loss: 29.4228, Validation Loss Current: 8.9008, Validation Loss AVG: 8.9008, lr: 0.001
Epoch [61/80], Training Loss: 27.7422, Validation Loss Current: 8.6323, Validation Loss AVG: 8.6323, lr: 0.001
Epoch [62/80], Training Loss: 30.8816, Validation Loss Current: 10.8434, Validation Loss AVG: 10.8434, lr: 0.001
Epoch [63/80], Training Loss: 32.5298, Validation Loss Current: 8.8543, Validation Loss AVG: 8.8543, lr: 0.001
Epoch [64/80], Training Loss: 29.1348, Validation Loss Current: 8.3957, Validation Loss AVG: 8.3957, lr: 0.001
Epoch [65/80], Training Loss: 26.9361, Validation Loss Current: 9.1883, Validation Loss AVG: 9.1883, lr: 0.001
Epoch [66/80], Training Loss: 27.9538, Validation Loss Current: 8.6776, Validation Loss AVG: 8.6776, lr: 0.001
Epoch [67/80], Training Loss: 27.2398, Validation Loss Current: 8.7643, Validation Loss AVG: 8.7643, lr: 0.001
Epoch [68/80], Training Loss: 26.0509, Validation Loss Current: 9.3493, Validation Loss AVG: 9.3493, lr: 0.001
Epoch [69/80], Training Loss: 26.1686, Validation Loss Current: 8.9078, Validation Loss AVG: 8.9078, lr: 0.001
Epoch [70/80], Training Loss: 26.2408, Validation Loss Current: 8.4328, Validation Loss AVG: 8.4328, lr: 0.001
Epoch [71/80], Training Loss: 27.1229, Validation Loss Current: 10.0428, Validation Loss AVG: 10.0428, lr: 0.001
Epoch [72/80], Training Loss: 27.3906, Validation Loss Current: 8.5473, Validation Loss AVG: 8.5473, lr: 0.001
Epoch [73/80], Training Loss: 26.4739, Validation Loss Current: 8.9507, Validation Loss AVG: 8.9507, lr: 0.001
Epoch [74/80], Training Loss: 26.1548, Validation Loss Current: 9.2835, Validation Loss AVG: 9.2835, lr: 0.001
Epoch [75/80], Training Loss: 25.2288, Validation Loss Current: 9.5222, Validation Loss AVG: 9.5222, lr: 0.001
Epoch [76/80], Training Loss: 25.4355, Validation Loss Current: 9.0867, Validation Loss AVG: 9.0867, lr: 0.001
Epoch [77/80], Training Loss: 26.0767, Validation Loss Current: 9.3412, Validation Loss AVG: 9.3412, lr: 0.001
Epoch [78/80], Training Loss: 28.5966, Validation Loss Current: 8.3665, Validation Loss AVG: 8.3665, lr: 0.001
Epoch [79/80], Training Loss: 26.0338, Validation Loss Current: 8.8418, Validation Loss AVG: 8.8418, lr: 0.001
Epoch [80/80], Training Loss: 24.3583, Validation Loss Current: 8.7838, Validation Loss AVG: 8.7838, lr: 0.001
Patch distance: 0.8 finished training. Best epoch: 59 Best val accuracy: [0.29276315789473684, 0.28125, 0.28881578947368425, 0.28717105263157894, 0.2884868421052632, 0.2825657894736842, 0.2875, 0.2914473684210526, 0.2967105263157895, 0.27532894736842106, 0.12006578947368421, 0.12006578947368421, 0.12006578947368421, 0.12006578947368421, 0.11875, 0.19802631578947366, 0.22269736842105264, 0.23881578947368426, 0.25296052631578947, 0.2549342105263158, 0.24342105263157893, 0.24144736842105266, 0.26710526315789473, 0.2769736842105263, 0.28125, 0.27763157894736845, 0.29078947368421054, 0.2694078947368421, 0.30625, 0.30723684210526314, 0.2950657894736842, 0.29210526315789476, 0.3111842105263158, 0.3292763157894737, 0.34078947368421053, 0.3263157894736842, 0.2822368421052631, 0.3381578947368421, 0.2888157894736842, 0.3358552631578947, 0.3486842105263158, 0.33980263157894736, 0.35789473684210527, 0.3513157894736842, 0.3486842105263158, 0.2578947368421053, 0.36973684210526314, 0.35296052631578945, 0.36578947368421055, 0.36348684210526316, 0.3648026315789473, 0.3585526315789474, 0.3625, 0.3447368421052632, 0.3059210526315789, 0.33322368421052634, 0.33322368421052634, 0.3703947368421053, 0.40394736842105267, 0.3453947368421053, 0.37565789473684214, 0.2930921052631579, 0.34638157894736843, 0.4042763157894737, 0.36710526315789477, 0.3782894736842105, 0.3894736842105263, 0.3703947368421053, 0.38355263157894737, 0.40394736842105267, 0.3391447368421053, 0.38684210526315793, 0.38651315789473684, 0.3805921052631579, 0.37203947368421053, 0.39276315789473687, 0.3848684210526315, 0.380921052631579, 0.3917763157894737, 0.4134868421052632] Best val loss: 8.182631015777588


Current group: 0.4
Epoch [1/80], Training Loss: 32.0803, Validation Loss Current: 9.3732, Validation Loss AVG: 9.3732, lr: 0.001
Epoch [2/80], Training Loss: 33.3694, Validation Loss Current: 8.1979, Validation Loss AVG: 8.1979, lr: 0.001
Epoch [3/80], Training Loss: 30.7250, Validation Loss Current: 7.8847, Validation Loss AVG: 7.8847, lr: 0.001
Epoch [4/80], Training Loss: 30.4625, Validation Loss Current: 8.1426, Validation Loss AVG: 8.1426, lr: 0.001
Epoch [5/80], Training Loss: 29.6220, Validation Loss Current: 7.7840, Validation Loss AVG: 7.7840, lr: 0.001
Epoch [6/80], Training Loss: 30.3667, Validation Loss Current: 9.0622, Validation Loss AVG: 9.0622, lr: 0.001
Epoch [7/80], Training Loss: 30.9118, Validation Loss Current: 8.2766, Validation Loss AVG: 8.2766, lr: 0.001
Epoch [8/80], Training Loss: 31.1679, Validation Loss Current: 7.8277, Validation Loss AVG: 7.8277, lr: 0.001
Epoch [9/80], Training Loss: 29.0995, Validation Loss Current: 8.4125, Validation Loss AVG: 8.4125, lr: 0.001
Epoch [10/80], Training Loss: 29.2226, Validation Loss Current: 7.8929, Validation Loss AVG: 7.8929, lr: 0.001
Epoch [11/80], Training Loss: 28.7990, Validation Loss Current: 8.0772, Validation Loss AVG: 8.0772, lr: 0.001
Epoch [12/80], Training Loss: 27.9505, Validation Loss Current: 8.4225, Validation Loss AVG: 8.4225, lr: 0.001
Epoch [13/80], Training Loss: 29.5754, Validation Loss Current: 8.2294, Validation Loss AVG: 8.2294, lr: 0.001
Epoch [14/80], Training Loss: 28.8587, Validation Loss Current: 7.8536, Validation Loss AVG: 7.8536, lr: 0.001
Epoch [15/80], Training Loss: 27.6073, Validation Loss Current: 8.8496, Validation Loss AVG: 8.8496, lr: 0.001
Epoch [16/80], Training Loss: 27.1134, Validation Loss Current: 8.2647, Validation Loss AVG: 8.2647, lr: 0.001
Epoch [17/80], Training Loss: 26.7482, Validation Loss Current: 8.5636, Validation Loss AVG: 8.5636, lr: 0.001
Epoch [18/80], Training Loss: 27.2488, Validation Loss Current: 7.9971, Validation Loss AVG: 7.9971, lr: 0.001
Epoch [19/80], Training Loss: 26.7242, Validation Loss Current: 7.9799, Validation Loss AVG: 7.9799, lr: 0.001
Epoch [20/80], Training Loss: 26.6557, Validation Loss Current: 8.2745, Validation Loss AVG: 8.2745, lr: 0.001
Epoch [21/80], Training Loss: 27.2866, Validation Loss Current: 9.3165, Validation Loss AVG: 9.3165, lr: 0.001
Epoch [22/80], Training Loss: 30.3067, Validation Loss Current: 8.1013, Validation Loss AVG: 8.1013, lr: 0.001
Epoch [23/80], Training Loss: 25.8719, Validation Loss Current: 8.2984, Validation Loss AVG: 8.2984, lr: 0.001
Epoch [24/80], Training Loss: 26.2697, Validation Loss Current: 8.3526, Validation Loss AVG: 8.3526, lr: 0.001
Epoch [25/80], Training Loss: 25.9988, Validation Loss Current: 10.4636, Validation Loss AVG: 10.4636, lr: 0.001
Epoch [26/80], Training Loss: 32.6267, Validation Loss Current: 8.5038, Validation Loss AVG: 8.5038, lr: 0.001
Epoch [27/80], Training Loss: 29.2918, Validation Loss Current: 8.1476, Validation Loss AVG: 8.1476, lr: 0.001
Epoch [28/80], Training Loss: 27.8090, Validation Loss Current: 8.2443, Validation Loss AVG: 8.2443, lr: 0.001
Epoch [29/80], Training Loss: 24.8118, Validation Loss Current: 8.4337, Validation Loss AVG: 8.4337, lr: 0.001
Epoch [30/80], Training Loss: 26.4100, Validation Loss Current: 8.3474, Validation Loss AVG: 8.3474, lr: 0.001
Epoch [31/80], Training Loss: 27.1980, Validation Loss Current: 7.7903, Validation Loss AVG: 7.7903, lr: 0.001
Epoch [32/80], Training Loss: 24.1578, Validation Loss Current: 8.3823, Validation Loss AVG: 8.3823, lr: 0.001
Epoch [33/80], Training Loss: 24.0031, Validation Loss Current: 8.2683, Validation Loss AVG: 8.2683, lr: 0.001
Epoch [34/80], Training Loss: 24.5592, Validation Loss Current: 8.1010, Validation Loss AVG: 8.1010, lr: 0.001
Epoch [35/80], Training Loss: 25.3228, Validation Loss Current: 9.0787, Validation Loss AVG: 9.0787, lr: 0.001
 --- Early Stopped ---
Patch distance: 0.4 finished training. Best epoch: 5 Best val accuracy: [0.3575657894736842, 0.4134868421052632, 0.43618421052631573, 0.42302631578947364, 0.4516447368421053, 0.3871710526315789, 0.4161184210526316, 0.46085526315789477, 0.40888157894736843, 0.4447368421052632, 0.4493421052631579, 0.42631578947368426, 0.4338815789473684, 0.45493421052631583, 0.39473684210526316, 0.42302631578947364, 0.41776315789473684, 0.46940789473684214, 0.4476973684210527, 0.43157894736842106, 0.3536184210526316, 0.42796052631578946, 0.4569078947368421, 0.4322368421052632, 0.32532894736842105, 0.3740131578947368, 0.43125, 0.43157894736842106, 0.4203947368421053, 0.4286184210526316, 0.4776315789473685, 0.4394736842105263, 0.44506578947368425, 0.47138157894736843, 0.38355263157894737] Best val loss: 7.784046602249146


Current group: 0.6
Epoch [1/80], Training Loss: 25.1608, Validation Loss Current: 7.8650, Validation Loss AVG: 7.8650, lr: 0.001
Epoch [2/80], Training Loss: 25.9262, Validation Loss Current: 9.2764, Validation Loss AVG: 9.2764, lr: 0.001
Epoch [3/80], Training Loss: 27.9902, Validation Loss Current: 8.0677, Validation Loss AVG: 8.0677, lr: 0.001
Epoch [4/80], Training Loss: 24.6277, Validation Loss Current: 7.8265, Validation Loss AVG: 7.8265, lr: 0.001
Epoch [5/80], Training Loss: 22.9656, Validation Loss Current: 8.2518, Validation Loss AVG: 8.2518, lr: 0.001
Epoch [6/80], Training Loss: 23.3666, Validation Loss Current: 8.3113, Validation Loss AVG: 8.3113, lr: 0.001
Epoch [7/80], Training Loss: 23.6736, Validation Loss Current: 8.1685, Validation Loss AVG: 8.1685, lr: 0.001
Epoch [8/80], Training Loss: 22.5113, Validation Loss Current: 8.2809, Validation Loss AVG: 8.2809, lr: 0.001
Epoch [9/80], Training Loss: 22.5158, Validation Loss Current: 8.1199, Validation Loss AVG: 8.1199, lr: 0.001
Epoch [10/80], Training Loss: 22.6211, Validation Loss Current: 8.6472, Validation Loss AVG: 8.6472, lr: 0.001
Epoch [11/80], Training Loss: 23.3809, Validation Loss Current: 7.9494, Validation Loss AVG: 7.9494, lr: 0.001
Epoch [12/80], Training Loss: 22.6018, Validation Loss Current: 8.2187, Validation Loss AVG: 8.2187, lr: 0.001
Epoch [13/80], Training Loss: 21.1079, Validation Loss Current: 8.6052, Validation Loss AVG: 8.6052, lr: 0.001
Epoch [14/80], Training Loss: 22.2933, Validation Loss Current: 7.9123, Validation Loss AVG: 7.9123, lr: 0.001
Epoch [15/80], Training Loss: 22.3997, Validation Loss Current: 8.4487, Validation Loss AVG: 8.4487, lr: 0.001
Epoch [16/80], Training Loss: 20.4458, Validation Loss Current: 8.2793, Validation Loss AVG: 8.2793, lr: 0.001
Epoch [17/80], Training Loss: 19.7131, Validation Loss Current: 8.0603, Validation Loss AVG: 8.0603, lr: 0.001
Epoch [18/80], Training Loss: 21.7927, Validation Loss Current: 8.9635, Validation Loss AVG: 8.9635, lr: 0.001
Epoch [19/80], Training Loss: 20.3004, Validation Loss Current: 7.8877, Validation Loss AVG: 7.8877, lr: 0.001
Epoch [20/80], Training Loss: 21.6096, Validation Loss Current: 8.4322, Validation Loss AVG: 8.4322, lr: 0.001
Epoch [21/80], Training Loss: 21.7321, Validation Loss Current: 8.0596, Validation Loss AVG: 8.0596, lr: 0.001
Epoch [22/80], Training Loss: 19.8700, Validation Loss Current: 8.7172, Validation Loss AVG: 8.7172, lr: 0.001
Epoch [23/80], Training Loss: 19.8999, Validation Loss Current: 7.7755, Validation Loss AVG: 7.7755, lr: 0.001
Epoch [24/80], Training Loss: 19.3772, Validation Loss Current: 8.6526, Validation Loss AVG: 8.6526, lr: 0.001
Epoch [25/80], Training Loss: 18.6623, Validation Loss Current: 8.1307, Validation Loss AVG: 8.1307, lr: 0.001
Epoch [26/80], Training Loss: 17.0492, Validation Loss Current: 8.4154, Validation Loss AVG: 8.4154, lr: 0.001
Epoch [27/80], Training Loss: 17.6161, Validation Loss Current: 9.6873, Validation Loss AVG: 9.6873, lr: 0.001
Epoch [28/80], Training Loss: 22.8720, Validation Loss Current: 8.5545, Validation Loss AVG: 8.5545, lr: 0.001
Epoch [29/80], Training Loss: 25.3134, Validation Loss Current: 7.7515, Validation Loss AVG: 7.7515, lr: 0.001
Epoch [30/80], Training Loss: 19.3076, Validation Loss Current: 7.9902, Validation Loss AVG: 7.9902, lr: 0.001
Epoch [31/80], Training Loss: 17.7695, Validation Loss Current: 8.0922, Validation Loss AVG: 8.0922, lr: 0.001
Epoch [32/80], Training Loss: 16.9821, Validation Loss Current: 8.3970, Validation Loss AVG: 8.3970, lr: 0.001
Epoch [33/80], Training Loss: 16.0266, Validation Loss Current: 8.8631, Validation Loss AVG: 8.8631, lr: 0.001
Epoch [34/80], Training Loss: 17.4276, Validation Loss Current: 8.1541, Validation Loss AVG: 8.1541, lr: 0.001
Epoch [35/80], Training Loss: 16.4529, Validation Loss Current: 8.7876, Validation Loss AVG: 8.7876, lr: 0.001
Epoch [36/80], Training Loss: 18.0252, Validation Loss Current: 8.2912, Validation Loss AVG: 8.2912, lr: 0.001
Epoch [37/80], Training Loss: 14.8772, Validation Loss Current: 9.8079, Validation Loss AVG: 9.8079, lr: 0.001
Epoch [38/80], Training Loss: 14.8659, Validation Loss Current: 8.7350, Validation Loss AVG: 8.7350, lr: 0.001
Epoch [39/80], Training Loss: 14.8350, Validation Loss Current: 9.9006, Validation Loss AVG: 9.9006, lr: 0.001
Epoch [40/80], Training Loss: 15.1827, Validation Loss Current: 9.2003, Validation Loss AVG: 9.2003, lr: 0.001
Epoch [41/80], Training Loss: 18.4077, Validation Loss Current: 10.2176, Validation Loss AVG: 10.2176, lr: 0.001
Epoch [42/80], Training Loss: 17.0635, Validation Loss Current: 9.0494, Validation Loss AVG: 9.0494, lr: 0.001
Epoch [43/80], Training Loss: 16.1070, Validation Loss Current: 10.5118, Validation Loss AVG: 10.5118, lr: 0.001
Epoch [44/80], Training Loss: 16.6458, Validation Loss Current: 9.9413, Validation Loss AVG: 9.9413, lr: 0.001
Epoch [45/80], Training Loss: 15.7207, Validation Loss Current: 9.3401, Validation Loss AVG: 9.3401, lr: 0.001
Epoch [46/80], Training Loss: 14.0669, Validation Loss Current: 10.5838, Validation Loss AVG: 10.5838, lr: 0.001
Epoch [47/80], Training Loss: 14.2639, Validation Loss Current: 10.1169, Validation Loss AVG: 10.1169, lr: 0.001
Epoch [48/80], Training Loss: 13.6196, Validation Loss Current: 9.4746, Validation Loss AVG: 9.4746, lr: 0.001
Epoch [49/80], Training Loss: 13.9982, Validation Loss Current: 9.1673, Validation Loss AVG: 9.1673, lr: 0.001
Epoch [50/80], Training Loss: 12.4956, Validation Loss Current: 11.0828, Validation Loss AVG: 11.0828, lr: 0.001
Epoch [51/80], Training Loss: 14.6838, Validation Loss Current: 10.2961, Validation Loss AVG: 10.2961, lr: 0.001
Epoch [52/80], Training Loss: 14.3043, Validation Loss Current: 29.4031, Validation Loss AVG: 29.4031, lr: 0.001
Epoch [53/80], Training Loss: 41.5905, Validation Loss Current: 8.9096, Validation Loss AVG: 8.9096, lr: 0.001
Epoch [54/80], Training Loss: 27.6031, Validation Loss Current: 8.8544, Validation Loss AVG: 8.8544, lr: 0.001
Epoch [55/80], Training Loss: 22.5175, Validation Loss Current: 8.3283, Validation Loss AVG: 8.3283, lr: 0.001
Epoch [56/80], Training Loss: 22.4036, Validation Loss Current: 10.1899, Validation Loss AVG: 10.1899, lr: 0.001
Epoch [57/80], Training Loss: 24.1436, Validation Loss Current: 8.4858, Validation Loss AVG: 8.4858, lr: 0.001
Epoch [58/80], Training Loss: 21.2267, Validation Loss Current: 8.6861, Validation Loss AVG: 8.6861, lr: 0.001
Epoch [59/80], Training Loss: 17.8496, Validation Loss Current: 8.9838, Validation Loss AVG: 8.9838, lr: 0.001
 --- Early Stopped ---
Patch distance: 0.6 finished training. Best epoch: 29 Best val accuracy: [0.4638157894736842, 0.40032894736842106, 0.43322368421052637, 0.48256578947368417, 0.4444078947368421, 0.45493421052631583, 0.43947368421052635, 0.45789473684210524, 0.46611842105263157, 0.4407894736842105, 0.47006578947368427, 0.4661184210526317, 0.4473684210526316, 0.4615131578947368, 0.43815789473684214, 0.45921052631578946, 0.4921052631578947, 0.4480263157894737, 0.5006578947368421, 0.4463815789473684, 0.4516447368421053, 0.44605263157894737, 0.5029605263157896, 0.46118421052631586, 0.4858552631578947, 0.4894736842105263, 0.4569078947368421, 0.47171052631578947, 0.4766447368421053, 0.4858552631578947, 0.4983552631578947, 0.4861842105263158, 0.48717105263157895, 0.47631578947368425, 0.4792763157894736, 0.48717105263157895, 0.4667763157894737, 0.5026315789473685, 0.45427631578947364, 0.47796052631578945, 0.43717105263157896, 0.5082236842105263, 0.41907894736842105, 0.45723684210526316, 0.4634868421052632, 0.4618421052631579, 0.46611842105263157, 0.4720394736842105, 0.5042763157894736, 0.4578947368421053, 0.45756578947368426, 0.318421052631579, 0.38618421052631585, 0.4082236842105263, 0.46480263157894736, 0.3848684210526315, 0.4480263157894737, 0.4427631578947368, 0.45296052631578954] Best val loss: 7.751481604576111


Current group: 1
Epoch [1/80], Training Loss: 25.9682, Validation Loss Current: 7.0905, Validation Loss AVG: 8.4807, lr: 0.001
Epoch [2/80], Training Loss: 25.4935, Validation Loss Current: 7.4392, Validation Loss AVG: 8.9893, lr: 0.001
Epoch [3/80], Training Loss: 26.9971, Validation Loss Current: 6.6136, Validation Loss AVG: 7.7116, lr: 0.001
Epoch [4/80], Training Loss: 23.7245, Validation Loss Current: 6.9260, Validation Loss AVG: 8.6823, lr: 0.001
Epoch [5/80], Training Loss: 25.4549, Validation Loss Current: 6.4527, Validation Loss AVG: 8.0588, lr: 0.001
Epoch [6/80], Training Loss: 24.2813, Validation Loss Current: 7.1698, Validation Loss AVG: 7.8383, lr: 0.001
Epoch [7/80], Training Loss: 24.2696, Validation Loss Current: 6.7925, Validation Loss AVG: 8.5873, lr: 0.001
Epoch [8/80], Training Loss: 21.7687, Validation Loss Current: 6.1653, Validation Loss AVG: 7.9696, lr: 0.001
Epoch [9/80], Training Loss: 20.4362, Validation Loss Current: 7.8718, Validation Loss AVG: 10.9361, lr: 0.001
Epoch [10/80], Training Loss: 23.6523, Validation Loss Current: 6.4287, Validation Loss AVG: 8.0446, lr: 0.001
Epoch [11/80], Training Loss: 21.5508, Validation Loss Current: 6.2400, Validation Loss AVG: 8.8579, lr: 0.001
Epoch [12/80], Training Loss: 19.8126, Validation Loss Current: 6.3229, Validation Loss AVG: 8.6805, lr: 0.001
Epoch [13/80], Training Loss: 19.2577, Validation Loss Current: 6.2753, Validation Loss AVG: 9.1932, lr: 0.001
Epoch [14/80], Training Loss: 18.3224, Validation Loss Current: 5.9541, Validation Loss AVG: 8.6739, lr: 0.001
Epoch [15/80], Training Loss: 17.2042, Validation Loss Current: 5.9949, Validation Loss AVG: 8.7594, lr: 0.001
Epoch [16/80], Training Loss: 16.5799, Validation Loss Current: 6.1513, Validation Loss AVG: 9.6703, lr: 0.001
Epoch [17/80], Training Loss: 16.2350, Validation Loss Current: 6.4152, Validation Loss AVG: 9.1704, lr: 0.001
Epoch [18/80], Training Loss: 15.9659, Validation Loss Current: 6.0846, Validation Loss AVG: 9.6764, lr: 0.001
Epoch [19/80], Training Loss: 16.0219, Validation Loss Current: 5.9230, Validation Loss AVG: 9.7276, lr: 0.001
Epoch [20/80], Training Loss: 15.4625, Validation Loss Current: 7.9043, Validation Loss AVG: 12.0550, lr: 0.001
Epoch [21/80], Training Loss: 18.5600, Validation Loss Current: 6.3570, Validation Loss AVG: 9.0554, lr: 0.001
Epoch [22/80], Training Loss: 16.0717, Validation Loss Current: 7.3363, Validation Loss AVG: 9.9834, lr: 0.001
Epoch [23/80], Training Loss: 21.2885, Validation Loss Current: 5.7162, Validation Loss AVG: 9.1213, lr: 0.001
Epoch [24/80], Training Loss: 17.2348, Validation Loss Current: 6.7986, Validation Loss AVG: 9.4345, lr: 0.001
Epoch [25/80], Training Loss: 16.4436, Validation Loss Current: 6.4557, Validation Loss AVG: 10.5057, lr: 0.001
Epoch [26/80], Training Loss: 16.2439, Validation Loss Current: 6.2115, Validation Loss AVG: 9.4735, lr: 0.001
Epoch [27/80], Training Loss: 14.0829, Validation Loss Current: 6.2665, Validation Loss AVG: 10.0613, lr: 0.001
Epoch [28/80], Training Loss: 12.3870, Validation Loss Current: 6.5611, Validation Loss AVG: 10.6920, lr: 0.001
Epoch [29/80], Training Loss: 11.5884, Validation Loss Current: 6.4067, Validation Loss AVG: 10.9108, lr: 0.001
Epoch [30/80], Training Loss: 12.5769, Validation Loss Current: 7.4962, Validation Loss AVG: 13.7308, lr: 0.001
Epoch [31/80], Training Loss: 18.0317, Validation Loss Current: 6.5814, Validation Loss AVG: 10.2749, lr: 0.001
Epoch [32/80], Training Loss: 13.8566, Validation Loss Current: 7.9966, Validation Loss AVG: 14.2592, lr: 0.001
Epoch [33/80], Training Loss: 17.6368, Validation Loss Current: 6.1699, Validation Loss AVG: 8.7878, lr: 0.001
Epoch [34/80], Training Loss: 13.6243, Validation Loss Current: 7.0257, Validation Loss AVG: 11.1719, lr: 0.001
Epoch [35/80], Training Loss: 12.6289, Validation Loss Current: 6.5619, Validation Loss AVG: 10.6718, lr: 0.001
Epoch [36/80], Training Loss: 10.8643, Validation Loss Current: 7.2252, Validation Loss AVG: 11.3281, lr: 0.001
Epoch [37/80], Training Loss: 14.6450, Validation Loss Current: 6.5858, Validation Loss AVG: 9.1008, lr: 0.001
Epoch [38/80], Training Loss: 12.4809, Validation Loss Current: 6.8919, Validation Loss AVG: 10.3345, lr: 0.001
Epoch [39/80], Training Loss: 13.3956, Validation Loss Current: 6.7169, Validation Loss AVG: 10.4187, lr: 0.001
Epoch [40/80], Training Loss: 10.8897, Validation Loss Current: 7.1130, Validation Loss AVG: 11.7331, lr: 0.001
Epoch [41/80], Training Loss: 13.3356, Validation Loss Current: 6.8830, Validation Loss AVG: 9.6303, lr: 0.001
Epoch [42/80], Training Loss: 15.4884, Validation Loss Current: 6.5273, Validation Loss AVG: 10.0211, lr: 0.001
Epoch [43/80], Training Loss: 11.9979, Validation Loss Current: 7.1036, Validation Loss AVG: 12.1633, lr: 0.001
Epoch [44/80], Training Loss: 9.7179, Validation Loss Current: 7.2124, Validation Loss AVG: 11.1830, lr: 0.001
Epoch [45/80], Training Loss: 12.4843, Validation Loss Current: 11.1583, Validation Loss AVG: 17.9915, lr: 0.001
Epoch [46/80], Training Loss: 23.1645, Validation Loss Current: 6.7983, Validation Loss AVG: 8.6198, lr: 0.001
Epoch [47/80], Training Loss: 14.9721, Validation Loss Current: 7.7663, Validation Loss AVG: 10.4036, lr: 0.001
Epoch [48/80], Training Loss: 13.2682, Validation Loss Current: 7.1070, Validation Loss AVG: 11.1879, lr: 0.001
Epoch [49/80], Training Loss: 13.5716, Validation Loss Current: 7.2036, Validation Loss AVG: 10.7058, lr: 0.001
Epoch [50/80], Training Loss: 12.9723, Validation Loss Current: 6.7710, Validation Loss AVG: 10.6935, lr: 0.001
Epoch [51/80], Training Loss: 9.1723, Validation Loss Current: 7.2234, Validation Loss AVG: 11.6432, lr: 0.001
Epoch [52/80], Training Loss: 8.1746, Validation Loss Current: 7.3862, Validation Loss AVG: 12.3373, lr: 0.001
Epoch [53/80], Training Loss: 8.1183, Validation Loss Current: 7.5220, Validation Loss AVG: 12.1417, lr: 0.001
 --- Early Stopped ---
Patch distance: 1 finished training. Best epoch: 23 Best val accuracy: [0.5263157894736842, 0.49835526315789475, 0.5509868421052632, 0.53125, 0.555921052631579, 0.524671052631579, 0.5394736842105263, 0.5740131578947368, 0.5049342105263158, 0.5592105263157895, 0.5921052631578947, 0.5855263157894737, 0.5904605263157895, 0.6085526315789473, 0.6151315789473685, 0.600328947368421, 0.5986842105263158, 0.6052631578947368, 0.6200657894736842, 0.5230263157894737, 0.5855263157894737, 0.5756578947368421, 0.6200657894736842, 0.600328947368421, 0.6217105263157895, 0.5970394736842105, 0.6019736842105263, 0.618421052631579, 0.6200657894736842, 0.5427631578947368, 0.5970394736842105, 0.5542763157894737, 0.618421052631579, 0.6118421052631579, 0.6398026315789473, 0.5756578947368421, 0.6151315789473685, 0.5888157894736842, 0.6101973684210527, 0.6299342105263158, 0.6052631578947368, 0.5970394736842105, 0.6151315789473685, 0.6299342105263158, 0.4605263157894737, 0.5838815789473685, 0.5789473684210527, 0.569078947368421, 0.5805921052631579, 0.6085526315789473, 0.631578947368421, 0.6414473684210527, 0.6299342105263158] Best val loss: 5.716205537319183


Fold: 1
----- Training alexnet with sequence: [0.2, 0.8, 0.4, 0.6, 1] -----
Current group: 0.2
Epoch [1/80], Training Loss: 41.5848, Validation Loss Current: 10.3782, Validation Loss AVG: 10.3782, lr: 0.001
Epoch [2/80], Training Loss: 41.4275, Validation Loss Current: 10.3416, Validation Loss AVG: 10.3416, lr: 0.001
Epoch [3/80], Training Loss: 41.2888, Validation Loss Current: 10.2939, Validation Loss AVG: 10.2939, lr: 0.001
Epoch [4/80], Training Loss: 41.1508, Validation Loss Current: 10.2430, Validation Loss AVG: 10.2430, lr: 0.001
Epoch [5/80], Training Loss: 40.8752, Validation Loss Current: 10.1749, Validation Loss AVG: 10.1749, lr: 0.001
Epoch [6/80], Training Loss: 40.2949, Validation Loss Current: 10.0554, Validation Loss AVG: 10.0554, lr: 0.001
Epoch [7/80], Training Loss: 40.5649, Validation Loss Current: 9.9852, Validation Loss AVG: 9.9852, lr: 0.001
Epoch [8/80], Training Loss: 40.1904, Validation Loss Current: 10.0258, Validation Loss AVG: 10.0258, lr: 0.001
Epoch [9/80], Training Loss: 40.3650, Validation Loss Current: 10.0134, Validation Loss AVG: 10.0134, lr: 0.001
Epoch [10/80], Training Loss: 40.0800, Validation Loss Current: 9.9995, Validation Loss AVG: 9.9995, lr: 0.001
Epoch [11/80], Training Loss: 40.0351, Validation Loss Current: 10.0115, Validation Loss AVG: 10.0115, lr: 0.001
Epoch [12/80], Training Loss: 40.2487, Validation Loss Current: 10.0047, Validation Loss AVG: 10.0047, lr: 0.001
Epoch [13/80], Training Loss: 40.0803, Validation Loss Current: 10.0226, Validation Loss AVG: 10.0226, lr: 0.001
Epoch [14/80], Training Loss: 40.3456, Validation Loss Current: 10.0009, Validation Loss AVG: 10.0009, lr: 0.001
Epoch [15/80], Training Loss: 39.9977, Validation Loss Current: 10.0026, Validation Loss AVG: 10.0026, lr: 0.001
Epoch [16/80], Training Loss: 39.8856, Validation Loss Current: 9.9882, Validation Loss AVG: 9.9882, lr: 0.001
Epoch [17/80], Training Loss: 40.0264, Validation Loss Current: 9.9814, Validation Loss AVG: 9.9814, lr: 0.001
Epoch [18/80], Training Loss: 39.5310, Validation Loss Current: 9.9982, Validation Loss AVG: 9.9982, lr: 0.001
Epoch [19/80], Training Loss: 39.7607, Validation Loss Current: 9.9905, Validation Loss AVG: 9.9905, lr: 0.001
Epoch [20/80], Training Loss: 39.7427, Validation Loss Current: 9.9879, Validation Loss AVG: 9.9879, lr: 0.001
Epoch [21/80], Training Loss: 40.0554, Validation Loss Current: 10.0030, Validation Loss AVG: 10.0030, lr: 0.001
Epoch [22/80], Training Loss: 39.9987, Validation Loss Current: 9.9979, Validation Loss AVG: 9.9979, lr: 0.001
Epoch [23/80], Training Loss: 39.8973, Validation Loss Current: 10.0052, Validation Loss AVG: 10.0052, lr: 0.001
Epoch [24/80], Training Loss: 39.7802, Validation Loss Current: 10.0026, Validation Loss AVG: 10.0026, lr: 0.001
Epoch [25/80], Training Loss: 39.7554, Validation Loss Current: 10.0024, Validation Loss AVG: 10.0024, lr: 0.001
Epoch [26/80], Training Loss: 40.4711, Validation Loss Current: 10.0117, Validation Loss AVG: 10.0117, lr: 0.001
Epoch [27/80], Training Loss: 40.1495, Validation Loss Current: 10.0496, Validation Loss AVG: 10.0496, lr: 0.001
Epoch [28/80], Training Loss: 39.7247, Validation Loss Current: 9.9891, Validation Loss AVG: 9.9891, lr: 0.001
Epoch [29/80], Training Loss: 39.6129, Validation Loss Current: 9.9734, Validation Loss AVG: 9.9734, lr: 0.001
Epoch [30/80], Training Loss: 39.7639, Validation Loss Current: 9.9951, Validation Loss AVG: 9.9951, lr: 0.001
Epoch [31/80], Training Loss: 39.7044, Validation Loss Current: 9.9947, Validation Loss AVG: 9.9947, lr: 0.001
Epoch [32/80], Training Loss: 39.7047, Validation Loss Current: 10.0115, Validation Loss AVG: 10.0115, lr: 0.001
Epoch [33/80], Training Loss: 39.6828, Validation Loss Current: 10.0105, Validation Loss AVG: 10.0105, lr: 0.001
Epoch [34/80], Training Loss: 40.2828, Validation Loss Current: 10.0291, Validation Loss AVG: 10.0291, lr: 0.001
Epoch [35/80], Training Loss: 40.3325, Validation Loss Current: 10.0446, Validation Loss AVG: 10.0446, lr: 0.001
Epoch [36/80], Training Loss: 39.6842, Validation Loss Current: 10.0084, Validation Loss AVG: 10.0084, lr: 0.001
Epoch [37/80], Training Loss: 40.1643, Validation Loss Current: 10.0354, Validation Loss AVG: 10.0354, lr: 0.001
Epoch [38/80], Training Loss: 39.7322, Validation Loss Current: 10.0209, Validation Loss AVG: 10.0209, lr: 0.001
Epoch [39/80], Training Loss: 39.9898, Validation Loss Current: 10.0254, Validation Loss AVG: 10.0254, lr: 0.001
Epoch [40/80], Training Loss: 40.0739, Validation Loss Current: 10.0183, Validation Loss AVG: 10.0183, lr: 0.001
Epoch [41/80], Training Loss: 39.6733, Validation Loss Current: 10.0093, Validation Loss AVG: 10.0093, lr: 0.001
Epoch [42/80], Training Loss: 39.9657, Validation Loss Current: 10.0118, Validation Loss AVG: 10.0118, lr: 0.001
Epoch [43/80], Training Loss: 40.3391, Validation Loss Current: 10.0285, Validation Loss AVG: 10.0285, lr: 0.001
Epoch [44/80], Training Loss: 39.8639, Validation Loss Current: 10.0224, Validation Loss AVG: 10.0224, lr: 0.001
Epoch [45/80], Training Loss: 39.6336, Validation Loss Current: 10.0033, Validation Loss AVG: 10.0033, lr: 0.001
Epoch [46/80], Training Loss: 40.1835, Validation Loss Current: 10.0195, Validation Loss AVG: 10.0195, lr: 0.001
Epoch [47/80], Training Loss: 39.8851, Validation Loss Current: 10.0094, Validation Loss AVG: 10.0094, lr: 0.001
Epoch [48/80], Training Loss: 39.6835, Validation Loss Current: 9.9995, Validation Loss AVG: 9.9995, lr: 0.001
Epoch [49/80], Training Loss: 39.8907, Validation Loss Current: 10.0330, Validation Loss AVG: 10.0330, lr: 0.001
Epoch [50/80], Training Loss: 39.8185, Validation Loss Current: 10.0085, Validation Loss AVG: 10.0085, lr: 0.001
Epoch [51/80], Training Loss: 39.6663, Validation Loss Current: 10.0357, Validation Loss AVG: 10.0357, lr: 0.001
Epoch [52/80], Training Loss: 40.0355, Validation Loss Current: 10.0141, Validation Loss AVG: 10.0141, lr: 0.001
Epoch [53/80], Training Loss: 39.9700, Validation Loss Current: 10.0478, Validation Loss AVG: 10.0478, lr: 0.001
Epoch [54/80], Training Loss: 39.4003, Validation Loss Current: 9.9973, Validation Loss AVG: 9.9973, lr: 0.001
Epoch [55/80], Training Loss: 39.6381, Validation Loss Current: 10.0668, Validation Loss AVG: 10.0668, lr: 0.001
Epoch [56/80], Training Loss: 39.4251, Validation Loss Current: 10.0362, Validation Loss AVG: 10.0362, lr: 0.001
Epoch [57/80], Training Loss: 40.0279, Validation Loss Current: 10.0761, Validation Loss AVG: 10.0761, lr: 0.001
Epoch [58/80], Training Loss: 40.0676, Validation Loss Current: 10.0400, Validation Loss AVG: 10.0400, lr: 0.001
Epoch [59/80], Training Loss: 39.6931, Validation Loss Current: 9.9752, Validation Loss AVG: 9.9752, lr: 0.001
 --- Early Stopped ---
Patch distance: 0.2 finished training. Best epoch: 29 Best val accuracy: [0.24473684210526314, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842] Best val loss: 9.973405528068543


Current group: 0.8
Epoch [1/80], Training Loss: 40.0482, Validation Loss Current: 10.0305, Validation Loss AVG: 10.0305, lr: 0.001
Epoch [2/80], Training Loss: 39.0838, Validation Loss Current: 9.9135, Validation Loss AVG: 9.9135, lr: 0.001
Epoch [3/80], Training Loss: 39.3087, Validation Loss Current: 9.9276, Validation Loss AVG: 9.9276, lr: 0.001
Epoch [4/80], Training Loss: 39.0104, Validation Loss Current: 9.8987, Validation Loss AVG: 9.8987, lr: 0.001
Epoch [5/80], Training Loss: 39.2931, Validation Loss Current: 9.8388, Validation Loss AVG: 9.8388, lr: 0.001
Epoch [6/80], Training Loss: 39.4529, Validation Loss Current: 9.7753, Validation Loss AVG: 9.7753, lr: 0.001
Epoch [7/80], Training Loss: 39.3238, Validation Loss Current: 9.7950, Validation Loss AVG: 9.7950, lr: 0.001
Epoch [8/80], Training Loss: 38.8303, Validation Loss Current: 9.8151, Validation Loss AVG: 9.8151, lr: 0.001
Epoch [9/80], Training Loss: 38.6138, Validation Loss Current: 9.6965, Validation Loss AVG: 9.6965, lr: 0.001
Epoch [10/80], Training Loss: 37.7697, Validation Loss Current: 9.7297, Validation Loss AVG: 9.7297, lr: 0.001
Epoch [11/80], Training Loss: 38.1455, Validation Loss Current: 9.8777, Validation Loss AVG: 9.8777, lr: 0.001
Epoch [12/80], Training Loss: 37.6493, Validation Loss Current: 9.8796, Validation Loss AVG: 9.8796, lr: 0.001
Epoch [13/80], Training Loss: 37.4848, Validation Loss Current: 9.8387, Validation Loss AVG: 9.8387, lr: 0.001
Epoch [14/80], Training Loss: 38.0493, Validation Loss Current: 9.6844, Validation Loss AVG: 9.6844, lr: 0.001
Epoch [15/80], Training Loss: 37.8140, Validation Loss Current: 9.5059, Validation Loss AVG: 9.5059, lr: 0.001
Epoch [16/80], Training Loss: 38.1610, Validation Loss Current: 9.4868, Validation Loss AVG: 9.4868, lr: 0.001
Epoch [17/80], Training Loss: 36.5785, Validation Loss Current: 9.7511, Validation Loss AVG: 9.7511, lr: 0.001
Epoch [18/80], Training Loss: 37.8786, Validation Loss Current: 9.5711, Validation Loss AVG: 9.5711, lr: 0.001
Epoch [19/80], Training Loss: 37.4335, Validation Loss Current: 9.3827, Validation Loss AVG: 9.3827, lr: 0.001
Epoch [20/80], Training Loss: 37.3422, Validation Loss Current: 9.5225, Validation Loss AVG: 9.5225, lr: 0.001
Epoch [21/80], Training Loss: 36.7520, Validation Loss Current: 9.3249, Validation Loss AVG: 9.3249, lr: 0.001
Epoch [22/80], Training Loss: 34.8388, Validation Loss Current: 9.3951, Validation Loss AVG: 9.3951, lr: 0.001
Epoch [23/80], Training Loss: 36.3392, Validation Loss Current: 9.2895, Validation Loss AVG: 9.2895, lr: 0.001
Epoch [24/80], Training Loss: 36.0038, Validation Loss Current: 9.4403, Validation Loss AVG: 9.4403, lr: 0.001
Epoch [25/80], Training Loss: 36.1840, Validation Loss Current: 9.5858, Validation Loss AVG: 9.5858, lr: 0.001
Epoch [26/80], Training Loss: 36.0514, Validation Loss Current: 10.2365, Validation Loss AVG: 10.2365, lr: 0.001
Epoch [27/80], Training Loss: 36.4670, Validation Loss Current: 9.3631, Validation Loss AVG: 9.3631, lr: 0.001
Epoch [28/80], Training Loss: 37.3095, Validation Loss Current: 9.2158, Validation Loss AVG: 9.2158, lr: 0.001
Epoch [29/80], Training Loss: 36.2420, Validation Loss Current: 9.4959, Validation Loss AVG: 9.4959, lr: 0.001
Epoch [30/80], Training Loss: 36.4483, Validation Loss Current: 9.3395, Validation Loss AVG: 9.3395, lr: 0.001
Epoch [31/80], Training Loss: 36.7948, Validation Loss Current: 9.3660, Validation Loss AVG: 9.3660, lr: 0.001
Epoch [32/80], Training Loss: 37.8412, Validation Loss Current: 9.5768, Validation Loss AVG: 9.5768, lr: 0.001
Epoch [33/80], Training Loss: 36.2495, Validation Loss Current: 9.0013, Validation Loss AVG: 9.0013, lr: 0.001
Epoch [34/80], Training Loss: 35.2716, Validation Loss Current: 8.8964, Validation Loss AVG: 8.8964, lr: 0.001
Epoch [35/80], Training Loss: 33.7829, Validation Loss Current: 9.7375, Validation Loss AVG: 9.7375, lr: 0.001
Epoch [36/80], Training Loss: 36.3195, Validation Loss Current: 10.3623, Validation Loss AVG: 10.3623, lr: 0.001
Epoch [37/80], Training Loss: 41.2223, Validation Loss Current: 10.9253, Validation Loss AVG: 10.9253, lr: 0.001
Epoch [38/80], Training Loss: 39.5025, Validation Loss Current: 10.1814, Validation Loss AVG: 10.1814, lr: 0.001
Epoch [39/80], Training Loss: 39.1025, Validation Loss Current: 10.1820, Validation Loss AVG: 10.1820, lr: 0.001
Epoch [40/80], Training Loss: 38.3034, Validation Loss Current: 10.3501, Validation Loss AVG: 10.3501, lr: 0.001
Epoch [41/80], Training Loss: 37.7763, Validation Loss Current: 9.7050, Validation Loss AVG: 9.7050, lr: 0.001
Epoch [42/80], Training Loss: 36.9605, Validation Loss Current: 9.4251, Validation Loss AVG: 9.4251, lr: 0.001
Epoch [43/80], Training Loss: 35.8121, Validation Loss Current: 9.2202, Validation Loss AVG: 9.2202, lr: 0.001
Epoch [44/80], Training Loss: 35.0542, Validation Loss Current: 9.5838, Validation Loss AVG: 9.5838, lr: 0.001
Epoch [45/80], Training Loss: 35.3226, Validation Loss Current: 8.8597, Validation Loss AVG: 8.8597, lr: 0.001
Epoch [46/80], Training Loss: 34.0556, Validation Loss Current: 8.8323, Validation Loss AVG: 8.8323, lr: 0.001
Epoch [47/80], Training Loss: 33.2786, Validation Loss Current: 9.5087, Validation Loss AVG: 9.5087, lr: 0.001
Epoch [48/80], Training Loss: 33.0202, Validation Loss Current: 9.0981, Validation Loss AVG: 9.0981, lr: 0.001
Epoch [49/80], Training Loss: 33.2623, Validation Loss Current: 9.5244, Validation Loss AVG: 9.5244, lr: 0.001
Epoch [50/80], Training Loss: 33.3212, Validation Loss Current: 8.6316, Validation Loss AVG: 8.6316, lr: 0.001
Epoch [51/80], Training Loss: 31.3990, Validation Loss Current: 8.4598, Validation Loss AVG: 8.4598, lr: 0.001
Epoch [52/80], Training Loss: 32.1330, Validation Loss Current: 8.4960, Validation Loss AVG: 8.4960, lr: 0.001
Epoch [53/80], Training Loss: 31.5756, Validation Loss Current: 8.4699, Validation Loss AVG: 8.4699, lr: 0.001
Epoch [54/80], Training Loss: 30.2919, Validation Loss Current: 9.3460, Validation Loss AVG: 9.3460, lr: 0.001
Epoch [55/80], Training Loss: 31.2223, Validation Loss Current: 8.5642, Validation Loss AVG: 8.5642, lr: 0.001
Epoch [56/80], Training Loss: 29.6642, Validation Loss Current: 8.4905, Validation Loss AVG: 8.4905, lr: 0.001
Epoch [57/80], Training Loss: 29.3565, Validation Loss Current: 9.3195, Validation Loss AVG: 9.3195, lr: 0.001
Epoch [58/80], Training Loss: 29.8229, Validation Loss Current: 8.3429, Validation Loss AVG: 8.3429, lr: 0.001
Epoch [59/80], Training Loss: 29.5489, Validation Loss Current: 8.2988, Validation Loss AVG: 8.2988, lr: 0.001
Epoch [60/80], Training Loss: 30.4063, Validation Loss Current: 9.6601, Validation Loss AVG: 9.6601, lr: 0.001
Epoch [61/80], Training Loss: 30.9756, Validation Loss Current: 8.5288, Validation Loss AVG: 8.5288, lr: 0.001
Epoch [62/80], Training Loss: 29.3332, Validation Loss Current: 8.4328, Validation Loss AVG: 8.4328, lr: 0.001
Epoch [63/80], Training Loss: 29.1258, Validation Loss Current: 8.3938, Validation Loss AVG: 8.3938, lr: 0.001
Epoch [64/80], Training Loss: 28.6792, Validation Loss Current: 8.2161, Validation Loss AVG: 8.2161, lr: 0.001
Epoch [65/80], Training Loss: 29.3178, Validation Loss Current: 9.3784, Validation Loss AVG: 9.3784, lr: 0.001
Epoch [66/80], Training Loss: 30.5152, Validation Loss Current: 9.7573, Validation Loss AVG: 9.7573, lr: 0.001
Epoch [67/80], Training Loss: 31.4453, Validation Loss Current: 8.1705, Validation Loss AVG: 8.1705, lr: 0.001
Epoch [68/80], Training Loss: 28.9497, Validation Loss Current: 8.1018, Validation Loss AVG: 8.1018, lr: 0.001
Epoch [69/80], Training Loss: 28.4404, Validation Loss Current: 8.7142, Validation Loss AVG: 8.7142, lr: 0.001
Epoch [70/80], Training Loss: 28.3462, Validation Loss Current: 8.7946, Validation Loss AVG: 8.7946, lr: 0.001
Epoch [71/80], Training Loss: 29.3869, Validation Loss Current: 7.9692, Validation Loss AVG: 7.9692, lr: 0.001
Epoch [72/80], Training Loss: 27.5324, Validation Loss Current: 9.6824, Validation Loss AVG: 9.6824, lr: 0.001
Epoch [73/80], Training Loss: 28.7086, Validation Loss Current: 8.3264, Validation Loss AVG: 8.3264, lr: 0.001
Epoch [74/80], Training Loss: 29.0142, Validation Loss Current: 10.2359, Validation Loss AVG: 10.2359, lr: 0.001
Epoch [75/80], Training Loss: 33.1635, Validation Loss Current: 8.9677, Validation Loss AVG: 8.9677, lr: 0.001
Epoch [76/80], Training Loss: 26.9021, Validation Loss Current: 7.9158, Validation Loss AVG: 7.9158, lr: 0.001
Epoch [77/80], Training Loss: 25.6145, Validation Loss Current: 8.1526, Validation Loss AVG: 8.1526, lr: 0.001
Epoch [78/80], Training Loss: 26.7425, Validation Loss Current: 8.3860, Validation Loss AVG: 8.3860, lr: 0.001
Epoch [79/80], Training Loss: 28.2430, Validation Loss Current: 8.1714, Validation Loss AVG: 8.1714, lr: 0.001
Epoch [80/80], Training Loss: 26.8665, Validation Loss Current: 8.3232, Validation Loss AVG: 8.3232, lr: 0.001
Patch distance: 0.8 finished training. Best epoch: 76 Best val accuracy: [0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.2450657894736842, 0.26085526315789476, 0.2694078947368421, 0.2802631578947368, 0.31217105263157896, 0.3046052631578947, 0.29243421052631585, 0.29934210526315785, 0.31315789473684214, 0.3019736842105263, 0.3161184210526316, 0.31414473684210525, 0.31776315789473686, 0.318421052631579, 0.31217105263157896, 0.2960526315789474, 0.3253289473684211, 0.3299342105263158, 0.31776315789473686, 0.33026315789473687, 0.330921052631579, 0.31677631578947374, 0.35098684210526315, 0.3703947368421052, 0.3332236842105263, 0.23717105263157895, 0.2473684210526316, 0.25427631578947374, 0.2720394736842105, 0.2901315789473684, 0.3069078947368421, 0.31644736842105264, 0.3177631578947368, 0.32664473684210527, 0.3529605263157895, 0.33848684210526314, 0.3335526315789473, 0.31644736842105264, 0.3417763157894737, 0.35296052631578945, 0.3736842105263158, 0.37269736842105267, 0.3773026315789474, 0.3421052631578948, 0.3805921052631579, 0.41381578947368425, 0.38651315789473684, 0.4213815789473684, 0.42467105263157895, 0.32368421052631574, 0.40328947368421053, 0.4098684210526316, 0.3921052631578948, 0.4217105263157895, 0.3733552631578948, 0.3167763157894737, 0.4125, 0.4338815789473684, 0.39276315789473687, 0.3851973684210527, 0.44243421052631576, 0.3536184210526316, 0.4046052631578947, 0.34375, 0.3414473684210526, 0.44572368421052627, 0.43717105263157896, 0.43980263157894733, 0.4230263157894737, 0.41875] Best val loss: 7.915760183334351


Current group: 0.4
Epoch [1/80], Training Loss: 33.9424, Validation Loss Current: 7.7682, Validation Loss AVG: 7.7682, lr: 0.001
Epoch [2/80], Training Loss: 31.4103, Validation Loss Current: 7.6339, Validation Loss AVG: 7.6339, lr: 0.001
Epoch [3/80], Training Loss: 29.7838, Validation Loss Current: 8.6842, Validation Loss AVG: 8.6842, lr: 0.001
Epoch [4/80], Training Loss: 32.3785, Validation Loss Current: 8.1905, Validation Loss AVG: 8.1905, lr: 0.001
Epoch [5/80], Training Loss: 32.3574, Validation Loss Current: 8.1025, Validation Loss AVG: 8.1025, lr: 0.001
Epoch [6/80], Training Loss: 32.3386, Validation Loss Current: 7.5934, Validation Loss AVG: 7.5934, lr: 0.001
Epoch [7/80], Training Loss: 28.8604, Validation Loss Current: 7.5082, Validation Loss AVG: 7.5082, lr: 0.001
Epoch [8/80], Training Loss: 29.3006, Validation Loss Current: 7.6886, Validation Loss AVG: 7.6886, lr: 0.001
Epoch [9/80], Training Loss: 31.2461, Validation Loss Current: 7.5477, Validation Loss AVG: 7.5477, lr: 0.001
Epoch [10/80], Training Loss: 29.0558, Validation Loss Current: 7.7400, Validation Loss AVG: 7.7400, lr: 0.001
Epoch [11/80], Training Loss: 28.5652, Validation Loss Current: 7.4984, Validation Loss AVG: 7.4984, lr: 0.001
Epoch [12/80], Training Loss: 27.4476, Validation Loss Current: 7.5582, Validation Loss AVG: 7.5582, lr: 0.001
Epoch [13/80], Training Loss: 28.6484, Validation Loss Current: 8.0023, Validation Loss AVG: 8.0023, lr: 0.001
Epoch [14/80], Training Loss: 29.7872, Validation Loss Current: 7.6436, Validation Loss AVG: 7.6436, lr: 0.001
Epoch [15/80], Training Loss: 28.9794, Validation Loss Current: 7.6842, Validation Loss AVG: 7.6842, lr: 0.001
Epoch [16/80], Training Loss: 29.9071, Validation Loss Current: 7.9972, Validation Loss AVG: 7.9972, lr: 0.001
Epoch [17/80], Training Loss: 29.7639, Validation Loss Current: 7.6746, Validation Loss AVG: 7.6746, lr: 0.001
Epoch [18/80], Training Loss: 28.3732, Validation Loss Current: 7.5011, Validation Loss AVG: 7.5011, lr: 0.001
Epoch [19/80], Training Loss: 27.2275, Validation Loss Current: 8.0047, Validation Loss AVG: 8.0047, lr: 0.001
Epoch [20/80], Training Loss: 28.0718, Validation Loss Current: 7.7061, Validation Loss AVG: 7.7061, lr: 0.001
Epoch [21/80], Training Loss: 28.1445, Validation Loss Current: 7.9138, Validation Loss AVG: 7.9138, lr: 0.001
Epoch [22/80], Training Loss: 26.1050, Validation Loss Current: 7.6157, Validation Loss AVG: 7.6157, lr: 0.001
Epoch [23/80], Training Loss: 25.6647, Validation Loss Current: 7.9260, Validation Loss AVG: 7.9260, lr: 0.001
Epoch [24/80], Training Loss: 27.3093, Validation Loss Current: 7.5890, Validation Loss AVG: 7.5890, lr: 0.001
Epoch [25/80], Training Loss: 27.1886, Validation Loss Current: 7.8880, Validation Loss AVG: 7.8880, lr: 0.001
Epoch [26/80], Training Loss: 27.4431, Validation Loss Current: 7.6904, Validation Loss AVG: 7.6904, lr: 0.001
Epoch [27/80], Training Loss: 25.8461, Validation Loss Current: 7.5378, Validation Loss AVG: 7.5378, lr: 0.001
Epoch [28/80], Training Loss: 26.1030, Validation Loss Current: 8.1408, Validation Loss AVG: 8.1408, lr: 0.001
Epoch [29/80], Training Loss: 26.0521, Validation Loss Current: 7.6724, Validation Loss AVG: 7.6724, lr: 0.001
Epoch [30/80], Training Loss: 25.6400, Validation Loss Current: 7.7997, Validation Loss AVG: 7.7997, lr: 0.001
Epoch [31/80], Training Loss: 27.0784, Validation Loss Current: 8.4734, Validation Loss AVG: 8.4734, lr: 0.001
Epoch [32/80], Training Loss: 32.3272, Validation Loss Current: 8.1909, Validation Loss AVG: 8.1909, lr: 0.001
Epoch [33/80], Training Loss: 28.2764, Validation Loss Current: 7.6910, Validation Loss AVG: 7.6910, lr: 0.001
Epoch [34/80], Training Loss: 25.1778, Validation Loss Current: 7.6281, Validation Loss AVG: 7.6281, lr: 0.001
Epoch [35/80], Training Loss: 24.0883, Validation Loss Current: 7.7788, Validation Loss AVG: 7.7788, lr: 0.001
Epoch [36/80], Training Loss: 23.9946, Validation Loss Current: 7.4982, Validation Loss AVG: 7.4982, lr: 0.001
Epoch [37/80], Training Loss: 24.4958, Validation Loss Current: 8.0105, Validation Loss AVG: 8.0105, lr: 0.001
Epoch [38/80], Training Loss: 25.2785, Validation Loss Current: 8.7013, Validation Loss AVG: 8.7013, lr: 0.001
Epoch [39/80], Training Loss: 26.7951, Validation Loss Current: 9.4055, Validation Loss AVG: 9.4055, lr: 0.001
Epoch [40/80], Training Loss: 31.2034, Validation Loss Current: 7.9111, Validation Loss AVG: 7.9111, lr: 0.001
Epoch [41/80], Training Loss: 26.1962, Validation Loss Current: 7.7398, Validation Loss AVG: 7.7398, lr: 0.001
Epoch [42/80], Training Loss: 24.6207, Validation Loss Current: 7.6534, Validation Loss AVG: 7.6534, lr: 0.001
Epoch [43/80], Training Loss: 24.5287, Validation Loss Current: 8.0792, Validation Loss AVG: 8.0792, lr: 0.001
Epoch [44/80], Training Loss: 24.2997, Validation Loss Current: 7.9939, Validation Loss AVG: 7.9939, lr: 0.001
Epoch [45/80], Training Loss: 23.0532, Validation Loss Current: 7.8048, Validation Loss AVG: 7.8048, lr: 0.001
Epoch [46/80], Training Loss: 22.6703, Validation Loss Current: 8.1011, Validation Loss AVG: 8.1011, lr: 0.001
Epoch [47/80], Training Loss: 23.3026, Validation Loss Current: 7.5690, Validation Loss AVG: 7.5690, lr: 0.001
Epoch [48/80], Training Loss: 24.0425, Validation Loss Current: 8.6070, Validation Loss AVG: 8.6070, lr: 0.001
Epoch [49/80], Training Loss: 28.2053, Validation Loss Current: 7.6165, Validation Loss AVG: 7.6165, lr: 0.001
Epoch [50/80], Training Loss: 24.9047, Validation Loss Current: 7.8579, Validation Loss AVG: 7.8579, lr: 0.001
Epoch [51/80], Training Loss: 25.5543, Validation Loss Current: 8.1287, Validation Loss AVG: 8.1287, lr: 0.001
Epoch [52/80], Training Loss: 22.2435, Validation Loss Current: 7.8683, Validation Loss AVG: 7.8683, lr: 0.001
Epoch [53/80], Training Loss: 20.9902, Validation Loss Current: 7.6531, Validation Loss AVG: 7.6531, lr: 0.001
Epoch [54/80], Training Loss: 20.5432, Validation Loss Current: 8.4030, Validation Loss AVG: 8.4030, lr: 0.001
Epoch [55/80], Training Loss: 23.2873, Validation Loss Current: 8.4185, Validation Loss AVG: 8.4185, lr: 0.001
Epoch [56/80], Training Loss: 24.9266, Validation Loss Current: 8.1494, Validation Loss AVG: 8.1494, lr: 0.001
Epoch [57/80], Training Loss: 23.8782, Validation Loss Current: 8.1716, Validation Loss AVG: 8.1716, lr: 0.001
Epoch [58/80], Training Loss: 25.2194, Validation Loss Current: 10.7177, Validation Loss AVG: 10.7177, lr: 0.001
Epoch [59/80], Training Loss: 30.9766, Validation Loss Current: 8.2681, Validation Loss AVG: 8.2681, lr: 0.001
Epoch [60/80], Training Loss: 25.6716, Validation Loss Current: 8.0242, Validation Loss AVG: 8.0242, lr: 0.001
Epoch [61/80], Training Loss: 22.6666, Validation Loss Current: 7.9605, Validation Loss AVG: 7.9605, lr: 0.001
Epoch [62/80], Training Loss: 21.6155, Validation Loss Current: 9.0213, Validation Loss AVG: 9.0213, lr: 0.001
Epoch [63/80], Training Loss: 22.5459, Validation Loss Current: 8.9699, Validation Loss AVG: 8.9699, lr: 0.001
Epoch [64/80], Training Loss: 24.3367, Validation Loss Current: 8.2643, Validation Loss AVG: 8.2643, lr: 0.001
Epoch [65/80], Training Loss: 26.4335, Validation Loss Current: 8.2812, Validation Loss AVG: 8.2812, lr: 0.001
Epoch [66/80], Training Loss: 23.2922, Validation Loss Current: 7.9000, Validation Loss AVG: 7.9000, lr: 0.001
 --- Early Stopped ---
Patch distance: 0.4 finished training. Best epoch: 36 Best val accuracy: [0.44703947368421054, 0.4598684210526316, 0.4269736842105264, 0.4131578947368421, 0.4217105263157895, 0.46940789473684214, 0.47993421052631574, 0.4588815789473684, 0.48453947368421063, 0.4697368421052632, 0.4848684210526316, 0.47993421052631574, 0.4430921052631579, 0.47302631578947363, 0.47664473684210523, 0.45625, 0.47105263157894733, 0.4898026315789473, 0.45625, 0.4861842105263158, 0.46184210526315794, 0.4858552631578947, 0.4631578947368421, 0.4740131578947369, 0.46282894736842106, 0.45921052631578946, 0.4894736842105264, 0.4532894736842105, 0.4671052631578948, 0.4723684210526316, 0.4223684210526316, 0.4009868421052632, 0.4648026315789474, 0.4891447368421053, 0.4776315789473684, 0.49572368421052626, 0.4569078947368421, 0.4184210526315789, 0.3743421052631579, 0.4391447368421053, 0.48026315789473684, 0.4789473684210527, 0.4625, 0.45000000000000007, 0.49144736842105263, 0.45230263157894746, 0.49407894736842106, 0.4305921052631579, 0.46118421052631575, 0.4828947368421052, 0.4365131578947368, 0.4786184210526316, 0.4911184210526316, 0.46052631578947373, 0.44835526315789476, 0.44144736842105264, 0.4319078947368421, 0.3203947368421053, 0.44177631578947374, 0.4240131578947369, 0.4743421052631579, 0.3917763157894737, 0.4197368421052632, 0.4325657894736842, 0.4391447368421052, 0.45723684210526316] Best val loss: 7.498198342323303


Current group: 0.6
Epoch [1/80], Training Loss: 24.3716, Validation Loss Current: 8.0967, Validation Loss AVG: 8.0967, lr: 0.001
Epoch [2/80], Training Loss: 26.7658, Validation Loss Current: 7.8108, Validation Loss AVG: 7.8108, lr: 0.001
Epoch [3/80], Training Loss: 23.5268, Validation Loss Current: 7.4421, Validation Loss AVG: 7.4421, lr: 0.001
Epoch [4/80], Training Loss: 22.0480, Validation Loss Current: 7.5069, Validation Loss AVG: 7.5069, lr: 0.001
Epoch [5/80], Training Loss: 20.7416, Validation Loss Current: 7.9157, Validation Loss AVG: 7.9157, lr: 0.001
Epoch [6/80], Training Loss: 20.0355, Validation Loss Current: 7.6306, Validation Loss AVG: 7.6306, lr: 0.001
Epoch [7/80], Training Loss: 20.0825, Validation Loss Current: 7.8988, Validation Loss AVG: 7.8988, lr: 0.001
Epoch [8/80], Training Loss: 19.4363, Validation Loss Current: 7.8293, Validation Loss AVG: 7.8293, lr: 0.001
Epoch [9/80], Training Loss: 20.4530, Validation Loss Current: 7.9275, Validation Loss AVG: 7.9275, lr: 0.001
Epoch [10/80], Training Loss: 20.7221, Validation Loss Current: 7.9941, Validation Loss AVG: 7.9941, lr: 0.001
Epoch [11/80], Training Loss: 19.2101, Validation Loss Current: 7.9545, Validation Loss AVG: 7.9545, lr: 0.001
Epoch [12/80], Training Loss: 20.2209, Validation Loss Current: 8.4353, Validation Loss AVG: 8.4353, lr: 0.001
Epoch [13/80], Training Loss: 18.2651, Validation Loss Current: 7.6398, Validation Loss AVG: 7.6398, lr: 0.001
Epoch [14/80], Training Loss: 17.6126, Validation Loss Current: 8.5041, Validation Loss AVG: 8.5041, lr: 0.001
Epoch [15/80], Training Loss: 18.2649, Validation Loss Current: 7.9436, Validation Loss AVG: 7.9436, lr: 0.001
Epoch [16/80], Training Loss: 17.3783, Validation Loss Current: 7.8970, Validation Loss AVG: 7.8970, lr: 0.001
Epoch [17/80], Training Loss: 17.9035, Validation Loss Current: 8.9797, Validation Loss AVG: 8.9797, lr: 0.001
Epoch [18/80], Training Loss: 17.8271, Validation Loss Current: 8.4048, Validation Loss AVG: 8.4048, lr: 0.001
Epoch [19/80], Training Loss: 17.0219, Validation Loss Current: 8.1323, Validation Loss AVG: 8.1323, lr: 0.001
Epoch [20/80], Training Loss: 16.3644, Validation Loss Current: 8.9546, Validation Loss AVG: 8.9546, lr: 0.001
Epoch [21/80], Training Loss: 16.0103, Validation Loss Current: 8.3302, Validation Loss AVG: 8.3302, lr: 0.001
Epoch [22/80], Training Loss: 15.1493, Validation Loss Current: 8.6017, Validation Loss AVG: 8.6017, lr: 0.001
Epoch [23/80], Training Loss: 16.0219, Validation Loss Current: 9.0501, Validation Loss AVG: 9.0501, lr: 0.001
Epoch [24/80], Training Loss: 17.2661, Validation Loss Current: 7.9056, Validation Loss AVG: 7.9056, lr: 0.001
Epoch [25/80], Training Loss: 15.7171, Validation Loss Current: 9.6546, Validation Loss AVG: 9.6546, lr: 0.001
Epoch [26/80], Training Loss: 14.2736, Validation Loss Current: 8.6490, Validation Loss AVG: 8.6490, lr: 0.001
Epoch [27/80], Training Loss: 14.1084, Validation Loss Current: 9.5073, Validation Loss AVG: 9.5073, lr: 0.001
Epoch [28/80], Training Loss: 13.2192, Validation Loss Current: 9.0609, Validation Loss AVG: 9.0609, lr: 0.001
Epoch [29/80], Training Loss: 11.9118, Validation Loss Current: 9.7382, Validation Loss AVG: 9.7382, lr: 0.001
Epoch [30/80], Training Loss: 12.2302, Validation Loss Current: 10.4322, Validation Loss AVG: 10.4322, lr: 0.001
Epoch [31/80], Training Loss: 17.2974, Validation Loss Current: 8.4072, Validation Loss AVG: 8.4072, lr: 0.001
Epoch [32/80], Training Loss: 16.7678, Validation Loss Current: 7.9859, Validation Loss AVG: 7.9859, lr: 0.001
Epoch [33/80], Training Loss: 14.6807, Validation Loss Current: 9.0912, Validation Loss AVG: 9.0912, lr: 0.001
 --- Early Stopped ---
Patch distance: 0.6 finished training. Best epoch: 3 Best val accuracy: [0.43881578947368427, 0.4697368421052632, 0.49572368421052626, 0.49703947368421053, 0.48552631578947364, 0.5, 0.4898026315789473, 0.5032894736842105, 0.5171052631578947, 0.5029605263157895, 0.48881578947368415, 0.4634868421052632, 0.5167763157894737, 0.4766447368421053, 0.5009868421052631, 0.5055921052631579, 0.45065789473684215, 0.4967105263157895, 0.5108552631578948, 0.48453947368421046, 0.5075657894736841, 0.5023026315789474, 0.48355263157894746, 0.5085526315789474, 0.47697368421052627, 0.5039473684210527, 0.4911184210526315, 0.500328947368421, 0.4875, 0.4703947368421052, 0.4927631578947368, 0.4822368421052631, 0.49703947368421053] Best val loss: 7.442065358161926


Current group: 1
Epoch [1/80], Training Loss: 25.3024, Validation Loss Current: 6.2780, Validation Loss AVG: 7.7718, lr: 0.001
Epoch [2/80], Training Loss: 22.1574, Validation Loss Current: 6.6872, Validation Loss AVG: 9.0636, lr: 0.001
Epoch [3/80], Training Loss: 21.1955, Validation Loss Current: 6.5988, Validation Loss AVG: 8.0697, lr: 0.001
Epoch [4/80], Training Loss: 21.8357, Validation Loss Current: 6.4229, Validation Loss AVG: 8.1065, lr: 0.001
Epoch [5/80], Training Loss: 20.9872, Validation Loss Current: 6.2005, Validation Loss AVG: 8.9772, lr: 0.001
Epoch [6/80], Training Loss: 21.1770, Validation Loss Current: 6.1773, Validation Loss AVG: 8.2420, lr: 0.001
Epoch [7/80], Training Loss: 19.0803, Validation Loss Current: 5.8834, Validation Loss AVG: 8.6098, lr: 0.001
Epoch [8/80], Training Loss: 17.9269, Validation Loss Current: 6.1556, Validation Loss AVG: 9.4301, lr: 0.001
Epoch [9/80], Training Loss: 19.4953, Validation Loss Current: 6.2827, Validation Loss AVG: 9.0749, lr: 0.001
Epoch [10/80], Training Loss: 20.4529, Validation Loss Current: 5.8878, Validation Loss AVG: 8.1460, lr: 0.001
Epoch [11/80], Training Loss: 17.6106, Validation Loss Current: 6.1587, Validation Loss AVG: 9.0399, lr: 0.001
Epoch [12/80], Training Loss: 18.6000, Validation Loss Current: 6.5477, Validation Loss AVG: 11.6883, lr: 0.001
Epoch [13/80], Training Loss: 16.6603, Validation Loss Current: 5.8968, Validation Loss AVG: 9.1756, lr: 0.001
Epoch [14/80], Training Loss: 15.8477, Validation Loss Current: 5.9942, Validation Loss AVG: 9.1902, lr: 0.001
Epoch [15/80], Training Loss: 14.9824, Validation Loss Current: 6.1817, Validation Loss AVG: 11.0605, lr: 0.001
Epoch [16/80], Training Loss: 14.1175, Validation Loss Current: 7.4712, Validation Loss AVG: 14.2840, lr: 0.001
Epoch [17/80], Training Loss: 16.4134, Validation Loss Current: 5.9093, Validation Loss AVG: 9.3507, lr: 0.001
Epoch [18/80], Training Loss: 13.9787, Validation Loss Current: 7.1770, Validation Loss AVG: 13.1079, lr: 0.001
Epoch [19/80], Training Loss: 13.9645, Validation Loss Current: 5.8822, Validation Loss AVG: 9.2596, lr: 0.001
Epoch [20/80], Training Loss: 12.4645, Validation Loss Current: 6.1438, Validation Loss AVG: 11.6821, lr: 0.001
Epoch [21/80], Training Loss: 11.6616, Validation Loss Current: 6.4592, Validation Loss AVG: 11.9587, lr: 0.001
Epoch [22/80], Training Loss: 13.9488, Validation Loss Current: 7.6173, Validation Loss AVG: 11.9908, lr: 0.001
Epoch [23/80], Training Loss: 14.5154, Validation Loss Current: 6.9666, Validation Loss AVG: 9.6818, lr: 0.001
Epoch [24/80], Training Loss: 12.2264, Validation Loss Current: 6.4411, Validation Loss AVG: 12.8884, lr: 0.001
Epoch [25/80], Training Loss: 11.0399, Validation Loss Current: 6.7508, Validation Loss AVG: 13.0331, lr: 0.001
Epoch [26/80], Training Loss: 11.5592, Validation Loss Current: 6.5768, Validation Loss AVG: 12.8168, lr: 0.001
Epoch [27/80], Training Loss: 10.2246, Validation Loss Current: 7.4894, Validation Loss AVG: 14.8091, lr: 0.001
Epoch [28/80], Training Loss: 12.2414, Validation Loss Current: 6.9847, Validation Loss AVG: 13.6684, lr: 0.001
Epoch [29/80], Training Loss: 10.5299, Validation Loss Current: 6.2433, Validation Loss AVG: 13.1327, lr: 0.001
Epoch [30/80], Training Loss: 8.4334, Validation Loss Current: 6.9184, Validation Loss AVG: 12.2622, lr: 0.001
Epoch [31/80], Training Loss: 7.5260, Validation Loss Current: 6.6999, Validation Loss AVG: 13.6251, lr: 0.001
Epoch [32/80], Training Loss: 8.9205, Validation Loss Current: 11.4244, Validation Loss AVG: 13.5315, lr: 0.001
Epoch [33/80], Training Loss: 25.4292, Validation Loss Current: 6.5459, Validation Loss AVG: 10.0975, lr: 0.001
Epoch [34/80], Training Loss: 18.0785, Validation Loss Current: 6.2247, Validation Loss AVG: 10.1317, lr: 0.001
Epoch [35/80], Training Loss: 16.2379, Validation Loss Current: 8.5782, Validation Loss AVG: 10.4927, lr: 0.001
Epoch [36/80], Training Loss: 15.9522, Validation Loss Current: 5.8501, Validation Loss AVG: 9.2470, lr: 0.001
Epoch [37/80], Training Loss: 11.4813, Validation Loss Current: 6.8139, Validation Loss AVG: 12.4522, lr: 0.001
Epoch [38/80], Training Loss: 9.4823, Validation Loss Current: 6.9979, Validation Loss AVG: 13.1816, lr: 0.001
Epoch [39/80], Training Loss: 9.6195, Validation Loss Current: 8.7071, Validation Loss AVG: 16.5961, lr: 0.001
Epoch [40/80], Training Loss: 17.5685, Validation Loss Current: 6.5491, Validation Loss AVG: 10.8798, lr: 0.001
Epoch [41/80], Training Loss: 12.4640, Validation Loss Current: 7.1780, Validation Loss AVG: 12.8271, lr: 0.001
Epoch [42/80], Training Loss: 11.1875, Validation Loss Current: 7.3172, Validation Loss AVG: 16.0251, lr: 0.001
Epoch [43/80], Training Loss: 10.2391, Validation Loss Current: 7.0761, Validation Loss AVG: 13.7258, lr: 0.001
Epoch [44/80], Training Loss: 11.7528, Validation Loss Current: 11.1206, Validation Loss AVG: 20.8969, lr: 0.001
Epoch [45/80], Training Loss: 31.5430, Validation Loss Current: 6.9075, Validation Loss AVG: 9.4768, lr: 0.001
Epoch [46/80], Training Loss: 23.6663, Validation Loss Current: 6.3048, Validation Loss AVG: 8.5469, lr: 0.001
Epoch [47/80], Training Loss: 18.1447, Validation Loss Current: 6.2267, Validation Loss AVG: 10.4536, lr: 0.001
Epoch [48/80], Training Loss: 13.5585, Validation Loss Current: 6.3804, Validation Loss AVG: 11.2785, lr: 0.001
Epoch [49/80], Training Loss: 12.5702, Validation Loss Current: 6.2954, Validation Loss AVG: 11.9358, lr: 0.001
Epoch [50/80], Training Loss: 9.3787, Validation Loss Current: 7.0473, Validation Loss AVG: 12.3593, lr: 0.001
Epoch [51/80], Training Loss: 8.4691, Validation Loss Current: 7.3939, Validation Loss AVG: 13.6919, lr: 0.001
Epoch [52/80], Training Loss: 7.5925, Validation Loss Current: 7.8343, Validation Loss AVG: 17.5781, lr: 0.001
Epoch [53/80], Training Loss: 6.5633, Validation Loss Current: 8.2671, Validation Loss AVG: 18.0189, lr: 0.001
Epoch [54/80], Training Loss: 7.8523, Validation Loss Current: 6.9820, Validation Loss AVG: 14.6346, lr: 0.001
Epoch [55/80], Training Loss: 5.9347, Validation Loss Current: 12.3592, Validation Loss AVG: 26.2042, lr: 0.001
Epoch [56/80], Training Loss: 12.4702, Validation Loss Current: 6.4601, Validation Loss AVG: 12.6395, lr: 0.001
Epoch [57/80], Training Loss: 7.9827, Validation Loss Current: 7.7695, Validation Loss AVG: 16.8459, lr: 0.001
Epoch [58/80], Training Loss: 5.3254, Validation Loss Current: 8.7139, Validation Loss AVG: 19.0207, lr: 0.001
Epoch [59/80], Training Loss: 9.1433, Validation Loss Current: 6.7062, Validation Loss AVG: 13.4725, lr: 0.001
Epoch [60/80], Training Loss: 6.5205, Validation Loss Current: 7.4829, Validation Loss AVG: 15.6195, lr: 0.001
Epoch [61/80], Training Loss: 5.0008, Validation Loss Current: 9.5487, Validation Loss AVG: 18.1842, lr: 0.001
Epoch [62/80], Training Loss: 8.6288, Validation Loss Current: 9.1998, Validation Loss AVG: 18.0363, lr: 0.001
Epoch [63/80], Training Loss: 7.7037, Validation Loss Current: 9.9415, Validation Loss AVG: 21.6996, lr: 0.001
Epoch [64/80], Training Loss: 12.4074, Validation Loss Current: 9.1646, Validation Loss AVG: 15.8939, lr: 0.001
Epoch [65/80], Training Loss: 22.7282, Validation Loss Current: 6.8971, Validation Loss AVG: 10.6362, lr: 0.001
Epoch [66/80], Training Loss: 16.1182, Validation Loss Current: 8.2590, Validation Loss AVG: 13.8861, lr: 0.001
 --- Early Stopped ---
Patch distance: 1 finished training. Best epoch: 36 Best val accuracy: [0.5773026315789473, 0.555921052631579, 0.5723684210526315, 0.5773026315789473, 0.5740131578947368, 0.587171052631579, 0.600328947368421, 0.5822368421052632, 0.5575657894736842, 0.6052631578947368, 0.5822368421052632, 0.5789473684210527, 0.6167763157894737, 0.6085526315789473, 0.5723684210526315, 0.5509868421052632, 0.5953947368421053, 0.5773026315789473, 0.6167763157894737, 0.6036184210526315, 0.5805921052631579, 0.4934210526315789, 0.618421052631579, 0.5970394736842105, 0.618421052631579, 0.6167763157894737, 0.5838815789473685, 0.5855263157894737, 0.600328947368421, 0.625, 0.618421052631579, 0.537828947368421, 0.5805921052631579, 0.6052631578947368, 0.5723684210526315, 0.6233552631578947, 0.6266447368421053, 0.6052631578947368, 0.5575657894736842, 0.5674342105263158, 0.5970394736842105, 0.6101973684210527, 0.5855263157894737, 0.4967105263157895, 0.4967105263157895, 0.5657894736842105, 0.5723684210526315, 0.5921052631578947, 0.618421052631579, 0.625, 0.6299342105263158, 0.6167763157894737, 0.5740131578947368, 0.6151315789473685, 0.5509868421052632, 0.6101973684210527, 0.6118421052631579, 0.587171052631579, 0.6282894736842105, 0.6200657894736842, 0.6085526315789473, 0.5674342105263158, 0.5279605263157895, 0.5460526315789473, 0.5674342105263158, 0.5345394736842105] Best val loss: 5.850091218948364


Fold: 2
----- Training alexnet with sequence: [0.2, 0.8, 0.4, 0.6, 1] -----
Current group: 0.2
Epoch [1/80], Training Loss: 41.5243, Validation Loss Current: 10.3665, Validation Loss AVG: 10.3665, lr: 0.001
Epoch [2/80], Training Loss: 41.3762, Validation Loss Current: 10.3381, Validation Loss AVG: 10.3381, lr: 0.001
Epoch [3/80], Training Loss: 41.2668, Validation Loss Current: 10.3048, Validation Loss AVG: 10.3048, lr: 0.001
Epoch [4/80], Training Loss: 41.1043, Validation Loss Current: 10.2700, Validation Loss AVG: 10.2700, lr: 0.001
Epoch [5/80], Training Loss: 40.9911, Validation Loss Current: 10.2294, Validation Loss AVG: 10.2294, lr: 0.001
Epoch [6/80], Training Loss: 40.7457, Validation Loss Current: 10.1841, Validation Loss AVG: 10.1841, lr: 0.001
Epoch [7/80], Training Loss: 40.4277, Validation Loss Current: 10.1258, Validation Loss AVG: 10.1258, lr: 0.001
Epoch [8/80], Training Loss: 40.2788, Validation Loss Current: 10.1041, Validation Loss AVG: 10.1041, lr: 0.001
Epoch [9/80], Training Loss: 40.0011, Validation Loss Current: 10.0920, Validation Loss AVG: 10.0920, lr: 0.001
Epoch [10/80], Training Loss: 40.0680, Validation Loss Current: 10.0870, Validation Loss AVG: 10.0870, lr: 0.001
Epoch [11/80], Training Loss: 39.7579, Validation Loss Current: 10.0815, Validation Loss AVG: 10.0815, lr: 0.001
Epoch [12/80], Training Loss: 40.8027, Validation Loss Current: 10.0845, Validation Loss AVG: 10.0845, lr: 0.001
Epoch [13/80], Training Loss: 39.8908, Validation Loss Current: 10.1219, Validation Loss AVG: 10.1219, lr: 0.001
Epoch [14/80], Training Loss: 39.8095, Validation Loss Current: 10.0822, Validation Loss AVG: 10.0822, lr: 0.001
Epoch [15/80], Training Loss: 39.9943, Validation Loss Current: 10.1027, Validation Loss AVG: 10.1027, lr: 0.001
Epoch [16/80], Training Loss: 40.3411, Validation Loss Current: 10.0995, Validation Loss AVG: 10.0995, lr: 0.001
Epoch [17/80], Training Loss: 40.1367, Validation Loss Current: 10.1097, Validation Loss AVG: 10.1097, lr: 0.001
Epoch [18/80], Training Loss: 39.6636, Validation Loss Current: 10.0928, Validation Loss AVG: 10.0928, lr: 0.001
Epoch [19/80], Training Loss: 40.0137, Validation Loss Current: 10.0861, Validation Loss AVG: 10.0861, lr: 0.001
Epoch [20/80], Training Loss: 40.0795, Validation Loss Current: 10.0919, Validation Loss AVG: 10.0919, lr: 0.001
Epoch [21/80], Training Loss: 39.6620, Validation Loss Current: 10.0759, Validation Loss AVG: 10.0759, lr: 0.001
Epoch [22/80], Training Loss: 39.6729, Validation Loss Current: 10.0948, Validation Loss AVG: 10.0948, lr: 0.001
Epoch [23/80], Training Loss: 40.1188, Validation Loss Current: 10.0997, Validation Loss AVG: 10.0997, lr: 0.001
Epoch [24/80], Training Loss: 39.6858, Validation Loss Current: 10.1095, Validation Loss AVG: 10.1095, lr: 0.001
Epoch [25/80], Training Loss: 39.7790, Validation Loss Current: 10.0908, Validation Loss AVG: 10.0908, lr: 0.001
Epoch [26/80], Training Loss: 40.2155, Validation Loss Current: 10.0913, Validation Loss AVG: 10.0913, lr: 0.001
Epoch [27/80], Training Loss: 40.0423, Validation Loss Current: 10.0932, Validation Loss AVG: 10.0932, lr: 0.001
Epoch [28/80], Training Loss: 39.5196, Validation Loss Current: 10.0671, Validation Loss AVG: 10.0671, lr: 0.001
Epoch [29/80], Training Loss: 40.3846, Validation Loss Current: 10.0837, Validation Loss AVG: 10.0837, lr: 0.001
Epoch [30/80], Training Loss: 40.2901, Validation Loss Current: 10.0933, Validation Loss AVG: 10.0933, lr: 0.001
Epoch [31/80], Training Loss: 40.0564, Validation Loss Current: 10.0896, Validation Loss AVG: 10.0896, lr: 0.001
Epoch [32/80], Training Loss: 39.3306, Validation Loss Current: 10.0850, Validation Loss AVG: 10.0850, lr: 0.001
Epoch [33/80], Training Loss: 39.7395, Validation Loss Current: 10.0911, Validation Loss AVG: 10.0911, lr: 0.001
Epoch [34/80], Training Loss: 39.5948, Validation Loss Current: 10.0826, Validation Loss AVG: 10.0826, lr: 0.001
Epoch [35/80], Training Loss: 39.5409, Validation Loss Current: 10.0832, Validation Loss AVG: 10.0832, lr: 0.001
Epoch [36/80], Training Loss: 39.3236, Validation Loss Current: 10.0951, Validation Loss AVG: 10.0951, lr: 0.001
Epoch [37/80], Training Loss: 39.6070, Validation Loss Current: 10.0916, Validation Loss AVG: 10.0916, lr: 0.001
Epoch [38/80], Training Loss: 39.5396, Validation Loss Current: 10.0793, Validation Loss AVG: 10.0793, lr: 0.001
Epoch [39/80], Training Loss: 40.1620, Validation Loss Current: 10.1113, Validation Loss AVG: 10.1113, lr: 0.001
Epoch [40/80], Training Loss: 39.7424, Validation Loss Current: 10.0974, Validation Loss AVG: 10.0974, lr: 0.001
Epoch [41/80], Training Loss: 39.9302, Validation Loss Current: 10.1011, Validation Loss AVG: 10.1011, lr: 0.001
Epoch [42/80], Training Loss: 39.4343, Validation Loss Current: 10.0925, Validation Loss AVG: 10.0925, lr: 0.001
Epoch [43/80], Training Loss: 39.9027, Validation Loss Current: 10.1118, Validation Loss AVG: 10.1118, lr: 0.001
Epoch [44/80], Training Loss: 39.3988, Validation Loss Current: 10.0858, Validation Loss AVG: 10.0858, lr: 0.001
Epoch [45/80], Training Loss: 39.8120, Validation Loss Current: 10.1038, Validation Loss AVG: 10.1038, lr: 0.001
Epoch [46/80], Training Loss: 39.4296, Validation Loss Current: 10.0892, Validation Loss AVG: 10.0892, lr: 0.001
Epoch [47/80], Training Loss: 39.9020, Validation Loss Current: 10.0980, Validation Loss AVG: 10.0980, lr: 0.001
Epoch [48/80], Training Loss: 39.4429, Validation Loss Current: 10.1041, Validation Loss AVG: 10.1041, lr: 0.001
Epoch [49/80], Training Loss: 39.9384, Validation Loss Current: 10.1108, Validation Loss AVG: 10.1108, lr: 0.001
Epoch [50/80], Training Loss: 39.4931, Validation Loss Current: 10.0821, Validation Loss AVG: 10.0821, lr: 0.001
Epoch [51/80], Training Loss: 39.4361, Validation Loss Current: 10.1145, Validation Loss AVG: 10.1145, lr: 0.001
Epoch [52/80], Training Loss: 39.5520, Validation Loss Current: 10.1311, Validation Loss AVG: 10.1311, lr: 0.001
Epoch [53/80], Training Loss: 39.3658, Validation Loss Current: 10.0778, Validation Loss AVG: 10.0778, lr: 0.001
Epoch [54/80], Training Loss: 39.7964, Validation Loss Current: 10.1096, Validation Loss AVG: 10.1096, lr: 0.001
Epoch [55/80], Training Loss: 39.5937, Validation Loss Current: 10.0854, Validation Loss AVG: 10.0854, lr: 0.001
Epoch [56/80], Training Loss: 39.6774, Validation Loss Current: 10.1034, Validation Loss AVG: 10.1034, lr: 0.001
Epoch [57/80], Training Loss: 39.3552, Validation Loss Current: 10.1102, Validation Loss AVG: 10.1102, lr: 0.001
Epoch [58/80], Training Loss: 39.6489, Validation Loss Current: 10.1155, Validation Loss AVG: 10.1155, lr: 0.001
 --- Early Stopped ---
Patch distance: 0.2 finished training. Best epoch: 28 Best val accuracy: [0.2302631578947368, 0.2302631578947368, 0.2302631578947368, 0.2302631578947368, 0.2302631578947368, 0.2302631578947368, 0.2302631578947368, 0.2302631578947368, 0.2302631578947368, 0.2302631578947368, 0.2302631578947368, 0.2302631578947368, 0.2302631578947368, 0.2302631578947368, 0.2302631578947368, 0.2302631578947368, 0.2302631578947368, 0.2302631578947368, 0.2302631578947368, 0.2302631578947368, 0.2302631578947368, 0.2302631578947368, 0.2302631578947368, 0.2302631578947368, 0.2302631578947368, 0.2302631578947368, 0.2302631578947368, 0.2302631578947368, 0.2302631578947368, 0.2302631578947368, 0.2302631578947368, 0.2302631578947368, 0.2302631578947368, 0.2302631578947368, 0.2302631578947368, 0.2302631578947368, 0.2302631578947368, 0.2302631578947368, 0.2302631578947368, 0.2302631578947368, 0.2302631578947368, 0.2302631578947368, 0.2302631578947368, 0.2302631578947368, 0.2302631578947368, 0.2302631578947368, 0.2302631578947368, 0.2302631578947368, 0.2302631578947368, 0.2302631578947368, 0.2302631578947368, 0.2302631578947368, 0.2302631578947368, 0.2302631578947368, 0.2302631578947368, 0.2302631578947368, 0.2302631578947368, 0.2302631578947368] Best val loss: 10.067138624191283


Current group: 0.8
Epoch [1/80], Training Loss: 39.9573, Validation Loss Current: 10.2475, Validation Loss AVG: 10.2475, lr: 0.001
Epoch [2/80], Training Loss: 39.6053, Validation Loss Current: 10.1365, Validation Loss AVG: 10.1365, lr: 0.001
Epoch [3/80], Training Loss: 39.4902, Validation Loss Current: 10.1622, Validation Loss AVG: 10.1622, lr: 0.001
Epoch [4/80], Training Loss: 39.4399, Validation Loss Current: 10.0883, Validation Loss AVG: 10.0883, lr: 0.001
Epoch [5/80], Training Loss: 38.7842, Validation Loss Current: 10.0980, Validation Loss AVG: 10.0980, lr: 0.001
Epoch [6/80], Training Loss: 39.2129, Validation Loss Current: 9.9916, Validation Loss AVG: 9.9916, lr: 0.001
Epoch [7/80], Training Loss: 39.9903, Validation Loss Current: 9.9333, Validation Loss AVG: 9.9333, lr: 0.001
Epoch [8/80], Training Loss: 39.2683, Validation Loss Current: 10.1066, Validation Loss AVG: 10.1066, lr: 0.001
Epoch [9/80], Training Loss: 38.6603, Validation Loss Current: 9.9578, Validation Loss AVG: 9.9578, lr: 0.001
Epoch [10/80], Training Loss: 37.3994, Validation Loss Current: 10.0208, Validation Loss AVG: 10.0208, lr: 0.001
Epoch [11/80], Training Loss: 37.0505, Validation Loss Current: 10.6027, Validation Loss AVG: 10.6027, lr: 0.001
Epoch [12/80], Training Loss: 38.3266, Validation Loss Current: 10.1536, Validation Loss AVG: 10.1536, lr: 0.001
Epoch [13/80], Training Loss: 38.5776, Validation Loss Current: 9.8696, Validation Loss AVG: 9.8696, lr: 0.001
Epoch [14/80], Training Loss: 37.9774, Validation Loss Current: 9.7174, Validation Loss AVG: 9.7174, lr: 0.001
Epoch [15/80], Training Loss: 37.7825, Validation Loss Current: 9.7749, Validation Loss AVG: 9.7749, lr: 0.001
Epoch [16/80], Training Loss: 37.8366, Validation Loss Current: 9.6350, Validation Loss AVG: 9.6350, lr: 0.001
Epoch [17/80], Training Loss: 37.6910, Validation Loss Current: 9.7695, Validation Loss AVG: 9.7695, lr: 0.001
Epoch [18/80], Training Loss: 36.8901, Validation Loss Current: 10.1912, Validation Loss AVG: 10.1912, lr: 0.001
Epoch [19/80], Training Loss: 36.2605, Validation Loss Current: 10.1386, Validation Loss AVG: 10.1386, lr: 0.001
Epoch [20/80], Training Loss: 38.1278, Validation Loss Current: 9.5668, Validation Loss AVG: 9.5668, lr: 0.001
Epoch [21/80], Training Loss: 38.2573, Validation Loss Current: 10.6215, Validation Loss AVG: 10.6215, lr: 0.001
Epoch [22/80], Training Loss: 36.8201, Validation Loss Current: 9.7466, Validation Loss AVG: 9.7466, lr: 0.001
Epoch [23/80], Training Loss: 37.2224, Validation Loss Current: 9.6936, Validation Loss AVG: 9.6936, lr: 0.001
Epoch [24/80], Training Loss: 35.0510, Validation Loss Current: 9.7537, Validation Loss AVG: 9.7537, lr: 0.001
Epoch [25/80], Training Loss: 36.6583, Validation Loss Current: 9.4027, Validation Loss AVG: 9.4027, lr: 0.001
Epoch [26/80], Training Loss: 36.2904, Validation Loss Current: 9.3840, Validation Loss AVG: 9.3840, lr: 0.001
Epoch [27/80], Training Loss: 36.1286, Validation Loss Current: 9.3823, Validation Loss AVG: 9.3823, lr: 0.001
Epoch [28/80], Training Loss: 35.4587, Validation Loss Current: 11.2370, Validation Loss AVG: 11.2370, lr: 0.001
Epoch [29/80], Training Loss: 36.3682, Validation Loss Current: 9.6762, Validation Loss AVG: 9.6762, lr: 0.001
Epoch [30/80], Training Loss: 35.8481, Validation Loss Current: 9.2716, Validation Loss AVG: 9.2716, lr: 0.001
Epoch [31/80], Training Loss: 33.9799, Validation Loss Current: 9.6266, Validation Loss AVG: 9.6266, lr: 0.001
Epoch [32/80], Training Loss: 35.2811, Validation Loss Current: 9.4631, Validation Loss AVG: 9.4631, lr: 0.001
Epoch [33/80], Training Loss: 34.7314, Validation Loss Current: 10.4776, Validation Loss AVG: 10.4776, lr: 0.001
Epoch [34/80], Training Loss: 34.0249, Validation Loss Current: 9.9577, Validation Loss AVG: 9.9577, lr: 0.001
Epoch [35/80], Training Loss: 34.9421, Validation Loss Current: 11.6316, Validation Loss AVG: 11.6316, lr: 0.001
Epoch [36/80], Training Loss: 37.3966, Validation Loss Current: 9.4341, Validation Loss AVG: 9.4341, lr: 0.001
Epoch [37/80], Training Loss: 34.3291, Validation Loss Current: 9.1120, Validation Loss AVG: 9.1120, lr: 0.001
Epoch [38/80], Training Loss: 34.4058, Validation Loss Current: 8.9376, Validation Loss AVG: 8.9376, lr: 0.001
Epoch [39/80], Training Loss: 33.9437, Validation Loss Current: 9.7296, Validation Loss AVG: 9.7296, lr: 0.001
Epoch [40/80], Training Loss: 43.3453, Validation Loss Current: 10.4238, Validation Loss AVG: 10.4238, lr: 0.001
Epoch [41/80], Training Loss: 38.5733, Validation Loss Current: 9.4693, Validation Loss AVG: 9.4693, lr: 0.001
Epoch [42/80], Training Loss: 34.7266, Validation Loss Current: 9.3014, Validation Loss AVG: 9.3014, lr: 0.001
Epoch [43/80], Training Loss: 34.4466, Validation Loss Current: 9.3687, Validation Loss AVG: 9.3687, lr: 0.001
Epoch [44/80], Training Loss: 34.9604, Validation Loss Current: 8.9826, Validation Loss AVG: 8.9826, lr: 0.001
Epoch [45/80], Training Loss: 32.8275, Validation Loss Current: 8.7906, Validation Loss AVG: 8.7906, lr: 0.001
Epoch [46/80], Training Loss: 31.9133, Validation Loss Current: 9.2235, Validation Loss AVG: 9.2235, lr: 0.001
Epoch [47/80], Training Loss: 33.9386, Validation Loss Current: 8.6918, Validation Loss AVG: 8.6918, lr: 0.001
Epoch [48/80], Training Loss: 31.4387, Validation Loss Current: 8.8311, Validation Loss AVG: 8.8311, lr: 0.001
Epoch [49/80], Training Loss: 32.2429, Validation Loss Current: 8.6368, Validation Loss AVG: 8.6368, lr: 0.001
Epoch [50/80], Training Loss: 32.0311, Validation Loss Current: 8.5779, Validation Loss AVG: 8.5779, lr: 0.001
Epoch [51/80], Training Loss: 31.7303, Validation Loss Current: 9.2699, Validation Loss AVG: 9.2699, lr: 0.001
Epoch [52/80], Training Loss: 31.7575, Validation Loss Current: 8.6946, Validation Loss AVG: 8.6946, lr: 0.001
Epoch [53/80], Training Loss: 30.3106, Validation Loss Current: 9.1806, Validation Loss AVG: 9.1806, lr: 0.001
Epoch [54/80], Training Loss: 31.0703, Validation Loss Current: 9.4960, Validation Loss AVG: 9.4960, lr: 0.001
Epoch [55/80], Training Loss: 32.5360, Validation Loss Current: 8.3669, Validation Loss AVG: 8.3669, lr: 0.001
Epoch [56/80], Training Loss: 31.0033, Validation Loss Current: 8.8397, Validation Loss AVG: 8.8397, lr: 0.001
Epoch [57/80], Training Loss: 30.8274, Validation Loss Current: 8.4129, Validation Loss AVG: 8.4129, lr: 0.001
Epoch [58/80], Training Loss: 29.4379, Validation Loss Current: 8.3592, Validation Loss AVG: 8.3592, lr: 0.001
Epoch [59/80], Training Loss: 29.8150, Validation Loss Current: 8.2767, Validation Loss AVG: 8.2767, lr: 0.001
Epoch [60/80], Training Loss: 29.6838, Validation Loss Current: 8.4026, Validation Loss AVG: 8.4026, lr: 0.001
Epoch [61/80], Training Loss: 29.5443, Validation Loss Current: 8.4488, Validation Loss AVG: 8.4488, lr: 0.001
Epoch [62/80], Training Loss: 30.4627, Validation Loss Current: 8.6031, Validation Loss AVG: 8.6031, lr: 0.001
Epoch [63/80], Training Loss: 29.4956, Validation Loss Current: 8.2632, Validation Loss AVG: 8.2632, lr: 0.001
Epoch [64/80], Training Loss: 28.5404, Validation Loss Current: 8.8340, Validation Loss AVG: 8.8340, lr: 0.001
Epoch [65/80], Training Loss: 29.3721, Validation Loss Current: 8.4643, Validation Loss AVG: 8.4643, lr: 0.001
Epoch [66/80], Training Loss: 28.0734, Validation Loss Current: 8.5980, Validation Loss AVG: 8.5980, lr: 0.001
Epoch [67/80], Training Loss: 29.7541, Validation Loss Current: 9.5663, Validation Loss AVG: 9.5663, lr: 0.001
Epoch [68/80], Training Loss: 30.3192, Validation Loss Current: 8.0016, Validation Loss AVG: 8.0016, lr: 0.001
Epoch [69/80], Training Loss: 26.6632, Validation Loss Current: 9.3685, Validation Loss AVG: 9.3685, lr: 0.001
Epoch [70/80], Training Loss: 27.0583, Validation Loss Current: 8.5118, Validation Loss AVG: 8.5118, lr: 0.001
Epoch [71/80], Training Loss: 25.9116, Validation Loss Current: 8.2982, Validation Loss AVG: 8.2982, lr: 0.001
Epoch [72/80], Training Loss: 27.1402, Validation Loss Current: 8.9977, Validation Loss AVG: 8.9977, lr: 0.001
Epoch [73/80], Training Loss: 26.8824, Validation Loss Current: 8.9925, Validation Loss AVG: 8.9925, lr: 0.001
Epoch [74/80], Training Loss: 27.0497, Validation Loss Current: 8.6649, Validation Loss AVG: 8.6649, lr: 0.001
Epoch [75/80], Training Loss: 27.0192, Validation Loss Current: 8.1557, Validation Loss AVG: 8.1557, lr: 0.001
Epoch [76/80], Training Loss: 26.1387, Validation Loss Current: 9.0898, Validation Loss AVG: 9.0898, lr: 0.001
Epoch [77/80], Training Loss: 27.7232, Validation Loss Current: 7.9681, Validation Loss AVG: 7.9681, lr: 0.001
Epoch [78/80], Training Loss: 26.0466, Validation Loss Current: 10.2487, Validation Loss AVG: 10.2487, lr: 0.001
Epoch [79/80], Training Loss: 28.3677, Validation Loss Current: 8.0823, Validation Loss AVG: 8.0823, lr: 0.001
Epoch [80/80], Training Loss: 28.7409, Validation Loss Current: 10.7831, Validation Loss AVG: 10.7831, lr: 0.001
Patch distance: 0.8 finished training. Best epoch: 77 Best val accuracy: [0.2302631578947368, 0.2302631578947368, 0.2302631578947368, 0.2302631578947368, 0.2302631578947368, 0.2302631578947368, 0.2302631578947368, 0.2302631578947368, 0.2302631578947368, 0.2302631578947368, 0.2302631578947368, 0.2302631578947368, 0.2302631578947368, 0.2319078947368421, 0.24703947368421053, 0.27072368421052634, 0.26973684210526316, 0.2618421052631579, 0.26414473684210527, 0.29901315789473687, 0.26052631578947366, 0.2661184210526316, 0.28519736842105264, 0.28848684210526315, 0.3125, 0.31019736842105267, 0.31644736842105264, 0.28256578947368427, 0.2986842105263158, 0.3226973684210526, 0.31118421052631573, 0.32072368421052627, 0.29901315789473687, 0.3046052631578947, 0.2805921052631579, 0.3026315789473684, 0.3236842105263158, 0.3427631578947368, 0.3078947368421053, 0.14375, 0.3174342105263158, 0.3072368421052632, 0.30625, 0.32105263157894737, 0.34144736842105267, 0.3398026315789474, 0.3516447368421053, 0.3473684210526316, 0.3601973684210526, 0.3700657894736842, 0.3223684210526315, 0.35559210526315793, 0.3618421052631579, 0.3427631578947368, 0.3861842105263158, 0.3473684210526316, 0.38618421052631574, 0.38881578947368417, 0.39802631578947373, 0.3871710526315789, 0.39769736842105263, 0.38388157894736846, 0.4125, 0.37203947368421053, 0.38651315789473684, 0.4006578947368421, 0.37335526315789475, 0.4161184210526316, 0.39046052631578954, 0.3980263157894737, 0.4148026315789474, 0.39276315789473687, 0.40328947368421053, 0.4115131578947368, 0.4256578947368421, 0.4141447368421053, 0.4338815789473685, 0.36447368421052634, 0.4220394736842105, 0.31513157894736843] Best val loss: 7.9681236982345585


Current group: 0.4
Epoch [1/80], Training Loss: 35.4501, Validation Loss Current: 7.9688, Validation Loss AVG: 7.9688, lr: 0.001
Epoch [2/80], Training Loss: 33.4658, Validation Loss Current: 7.9602, Validation Loss AVG: 7.9602, lr: 0.001
Epoch [3/80], Training Loss: 34.1932, Validation Loss Current: 8.5461, Validation Loss AVG: 8.5461, lr: 0.001
Epoch [4/80], Training Loss: 36.2385, Validation Loss Current: 8.3415, Validation Loss AVG: 8.3415, lr: 0.001
Epoch [5/80], Training Loss: 32.8909, Validation Loss Current: 9.1049, Validation Loss AVG: 9.1049, lr: 0.001
Epoch [6/80], Training Loss: 33.8096, Validation Loss Current: 8.1646, Validation Loss AVG: 8.1646, lr: 0.001
Epoch [7/80], Training Loss: 30.1229, Validation Loss Current: 9.2431, Validation Loss AVG: 9.2431, lr: 0.001
Epoch [8/80], Training Loss: 32.2073, Validation Loss Current: 7.7460, Validation Loss AVG: 7.7460, lr: 0.001
Epoch [9/80], Training Loss: 29.3641, Validation Loss Current: 8.4343, Validation Loss AVG: 8.4343, lr: 0.001
Epoch [10/80], Training Loss: 31.7581, Validation Loss Current: 7.9811, Validation Loss AVG: 7.9811, lr: 0.001
Epoch [11/80], Training Loss: 29.8471, Validation Loss Current: 7.7001, Validation Loss AVG: 7.7001, lr: 0.001
Epoch [12/80], Training Loss: 28.8140, Validation Loss Current: 7.7148, Validation Loss AVG: 7.7148, lr: 0.001
Epoch [13/80], Training Loss: 28.2675, Validation Loss Current: 7.6934, Validation Loss AVG: 7.6934, lr: 0.001
Epoch [14/80], Training Loss: 28.8205, Validation Loss Current: 7.6404, Validation Loss AVG: 7.6404, lr: 0.001
Epoch [15/80], Training Loss: 27.2114, Validation Loss Current: 7.8190, Validation Loss AVG: 7.8190, lr: 0.001
Epoch [16/80], Training Loss: 27.9597, Validation Loss Current: 7.6542, Validation Loss AVG: 7.6542, lr: 0.001
Epoch [17/80], Training Loss: 28.3512, Validation Loss Current: 8.0054, Validation Loss AVG: 8.0054, lr: 0.001
Epoch [18/80], Training Loss: 30.0465, Validation Loss Current: 7.8845, Validation Loss AVG: 7.8845, lr: 0.001
Epoch [19/80], Training Loss: 27.4275, Validation Loss Current: 7.6302, Validation Loss AVG: 7.6302, lr: 0.001
Epoch [20/80], Training Loss: 26.1655, Validation Loss Current: 8.0354, Validation Loss AVG: 8.0354, lr: 0.001
Epoch [21/80], Training Loss: 26.9605, Validation Loss Current: 8.0181, Validation Loss AVG: 8.0181, lr: 0.001
Epoch [22/80], Training Loss: 25.6613, Validation Loss Current: 7.6763, Validation Loss AVG: 7.6763, lr: 0.001
Epoch [23/80], Training Loss: 25.7874, Validation Loss Current: 8.2123, Validation Loss AVG: 8.2123, lr: 0.001
Epoch [24/80], Training Loss: 25.7922, Validation Loss Current: 7.7949, Validation Loss AVG: 7.7949, lr: 0.001
Epoch [25/80], Training Loss: 25.5767, Validation Loss Current: 7.8617, Validation Loss AVG: 7.8617, lr: 0.001
Epoch [26/80], Training Loss: 25.3551, Validation Loss Current: 7.8222, Validation Loss AVG: 7.8222, lr: 0.001
Epoch [27/80], Training Loss: 26.6841, Validation Loss Current: 9.0122, Validation Loss AVG: 9.0122, lr: 0.001
Epoch [28/80], Training Loss: 28.2253, Validation Loss Current: 7.5434, Validation Loss AVG: 7.5434, lr: 0.001
Epoch [29/80], Training Loss: 25.0242, Validation Loss Current: 8.0854, Validation Loss AVG: 8.0854, lr: 0.001
Epoch [30/80], Training Loss: 25.0083, Validation Loss Current: 8.1397, Validation Loss AVG: 8.1397, lr: 0.001
Epoch [31/80], Training Loss: 24.9530, Validation Loss Current: 8.3898, Validation Loss AVG: 8.3898, lr: 0.001
Epoch [32/80], Training Loss: 25.3367, Validation Loss Current: 7.8358, Validation Loss AVG: 7.8358, lr: 0.001
Epoch [33/80], Training Loss: 24.4505, Validation Loss Current: 7.9544, Validation Loss AVG: 7.9544, lr: 0.001
Epoch [34/80], Training Loss: 24.6315, Validation Loss Current: 8.0907, Validation Loss AVG: 8.0907, lr: 0.001
Epoch [35/80], Training Loss: 26.6275, Validation Loss Current: 7.9602, Validation Loss AVG: 7.9602, lr: 0.001
Epoch [36/80], Training Loss: 25.4878, Validation Loss Current: 8.9050, Validation Loss AVG: 8.9050, lr: 0.001
Epoch [37/80], Training Loss: 26.9285, Validation Loss Current: 7.7445, Validation Loss AVG: 7.7445, lr: 0.001
Epoch [38/80], Training Loss: 23.8084, Validation Loss Current: 7.9987, Validation Loss AVG: 7.9987, lr: 0.001
Epoch [39/80], Training Loss: 24.1199, Validation Loss Current: 7.9632, Validation Loss AVG: 7.9632, lr: 0.001
Epoch [40/80], Training Loss: 23.1125, Validation Loss Current: 7.9473, Validation Loss AVG: 7.9473, lr: 0.001
Epoch [41/80], Training Loss: 22.2169, Validation Loss Current: 8.1400, Validation Loss AVG: 8.1400, lr: 0.001
Epoch [42/80], Training Loss: 22.1612, Validation Loss Current: 8.1119, Validation Loss AVG: 8.1119, lr: 0.001
Epoch [43/80], Training Loss: 21.8549, Validation Loss Current: 9.8493, Validation Loss AVG: 9.8493, lr: 0.001
Epoch [44/80], Training Loss: 26.0924, Validation Loss Current: 7.7833, Validation Loss AVG: 7.7833, lr: 0.001
Epoch [45/80], Training Loss: 22.5072, Validation Loss Current: 8.1603, Validation Loss AVG: 8.1603, lr: 0.001
Epoch [46/80], Training Loss: 22.1632, Validation Loss Current: 8.1721, Validation Loss AVG: 8.1721, lr: 0.001
Epoch [47/80], Training Loss: 22.0731, Validation Loss Current: 8.4101, Validation Loss AVG: 8.4101, lr: 0.001
Epoch [48/80], Training Loss: 22.9315, Validation Loss Current: 7.8062, Validation Loss AVG: 7.8062, lr: 0.001
Epoch [49/80], Training Loss: 22.0068, Validation Loss Current: 7.9649, Validation Loss AVG: 7.9649, lr: 0.001
Epoch [50/80], Training Loss: 20.8520, Validation Loss Current: 8.6141, Validation Loss AVG: 8.6141, lr: 0.001
Epoch [51/80], Training Loss: 21.8359, Validation Loss Current: 8.7193, Validation Loss AVG: 8.7193, lr: 0.001
Epoch [52/80], Training Loss: 22.3628, Validation Loss Current: 8.3019, Validation Loss AVG: 8.3019, lr: 0.001
Epoch [53/80], Training Loss: 21.3404, Validation Loss Current: 8.1292, Validation Loss AVG: 8.1292, lr: 0.001
Epoch [54/80], Training Loss: 19.7756, Validation Loss Current: 8.1722, Validation Loss AVG: 8.1722, lr: 0.001
Epoch [55/80], Training Loss: 19.5296, Validation Loss Current: 8.6642, Validation Loss AVG: 8.6642, lr: 0.001
Epoch [56/80], Training Loss: 20.1108, Validation Loss Current: 11.3517, Validation Loss AVG: 11.3517, lr: 0.001
Epoch [57/80], Training Loss: 25.8588, Validation Loss Current: 7.5821, Validation Loss AVG: 7.5821, lr: 0.001
Epoch [58/80], Training Loss: 20.4006, Validation Loss Current: 8.5178, Validation Loss AVG: 8.5178, lr: 0.001
 --- Early Stopped ---
Patch distance: 0.4 finished training. Best epoch: 28 Best val accuracy: [0.4276315789473684, 0.436842105263158, 0.38355263157894737, 0.4, 0.3832236842105263, 0.43059210526315794, 0.3914473684210526, 0.44901315789473684, 0.4029605263157895, 0.4342105263157895, 0.4598684210526316, 0.44342105263157894, 0.45, 0.45, 0.45131578947368417, 0.45394736842105265, 0.4217105263157895, 0.42269736842105265, 0.4671052631578947, 0.44144736842105264, 0.44210526315789467, 0.46118421052631575, 0.4322368421052632, 0.46118421052631575, 0.46842105263157896, 0.45756578947368426, 0.3736842105263158, 0.46282894736842106, 0.4608552631578947, 0.4289473684210526, 0.43486842105263157, 0.46743421052631573, 0.47171052631578947, 0.43815789473684214, 0.44506578947368425, 0.41578947368421054, 0.44605263157894737, 0.4536184210526316, 0.47171052631578947, 0.4648026315789474, 0.4631578947368421, 0.45592105263157895, 0.38815789473684215, 0.4569078947368421, 0.46381578947368424, 0.46282894736842106, 0.43157894736842106, 0.4792763157894736, 0.46644736842105267, 0.43881578947368427, 0.43289473684210533, 0.4519736842105263, 0.4588815789473684, 0.4671052631578947, 0.425, 0.3457236842105263, 0.47960526315789476, 0.4496710526315789] Best val loss: 7.543392586708069


Current group: 0.6
Epoch [1/80], Training Loss: 23.5083, Validation Loss Current: 7.7268, Validation Loss AVG: 7.7268, lr: 0.001
Epoch [2/80], Training Loss: 23.8175, Validation Loss Current: 7.8129, Validation Loss AVG: 7.8129, lr: 0.001
Epoch [3/80], Training Loss: 23.5456, Validation Loss Current: 7.7819, Validation Loss AVG: 7.7819, lr: 0.001
Epoch [4/80], Training Loss: 22.6604, Validation Loss Current: 7.8131, Validation Loss AVG: 7.8131, lr: 0.001
Epoch [5/80], Training Loss: 21.8542, Validation Loss Current: 7.8866, Validation Loss AVG: 7.8866, lr: 0.001
Epoch [6/80], Training Loss: 21.8473, Validation Loss Current: 8.0032, Validation Loss AVG: 8.0032, lr: 0.001
Epoch [7/80], Training Loss: 20.8630, Validation Loss Current: 8.8237, Validation Loss AVG: 8.8237, lr: 0.001
Epoch [8/80], Training Loss: 22.2374, Validation Loss Current: 8.6882, Validation Loss AVG: 8.6882, lr: 0.001
Epoch [9/80], Training Loss: 20.0299, Validation Loss Current: 7.6527, Validation Loss AVG: 7.6527, lr: 0.001
Epoch [10/80], Training Loss: 19.9503, Validation Loss Current: 8.1085, Validation Loss AVG: 8.1085, lr: 0.001
Epoch [11/80], Training Loss: 19.3624, Validation Loss Current: 7.6173, Validation Loss AVG: 7.6173, lr: 0.001
Epoch [12/80], Training Loss: 19.0650, Validation Loss Current: 8.8404, Validation Loss AVG: 8.8404, lr: 0.001
Epoch [13/80], Training Loss: 19.4696, Validation Loss Current: 7.8659, Validation Loss AVG: 7.8659, lr: 0.001
Epoch [14/80], Training Loss: 19.1620, Validation Loss Current: 8.4354, Validation Loss AVG: 8.4354, lr: 0.001
Epoch [15/80], Training Loss: 18.9387, Validation Loss Current: 8.1160, Validation Loss AVG: 8.1160, lr: 0.001
Epoch [16/80], Training Loss: 19.0867, Validation Loss Current: 8.4838, Validation Loss AVG: 8.4838, lr: 0.001
Epoch [17/80], Training Loss: 20.5964, Validation Loss Current: 8.3527, Validation Loss AVG: 8.3527, lr: 0.001
Epoch [18/80], Training Loss: 20.6806, Validation Loss Current: 9.0701, Validation Loss AVG: 9.0701, lr: 0.001
Epoch [19/80], Training Loss: 19.0449, Validation Loss Current: 8.2082, Validation Loss AVG: 8.2082, lr: 0.001
Epoch [20/80], Training Loss: 17.2303, Validation Loss Current: 9.7123, Validation Loss AVG: 9.7123, lr: 0.001
Epoch [21/80], Training Loss: 21.0946, Validation Loss Current: 8.7952, Validation Loss AVG: 8.7952, lr: 0.001
Epoch [22/80], Training Loss: 21.2206, Validation Loss Current: 7.9926, Validation Loss AVG: 7.9926, lr: 0.001
Epoch [23/80], Training Loss: 17.1577, Validation Loss Current: 8.6356, Validation Loss AVG: 8.6356, lr: 0.001
Epoch [24/80], Training Loss: 16.4850, Validation Loss Current: 9.7522, Validation Loss AVG: 9.7522, lr: 0.001
Epoch [25/80], Training Loss: 19.7717, Validation Loss Current: 8.1122, Validation Loss AVG: 8.1122, lr: 0.001
Epoch [26/80], Training Loss: 16.7211, Validation Loss Current: 9.2297, Validation Loss AVG: 9.2297, lr: 0.001
Epoch [27/80], Training Loss: 16.2162, Validation Loss Current: 8.4431, Validation Loss AVG: 8.4431, lr: 0.001
Epoch [28/80], Training Loss: 16.0335, Validation Loss Current: 11.3549, Validation Loss AVG: 11.3549, lr: 0.001
Epoch [29/80], Training Loss: 18.7281, Validation Loss Current: 8.8119, Validation Loss AVG: 8.8119, lr: 0.001
Epoch [30/80], Training Loss: 18.1927, Validation Loss Current: 8.5446, Validation Loss AVG: 8.5446, lr: 0.001
Epoch [31/80], Training Loss: 16.1856, Validation Loss Current: 8.9678, Validation Loss AVG: 8.9678, lr: 0.001
Epoch [32/80], Training Loss: 16.5370, Validation Loss Current: 8.5468, Validation Loss AVG: 8.5468, lr: 0.001
Epoch [33/80], Training Loss: 15.1351, Validation Loss Current: 11.9923, Validation Loss AVG: 11.9923, lr: 0.001
Epoch [34/80], Training Loss: 18.6417, Validation Loss Current: 9.1193, Validation Loss AVG: 9.1193, lr: 0.001
Epoch [35/80], Training Loss: 14.7293, Validation Loss Current: 9.0758, Validation Loss AVG: 9.0758, lr: 0.001
Epoch [36/80], Training Loss: 14.5194, Validation Loss Current: 12.5526, Validation Loss AVG: 12.5526, lr: 0.001
Epoch [37/80], Training Loss: 19.7225, Validation Loss Current: 8.1346, Validation Loss AVG: 8.1346, lr: 0.001
Epoch [38/80], Training Loss: 18.4922, Validation Loss Current: 8.2312, Validation Loss AVG: 8.2312, lr: 0.001
Epoch [39/80], Training Loss: 18.5214, Validation Loss Current: 8.5588, Validation Loss AVG: 8.5588, lr: 0.001
Epoch [40/80], Training Loss: 18.5012, Validation Loss Current: 8.5175, Validation Loss AVG: 8.5175, lr: 0.001
Epoch [41/80], Training Loss: 17.6385, Validation Loss Current: 8.5098, Validation Loss AVG: 8.5098, lr: 0.001
 --- Early Stopped ---
Patch distance: 0.6 finished training. Best epoch: 11 Best val accuracy: [0.47960526315789476, 0.49703947368421053, 0.4894736842105263, 0.4796052631578947, 0.4743421052631579, 0.4865131578947368, 0.43717105263157896, 0.42894736842105263, 0.49967105263157896, 0.49407894736842106, 0.5029605263157895, 0.4625, 0.4825657894736842, 0.4842105263157895, 0.4980263157894737, 0.4786184210526316, 0.4493421052631579, 0.4549342105263158, 0.5052631578947369, 0.45592105263157895, 0.4391447368421053, 0.4805921052631579, 0.4805921052631579, 0.4776315789473684, 0.48157894736842105, 0.4582236842105263, 0.4898026315789473, 0.40855263157894733, 0.48618421052631583, 0.47006578947368427, 0.47105263157894733, 0.4917763157894736, 0.39407894736842103, 0.46282894736842106, 0.47467105263157894, 0.39999999999999997, 0.48190789473684215, 0.4861842105263158, 0.48157894736842105, 0.4743421052631579, 0.47269736842105264] Best val loss: 7.617294239997864


Current group: 1
Epoch [1/80], Training Loss: 24.5823, Validation Loss Current: 6.9233, Validation Loss AVG: 9.0789, lr: 0.001
Epoch [2/80], Training Loss: 23.6532, Validation Loss Current: 6.4212, Validation Loss AVG: 9.0721, lr: 0.001
Epoch [3/80], Training Loss: 21.2927, Validation Loss Current: 6.3974, Validation Loss AVG: 7.9005, lr: 0.001
Epoch [4/80], Training Loss: 19.8231, Validation Loss Current: 6.1637, Validation Loss AVG: 9.2964, lr: 0.001
Epoch [5/80], Training Loss: 20.3275, Validation Loss Current: 6.1470, Validation Loss AVG: 8.0876, lr: 0.001
Epoch [6/80], Training Loss: 22.3220, Validation Loss Current: 6.1856, Validation Loss AVG: 8.6504, lr: 0.001
Epoch [7/80], Training Loss: 20.6367, Validation Loss Current: 6.5864, Validation Loss AVG: 10.2242, lr: 0.001
Epoch [8/80], Training Loss: 19.6612, Validation Loss Current: 5.8048, Validation Loss AVG: 8.2038, lr: 0.001
Epoch [9/80], Training Loss: 18.7888, Validation Loss Current: 6.0504, Validation Loss AVG: 9.5891, lr: 0.001
Epoch [10/80], Training Loss: 18.0162, Validation Loss Current: 5.8030, Validation Loss AVG: 9.5561, lr: 0.001
Epoch [11/80], Training Loss: 18.4225, Validation Loss Current: 5.9176, Validation Loss AVG: 9.6067, lr: 0.001
Epoch [12/80], Training Loss: 18.7024, Validation Loss Current: 6.8033, Validation Loss AVG: 10.7951, lr: 0.001
Epoch [13/80], Training Loss: 21.5598, Validation Loss Current: 5.6980, Validation Loss AVG: 8.3324, lr: 0.001
Epoch [14/80], Training Loss: 17.0180, Validation Loss Current: 5.6628, Validation Loss AVG: 9.6882, lr: 0.001
Epoch [15/80], Training Loss: 18.2848, Validation Loss Current: 10.1762, Validation Loss AVG: 17.4723, lr: 0.001
Epoch [16/80], Training Loss: 28.9810, Validation Loss Current: 6.2366, Validation Loss AVG: 7.9518, lr: 0.001
Epoch [17/80], Training Loss: 22.2212, Validation Loss Current: 6.3056, Validation Loss AVG: 10.1586, lr: 0.001
Epoch [18/80], Training Loss: 19.0284, Validation Loss Current: 5.9315, Validation Loss AVG: 9.5483, lr: 0.001
Epoch [19/80], Training Loss: 18.2492, Validation Loss Current: 5.6268, Validation Loss AVG: 8.7869, lr: 0.001
Epoch [20/80], Training Loss: 17.3767, Validation Loss Current: 5.9697, Validation Loss AVG: 9.0143, lr: 0.001
Epoch [21/80], Training Loss: 16.0636, Validation Loss Current: 5.7057, Validation Loss AVG: 9.7057, lr: 0.001
Epoch [22/80], Training Loss: 15.6469, Validation Loss Current: 5.6134, Validation Loss AVG: 9.3599, lr: 0.001
Epoch [23/80], Training Loss: 14.4701, Validation Loss Current: 6.1367, Validation Loss AVG: 11.2016, lr: 0.001
Epoch [24/80], Training Loss: 16.2218, Validation Loss Current: 5.8238, Validation Loss AVG: 10.4555, lr: 0.001
Epoch [25/80], Training Loss: 15.3083, Validation Loss Current: 6.0845, Validation Loss AVG: 10.6317, lr: 0.001
Epoch [26/80], Training Loss: 15.2672, Validation Loss Current: 5.8211, Validation Loss AVG: 10.6143, lr: 0.001
Epoch [27/80], Training Loss: 12.7818, Validation Loss Current: 5.8158, Validation Loss AVG: 9.4977, lr: 0.001
Epoch [28/80], Training Loss: 12.1248, Validation Loss Current: 5.9151, Validation Loss AVG: 12.2890, lr: 0.001
Epoch [29/80], Training Loss: 13.4193, Validation Loss Current: 8.8789, Validation Loss AVG: 11.8911, lr: 0.001
Epoch [30/80], Training Loss: 18.5480, Validation Loss Current: 5.7997, Validation Loss AVG: 8.7839, lr: 0.001
Epoch [31/80], Training Loss: 16.9927, Validation Loss Current: 7.7116, Validation Loss AVG: 12.6189, lr: 0.001
Epoch [32/80], Training Loss: 25.4176, Validation Loss Current: 7.2305, Validation Loss AVG: 8.6274, lr: 0.001
Epoch [33/80], Training Loss: 20.8861, Validation Loss Current: 6.5256, Validation Loss AVG: 12.3097, lr: 0.001
Epoch [34/80], Training Loss: 17.8008, Validation Loss Current: 5.9146, Validation Loss AVG: 10.6690, lr: 0.001
Epoch [35/80], Training Loss: 14.4275, Validation Loss Current: 5.6098, Validation Loss AVG: 11.3968, lr: 0.001
Epoch [36/80], Training Loss: 13.7915, Validation Loss Current: 6.4331, Validation Loss AVG: 11.2966, lr: 0.001
Epoch [37/80], Training Loss: 17.0404, Validation Loss Current: 7.0190, Validation Loss AVG: 9.1108, lr: 0.001
Epoch [38/80], Training Loss: 18.7414, Validation Loss Current: 5.8004, Validation Loss AVG: 8.8524, lr: 0.001
Epoch [39/80], Training Loss: 14.3570, Validation Loss Current: 6.2098, Validation Loss AVG: 10.6577, lr: 0.001
Epoch [40/80], Training Loss: 13.5810, Validation Loss Current: 6.2360, Validation Loss AVG: 11.2188, lr: 0.001
Epoch [41/80], Training Loss: 11.5015, Validation Loss Current: 6.2845, Validation Loss AVG: 11.6284, lr: 0.001
Epoch [42/80], Training Loss: 11.3171, Validation Loss Current: 6.1756, Validation Loss AVG: 9.4210, lr: 0.001
Epoch [43/80], Training Loss: 12.7855, Validation Loss Current: 6.2657, Validation Loss AVG: 12.7767, lr: 0.001
Epoch [44/80], Training Loss: 13.8819, Validation Loss Current: 5.7820, Validation Loss AVG: 9.9553, lr: 0.001
Epoch [45/80], Training Loss: 11.2163, Validation Loss Current: 6.0846, Validation Loss AVG: 11.9550, lr: 0.001
Epoch [46/80], Training Loss: 9.3364, Validation Loss Current: 6.6960, Validation Loss AVG: 12.7176, lr: 0.001
Epoch [47/80], Training Loss: 9.5426, Validation Loss Current: 7.2117, Validation Loss AVG: 13.5522, lr: 0.001
Epoch [48/80], Training Loss: 13.4855, Validation Loss Current: 6.1704, Validation Loss AVG: 11.5059, lr: 0.001
Epoch [49/80], Training Loss: 9.8701, Validation Loss Current: 6.7349, Validation Loss AVG: 12.4926, lr: 0.001
Epoch [50/80], Training Loss: 10.7049, Validation Loss Current: 7.4493, Validation Loss AVG: 15.3381, lr: 0.001
Epoch [51/80], Training Loss: 11.0490, Validation Loss Current: 7.5888, Validation Loss AVG: 13.1460, lr: 0.001
Epoch [52/80], Training Loss: 14.3371, Validation Loss Current: 6.6856, Validation Loss AVG: 12.9088, lr: 0.001
Epoch [53/80], Training Loss: 10.8602, Validation Loss Current: 6.4194, Validation Loss AVG: 10.8949, lr: 0.001
Epoch [54/80], Training Loss: 10.1331, Validation Loss Current: 6.4624, Validation Loss AVG: 11.3835, lr: 0.001
Epoch [55/80], Training Loss: 8.3707, Validation Loss Current: 7.7414, Validation Loss AVG: 11.7986, lr: 0.001
Epoch [56/80], Training Loss: 9.2174, Validation Loss Current: 6.6788, Validation Loss AVG: 13.1736, lr: 0.001
Epoch [57/80], Training Loss: 8.3647, Validation Loss Current: 7.3515, Validation Loss AVG: 13.3098, lr: 0.001
Epoch [58/80], Training Loss: 8.0724, Validation Loss Current: 9.7883, Validation Loss AVG: 17.5396, lr: 0.001
Epoch [59/80], Training Loss: 14.9651, Validation Loss Current: 6.2729, Validation Loss AVG: 10.9768, lr: 0.001
Epoch [60/80], Training Loss: 8.8965, Validation Loss Current: 6.4284, Validation Loss AVG: 14.8960, lr: 0.001
Epoch [61/80], Training Loss: 6.1794, Validation Loss Current: 7.4394, Validation Loss AVG: 14.6334, lr: 0.001
Epoch [62/80], Training Loss: 5.5819, Validation Loss Current: 7.2920, Validation Loss AVG: 15.7719, lr: 0.001
Epoch [63/80], Training Loss: 6.5123, Validation Loss Current: 9.4029, Validation Loss AVG: 23.6475, lr: 0.001
Epoch [64/80], Training Loss: 13.1237, Validation Loss Current: 6.9070, Validation Loss AVG: 12.3961, lr: 0.001
Epoch [65/80], Training Loss: 6.9647, Validation Loss Current: 7.4952, Validation Loss AVG: 14.5816, lr: 0.001
 --- Early Stopped ---
Patch distance: 1 finished training. Best epoch: 35 Best val accuracy: [0.5230263157894737, 0.5625, 0.5657894736842105, 0.5723684210526315, 0.587171052631579, 0.5838815789473685, 0.5575657894736842, 0.6036184210526315, 0.6019736842105263, 0.5986842105263158, 0.5723684210526315, 0.5707236842105263, 0.6019736842105263, 0.6282894736842105, 0.4375, 0.5674342105263158, 0.5904605263157895, 0.5953947368421053, 0.6200657894736842, 0.569078947368421, 0.6134868421052632, 0.6134868421052632, 0.6101973684210527, 0.6019736842105263, 0.6085526315789473, 0.600328947368421, 0.6217105263157895, 0.631578947368421, 0.5707236842105263, 0.6134868421052632, 0.5345394736842105, 0.5049342105263158, 0.5773026315789473, 0.5641447368421053, 0.6266447368421053, 0.6052631578947368, 0.569078947368421, 0.6414473684210527, 0.6101973684210527, 0.6118421052631579, 0.6414473684210527, 0.6381578947368421, 0.6348684210526315, 0.625, 0.6398026315789473, 0.6365131578947368, 0.5773026315789473, 0.6365131578947368, 0.6118421052631579, 0.6299342105263158, 0.5279605263157895, 0.5970394736842105, 0.6348684210526315, 0.6398026315789473, 0.6200657894736842, 0.6398026315789473, 0.6430921052631579, 0.5953947368421053, 0.6134868421052632, 0.631578947368421, 0.6299342105263158, 0.6447368421052632, 0.6101973684210527, 0.625, 0.6365131578947368] Best val loss: 5.609842479228973


Fold: 3
----- Training alexnet with sequence: [0.2, 0.8, 0.4, 0.6, 1] -----
Current group: 0.2
Epoch [1/80], Training Loss: 41.5631, Validation Loss Current: 10.3796, Validation Loss AVG: 10.3796, lr: 0.001
Epoch [2/80], Training Loss: 41.4204, Validation Loss Current: 10.3428, Validation Loss AVG: 10.3428, lr: 0.001
Epoch [3/80], Training Loss: 41.2259, Validation Loss Current: 10.3025, Validation Loss AVG: 10.3025, lr: 0.001
Epoch [4/80], Training Loss: 41.1111, Validation Loss Current: 10.2522, Validation Loss AVG: 10.2522, lr: 0.001
Epoch [5/80], Training Loss: 40.8130, Validation Loss Current: 10.1904, Validation Loss AVG: 10.1904, lr: 0.001
Epoch [6/80], Training Loss: 40.2797, Validation Loss Current: 10.0922, Validation Loss AVG: 10.0922, lr: 0.001
Epoch [7/80], Training Loss: 40.0780, Validation Loss Current: 10.0355, Validation Loss AVG: 10.0355, lr: 0.001
Epoch [8/80], Training Loss: 40.1834, Validation Loss Current: 10.0365, Validation Loss AVG: 10.0365, lr: 0.001
Epoch [9/80], Training Loss: 40.4571, Validation Loss Current: 10.0484, Validation Loss AVG: 10.0484, lr: 0.001
Epoch [10/80], Training Loss: 40.0115, Validation Loss Current: 10.0444, Validation Loss AVG: 10.0444, lr: 0.001
Epoch [11/80], Training Loss: 39.7710, Validation Loss Current: 10.0312, Validation Loss AVG: 10.0312, lr: 0.001
Epoch [12/80], Training Loss: 40.0179, Validation Loss Current: 10.0285, Validation Loss AVG: 10.0285, lr: 0.001
Epoch [13/80], Training Loss: 40.2539, Validation Loss Current: 10.0456, Validation Loss AVG: 10.0456, lr: 0.001
Epoch [14/80], Training Loss: 39.8724, Validation Loss Current: 10.0541, Validation Loss AVG: 10.0541, lr: 0.001
Epoch [15/80], Training Loss: 39.8583, Validation Loss Current: 10.0354, Validation Loss AVG: 10.0354, lr: 0.001
Epoch [16/80], Training Loss: 39.3595, Validation Loss Current: 10.0248, Validation Loss AVG: 10.0248, lr: 0.001
Epoch [17/80], Training Loss: 40.3632, Validation Loss Current: 10.0481, Validation Loss AVG: 10.0481, lr: 0.001
Epoch [18/80], Training Loss: 40.1447, Validation Loss Current: 10.0648, Validation Loss AVG: 10.0648, lr: 0.001
Epoch [19/80], Training Loss: 40.3511, Validation Loss Current: 10.0415, Validation Loss AVG: 10.0415, lr: 0.001
Epoch [20/80], Training Loss: 39.7291, Validation Loss Current: 10.0401, Validation Loss AVG: 10.0401, lr: 0.001
Epoch [21/80], Training Loss: 40.4438, Validation Loss Current: 10.0458, Validation Loss AVG: 10.0458, lr: 0.001
Epoch [22/80], Training Loss: 39.7680, Validation Loss Current: 10.0713, Validation Loss AVG: 10.0713, lr: 0.001
Epoch [23/80], Training Loss: 40.1504, Validation Loss Current: 10.0369, Validation Loss AVG: 10.0369, lr: 0.001
Epoch [24/80], Training Loss: 40.0791, Validation Loss Current: 10.0587, Validation Loss AVG: 10.0587, lr: 0.001
Epoch [25/80], Training Loss: 39.7090, Validation Loss Current: 10.0452, Validation Loss AVG: 10.0452, lr: 0.001
Epoch [26/80], Training Loss: 40.6784, Validation Loss Current: 10.0559, Validation Loss AVG: 10.0559, lr: 0.001
Epoch [27/80], Training Loss: 40.1247, Validation Loss Current: 10.0791, Validation Loss AVG: 10.0791, lr: 0.001
Epoch [28/80], Training Loss: 40.2419, Validation Loss Current: 10.0336, Validation Loss AVG: 10.0336, lr: 0.001
Epoch [29/80], Training Loss: 39.6428, Validation Loss Current: 10.0516, Validation Loss AVG: 10.0516, lr: 0.001
Epoch [30/80], Training Loss: 39.9325, Validation Loss Current: 10.0239, Validation Loss AVG: 10.0239, lr: 0.001
Epoch [31/80], Training Loss: 40.1167, Validation Loss Current: 10.0470, Validation Loss AVG: 10.0470, lr: 0.001
Epoch [32/80], Training Loss: 39.9398, Validation Loss Current: 10.0409, Validation Loss AVG: 10.0409, lr: 0.001
Epoch [33/80], Training Loss: 40.0213, Validation Loss Current: 10.0410, Validation Loss AVG: 10.0410, lr: 0.001
Epoch [34/80], Training Loss: 39.7575, Validation Loss Current: 10.0300, Validation Loss AVG: 10.0300, lr: 0.001
Epoch [35/80], Training Loss: 39.5578, Validation Loss Current: 10.0211, Validation Loss AVG: 10.0211, lr: 0.001
Epoch [36/80], Training Loss: 39.8604, Validation Loss Current: 10.0407, Validation Loss AVG: 10.0407, lr: 0.001
Epoch [37/80], Training Loss: 40.1664, Validation Loss Current: 10.0388, Validation Loss AVG: 10.0388, lr: 0.001
Epoch [38/80], Training Loss: 40.2057, Validation Loss Current: 10.0637, Validation Loss AVG: 10.0637, lr: 0.001
Epoch [39/80], Training Loss: 39.9950, Validation Loss Current: 10.0382, Validation Loss AVG: 10.0382, lr: 0.001
Epoch [40/80], Training Loss: 39.8369, Validation Loss Current: 10.0563, Validation Loss AVG: 10.0563, lr: 0.001
Epoch [41/80], Training Loss: 39.8833, Validation Loss Current: 10.0278, Validation Loss AVG: 10.0278, lr: 0.001
Epoch [42/80], Training Loss: 39.7338, Validation Loss Current: 10.0221, Validation Loss AVG: 10.0221, lr: 0.001
Epoch [43/80], Training Loss: 40.2632, Validation Loss Current: 10.0631, Validation Loss AVG: 10.0631, lr: 0.001
Epoch [44/80], Training Loss: 39.4462, Validation Loss Current: 10.0166, Validation Loss AVG: 10.0166, lr: 0.001
Epoch [45/80], Training Loss: 39.8198, Validation Loss Current: 10.0648, Validation Loss AVG: 10.0648, lr: 0.001
Epoch [46/80], Training Loss: 40.0451, Validation Loss Current: 10.0282, Validation Loss AVG: 10.0282, lr: 0.001
Epoch [47/80], Training Loss: 39.9196, Validation Loss Current: 10.0311, Validation Loss AVG: 10.0311, lr: 0.001
Epoch [48/80], Training Loss: 39.2943, Validation Loss Current: 10.0065, Validation Loss AVG: 10.0065, lr: 0.001
Epoch [49/80], Training Loss: 39.6714, Validation Loss Current: 10.0332, Validation Loss AVG: 10.0332, lr: 0.001
Epoch [50/80], Training Loss: 39.4230, Validation Loss Current: 10.0326, Validation Loss AVG: 10.0326, lr: 0.001
Epoch [51/80], Training Loss: 39.8386, Validation Loss Current: 10.0415, Validation Loss AVG: 10.0415, lr: 0.001
Epoch [52/80], Training Loss: 39.3554, Validation Loss Current: 9.9770, Validation Loss AVG: 9.9770, lr: 0.001
Epoch [53/80], Training Loss: 39.7072, Validation Loss Current: 10.0374, Validation Loss AVG: 10.0374, lr: 0.001
Epoch [54/80], Training Loss: 39.1763, Validation Loss Current: 10.0066, Validation Loss AVG: 10.0066, lr: 0.001
Epoch [55/80], Training Loss: 39.6502, Validation Loss Current: 10.0598, Validation Loss AVG: 10.0598, lr: 0.001
Epoch [56/80], Training Loss: 39.1740, Validation Loss Current: 9.9676, Validation Loss AVG: 9.9676, lr: 0.001
Epoch [57/80], Training Loss: 39.5112, Validation Loss Current: 9.9786, Validation Loss AVG: 9.9786, lr: 0.001
Epoch [58/80], Training Loss: 39.6278, Validation Loss Current: 9.9917, Validation Loss AVG: 9.9917, lr: 0.001
Epoch [59/80], Training Loss: 39.3923, Validation Loss Current: 9.9528, Validation Loss AVG: 9.9528, lr: 0.001
Epoch [60/80], Training Loss: 39.0754, Validation Loss Current: 9.9312, Validation Loss AVG: 9.9312, lr: 0.001
Epoch [61/80], Training Loss: 39.1493, Validation Loss Current: 9.8760, Validation Loss AVG: 9.8760, lr: 0.001
Epoch [62/80], Training Loss: 39.2506, Validation Loss Current: 9.8959, Validation Loss AVG: 9.8959, lr: 0.001
Epoch [63/80], Training Loss: 38.5941, Validation Loss Current: 9.8588, Validation Loss AVG: 9.8588, lr: 0.001
Epoch [64/80], Training Loss: 39.2051, Validation Loss Current: 9.8338, Validation Loss AVG: 9.8338, lr: 0.001
Epoch [65/80], Training Loss: 39.0205, Validation Loss Current: 9.8705, Validation Loss AVG: 9.8705, lr: 0.001
Epoch [66/80], Training Loss: 38.3029, Validation Loss Current: 9.7925, Validation Loss AVG: 9.7925, lr: 0.001
Epoch [67/80], Training Loss: 38.6870, Validation Loss Current: 9.8074, Validation Loss AVG: 9.8074, lr: 0.001
Epoch [68/80], Training Loss: 38.5787, Validation Loss Current: 9.8191, Validation Loss AVG: 9.8191, lr: 0.001
Epoch [69/80], Training Loss: 38.6776, Validation Loss Current: 9.7977, Validation Loss AVG: 9.7977, lr: 0.001
Epoch [70/80], Training Loss: 38.1319, Validation Loss Current: 9.7494, Validation Loss AVG: 9.7494, lr: 0.001
Epoch [71/80], Training Loss: 38.2670, Validation Loss Current: 9.7279, Validation Loss AVG: 9.7279, lr: 0.001
Epoch [72/80], Training Loss: 37.9483, Validation Loss Current: 9.6793, Validation Loss AVG: 9.6793, lr: 0.001
Epoch [73/80], Training Loss: 38.4077, Validation Loss Current: 9.6960, Validation Loss AVG: 9.6960, lr: 0.001
Epoch [74/80], Training Loss: 37.9791, Validation Loss Current: 9.6972, Validation Loss AVG: 9.6972, lr: 0.001
Epoch [75/80], Training Loss: 36.6252, Validation Loss Current: 9.7545, Validation Loss AVG: 9.7545, lr: 0.001
Epoch [76/80], Training Loss: 37.8975, Validation Loss Current: 9.7751, Validation Loss AVG: 9.7751, lr: 0.001
Epoch [77/80], Training Loss: 38.7856, Validation Loss Current: 9.6093, Validation Loss AVG: 9.6093, lr: 0.001
Epoch [78/80], Training Loss: 38.6708, Validation Loss Current: 9.7019, Validation Loss AVG: 9.7019, lr: 0.001
Epoch [79/80], Training Loss: 37.8405, Validation Loss Current: 9.7096, Validation Loss AVG: 9.7096, lr: 0.001
Epoch [80/80], Training Loss: 37.4657, Validation Loss Current: 9.6224, Validation Loss AVG: 9.6224, lr: 0.001
Patch distance: 0.2 finished training. Best epoch: 77 Best val accuracy: [0.2351973684210526, 0.2351973684210526, 0.2351973684210526, 0.2351973684210526, 0.2351973684210526, 0.2351973684210526, 0.2351973684210526, 0.2351973684210526, 0.2351973684210526, 0.2351973684210526, 0.2351973684210526, 0.2351973684210526, 0.2351973684210526, 0.2351973684210526, 0.2351973684210526, 0.2351973684210526, 0.2351973684210526, 0.2351973684210526, 0.2351973684210526, 0.2351973684210526, 0.2351973684210526, 0.2351973684210526, 0.2351973684210526, 0.2351973684210526, 0.2351973684210526, 0.2351973684210526, 0.2351973684210526, 0.2351973684210526, 0.2351973684210526, 0.2351973684210526, 0.2351973684210526, 0.2351973684210526, 0.2351973684210526, 0.2351973684210526, 0.2351973684210526, 0.2351973684210526, 0.2351973684210526, 0.2351973684210526, 0.2351973684210526, 0.2351973684210526, 0.2351973684210526, 0.2351973684210526, 0.2351973684210526, 0.2351973684210526, 0.2351973684210526, 0.2351973684210526, 0.2351973684210526, 0.2351973684210526, 0.2351973684210526, 0.2351973684210526, 0.2351973684210526, 0.2351973684210526, 0.2351973684210526, 0.2351973684210526, 0.2351973684210526, 0.2351973684210526, 0.2351973684210526, 0.2351973684210526, 0.2351973684210526, 0.2351973684210526, 0.2351973684210526, 0.2351973684210526, 0.2351973684210526, 0.2351973684210526, 0.2351973684210526, 0.2351973684210526, 0.2351973684210526, 0.3026315789473684, 0.3003289473684211, 0.3023026315789473, 0.30164473684210524, 0.3082236842105263, 0.3088815789473684, 0.30493421052631586, 0.30657894736842106, 0.30394736842105263, 0.3226973684210526, 0.3069078947368421, 0.30559210526315794, 0.31940789473684206] Best val loss: 9.609252953529358


Current group: 0.8
Epoch [1/80], Training Loss: 38.2006, Validation Loss Current: 9.5355, Validation Loss AVG: 9.5355, lr: 0.001
Epoch [2/80], Training Loss: 37.5312, Validation Loss Current: 9.8689, Validation Loss AVG: 9.8689, lr: 0.001
Epoch [3/80], Training Loss: 37.4635, Validation Loss Current: 9.8769, Validation Loss AVG: 9.8769, lr: 0.001
Epoch [4/80], Training Loss: 37.1006, Validation Loss Current: 9.5767, Validation Loss AVG: 9.5767, lr: 0.001
Epoch [5/80], Training Loss: 36.7919, Validation Loss Current: 9.6862, Validation Loss AVG: 9.6862, lr: 0.001
Epoch [6/80], Training Loss: 36.4743, Validation Loss Current: 9.5282, Validation Loss AVG: 9.5282, lr: 0.001
Epoch [7/80], Training Loss: 36.8108, Validation Loss Current: 9.3492, Validation Loss AVG: 9.3492, lr: 0.001
Epoch [8/80], Training Loss: 37.1151, Validation Loss Current: 9.3996, Validation Loss AVG: 9.3996, lr: 0.001
Epoch [9/80], Training Loss: 36.3447, Validation Loss Current: 9.4135, Validation Loss AVG: 9.4135, lr: 0.001
Epoch [10/80], Training Loss: 37.1147, Validation Loss Current: 9.3029, Validation Loss AVG: 9.3029, lr: 0.001
Epoch [11/80], Training Loss: 38.2429, Validation Loss Current: 9.9937, Validation Loss AVG: 9.9937, lr: 0.001
Epoch [12/80], Training Loss: 37.6864, Validation Loss Current: 9.3503, Validation Loss AVG: 9.3503, lr: 0.001
Epoch [13/80], Training Loss: 36.0140, Validation Loss Current: 9.3366, Validation Loss AVG: 9.3366, lr: 0.001
Epoch [14/80], Training Loss: 35.8907, Validation Loss Current: 9.5908, Validation Loss AVG: 9.5908, lr: 0.001
Epoch [15/80], Training Loss: 36.5029, Validation Loss Current: 9.1824, Validation Loss AVG: 9.1824, lr: 0.001
Epoch [16/80], Training Loss: 35.0601, Validation Loss Current: 9.1309, Validation Loss AVG: 9.1309, lr: 0.001
Epoch [17/80], Training Loss: 34.7938, Validation Loss Current: 9.1189, Validation Loss AVG: 9.1189, lr: 0.001
Epoch [18/80], Training Loss: 34.8621, Validation Loss Current: 9.1300, Validation Loss AVG: 9.1300, lr: 0.001
Epoch [19/80], Training Loss: 34.7670, Validation Loss Current: 9.4100, Validation Loss AVG: 9.4100, lr: 0.001
Epoch [20/80], Training Loss: 35.1686, Validation Loss Current: 9.0394, Validation Loss AVG: 9.0394, lr: 0.001
Epoch [21/80], Training Loss: 35.1220, Validation Loss Current: 8.9264, Validation Loss AVG: 8.9264, lr: 0.001
Epoch [22/80], Training Loss: 36.1816, Validation Loss Current: 10.1114, Validation Loss AVG: 10.1114, lr: 0.001
Epoch [23/80], Training Loss: 41.9486, Validation Loss Current: 10.5035, Validation Loss AVG: 10.5035, lr: 0.001
Epoch [24/80], Training Loss: 41.2105, Validation Loss Current: 10.8522, Validation Loss AVG: 10.8522, lr: 0.001
Epoch [25/80], Training Loss: 40.6830, Validation Loss Current: 10.4527, Validation Loss AVG: 10.4527, lr: 0.001
Epoch [26/80], Training Loss: 39.5949, Validation Loss Current: 10.3600, Validation Loss AVG: 10.3600, lr: 0.001
Epoch [27/80], Training Loss: 38.5275, Validation Loss Current: 10.4248, Validation Loss AVG: 10.4248, lr: 0.001
Epoch [28/80], Training Loss: 37.9181, Validation Loss Current: 9.6096, Validation Loss AVG: 9.6096, lr: 0.001
Epoch [29/80], Training Loss: 36.5184, Validation Loss Current: 9.1772, Validation Loss AVG: 9.1772, lr: 0.001
Epoch [30/80], Training Loss: 35.5434, Validation Loss Current: 9.4883, Validation Loss AVG: 9.4883, lr: 0.001
Epoch [31/80], Training Loss: 37.2030, Validation Loss Current: 9.5027, Validation Loss AVG: 9.5027, lr: 0.001
Epoch [32/80], Training Loss: 34.7238, Validation Loss Current: 8.9280, Validation Loss AVG: 8.9280, lr: 0.001
Epoch [33/80], Training Loss: 34.2008, Validation Loss Current: 8.9905, Validation Loss AVG: 8.9905, lr: 0.001
Epoch [34/80], Training Loss: 35.2999, Validation Loss Current: 9.1927, Validation Loss AVG: 9.1927, lr: 0.001
Epoch [35/80], Training Loss: 34.9235, Validation Loss Current: 8.7636, Validation Loss AVG: 8.7636, lr: 0.001
Epoch [36/80], Training Loss: 33.2854, Validation Loss Current: 8.6817, Validation Loss AVG: 8.6817, lr: 0.001
Epoch [37/80], Training Loss: 32.9389, Validation Loss Current: 8.5930, Validation Loss AVG: 8.5930, lr: 0.001
Epoch [38/80], Training Loss: 32.1432, Validation Loss Current: 8.5551, Validation Loss AVG: 8.5551, lr: 0.001
Epoch [39/80], Training Loss: 32.1842, Validation Loss Current: 8.5831, Validation Loss AVG: 8.5831, lr: 0.001
Epoch [40/80], Training Loss: 31.7922, Validation Loss Current: 8.8036, Validation Loss AVG: 8.8036, lr: 0.001
Epoch [41/80], Training Loss: 31.6586, Validation Loss Current: 9.7668, Validation Loss AVG: 9.7668, lr: 0.001
Epoch [42/80], Training Loss: 31.5333, Validation Loss Current: 8.7892, Validation Loss AVG: 8.7892, lr: 0.001
Epoch [43/80], Training Loss: 31.1297, Validation Loss Current: 8.5478, Validation Loss AVG: 8.5478, lr: 0.001
Epoch [44/80], Training Loss: 31.6558, Validation Loss Current: 9.1962, Validation Loss AVG: 9.1962, lr: 0.001
Epoch [45/80], Training Loss: 30.8477, Validation Loss Current: 9.0796, Validation Loss AVG: 9.0796, lr: 0.001
Epoch [46/80], Training Loss: 33.1912, Validation Loss Current: 8.7041, Validation Loss AVG: 8.7041, lr: 0.001
Epoch [47/80], Training Loss: 31.8924, Validation Loss Current: 8.1266, Validation Loss AVG: 8.1266, lr: 0.001
Epoch [48/80], Training Loss: 30.1696, Validation Loss Current: 8.3415, Validation Loss AVG: 8.3415, lr: 0.001
Epoch [49/80], Training Loss: 30.4187, Validation Loss Current: 8.0475, Validation Loss AVG: 8.0475, lr: 0.001
Epoch [50/80], Training Loss: 27.9435, Validation Loss Current: 8.1416, Validation Loss AVG: 8.1416, lr: 0.001
Epoch [51/80], Training Loss: 29.1338, Validation Loss Current: 8.2012, Validation Loss AVG: 8.2012, lr: 0.001
Epoch [52/80], Training Loss: 29.4447, Validation Loss Current: 11.2394, Validation Loss AVG: 11.2394, lr: 0.001
Epoch [53/80], Training Loss: 36.1175, Validation Loss Current: 9.6470, Validation Loss AVG: 9.6470, lr: 0.001
Epoch [54/80], Training Loss: 32.5717, Validation Loss Current: 8.5411, Validation Loss AVG: 8.5411, lr: 0.001
Epoch [55/80], Training Loss: 29.5667, Validation Loss Current: 8.1078, Validation Loss AVG: 8.1078, lr: 0.001
Epoch [56/80], Training Loss: 28.9121, Validation Loss Current: 8.7125, Validation Loss AVG: 8.7125, lr: 0.001
Epoch [57/80], Training Loss: 29.0871, Validation Loss Current: 8.3085, Validation Loss AVG: 8.3085, lr: 0.001
Epoch [58/80], Training Loss: 29.6470, Validation Loss Current: 8.5501, Validation Loss AVG: 8.5501, lr: 0.001
Epoch [59/80], Training Loss: 28.5331, Validation Loss Current: 8.1493, Validation Loss AVG: 8.1493, lr: 0.001
Epoch [60/80], Training Loss: 26.9003, Validation Loss Current: 8.1286, Validation Loss AVG: 8.1286, lr: 0.001
Epoch [61/80], Training Loss: 26.5053, Validation Loss Current: 8.1015, Validation Loss AVG: 8.1015, lr: 0.001
Epoch [62/80], Training Loss: 28.0086, Validation Loss Current: 8.2903, Validation Loss AVG: 8.2903, lr: 0.001
Epoch [63/80], Training Loss: 25.7823, Validation Loss Current: 7.8579, Validation Loss AVG: 7.8579, lr: 0.001
Epoch [64/80], Training Loss: 25.7972, Validation Loss Current: 8.1707, Validation Loss AVG: 8.1707, lr: 0.001
Epoch [65/80], Training Loss: 25.3301, Validation Loss Current: 9.4737, Validation Loss AVG: 9.4737, lr: 0.001
Epoch [66/80], Training Loss: 26.7685, Validation Loss Current: 8.0853, Validation Loss AVG: 8.0853, lr: 0.001
Epoch [67/80], Training Loss: 26.6914, Validation Loss Current: 11.0666, Validation Loss AVG: 11.0666, lr: 0.001
Epoch [68/80], Training Loss: 33.1912, Validation Loss Current: 8.8319, Validation Loss AVG: 8.8319, lr: 0.001
Epoch [69/80], Training Loss: 28.1673, Validation Loss Current: 9.5214, Validation Loss AVG: 9.5214, lr: 0.001
Epoch [70/80], Training Loss: 26.0935, Validation Loss Current: 7.8918, Validation Loss AVG: 7.8918, lr: 0.001
Epoch [71/80], Training Loss: 25.6663, Validation Loss Current: 8.0634, Validation Loss AVG: 8.0634, lr: 0.001
Epoch [72/80], Training Loss: 27.6757, Validation Loss Current: 8.3175, Validation Loss AVG: 8.3175, lr: 0.001
Epoch [73/80], Training Loss: 26.0154, Validation Loss Current: 7.7750, Validation Loss AVG: 7.7750, lr: 0.001
Epoch [74/80], Training Loss: 25.1639, Validation Loss Current: 8.6920, Validation Loss AVG: 8.6920, lr: 0.001
Epoch [75/80], Training Loss: 24.8875, Validation Loss Current: 8.0304, Validation Loss AVG: 8.0304, lr: 0.001
Epoch [76/80], Training Loss: 23.4520, Validation Loss Current: 8.6665, Validation Loss AVG: 8.6665, lr: 0.001
Epoch [77/80], Training Loss: 23.7890, Validation Loss Current: 8.2684, Validation Loss AVG: 8.2684, lr: 0.001
Epoch [78/80], Training Loss: 24.1986, Validation Loss Current: 8.2178, Validation Loss AVG: 8.2178, lr: 0.001
Epoch [79/80], Training Loss: 24.3590, Validation Loss Current: 12.7488, Validation Loss AVG: 12.7488, lr: 0.001
Epoch [80/80], Training Loss: 32.0913, Validation Loss Current: 8.4567, Validation Loss AVG: 8.4567, lr: 0.001
Patch distance: 0.8 finished training. Best epoch: 73 Best val accuracy: [0.3282894736842105, 0.2957236842105263, 0.29802631578947364, 0.3203947368421053, 0.3141447368421053, 0.3171052631578947, 0.33848684210526314, 0.32960526315789473, 0.3269736842105263, 0.32993421052631583, 0.2802631578947369, 0.34111842105263157, 0.32532894736842105, 0.3101973684210526, 0.3473684210526316, 0.35197368421052627, 0.35328947368421054, 0.33980263157894736, 0.3243421052631579, 0.3447368421052631, 0.3539473684210527, 0.20296052631578948, 0.14473684210526314, 0.2138157894736842, 0.24013157894736842, 0.2552631578947368, 0.2644736842105263, 0.2944078947368421, 0.35657894736842105, 0.31085526315789475, 0.3092105263157895, 0.3519736842105263, 0.3483552631578948, 0.3348684210526316, 0.36447368421052634, 0.37796052631578947, 0.37763157894736843, 0.3828947368421053, 0.3707236842105263, 0.3549342105263158, 0.2868421052631579, 0.36611842105263154, 0.36546052631578946, 0.33092105263157895, 0.36644736842105263, 0.36875, 0.40493421052631573, 0.38815789473684215, 0.4184210526315789, 0.42006578947368417, 0.3904605263157895, 0.3019736842105263, 0.2447368421052632, 0.37993421052631576, 0.3960526315789474, 0.38026315789473686, 0.39769736842105263, 0.39473684210526316, 0.40855263157894744, 0.4125, 0.4177631578947369, 0.40328947368421053, 0.4269736842105263, 0.4151315789473684, 0.3773026315789474, 0.4207236842105263, 0.31875, 0.3473684210526316, 0.3516447368421053, 0.42960526315789477, 0.42434210526315785, 0.4197368421052632, 0.45197368421052636, 0.42467105263157895, 0.4427631578947368, 0.41743421052631585, 0.4302631578947368, 0.4391447368421053, 0.2881578947368421, 0.38125000000000003] Best val loss: 7.774969959259034


Current group: 0.4
Epoch [1/80], Training Loss: 33.2930, Validation Loss Current: 7.5087, Validation Loss AVG: 7.5087, lr: 0.001
Epoch [2/80], Training Loss: 30.6723, Validation Loss Current: 7.4923, Validation Loss AVG: 7.4923, lr: 0.001
Epoch [3/80], Training Loss: 29.3484, Validation Loss Current: 7.6442, Validation Loss AVG: 7.6442, lr: 0.001
Epoch [4/80], Training Loss: 28.8261, Validation Loss Current: 7.4572, Validation Loss AVG: 7.4572, lr: 0.001
Epoch [5/80], Training Loss: 28.1119, Validation Loss Current: 8.0510, Validation Loss AVG: 8.0510, lr: 0.001
Epoch [6/80], Training Loss: 29.5828, Validation Loss Current: 7.8186, Validation Loss AVG: 7.8186, lr: 0.001
Epoch [7/80], Training Loss: 28.5318, Validation Loss Current: 8.1674, Validation Loss AVG: 8.1674, lr: 0.001
Epoch [8/80], Training Loss: 29.5604, Validation Loss Current: 7.7752, Validation Loss AVG: 7.7752, lr: 0.001
Epoch [9/80], Training Loss: 27.9737, Validation Loss Current: 7.4255, Validation Loss AVG: 7.4255, lr: 0.001
Epoch [10/80], Training Loss: 28.0086, Validation Loss Current: 7.8009, Validation Loss AVG: 7.8009, lr: 0.001
Epoch [11/80], Training Loss: 28.6257, Validation Loss Current: 7.3762, Validation Loss AVG: 7.3762, lr: 0.001
Epoch [12/80], Training Loss: 27.5547, Validation Loss Current: 7.3441, Validation Loss AVG: 7.3441, lr: 0.001
Epoch [13/80], Training Loss: 27.8157, Validation Loss Current: 7.2558, Validation Loss AVG: 7.2558, lr: 0.001
Epoch [14/80], Training Loss: 27.3684, Validation Loss Current: 7.4489, Validation Loss AVG: 7.4489, lr: 0.001
Epoch [15/80], Training Loss: 28.7053, Validation Loss Current: 7.4167, Validation Loss AVG: 7.4167, lr: 0.001
Epoch [16/80], Training Loss: 27.2043, Validation Loss Current: 7.3713, Validation Loss AVG: 7.3713, lr: 0.001
Epoch [17/80], Training Loss: 27.4577, Validation Loss Current: 7.4394, Validation Loss AVG: 7.4394, lr: 0.001
Epoch [18/80], Training Loss: 27.6497, Validation Loss Current: 7.3523, Validation Loss AVG: 7.3523, lr: 0.001
Epoch [19/80], Training Loss: 27.8411, Validation Loss Current: 7.3827, Validation Loss AVG: 7.3827, lr: 0.001
Epoch [20/80], Training Loss: 26.6872, Validation Loss Current: 7.5679, Validation Loss AVG: 7.5679, lr: 0.001
Epoch [21/80], Training Loss: 26.1682, Validation Loss Current: 7.7130, Validation Loss AVG: 7.7130, lr: 0.001
Epoch [22/80], Training Loss: 26.1474, Validation Loss Current: 7.3236, Validation Loss AVG: 7.3236, lr: 0.001
Epoch [23/80], Training Loss: 24.3411, Validation Loss Current: 8.3129, Validation Loss AVG: 8.3129, lr: 0.001
Epoch [24/80], Training Loss: 28.3210, Validation Loss Current: 7.4126, Validation Loss AVG: 7.4126, lr: 0.001
Epoch [25/80], Training Loss: 27.0551, Validation Loss Current: 7.4314, Validation Loss AVG: 7.4314, lr: 0.001
Epoch [26/80], Training Loss: 25.9119, Validation Loss Current: 7.5590, Validation Loss AVG: 7.5590, lr: 0.001
Epoch [27/80], Training Loss: 24.7869, Validation Loss Current: 7.2481, Validation Loss AVG: 7.2481, lr: 0.001
Epoch [28/80], Training Loss: 25.9440, Validation Loss Current: 8.5229, Validation Loss AVG: 8.5229, lr: 0.001
Epoch [29/80], Training Loss: 28.0491, Validation Loss Current: 7.2054, Validation Loss AVG: 7.2054, lr: 0.001
Epoch [30/80], Training Loss: 26.0025, Validation Loss Current: 8.2345, Validation Loss AVG: 8.2345, lr: 0.001
Epoch [31/80], Training Loss: 29.4713, Validation Loss Current: 7.9292, Validation Loss AVG: 7.9292, lr: 0.001
Epoch [32/80], Training Loss: 25.6559, Validation Loss Current: 7.3763, Validation Loss AVG: 7.3763, lr: 0.001
Epoch [33/80], Training Loss: 27.0374, Validation Loss Current: 7.6466, Validation Loss AVG: 7.6466, lr: 0.001
Epoch [34/80], Training Loss: 27.9380, Validation Loss Current: 7.6021, Validation Loss AVG: 7.6021, lr: 0.001
Epoch [35/80], Training Loss: 24.7983, Validation Loss Current: 7.9176, Validation Loss AVG: 7.9176, lr: 0.001
Epoch [36/80], Training Loss: 23.5781, Validation Loss Current: 7.5598, Validation Loss AVG: 7.5598, lr: 0.001
Epoch [37/80], Training Loss: 22.7580, Validation Loss Current: 7.6991, Validation Loss AVG: 7.6991, lr: 0.001
Epoch [38/80], Training Loss: 24.2671, Validation Loss Current: 7.3441, Validation Loss AVG: 7.3441, lr: 0.001
Epoch [39/80], Training Loss: 22.4102, Validation Loss Current: 7.3671, Validation Loss AVG: 7.3671, lr: 0.001
Epoch [40/80], Training Loss: 21.1073, Validation Loss Current: 7.4539, Validation Loss AVG: 7.4539, lr: 0.001
Epoch [41/80], Training Loss: 21.2814, Validation Loss Current: 7.9999, Validation Loss AVG: 7.9999, lr: 0.001
Epoch [42/80], Training Loss: 22.6080, Validation Loss Current: 7.4201, Validation Loss AVG: 7.4201, lr: 0.001
Epoch [43/80], Training Loss: 23.2638, Validation Loss Current: 7.5361, Validation Loss AVG: 7.5361, lr: 0.001
Epoch [44/80], Training Loss: 22.6024, Validation Loss Current: 7.4127, Validation Loss AVG: 7.4127, lr: 0.001
Epoch [45/80], Training Loss: 21.6819, Validation Loss Current: 7.4092, Validation Loss AVG: 7.4092, lr: 0.001
Epoch [46/80], Training Loss: 20.2083, Validation Loss Current: 7.5949, Validation Loss AVG: 7.5949, lr: 0.001
Epoch [47/80], Training Loss: 20.3995, Validation Loss Current: 7.4870, Validation Loss AVG: 7.4870, lr: 0.001
Epoch [48/80], Training Loss: 20.1338, Validation Loss Current: 9.6833, Validation Loss AVG: 9.6833, lr: 0.001
Epoch [49/80], Training Loss: 24.4963, Validation Loss Current: 7.5860, Validation Loss AVG: 7.5860, lr: 0.001
Epoch [50/80], Training Loss: 22.1863, Validation Loss Current: 7.6063, Validation Loss AVG: 7.6063, lr: 0.001
Epoch [51/80], Training Loss: 21.8092, Validation Loss Current: 8.3268, Validation Loss AVG: 8.3268, lr: 0.001
Epoch [52/80], Training Loss: 24.8298, Validation Loss Current: 7.3106, Validation Loss AVG: 7.3106, lr: 0.001
Epoch [53/80], Training Loss: 20.6999, Validation Loss Current: 7.5626, Validation Loss AVG: 7.5626, lr: 0.001
Epoch [54/80], Training Loss: 20.2977, Validation Loss Current: 7.7357, Validation Loss AVG: 7.7357, lr: 0.001
Epoch [55/80], Training Loss: 21.3091, Validation Loss Current: 7.7697, Validation Loss AVG: 7.7697, lr: 0.001
Epoch [56/80], Training Loss: 19.7553, Validation Loss Current: 7.6195, Validation Loss AVG: 7.6195, lr: 0.001
Epoch [57/80], Training Loss: 18.9609, Validation Loss Current: 8.6531, Validation Loss AVG: 8.6531, lr: 0.001
Epoch [58/80], Training Loss: 18.6800, Validation Loss Current: 7.5320, Validation Loss AVG: 7.5320, lr: 0.001
Epoch [59/80], Training Loss: 17.0923, Validation Loss Current: 7.5515, Validation Loss AVG: 7.5515, lr: 0.001
 --- Early Stopped ---
Patch distance: 0.4 finished training. Best epoch: 29 Best val accuracy: [0.46019736842105263, 0.4513157894736842, 0.46611842105263157, 0.4756578947368421, 0.4319078947368421, 0.4365131578947368, 0.4509868421052632, 0.44835526315789476, 0.4756578947368421, 0.4625, 0.4667763157894737, 0.48914473684210524, 0.48848684210526316, 0.4621710526315789, 0.47072368421052635, 0.46940789473684214, 0.46315789473684205, 0.49046052631578946, 0.47565789473684206, 0.4740131578947369, 0.4805921052631579, 0.48355263157894735, 0.42796052631578946, 0.46447368421052626, 0.4861842105263158, 0.4667763157894737, 0.4999999999999999, 0.42434210526315785, 0.4842105263157895, 0.43125, 0.43355263157894736, 0.48519736842105254, 0.4618421052631579, 0.4707236842105263, 0.47368421052631576, 0.4677631578947368, 0.4838815789473684, 0.48190789473684215, 0.4858552631578947, 0.49046052631578946, 0.4733552631578948, 0.48914473684210524, 0.4934210526315789, 0.500328947368421, 0.5023026315789474, 0.5069078947368421, 0.4878289473684211, 0.4411184210526316, 0.4838815789473684, 0.47697368421052627, 0.46644736842105267, 0.4809210526315789, 0.48519736842105265, 0.49243421052631575, 0.4753289473684211, 0.5036184210526315, 0.46809210526315786, 0.49802631578947365, 0.5118421052631579] Best val loss: 7.205365371704102


Current group: 0.6
Epoch [1/80], Training Loss: 21.0366, Validation Loss Current: 7.4225, Validation Loss AVG: 7.4225, lr: 0.001
Epoch [2/80], Training Loss: 21.4683, Validation Loss Current: 7.5448, Validation Loss AVG: 7.5448, lr: 0.001
Epoch [3/80], Training Loss: 21.1910, Validation Loss Current: 7.3754, Validation Loss AVG: 7.3754, lr: 0.001
Epoch [4/80], Training Loss: 20.1564, Validation Loss Current: 7.7668, Validation Loss AVG: 7.7668, lr: 0.001
Epoch [5/80], Training Loss: 20.1583, Validation Loss Current: 8.3197, Validation Loss AVG: 8.3197, lr: 0.001
Epoch [6/80], Training Loss: 19.9534, Validation Loss Current: 7.3784, Validation Loss AVG: 7.3784, lr: 0.001
Epoch [7/80], Training Loss: 18.1849, Validation Loss Current: 8.0284, Validation Loss AVG: 8.0284, lr: 0.001
Epoch [8/80], Training Loss: 17.7011, Validation Loss Current: 8.3449, Validation Loss AVG: 8.3449, lr: 0.001
Epoch [9/80], Training Loss: 18.8968, Validation Loss Current: 8.4150, Validation Loss AVG: 8.4150, lr: 0.001
Epoch [10/80], Training Loss: 17.8650, Validation Loss Current: 8.3958, Validation Loss AVG: 8.3958, lr: 0.001
Epoch [11/80], Training Loss: 17.0173, Validation Loss Current: 8.9894, Validation Loss AVG: 8.9894, lr: 0.001
Epoch [12/80], Training Loss: 17.4034, Validation Loss Current: 8.1176, Validation Loss AVG: 8.1176, lr: 0.001
Epoch [13/80], Training Loss: 18.9642, Validation Loss Current: 8.3272, Validation Loss AVG: 8.3272, lr: 0.001
Epoch [14/80], Training Loss: 20.7640, Validation Loss Current: 8.2053, Validation Loss AVG: 8.2053, lr: 0.001
Epoch [15/80], Training Loss: 17.2438, Validation Loss Current: 8.2289, Validation Loss AVG: 8.2289, lr: 0.001
Epoch [16/80], Training Loss: 15.9648, Validation Loss Current: 8.2775, Validation Loss AVG: 8.2775, lr: 0.001
Epoch [17/80], Training Loss: 15.5345, Validation Loss Current: 9.5856, Validation Loss AVG: 9.5856, lr: 0.001
Epoch [18/80], Training Loss: 18.6655, Validation Loss Current: 7.5562, Validation Loss AVG: 7.5562, lr: 0.001
Epoch [19/80], Training Loss: 16.6683, Validation Loss Current: 8.0762, Validation Loss AVG: 8.0762, lr: 0.001
Epoch [20/80], Training Loss: 16.0770, Validation Loss Current: 7.9926, Validation Loss AVG: 7.9926, lr: 0.001
Epoch [21/80], Training Loss: 15.0530, Validation Loss Current: 9.1277, Validation Loss AVG: 9.1277, lr: 0.001
Epoch [22/80], Training Loss: 14.4732, Validation Loss Current: 8.6811, Validation Loss AVG: 8.6811, lr: 0.001
Epoch [23/80], Training Loss: 14.7897, Validation Loss Current: 8.6741, Validation Loss AVG: 8.6741, lr: 0.001
Epoch [24/80], Training Loss: 13.1530, Validation Loss Current: 9.7908, Validation Loss AVG: 9.7908, lr: 0.001
Epoch [25/80], Training Loss: 12.6664, Validation Loss Current: 10.9980, Validation Loss AVG: 10.9980, lr: 0.001
Epoch [26/80], Training Loss: 17.1074, Validation Loss Current: 8.9774, Validation Loss AVG: 8.9774, lr: 0.001
Epoch [27/80], Training Loss: 15.9999, Validation Loss Current: 8.1483, Validation Loss AVG: 8.1483, lr: 0.001
Epoch [28/80], Training Loss: 13.6718, Validation Loss Current: 8.6253, Validation Loss AVG: 8.6253, lr: 0.001
Epoch [29/80], Training Loss: 14.9057, Validation Loss Current: 9.2241, Validation Loss AVG: 9.2241, lr: 0.001
Epoch [30/80], Training Loss: 12.4668, Validation Loss Current: 9.3646, Validation Loss AVG: 9.3646, lr: 0.001
Epoch [31/80], Training Loss: 12.0902, Validation Loss Current: 9.0537, Validation Loss AVG: 9.0537, lr: 0.001
Epoch [32/80], Training Loss: 10.5672, Validation Loss Current: 9.7196, Validation Loss AVG: 9.7196, lr: 0.001
Epoch [33/80], Training Loss: 9.9950, Validation Loss Current: 11.4257, Validation Loss AVG: 11.4257, lr: 0.001
 --- Early Stopped ---
Patch distance: 0.6 finished training. Best epoch: 3 Best val accuracy: [0.5, 0.5023026315789474, 0.5072368421052631, 0.50625, 0.4819078947368422, 0.5125, 0.5006578947368421, 0.4822368421052632, 0.4805921052631579, 0.49375, 0.47664473684210523, 0.4973684210526315, 0.49407894736842106, 0.506578947368421, 0.5009868421052631, 0.5046052631578947, 0.46381578947368424, 0.5128289473684211, 0.506578947368421, 0.5164473684210525, 0.4990131578947368, 0.5161184210526315, 0.5082236842105263, 0.48355263157894735, 0.47993421052631574, 0.5115131578947368, 0.5167763157894737, 0.5055921052631579, 0.5042763157894737, 0.5042763157894737, 0.5200657894736842, 0.5194078947368421, 0.47993421052631574] Best val loss: 7.375443434715271


Current group: 1
Epoch [1/80], Training Loss: 22.1470, Validation Loss Current: 6.1409, Validation Loss AVG: 8.5173, lr: 0.001
Epoch [2/80], Training Loss: 20.1378, Validation Loss Current: 5.8102, Validation Loss AVG: 9.0760, lr: 0.001
Epoch [3/80], Training Loss: 20.1429, Validation Loss Current: 7.9732, Validation Loss AVG: 11.1666, lr: 0.001
Epoch [4/80], Training Loss: 25.0442, Validation Loss Current: 6.6914, Validation Loss AVG: 12.1128, lr: 0.001
Epoch [5/80], Training Loss: 25.7932, Validation Loss Current: 7.0169, Validation Loss AVG: 8.5070, lr: 0.001
Epoch [6/80], Training Loss: 21.1120, Validation Loss Current: 5.9627, Validation Loss AVG: 8.1370, lr: 0.001
Epoch [7/80], Training Loss: 19.8854, Validation Loss Current: 6.0199, Validation Loss AVG: 7.7989, lr: 0.001
Epoch [8/80], Training Loss: 17.1933, Validation Loss Current: 6.6031, Validation Loss AVG: 8.3030, lr: 0.001
Epoch [9/80], Training Loss: 16.8169, Validation Loss Current: 7.1095, Validation Loss AVG: 8.6715, lr: 0.001
Epoch [10/80], Training Loss: 18.8087, Validation Loss Current: 5.9507, Validation Loss AVG: 8.3898, lr: 0.001
Epoch [11/80], Training Loss: 17.7231, Validation Loss Current: 8.4170, Validation Loss AVG: 10.4224, lr: 0.001
Epoch [12/80], Training Loss: 22.7417, Validation Loss Current: 6.2632, Validation Loss AVG: 10.0561, lr: 0.001
Epoch [13/80], Training Loss: 20.0747, Validation Loss Current: 6.3913, Validation Loss AVG: 8.7797, lr: 0.001
Epoch [14/80], Training Loss: 17.7199, Validation Loss Current: 6.1290, Validation Loss AVG: 10.3846, lr: 0.001
Epoch [15/80], Training Loss: 17.3178, Validation Loss Current: 5.8358, Validation Loss AVG: 9.1914, lr: 0.001
Epoch [16/80], Training Loss: 16.3802, Validation Loss Current: 6.5585, Validation Loss AVG: 9.6082, lr: 0.001
Epoch [17/80], Training Loss: 16.9609, Validation Loss Current: 5.4568, Validation Loss AVG: 9.4947, lr: 0.001
Epoch [18/80], Training Loss: 13.9929, Validation Loss Current: 5.8874, Validation Loss AVG: 10.7470, lr: 0.001
Epoch [19/80], Training Loss: 13.0115, Validation Loss Current: 5.5967, Validation Loss AVG: 11.7708, lr: 0.001
Epoch [20/80], Training Loss: 12.6580, Validation Loss Current: 5.7895, Validation Loss AVG: 10.8217, lr: 0.001
Epoch [21/80], Training Loss: 12.0239, Validation Loss Current: 6.1838, Validation Loss AVG: 11.4654, lr: 0.001
Epoch [22/80], Training Loss: 12.9771, Validation Loss Current: 6.5865, Validation Loss AVG: 11.1462, lr: 0.001
Epoch [23/80], Training Loss: 14.6364, Validation Loss Current: 6.3573, Validation Loss AVG: 10.7168, lr: 0.001
Epoch [24/80], Training Loss: 16.1724, Validation Loss Current: 6.9763, Validation Loss AVG: 9.3545, lr: 0.001
Epoch [25/80], Training Loss: 13.9819, Validation Loss Current: 5.8433, Validation Loss AVG: 10.1880, lr: 0.001
Epoch [26/80], Training Loss: 15.4684, Validation Loss Current: 5.7218, Validation Loss AVG: 10.2798, lr: 0.001
Epoch [27/80], Training Loss: 11.9555, Validation Loss Current: 5.8158, Validation Loss AVG: 11.8045, lr: 0.001
Epoch [28/80], Training Loss: 10.5836, Validation Loss Current: 5.9897, Validation Loss AVG: 13.7563, lr: 0.001
Epoch [29/80], Training Loss: 11.8254, Validation Loss Current: 5.7007, Validation Loss AVG: 11.1583, lr: 0.001
Epoch [30/80], Training Loss: 11.6785, Validation Loss Current: 11.0592, Validation Loss AVG: 19.5781, lr: 0.001
Epoch [31/80], Training Loss: 24.8554, Validation Loss Current: 7.2963, Validation Loss AVG: 11.5044, lr: 0.001
Epoch [32/80], Training Loss: 15.9775, Validation Loss Current: 5.9393, Validation Loss AVG: 10.4934, lr: 0.001
Epoch [33/80], Training Loss: 12.1553, Validation Loss Current: 5.9872, Validation Loss AVG: 11.4115, lr: 0.001
Epoch [34/80], Training Loss: 10.5905, Validation Loss Current: 6.1932, Validation Loss AVG: 12.3769, lr: 0.001
Epoch [35/80], Training Loss: 10.5589, Validation Loss Current: 6.7829, Validation Loss AVG: 12.9282, lr: 0.001
Epoch [36/80], Training Loss: 9.2696, Validation Loss Current: 6.1535, Validation Loss AVG: 13.3761, lr: 0.001
Epoch [37/80], Training Loss: 8.7245, Validation Loss Current: 6.2503, Validation Loss AVG: 14.7350, lr: 0.001
Epoch [38/80], Training Loss: 9.2450, Validation Loss Current: 7.3625, Validation Loss AVG: 12.5759, lr: 0.001
Epoch [39/80], Training Loss: 9.0516, Validation Loss Current: 6.0275, Validation Loss AVG: 13.8599, lr: 0.001
Epoch [40/80], Training Loss: 6.9028, Validation Loss Current: 6.7167, Validation Loss AVG: 14.5089, lr: 0.001
Epoch [41/80], Training Loss: 6.1735, Validation Loss Current: 6.1423, Validation Loss AVG: 15.7716, lr: 0.001
Epoch [42/80], Training Loss: 5.7571, Validation Loss Current: 7.8795, Validation Loss AVG: 18.2809, lr: 0.001
Epoch [43/80], Training Loss: 5.8160, Validation Loss Current: 6.8294, Validation Loss AVG: 16.7106, lr: 0.001
Epoch [44/80], Training Loss: 4.9090, Validation Loss Current: 7.6175, Validation Loss AVG: 17.8848, lr: 0.001
Epoch [45/80], Training Loss: 5.6657, Validation Loss Current: 7.8137, Validation Loss AVG: 16.3238, lr: 0.001
Epoch [46/80], Training Loss: 7.5770, Validation Loss Current: 7.1857, Validation Loss AVG: 15.6755, lr: 0.001
Epoch [47/80], Training Loss: 5.0968, Validation Loss Current: 7.8861, Validation Loss AVG: 17.9533, lr: 0.001
 --- Early Stopped ---
Patch distance: 1 finished training. Best epoch: 17 Best val accuracy: [0.5838815789473685, 0.5756578947368421, 0.45394736842105265, 0.5296052631578947, 0.5411184210526315, 0.5855263157894737, 0.6052631578947368, 0.5805921052631579, 0.5855263157894737, 0.5970394736842105, 0.5641447368421053, 0.5838815789473685, 0.5740131578947368, 0.5805921052631579, 0.6036184210526315, 0.5888157894736842, 0.631578947368421, 0.6348684210526315, 0.6332236842105263, 0.6381578947368421, 0.6447368421052632, 0.6167763157894737, 0.6200657894736842, 0.6134868421052632, 0.6266447368421053, 0.6348684210526315, 0.6447368421052632, 0.6578947368421053, 0.6398026315789473, 0.5032894736842105, 0.4917763157894737, 0.618421052631579, 0.6529605263157895, 0.6299342105263158, 0.6085526315789473, 0.6414473684210527, 0.6414473684210527, 0.6217105263157895, 0.6694078947368421, 0.6430921052631579, 0.6463815789473685, 0.631578947368421, 0.6480263157894737, 0.649671052631579, 0.631578947368421, 0.6282894736842105, 0.6414473684210527] Best val loss: 5.456779718399048


Fold: 4
----- Training alexnet with sequence: [0.2, 0.8, 0.4, 0.6, 1] -----
Current group: 0.2
Epoch [1/80], Training Loss: 41.4949, Validation Loss Current: 10.3578, Validation Loss AVG: 10.3578, lr: 0.001
Epoch [2/80], Training Loss: 41.3986, Validation Loss Current: 10.3254, Validation Loss AVG: 10.3254, lr: 0.001
Epoch [3/80], Training Loss: 41.1894, Validation Loss Current: 10.2925, Validation Loss AVG: 10.2925, lr: 0.001
Epoch [4/80], Training Loss: 41.0869, Validation Loss Current: 10.2469, Validation Loss AVG: 10.2469, lr: 0.001
Epoch [5/80], Training Loss: 40.9400, Validation Loss Current: 10.2077, Validation Loss AVG: 10.2077, lr: 0.001
Epoch [6/80], Training Loss: 40.9239, Validation Loss Current: 10.1504, Validation Loss AVG: 10.1504, lr: 0.001
Epoch [7/80], Training Loss: 40.1836, Validation Loss Current: 10.0793, Validation Loss AVG: 10.0793, lr: 0.001
Epoch [8/80], Training Loss: 40.1530, Validation Loss Current: 9.9849, Validation Loss AVG: 9.9849, lr: 0.001
Epoch [9/80], Training Loss: 39.7417, Validation Loss Current: 10.0021, Validation Loss AVG: 10.0021, lr: 0.001
Epoch [10/80], Training Loss: 40.3271, Validation Loss Current: 10.0065, Validation Loss AVG: 10.0065, lr: 0.001
Epoch [11/80], Training Loss: 40.1653, Validation Loss Current: 10.0089, Validation Loss AVG: 10.0089, lr: 0.001
Epoch [12/80], Training Loss: 40.2862, Validation Loss Current: 9.9950, Validation Loss AVG: 9.9950, lr: 0.001
Epoch [13/80], Training Loss: 40.1306, Validation Loss Current: 10.0016, Validation Loss AVG: 10.0016, lr: 0.001
Epoch [14/80], Training Loss: 40.0465, Validation Loss Current: 10.0015, Validation Loss AVG: 10.0015, lr: 0.001
Epoch [15/80], Training Loss: 40.1672, Validation Loss Current: 10.0099, Validation Loss AVG: 10.0099, lr: 0.001
Epoch [16/80], Training Loss: 40.3416, Validation Loss Current: 10.0274, Validation Loss AVG: 10.0274, lr: 0.001
Epoch [17/80], Training Loss: 39.9734, Validation Loss Current: 10.0136, Validation Loss AVG: 10.0136, lr: 0.001
Epoch [18/80], Training Loss: 40.6606, Validation Loss Current: 9.9991, Validation Loss AVG: 9.9991, lr: 0.001
Epoch [19/80], Training Loss: 39.7111, Validation Loss Current: 10.0363, Validation Loss AVG: 10.0363, lr: 0.001
Epoch [20/80], Training Loss: 40.1380, Validation Loss Current: 9.9800, Validation Loss AVG: 9.9800, lr: 0.001
Epoch [21/80], Training Loss: 39.9712, Validation Loss Current: 10.0174, Validation Loss AVG: 10.0174, lr: 0.001
Epoch [22/80], Training Loss: 40.5320, Validation Loss Current: 9.9920, Validation Loss AVG: 9.9920, lr: 0.001
Epoch [23/80], Training Loss: 39.9833, Validation Loss Current: 10.0275, Validation Loss AVG: 10.0275, lr: 0.001
Epoch [24/80], Training Loss: 39.8865, Validation Loss Current: 10.0068, Validation Loss AVG: 10.0068, lr: 0.001
Epoch [25/80], Training Loss: 39.8678, Validation Loss Current: 10.0085, Validation Loss AVG: 10.0085, lr: 0.001
Epoch [26/80], Training Loss: 40.3717, Validation Loss Current: 10.0215, Validation Loss AVG: 10.0215, lr: 0.001
Epoch [27/80], Training Loss: 40.1929, Validation Loss Current: 10.0390, Validation Loss AVG: 10.0390, lr: 0.001
Epoch [28/80], Training Loss: 40.2798, Validation Loss Current: 10.0181, Validation Loss AVG: 10.0181, lr: 0.001
Epoch [29/80], Training Loss: 40.0532, Validation Loss Current: 10.0136, Validation Loss AVG: 10.0136, lr: 0.001
Epoch [30/80], Training Loss: 40.0934, Validation Loss Current: 10.0039, Validation Loss AVG: 10.0039, lr: 0.001
Epoch [31/80], Training Loss: 40.0555, Validation Loss Current: 10.0158, Validation Loss AVG: 10.0158, lr: 0.001
Epoch [32/80], Training Loss: 39.7230, Validation Loss Current: 10.0109, Validation Loss AVG: 10.0109, lr: 0.001
Epoch [33/80], Training Loss: 39.7893, Validation Loss Current: 10.0091, Validation Loss AVG: 10.0091, lr: 0.001
Epoch [34/80], Training Loss: 40.2081, Validation Loss Current: 10.0215, Validation Loss AVG: 10.0215, lr: 0.001
Epoch [35/80], Training Loss: 39.7801, Validation Loss Current: 10.0120, Validation Loss AVG: 10.0120, lr: 0.001
Epoch [36/80], Training Loss: 39.9409, Validation Loss Current: 10.0009, Validation Loss AVG: 10.0009, lr: 0.001
Epoch [37/80], Training Loss: 40.3399, Validation Loss Current: 10.0207, Validation Loss AVG: 10.0207, lr: 0.001
Epoch [38/80], Training Loss: 39.9940, Validation Loss Current: 10.0331, Validation Loss AVG: 10.0331, lr: 0.001
Epoch [39/80], Training Loss: 39.8532, Validation Loss Current: 10.0288, Validation Loss AVG: 10.0288, lr: 0.001
Epoch [40/80], Training Loss: 40.2228, Validation Loss Current: 10.0284, Validation Loss AVG: 10.0284, lr: 0.001
Epoch [41/80], Training Loss: 39.6394, Validation Loss Current: 10.0372, Validation Loss AVG: 10.0372, lr: 0.001
Epoch [42/80], Training Loss: 40.0108, Validation Loss Current: 10.0376, Validation Loss AVG: 10.0376, lr: 0.001
Epoch [43/80], Training Loss: 39.2894, Validation Loss Current: 10.0192, Validation Loss AVG: 10.0192, lr: 0.001
Epoch [44/80], Training Loss: 40.0416, Validation Loss Current: 10.0530, Validation Loss AVG: 10.0530, lr: 0.001
Epoch [45/80], Training Loss: 39.7843, Validation Loss Current: 10.0286, Validation Loss AVG: 10.0286, lr: 0.001
Epoch [46/80], Training Loss: 40.1051, Validation Loss Current: 10.0581, Validation Loss AVG: 10.0581, lr: 0.001
Epoch [47/80], Training Loss: 39.8943, Validation Loss Current: 10.0619, Validation Loss AVG: 10.0619, lr: 0.001
Epoch [48/80], Training Loss: 39.7656, Validation Loss Current: 10.0489, Validation Loss AVG: 10.0489, lr: 0.001
Epoch [49/80], Training Loss: 39.8337, Validation Loss Current: 10.0711, Validation Loss AVG: 10.0711, lr: 0.001
Epoch [50/80], Training Loss: 40.0360, Validation Loss Current: 10.0612, Validation Loss AVG: 10.0612, lr: 0.001
 --- Early Stopped ---
Patch distance: 0.2 finished training. Best epoch: 20 Best val accuracy: [0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421] Best val loss: 9.979979085922242


Current group: 0.8
Epoch [1/80], Training Loss: 40.6946, Validation Loss Current: 10.0028, Validation Loss AVG: 10.0028, lr: 0.001
Epoch [2/80], Training Loss: 40.2076, Validation Loss Current: 10.0190, Validation Loss AVG: 10.0190, lr: 0.001
Epoch [3/80], Training Loss: 39.6380, Validation Loss Current: 9.9924, Validation Loss AVG: 9.9924, lr: 0.001
Epoch [4/80], Training Loss: 39.3538, Validation Loss Current: 9.9896, Validation Loss AVG: 9.9896, lr: 0.001
Epoch [5/80], Training Loss: 39.5224, Validation Loss Current: 9.9535, Validation Loss AVG: 9.9535, lr: 0.001
Epoch [6/80], Training Loss: 39.2604, Validation Loss Current: 9.9589, Validation Loss AVG: 9.9589, lr: 0.001
Epoch [7/80], Training Loss: 39.6525, Validation Loss Current: 9.9042, Validation Loss AVG: 9.9042, lr: 0.001
Epoch [8/80], Training Loss: 39.1697, Validation Loss Current: 9.9736, Validation Loss AVG: 9.9736, lr: 0.001
Epoch [9/80], Training Loss: 39.2535, Validation Loss Current: 9.9007, Validation Loss AVG: 9.9007, lr: 0.001
Epoch [10/80], Training Loss: 39.3972, Validation Loss Current: 9.9065, Validation Loss AVG: 9.9065, lr: 0.001
Epoch [11/80], Training Loss: 39.4961, Validation Loss Current: 9.8851, Validation Loss AVG: 9.8851, lr: 0.001
Epoch [12/80], Training Loss: 38.9112, Validation Loss Current: 9.8939, Validation Loss AVG: 9.8939, lr: 0.001
Epoch [13/80], Training Loss: 38.3039, Validation Loss Current: 9.9405, Validation Loss AVG: 9.9405, lr: 0.001
Epoch [14/80], Training Loss: 39.0571, Validation Loss Current: 9.7171, Validation Loss AVG: 9.7171, lr: 0.001
Epoch [15/80], Training Loss: 38.8460, Validation Loss Current: 9.6571, Validation Loss AVG: 9.6571, lr: 0.001
Epoch [16/80], Training Loss: 38.4391, Validation Loss Current: 9.6416, Validation Loss AVG: 9.6416, lr: 0.001
Epoch [17/80], Training Loss: 38.4031, Validation Loss Current: 9.5636, Validation Loss AVG: 9.5636, lr: 0.001
Epoch [18/80], Training Loss: 38.1434, Validation Loss Current: 9.6125, Validation Loss AVG: 9.6125, lr: 0.001
Epoch [19/80], Training Loss: 37.8339, Validation Loss Current: 9.5102, Validation Loss AVG: 9.5102, lr: 0.001
Epoch [20/80], Training Loss: 36.2670, Validation Loss Current: 10.4177, Validation Loss AVG: 10.4177, lr: 0.001
Epoch [21/80], Training Loss: 38.5120, Validation Loss Current: 9.5413, Validation Loss AVG: 9.5413, lr: 0.001
Epoch [22/80], Training Loss: 39.6498, Validation Loss Current: 9.4887, Validation Loss AVG: 9.4887, lr: 0.001
Epoch [23/80], Training Loss: 38.3819, Validation Loss Current: 9.5342, Validation Loss AVG: 9.5342, lr: 0.001
Epoch [24/80], Training Loss: 37.5609, Validation Loss Current: 9.3491, Validation Loss AVG: 9.3491, lr: 0.001
Epoch [25/80], Training Loss: 36.8593, Validation Loss Current: 9.3460, Validation Loss AVG: 9.3460, lr: 0.001
Epoch [26/80], Training Loss: 36.5545, Validation Loss Current: 9.2233, Validation Loss AVG: 9.2233, lr: 0.001
Epoch [27/80], Training Loss: 36.2218, Validation Loss Current: 9.2342, Validation Loss AVG: 9.2342, lr: 0.001
Epoch [28/80], Training Loss: 35.7419, Validation Loss Current: 9.1385, Validation Loss AVG: 9.1385, lr: 0.001
Epoch [29/80], Training Loss: 36.0107, Validation Loss Current: 9.0754, Validation Loss AVG: 9.0754, lr: 0.001
Epoch [30/80], Training Loss: 35.8188, Validation Loss Current: 8.9689, Validation Loss AVG: 8.9689, lr: 0.001
Epoch [31/80], Training Loss: 34.6781, Validation Loss Current: 8.9712, Validation Loss AVG: 8.9712, lr: 0.001
Epoch [32/80], Training Loss: 35.8793, Validation Loss Current: 8.8948, Validation Loss AVG: 8.8948, lr: 0.001
Epoch [33/80], Training Loss: 34.0657, Validation Loss Current: 8.8956, Validation Loss AVG: 8.8956, lr: 0.001
Epoch [34/80], Training Loss: 34.9215, Validation Loss Current: 9.7234, Validation Loss AVG: 9.7234, lr: 0.001
Epoch [35/80], Training Loss: 36.4984, Validation Loss Current: 9.5130, Validation Loss AVG: 9.5130, lr: 0.001
Epoch [36/80], Training Loss: 33.8824, Validation Loss Current: 8.7831, Validation Loss AVG: 8.7831, lr: 0.001
Epoch [37/80], Training Loss: 33.0937, Validation Loss Current: 8.6167, Validation Loss AVG: 8.6167, lr: 0.001
Epoch [38/80], Training Loss: 32.7563, Validation Loss Current: 8.6889, Validation Loss AVG: 8.6889, lr: 0.001
Epoch [39/80], Training Loss: 31.1084, Validation Loss Current: 8.6269, Validation Loss AVG: 8.6269, lr: 0.001
Epoch [40/80], Training Loss: 31.5166, Validation Loss Current: 9.6674, Validation Loss AVG: 9.6674, lr: 0.001
Epoch [41/80], Training Loss: 33.1798, Validation Loss Current: 8.6793, Validation Loss AVG: 8.6793, lr: 0.001
Epoch [42/80], Training Loss: 31.1996, Validation Loss Current: 8.9994, Validation Loss AVG: 8.9994, lr: 0.001
Epoch [43/80], Training Loss: 31.1790, Validation Loss Current: 8.7734, Validation Loss AVG: 8.7734, lr: 0.001
Epoch [44/80], Training Loss: 31.2487, Validation Loss Current: 8.6346, Validation Loss AVG: 8.6346, lr: 0.001
Epoch [45/80], Training Loss: 30.6443, Validation Loss Current: 9.6524, Validation Loss AVG: 9.6524, lr: 0.001
Epoch [46/80], Training Loss: 33.8377, Validation Loss Current: 8.4777, Validation Loss AVG: 8.4777, lr: 0.001
Epoch [47/80], Training Loss: 31.5001, Validation Loss Current: 8.6819, Validation Loss AVG: 8.6819, lr: 0.001
Epoch [48/80], Training Loss: 29.6720, Validation Loss Current: 8.5420, Validation Loss AVG: 8.5420, lr: 0.001
Epoch [49/80], Training Loss: 30.2556, Validation Loss Current: 10.4472, Validation Loss AVG: 10.4472, lr: 0.001
Epoch [50/80], Training Loss: 32.7466, Validation Loss Current: 8.1572, Validation Loss AVG: 8.1572, lr: 0.001
Epoch [51/80], Training Loss: 30.5145, Validation Loss Current: 8.6194, Validation Loss AVG: 8.6194, lr: 0.001
Epoch [52/80], Training Loss: 30.6894, Validation Loss Current: 8.3125, Validation Loss AVG: 8.3125, lr: 0.001
Epoch [53/80], Training Loss: 29.9470, Validation Loss Current: 8.3270, Validation Loss AVG: 8.3270, lr: 0.001
Epoch [54/80], Training Loss: 28.4058, Validation Loss Current: 9.4717, Validation Loss AVG: 9.4717, lr: 0.001
Epoch [55/80], Training Loss: 28.3167, Validation Loss Current: 8.7604, Validation Loss AVG: 8.7604, lr: 0.001
Epoch [56/80], Training Loss: 27.9271, Validation Loss Current: 8.1495, Validation Loss AVG: 8.1495, lr: 0.001
Epoch [57/80], Training Loss: 29.3772, Validation Loss Current: 8.1696, Validation Loss AVG: 8.1696, lr: 0.001
Epoch [58/80], Training Loss: 27.7905, Validation Loss Current: 8.9922, Validation Loss AVG: 8.9922, lr: 0.001
Epoch [59/80], Training Loss: 27.9660, Validation Loss Current: 10.5640, Validation Loss AVG: 10.5640, lr: 0.001
Epoch [60/80], Training Loss: 30.7158, Validation Loss Current: 7.9322, Validation Loss AVG: 7.9322, lr: 0.001
Epoch [61/80], Training Loss: 27.9383, Validation Loss Current: 8.0000, Validation Loss AVG: 8.0000, lr: 0.001
Epoch [62/80], Training Loss: 27.1204, Validation Loss Current: 7.9544, Validation Loss AVG: 7.9544, lr: 0.001
Epoch [63/80], Training Loss: 26.7859, Validation Loss Current: 9.9066, Validation Loss AVG: 9.9066, lr: 0.001
Epoch [64/80], Training Loss: 28.6779, Validation Loss Current: 8.5538, Validation Loss AVG: 8.5538, lr: 0.001
Epoch [65/80], Training Loss: 30.0489, Validation Loss Current: 8.4682, Validation Loss AVG: 8.4682, lr: 0.001
Epoch [66/80], Training Loss: 27.4374, Validation Loss Current: 8.9642, Validation Loss AVG: 8.9642, lr: 0.001
Epoch [67/80], Training Loss: 26.5206, Validation Loss Current: 8.5632, Validation Loss AVG: 8.5632, lr: 0.001
Epoch [68/80], Training Loss: 26.2921, Validation Loss Current: 10.8795, Validation Loss AVG: 10.8795, lr: 0.001
Epoch [69/80], Training Loss: 27.1531, Validation Loss Current: 9.5488, Validation Loss AVG: 9.5488, lr: 0.001
Epoch [70/80], Training Loss: 28.5193, Validation Loss Current: 8.1448, Validation Loss AVG: 8.1448, lr: 0.001
Epoch [71/80], Training Loss: 27.1591, Validation Loss Current: 8.5136, Validation Loss AVG: 8.5136, lr: 0.001
Epoch [72/80], Training Loss: 26.5918, Validation Loss Current: 8.5853, Validation Loss AVG: 8.5853, lr: 0.001
Epoch [73/80], Training Loss: 26.5038, Validation Loss Current: 8.4809, Validation Loss AVG: 8.4809, lr: 0.001
Epoch [74/80], Training Loss: 25.7803, Validation Loss Current: 8.1461, Validation Loss AVG: 8.1461, lr: 0.001
Epoch [75/80], Training Loss: 26.4169, Validation Loss Current: 8.0060, Validation Loss AVG: 8.0060, lr: 0.001
Epoch [76/80], Training Loss: 25.6003, Validation Loss Current: 8.6847, Validation Loss AVG: 8.6847, lr: 0.001
Epoch [77/80], Training Loss: 25.6652, Validation Loss Current: 9.8429, Validation Loss AVG: 9.8429, lr: 0.001
Epoch [78/80], Training Loss: 25.4097, Validation Loss Current: 9.3133, Validation Loss AVG: 9.3133, lr: 0.001
Epoch [79/80], Training Loss: 24.7523, Validation Loss Current: 8.1157, Validation Loss AVG: 8.1157, lr: 0.001
Epoch [80/80], Training Loss: 24.8981, Validation Loss Current: 7.9781, Validation Loss AVG: 7.9781, lr: 0.001
Patch distance: 0.8 finished training. Best epoch: 60 Best val accuracy: [0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.27072368421052634, 0.29375, 0.2799342105263158, 0.3194078947368421, 0.3292763157894737, 0.31217105263157896, 0.31184210526315786, 0.30986842105263157, 0.3319078947368421, 0.3292763157894737, 0.3424342105263158, 0.33519736842105263, 0.337828947368421, 0.3299342105263158, 0.3450657894736842, 0.3598684210526316, 0.27269736842105263, 0.3256578947368421, 0.3424342105263158, 0.36019736842105265, 0.34967105263157894, 0.3648026315789473, 0.28388157894736843, 0.34605263157894733, 0.368421052631579, 0.36776315789473685, 0.3661184210526316, 0.3713815789473684, 0.3825657894736842, 0.3509868421052632, 0.3730263157894737, 0.28289473684210525, 0.3996710526315789, 0.3996710526315789, 0.3884868421052632, 0.3907894736842105, 0.3391447368421052, 0.3782894736842105, 0.41381578947368414, 0.4230263157894737, 0.37565789473684214, 0.3180921052631579, 0.42960526315789477, 0.42269736842105265, 0.43157894736842106, 0.34671052631578947, 0.4167763157894737, 0.38782894736842105, 0.37203947368421053, 0.40328947368421053, 0.3282894736842105, 0.36151315789473687, 0.4164473684210527, 0.39769736842105263, 0.39901315789473685, 0.4197368421052632, 0.44506578947368425, 0.4394736842105263, 0.41546052631578945, 0.36282894736842103, 0.3871710526315789, 0.45789473684210524, 0.4532894736842105] Best val loss: 7.932150936126709


Current group: 0.4
Epoch [1/80], Training Loss: 32.5241, Validation Loss Current: 7.8669, Validation Loss AVG: 7.8669, lr: 0.001
Epoch [2/80], Training Loss: 32.1484, Validation Loss Current: 8.1392, Validation Loss AVG: 8.1392, lr: 0.001
Epoch [3/80], Training Loss: 29.4538, Validation Loss Current: 7.5953, Validation Loss AVG: 7.5953, lr: 0.001
Epoch [4/80], Training Loss: 30.7924, Validation Loss Current: 7.4755, Validation Loss AVG: 7.4755, lr: 0.001
Epoch [5/80], Training Loss: 29.6031, Validation Loss Current: 9.0575, Validation Loss AVG: 9.0575, lr: 0.001
Epoch [6/80], Training Loss: 33.4988, Validation Loss Current: 7.7866, Validation Loss AVG: 7.7866, lr: 0.001
Epoch [7/80], Training Loss: 29.8884, Validation Loss Current: 7.6567, Validation Loss AVG: 7.6567, lr: 0.001
Epoch [8/80], Training Loss: 28.9311, Validation Loss Current: 8.6773, Validation Loss AVG: 8.6773, lr: 0.001
Epoch [9/80], Training Loss: 31.8133, Validation Loss Current: 8.0437, Validation Loss AVG: 8.0437, lr: 0.001
Epoch [10/80], Training Loss: 29.3105, Validation Loss Current: 8.1569, Validation Loss AVG: 8.1569, lr: 0.001
Epoch [11/80], Training Loss: 31.4296, Validation Loss Current: 7.7219, Validation Loss AVG: 7.7219, lr: 0.001
Epoch [12/80], Training Loss: 28.1626, Validation Loss Current: 7.6380, Validation Loss AVG: 7.6380, lr: 0.001
Epoch [13/80], Training Loss: 27.6650, Validation Loss Current: 7.8166, Validation Loss AVG: 7.8166, lr: 0.001
Epoch [14/80], Training Loss: 27.7534, Validation Loss Current: 7.5933, Validation Loss AVG: 7.5933, lr: 0.001
Epoch [15/80], Training Loss: 27.6381, Validation Loss Current: 7.3660, Validation Loss AVG: 7.3660, lr: 0.001
Epoch [16/80], Training Loss: 26.7885, Validation Loss Current: 7.3133, Validation Loss AVG: 7.3133, lr: 0.001
Epoch [17/80], Training Loss: 26.3875, Validation Loss Current: 7.5234, Validation Loss AVG: 7.5234, lr: 0.001
Epoch [18/80], Training Loss: 26.6155, Validation Loss Current: 7.4653, Validation Loss AVG: 7.4653, lr: 0.001
Epoch [19/80], Training Loss: 25.9503, Validation Loss Current: 7.3589, Validation Loss AVG: 7.3589, lr: 0.001
Epoch [20/80], Training Loss: 25.7208, Validation Loss Current: 7.5310, Validation Loss AVG: 7.5310, lr: 0.001
Epoch [21/80], Training Loss: 25.5599, Validation Loss Current: 7.4650, Validation Loss AVG: 7.4650, lr: 0.001
Epoch [22/80], Training Loss: 25.3341, Validation Loss Current: 7.6248, Validation Loss AVG: 7.6248, lr: 0.001
Epoch [23/80], Training Loss: 26.6027, Validation Loss Current: 7.2619, Validation Loss AVG: 7.2619, lr: 0.001
Epoch [24/80], Training Loss: 25.1156, Validation Loss Current: 8.0858, Validation Loss AVG: 8.0858, lr: 0.001
Epoch [25/80], Training Loss: 27.2623, Validation Loss Current: 7.8203, Validation Loss AVG: 7.8203, lr: 0.001
Epoch [26/80], Training Loss: 27.4056, Validation Loss Current: 7.3275, Validation Loss AVG: 7.3275, lr: 0.001
Epoch [27/80], Training Loss: 28.1451, Validation Loss Current: 7.1331, Validation Loss AVG: 7.1331, lr: 0.001
Epoch [28/80], Training Loss: 25.3152, Validation Loss Current: 8.0448, Validation Loss AVG: 8.0448, lr: 0.001
Epoch [29/80], Training Loss: 26.7090, Validation Loss Current: 7.4921, Validation Loss AVG: 7.4921, lr: 0.001
Epoch [30/80], Training Loss: 24.5805, Validation Loss Current: 7.5894, Validation Loss AVG: 7.5894, lr: 0.001
Epoch [31/80], Training Loss: 23.4816, Validation Loss Current: 7.4116, Validation Loss AVG: 7.4116, lr: 0.001
Epoch [32/80], Training Loss: 24.8291, Validation Loss Current: 7.8160, Validation Loss AVG: 7.8160, lr: 0.001
Epoch [33/80], Training Loss: 24.1315, Validation Loss Current: 7.3474, Validation Loss AVG: 7.3474, lr: 0.001
Epoch [34/80], Training Loss: 23.2770, Validation Loss Current: 7.6569, Validation Loss AVG: 7.6569, lr: 0.001
Epoch [35/80], Training Loss: 24.9910, Validation Loss Current: 7.6132, Validation Loss AVG: 7.6132, lr: 0.001
Epoch [36/80], Training Loss: 24.4483, Validation Loss Current: 7.5328, Validation Loss AVG: 7.5328, lr: 0.001
Epoch [37/80], Training Loss: 22.3311, Validation Loss Current: 7.4064, Validation Loss AVG: 7.4064, lr: 0.001
Epoch [38/80], Training Loss: 22.9026, Validation Loss Current: 7.5041, Validation Loss AVG: 7.5041, lr: 0.001
Epoch [39/80], Training Loss: 22.4546, Validation Loss Current: 7.5719, Validation Loss AVG: 7.5719, lr: 0.001
Epoch [40/80], Training Loss: 21.9055, Validation Loss Current: 7.5954, Validation Loss AVG: 7.5954, lr: 0.001
Epoch [41/80], Training Loss: 21.0083, Validation Loss Current: 7.8204, Validation Loss AVG: 7.8204, lr: 0.001
Epoch [42/80], Training Loss: 23.8201, Validation Loss Current: 7.7356, Validation Loss AVG: 7.7356, lr: 0.001
Epoch [43/80], Training Loss: 24.1156, Validation Loss Current: 7.5073, Validation Loss AVG: 7.5073, lr: 0.001
Epoch [44/80], Training Loss: 22.2645, Validation Loss Current: 7.7028, Validation Loss AVG: 7.7028, lr: 0.001
Epoch [45/80], Training Loss: 21.2698, Validation Loss Current: 7.8390, Validation Loss AVG: 7.8390, lr: 0.001
Epoch [46/80], Training Loss: 20.3026, Validation Loss Current: 8.1265, Validation Loss AVG: 8.1265, lr: 0.001
Epoch [47/80], Training Loss: 22.4124, Validation Loss Current: 7.5253, Validation Loss AVG: 7.5253, lr: 0.001
Epoch [48/80], Training Loss: 20.7541, Validation Loss Current: 8.0861, Validation Loss AVG: 8.0861, lr: 0.001
Epoch [49/80], Training Loss: 21.5494, Validation Loss Current: 7.8074, Validation Loss AVG: 7.8074, lr: 0.001
Epoch [50/80], Training Loss: 22.5274, Validation Loss Current: 7.7094, Validation Loss AVG: 7.7094, lr: 0.001
Epoch [51/80], Training Loss: 20.5869, Validation Loss Current: 7.9079, Validation Loss AVG: 7.9079, lr: 0.001
Epoch [52/80], Training Loss: 18.9493, Validation Loss Current: 8.6656, Validation Loss AVG: 8.6656, lr: 0.001
Epoch [53/80], Training Loss: 19.6712, Validation Loss Current: 7.8540, Validation Loss AVG: 7.8540, lr: 0.001
Epoch [54/80], Training Loss: 18.7398, Validation Loss Current: 9.2017, Validation Loss AVG: 9.2017, lr: 0.001
Epoch [55/80], Training Loss: 23.4381, Validation Loss Current: 8.2273, Validation Loss AVG: 8.2273, lr: 0.001
Epoch [56/80], Training Loss: 20.3926, Validation Loss Current: 7.6898, Validation Loss AVG: 7.6898, lr: 0.001
Epoch [57/80], Training Loss: 19.2637, Validation Loss Current: 7.8756, Validation Loss AVG: 7.8756, lr: 0.001
 --- Early Stopped ---
Patch distance: 0.4 finished training. Best epoch: 27 Best val accuracy: [0.43256578947368424, 0.42828947368421055, 0.45559210526315785, 0.4707236842105263, 0.4052631578947369, 0.4345394736842104, 0.47105263157894733, 0.425, 0.4256578947368421, 0.43519736842105267, 0.44703947368421054, 0.4509868421052632, 0.44013157894736843, 0.46611842105263157, 0.4809210526315789, 0.488157894736842, 0.4717105263157896, 0.48322368421052636, 0.4881578947368421, 0.48355263157894735, 0.4588815789473683, 0.45230263157894746, 0.4930921052631579, 0.4677631578947368, 0.44243421052631576, 0.4697368421052631, 0.4967105263157895, 0.4634868421052631, 0.4838815789473684, 0.4572368421052631, 0.49539473684210533, 0.4480263157894737, 0.4934210526315789, 0.46875, 0.4625, 0.47171052631578947, 0.48157894736842105, 0.4832236842105263, 0.49802631578947365, 0.4960526315789474, 0.4756578947368421, 0.45756578947368415, 0.475, 0.47828947368421054, 0.45756578947368415, 0.49539473684210533, 0.45625, 0.46842105263157896, 0.4743421052631579, 0.46809210526315786, 0.4509868421052632, 0.4273026315789473, 0.48519736842105277, 0.4667763157894737, 0.4240131578947368, 0.47138157894736843, 0.48026315789473684] Best val loss: 7.133111357688904


Current group: 0.6
Epoch [1/80], Training Loss: 22.6471, Validation Loss Current: 7.7847, Validation Loss AVG: 7.7847, lr: 0.001
Epoch [2/80], Training Loss: 21.7206, Validation Loss Current: 7.5938, Validation Loss AVG: 7.5938, lr: 0.001
Epoch [3/80], Training Loss: 22.3616, Validation Loss Current: 7.4526, Validation Loss AVG: 7.4526, lr: 0.001
Epoch [4/80], Training Loss: 22.5459, Validation Loss Current: 7.9039, Validation Loss AVG: 7.9039, lr: 0.001
Epoch [5/80], Training Loss: 21.3172, Validation Loss Current: 7.6043, Validation Loss AVG: 7.6043, lr: 0.001
Epoch [6/80], Training Loss: 21.4143, Validation Loss Current: 8.4938, Validation Loss AVG: 8.4938, lr: 0.001
Epoch [7/80], Training Loss: 25.1786, Validation Loss Current: 8.1391, Validation Loss AVG: 8.1391, lr: 0.001
Epoch [8/80], Training Loss: 21.0629, Validation Loss Current: 7.6394, Validation Loss AVG: 7.6394, lr: 0.001
Epoch [9/80], Training Loss: 19.2054, Validation Loss Current: 7.6732, Validation Loss AVG: 7.6732, lr: 0.001
Epoch [10/80], Training Loss: 21.1443, Validation Loss Current: 7.8560, Validation Loss AVG: 7.8560, lr: 0.001
Epoch [11/80], Training Loss: 20.7467, Validation Loss Current: 7.7763, Validation Loss AVG: 7.7763, lr: 0.001
Epoch [12/80], Training Loss: 19.9483, Validation Loss Current: 7.9662, Validation Loss AVG: 7.9662, lr: 0.001
Epoch [13/80], Training Loss: 20.0283, Validation Loss Current: 7.5051, Validation Loss AVG: 7.5051, lr: 0.001
Epoch [14/80], Training Loss: 20.7593, Validation Loss Current: 7.5011, Validation Loss AVG: 7.5011, lr: 0.001
Epoch [15/80], Training Loss: 20.5915, Validation Loss Current: 9.3560, Validation Loss AVG: 9.3560, lr: 0.001
Epoch [16/80], Training Loss: 23.2734, Validation Loss Current: 7.7885, Validation Loss AVG: 7.7885, lr: 0.001
Epoch [17/80], Training Loss: 21.2216, Validation Loss Current: 7.8735, Validation Loss AVG: 7.8735, lr: 0.001
Epoch [18/80], Training Loss: 23.9896, Validation Loss Current: 7.8322, Validation Loss AVG: 7.8322, lr: 0.001
Epoch [19/80], Training Loss: 19.3292, Validation Loss Current: 7.2674, Validation Loss AVG: 7.2674, lr: 0.001
Epoch [20/80], Training Loss: 19.4050, Validation Loss Current: 7.9740, Validation Loss AVG: 7.9740, lr: 0.001
Epoch [21/80], Training Loss: 18.4461, Validation Loss Current: 7.8940, Validation Loss AVG: 7.8940, lr: 0.001
Epoch [22/80], Training Loss: 18.0165, Validation Loss Current: 7.8256, Validation Loss AVG: 7.8256, lr: 0.001
Epoch [23/80], Training Loss: 18.8795, Validation Loss Current: 7.9964, Validation Loss AVG: 7.9964, lr: 0.001
Epoch [24/80], Training Loss: 16.7625, Validation Loss Current: 8.2764, Validation Loss AVG: 8.2764, lr: 0.001
Epoch [25/80], Training Loss: 16.5073, Validation Loss Current: 8.2319, Validation Loss AVG: 8.2319, lr: 0.001
Epoch [26/80], Training Loss: 19.8871, Validation Loss Current: 8.8548, Validation Loss AVG: 8.8548, lr: 0.001
Epoch [27/80], Training Loss: 18.7253, Validation Loss Current: 7.8911, Validation Loss AVG: 7.8911, lr: 0.001
Epoch [28/80], Training Loss: 17.0380, Validation Loss Current: 9.1602, Validation Loss AVG: 9.1602, lr: 0.001
Epoch [29/80], Training Loss: 21.8171, Validation Loss Current: 9.3313, Validation Loss AVG: 9.3313, lr: 0.001
Epoch [30/80], Training Loss: 23.4046, Validation Loss Current: 7.7517, Validation Loss AVG: 7.7517, lr: 0.001
Epoch [31/80], Training Loss: 18.7146, Validation Loss Current: 8.3586, Validation Loss AVG: 8.3586, lr: 0.001
Epoch [32/80], Training Loss: 16.7622, Validation Loss Current: 7.8286, Validation Loss AVG: 7.8286, lr: 0.001
Epoch [33/80], Training Loss: 15.7344, Validation Loss Current: 9.2151, Validation Loss AVG: 9.2151, lr: 0.001
Epoch [34/80], Training Loss: 17.2742, Validation Loss Current: 9.0275, Validation Loss AVG: 9.0275, lr: 0.001
Epoch [35/80], Training Loss: 15.1291, Validation Loss Current: 9.3273, Validation Loss AVG: 9.3273, lr: 0.001
Epoch [36/80], Training Loss: 13.5582, Validation Loss Current: 9.3727, Validation Loss AVG: 9.3727, lr: 0.001
Epoch [37/80], Training Loss: 13.0814, Validation Loss Current: 9.7175, Validation Loss AVG: 9.7175, lr: 0.001
Epoch [38/80], Training Loss: 13.5559, Validation Loss Current: 9.8743, Validation Loss AVG: 9.8743, lr: 0.001
Epoch [39/80], Training Loss: 12.9985, Validation Loss Current: 9.3773, Validation Loss AVG: 9.3773, lr: 0.001
Epoch [40/80], Training Loss: 14.7285, Validation Loss Current: 8.8463, Validation Loss AVG: 8.8463, lr: 0.001
Epoch [41/80], Training Loss: 12.1329, Validation Loss Current: 9.0319, Validation Loss AVG: 9.0319, lr: 0.001
Epoch [42/80], Training Loss: 13.9926, Validation Loss Current: 10.1245, Validation Loss AVG: 10.1245, lr: 0.001
Epoch [43/80], Training Loss: 19.8372, Validation Loss Current: 7.7711, Validation Loss AVG: 7.7711, lr: 0.001
Epoch [44/80], Training Loss: 14.0546, Validation Loss Current: 9.5937, Validation Loss AVG: 9.5937, lr: 0.001
Epoch [45/80], Training Loss: 12.5501, Validation Loss Current: 10.4784, Validation Loss AVG: 10.4784, lr: 0.001
Epoch [46/80], Training Loss: 12.1577, Validation Loss Current: 9.9812, Validation Loss AVG: 9.9812, lr: 0.001
Epoch [47/80], Training Loss: 11.1074, Validation Loss Current: 9.6482, Validation Loss AVG: 9.6482, lr: 0.001
Epoch [48/80], Training Loss: 12.6449, Validation Loss Current: 12.2781, Validation Loss AVG: 12.2781, lr: 0.001
Epoch [49/80], Training Loss: 21.0438, Validation Loss Current: 9.1221, Validation Loss AVG: 9.1221, lr: 0.001
 --- Early Stopped ---
Patch distance: 0.6 finished training. Best epoch: 19 Best val accuracy: [0.4848684210526316, 0.5075657894736841, 0.5023026315789474, 0.4805921052631579, 0.5062499999999999, 0.4865131578947368, 0.46414473684210533, 0.5085526315789474, 0.5151315789473685, 0.5121710526315789, 0.506578947368421, 0.4858552631578948, 0.4983552631578948, 0.5023026315789474, 0.41315789473684206, 0.48355263157894735, 0.4967105263157895, 0.47960526315789476, 0.5157894736842106, 0.5197368421052632, 0.4914473684210526, 0.4878289473684211, 0.49934210526315786, 0.5180921052631579, 0.5148026315789475, 0.46447368421052626, 0.5075657894736842, 0.47697368421052627, 0.43125, 0.4809210526315789, 0.4881578947368421, 0.5180921052631579, 0.44013157894736843, 0.49802631578947365, 0.4881578947368421, 0.500328947368421, 0.48881578947368426, 0.49407894736842106, 0.4953947368421052, 0.5141447368421053, 0.518421052631579, 0.4615131578947368, 0.48552631578947364, 0.49539473684210533, 0.49506578947368424, 0.47796052631578945, 0.5032894736842105, 0.4161184210526316, 0.4753289473684211] Best val loss: 7.267387342453003


Current group: 1
Epoch [1/80], Training Loss: 24.6366, Validation Loss Current: 6.3400, Validation Loss AVG: 8.1598, lr: 0.001
Epoch [2/80], Training Loss: 22.8003, Validation Loss Current: 6.1887, Validation Loss AVG: 8.3090, lr: 0.001
Epoch [3/80], Training Loss: 21.1826, Validation Loss Current: 7.1133, Validation Loss AVG: 10.0306, lr: 0.001
Epoch [4/80], Training Loss: 22.2786, Validation Loss Current: 6.1592, Validation Loss AVG: 8.7213, lr: 0.001
Epoch [5/80], Training Loss: 22.3686, Validation Loss Current: 7.1944, Validation Loss AVG: 11.5262, lr: 0.001
Epoch [6/80], Training Loss: 27.4883, Validation Loss Current: 6.3384, Validation Loss AVG: 8.1252, lr: 0.001
Epoch [7/80], Training Loss: 22.6196, Validation Loss Current: 6.0202, Validation Loss AVG: 8.5053, lr: 0.001
Epoch [8/80], Training Loss: 21.3554, Validation Loss Current: 7.8708, Validation Loss AVG: 12.3023, lr: 0.001
Epoch [9/80], Training Loss: 26.1347, Validation Loss Current: 6.2853, Validation Loss AVG: 8.2752, lr: 0.001
Epoch [10/80], Training Loss: 21.1393, Validation Loss Current: 5.9660, Validation Loss AVG: 8.4118, lr: 0.001
Epoch [11/80], Training Loss: 19.0260, Validation Loss Current: 5.9601, Validation Loss AVG: 8.3965, lr: 0.001
Epoch [12/80], Training Loss: 17.7901, Validation Loss Current: 6.1495, Validation Loss AVG: 9.1296, lr: 0.001
Epoch [13/80], Training Loss: 18.1406, Validation Loss Current: 5.8173, Validation Loss AVG: 9.0592, lr: 0.001
Epoch [14/80], Training Loss: 16.3862, Validation Loss Current: 6.2464, Validation Loss AVG: 10.4810, lr: 0.001
Epoch [15/80], Training Loss: 16.1438, Validation Loss Current: 6.1723, Validation Loss AVG: 8.9457, lr: 0.001
Epoch [16/80], Training Loss: 17.3081, Validation Loss Current: 6.5278, Validation Loss AVG: 8.8506, lr: 0.001
Epoch [17/80], Training Loss: 21.2960, Validation Loss Current: 6.2555, Validation Loss AVG: 8.2024, lr: 0.001
Epoch [18/80], Training Loss: 20.2972, Validation Loss Current: 6.1962, Validation Loss AVG: 8.7219, lr: 0.001
Epoch [19/80], Training Loss: 17.1608, Validation Loss Current: 5.7517, Validation Loss AVG: 11.1094, lr: 0.001
Epoch [20/80], Training Loss: 14.9703, Validation Loss Current: 5.7685, Validation Loss AVG: 9.5128, lr: 0.001
Epoch [21/80], Training Loss: 14.3584, Validation Loss Current: 5.8913, Validation Loss AVG: 11.6883, lr: 0.001
Epoch [22/80], Training Loss: 14.2435, Validation Loss Current: 5.8058, Validation Loss AVG: 11.2663, lr: 0.001
Epoch [23/80], Training Loss: 13.7445, Validation Loss Current: 7.0047, Validation Loss AVG: 10.6309, lr: 0.001
Epoch [24/80], Training Loss: 14.1566, Validation Loss Current: 6.1262, Validation Loss AVG: 11.2074, lr: 0.001
Epoch [25/80], Training Loss: 13.6393, Validation Loss Current: 6.8676, Validation Loss AVG: 12.6570, lr: 0.001
Epoch [26/80], Training Loss: 14.0745, Validation Loss Current: 6.1315, Validation Loss AVG: 12.0494, lr: 0.001
Epoch [27/80], Training Loss: 12.0069, Validation Loss Current: 6.2951, Validation Loss AVG: 13.6663, lr: 0.001
Epoch [28/80], Training Loss: 13.8846, Validation Loss Current: 6.5453, Validation Loss AVG: 11.7927, lr: 0.001
Epoch [29/80], Training Loss: 15.7404, Validation Loss Current: 5.9378, Validation Loss AVG: 10.3199, lr: 0.001
Epoch [30/80], Training Loss: 14.1987, Validation Loss Current: 6.2792, Validation Loss AVG: 10.4271, lr: 0.001
Epoch [31/80], Training Loss: 12.3958, Validation Loss Current: 6.3064, Validation Loss AVG: 12.9913, lr: 0.001
Epoch [32/80], Training Loss: 11.6225, Validation Loss Current: 6.7592, Validation Loss AVG: 10.5232, lr: 0.001
Epoch [33/80], Training Loss: 13.3558, Validation Loss Current: 6.3039, Validation Loss AVG: 13.7547, lr: 0.001
Epoch [34/80], Training Loss: 10.2935, Validation Loss Current: 6.3684, Validation Loss AVG: 11.7445, lr: 0.001
Epoch [35/80], Training Loss: 8.6688, Validation Loss Current: 6.8863, Validation Loss AVG: 13.7864, lr: 0.001
Epoch [36/80], Training Loss: 8.8702, Validation Loss Current: 7.6770, Validation Loss AVG: 17.0696, lr: 0.001
Epoch [37/80], Training Loss: 11.8189, Validation Loss Current: 6.0557, Validation Loss AVG: 12.3777, lr: 0.001
Epoch [38/80], Training Loss: 8.6634, Validation Loss Current: 10.5017, Validation Loss AVG: 22.9588, lr: 0.001
Epoch [39/80], Training Loss: 19.8534, Validation Loss Current: 6.0592, Validation Loss AVG: 9.4756, lr: 0.001
Epoch [40/80], Training Loss: 12.4865, Validation Loss Current: 6.5079, Validation Loss AVG: 12.7257, lr: 0.001
Epoch [41/80], Training Loss: 10.5010, Validation Loss Current: 6.6980, Validation Loss AVG: 13.3755, lr: 0.001
Epoch [42/80], Training Loss: 10.1198, Validation Loss Current: 7.4906, Validation Loss AVG: 11.8340, lr: 0.001
Epoch [43/80], Training Loss: 14.5310, Validation Loss Current: 6.5213, Validation Loss AVG: 12.6561, lr: 0.001
Epoch [44/80], Training Loss: 11.3678, Validation Loss Current: 6.2517, Validation Loss AVG: 11.9931, lr: 0.001
Epoch [45/80], Training Loss: 8.0682, Validation Loss Current: 6.9848, Validation Loss AVG: 14.4216, lr: 0.001
Epoch [46/80], Training Loss: 6.6943, Validation Loss Current: 7.1608, Validation Loss AVG: 14.6048, lr: 0.001
Epoch [47/80], Training Loss: 6.2264, Validation Loss Current: 7.6658, Validation Loss AVG: 16.9193, lr: 0.001
Epoch [48/80], Training Loss: 7.0285, Validation Loss Current: 7.6361, Validation Loss AVG: 18.4356, lr: 0.001
Epoch [49/80], Training Loss: 8.2714, Validation Loss Current: 7.0162, Validation Loss AVG: 15.1757, lr: 0.001
 --- Early Stopped ---
Patch distance: 1 finished training. Best epoch: 19 Best val accuracy: [0.5641447368421053, 0.587171052631579, 0.5361842105263158, 0.5970394736842105, 0.5164473684210527, 0.569078947368421, 0.59375, 0.48026315789473684, 0.5953947368421053, 0.6118421052631579, 0.6134868421052632, 0.6233552631578947, 0.6200657894736842, 0.6200657894736842, 0.6167763157894737, 0.5953947368421053, 0.5970394736842105, 0.6151315789473685, 0.6200657894736842, 0.625, 0.649671052631579, 0.6332236842105263, 0.6167763157894737, 0.6365131578947368, 0.6118421052631579, 0.6513157894736842, 0.6200657894736842, 0.6447368421052632, 0.6167763157894737, 0.6200657894736842, 0.649671052631579, 0.6052631578947368, 0.662828947368421, 0.6233552631578947, 0.6546052631578947, 0.5855263157894737, 0.6381578947368421, 0.5361842105263158, 0.5986842105263158, 0.6299342105263158, 0.6480263157894737, 0.6348684210526315, 0.6052631578947368, 0.6365131578947368, 0.6546052631578947, 0.649671052631579, 0.6546052631578947, 0.6644736842105263, 0.662828947368421] Best val loss: 5.751670956611633


-------------------- All training done --------------------


 --- Evaluating ---
Fold: 0
---- Testing model trained on sequence: [0.2, 0.8, 0.4, 0.6, 1] ----
Test set distance: 0.2 Top 1 Accuracy: 0.21695951107715813
Test set distance: 0.8 Top 1 Accuracy: 0.6233766233766234
Test set distance: 0.4 Top 1 Accuracy: 0.3384262796027502
Test set distance: 0.6 Top 1 Accuracy: 0.5156608097784569
Test set distance: 1 Top 1 Accuracy: 0.6348357524828113
Fold: 1
---- Testing model trained on sequence: [0.2, 0.8, 0.4, 0.6, 1] ----
Test set distance: 0.2 Top 1 Accuracy: 0.1306340718105424
Test set distance: 0.8 Top 1 Accuracy: 0.4988540870893812
Test set distance: 0.4 Top 1 Accuracy: 0.25668449197860965
Test set distance: 0.6 Top 1 Accuracy: 0.39877769289533993
Test set distance: 1 Top 1 Accuracy: 0.5515660809778457
Fold: 2
---- Testing model trained on sequence: [0.2, 0.8, 0.4, 0.6, 1] ----
Test set distance: 0.2 Top 1 Accuracy: 0.1779984721161192
Test set distance: 0.8 Top 1 Accuracy: 0.612681436210848
Test set distance: 0.4 Top 1 Accuracy: 0.33689839572192515
Test set distance: 0.6 Top 1 Accuracy: 0.5294117647058824
Test set distance: 1 Top 1 Accuracy: 0.6195569136745608
Fold: 3
---- Testing model trained on sequence: [0.2, 0.8, 0.4, 0.6, 1] ----
Test set distance: 0.2 Top 1 Accuracy: 0.22459893048128343
Test set distance: 0.8 Top 1 Accuracy: 0.6073338426279603
Test set distance: 0.4 Top 1 Accuracy: 0.3284950343773873
Test set distance: 0.6 Top 1 Accuracy: 0.5095492742551566
Test set distance: 1 Top 1 Accuracy: 0.6294881588999236
Fold: 4
---- Testing model trained on sequence: [0.2, 0.8, 0.4, 0.6, 1] ----
Test set distance: 0.2 Top 1 Accuracy: 0.18487394957983194
Test set distance: 0.8 Top 1 Accuracy: 0.640183346065699
Test set distance: 0.4 Top 1 Accuracy: 0.32085561497326204
Test set distance: 0.6 Top 1 Accuracy: 0.5370511841100076
Test set distance: 1 Top 1 Accuracy: 0.6508785332314744
------------------------------ End ------------------------------








