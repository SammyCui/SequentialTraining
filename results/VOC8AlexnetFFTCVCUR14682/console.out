Loading openmpi/cuda/64/3.1.4
  Loading requirement: hpcx/2.4.0
Loading pytorch-py36-cuda10.1-gcc/1.5.0
  Loading requirement: python36 ml-pythondeps-py36-cuda10.1-gcc/3.3.0
    openblas/dynamic/0.2.20 cudnn7.6-cuda10.1/7.6.5.32 hdf5_18/1.8.20
    nccl2-cuda10.1-gcc/2.7.8
['/u/erdos/students/xcui32/cnslab', '/cm/shared/apps/pytorch-py36-cuda10.1-gcc/1.5.0/lib64/python3.6/site-packages', '/cm/shared/apps/pytorch-py36-cuda10.1-gcc/1.5.0/lib/python3.6/site-packages', '/cm/shared/apps/ml-pythondeps-py36-cuda10.1-gcc/safeimport', '/cm/local/apps/cuda/libs/current/pynvml', '/cm/local/apps/python3/lib/python36.zip', '/cm/local/apps/python3/lib/python3.6', '/cm/local/apps/python3/lib/python3.6/lib-dynload', '/u/erdos/students/xcui32/.local/lib/python3.6/site-packages', '/cm/local/apps/python3/lib/python3.6/site-packages', '/usr/local/lib/python3.6/site-packages', '/usr/lib64/python3.6/site-packages', '/u/erdos/students/xcui32/.local/lib/python3.6/site-packages']
Run:  0
 # ------------------ Running pipeline on as_is fft run_0 -------------------- #
cuda:0
 ------ Pipeline with following parameters ------
training_root_path :  /u/erdos/students/xcui32/cnslab/datasets/VOC2012/VOC2012_filtered/train
val_root_path :  /u/erdos/students/xcui32/cnslab/datasets/VOC2012/VOC2012_filtered/val
test_root_path :  /u/erdos/students/xcui32/cnslab/datasets/VOC2012/VOC2012_filtered/test
dataset_name :  VOC
target_distances :  [1, 0.4, 0.6, 0.8, 0.2]
training_mode :  as_is
n_distances :  None
background :  fft
size :  (150, 150)
cls_to_use :  ['aeroplane', 'bicycle', 'bird', 'boat', 'car', 'cat', 'train', 'tvmonitor']
batch_size :  128
val_size :  1
epochs :  400
resize_method :  long
n_folds :  5
val_target :  ('avg',)
num_workers :  16
model_name :  alexnet
device :  cuda:0
random_seed :  40
result_dirpath :  /u/erdos/students/xcui32/cnslab/results/VOC8AlexnetFFTCVCUR14682
save_checkpoints :  False
verbose :  0
 ---  Loading datasets ---
 ---  Running  ---
Fold: 0
----- Training alexnet with sequence: [1, 0.4, 0.6, 0.8, 0.2] -----
Current group: 1
Epoch [1/80], Training Loss: 41.5392, Validation Loss Current: 10.3759, Validation Loss AVG: 10.3757
Epoch [2/80], Training Loss: 41.4323, Validation Loss Current: 10.3434, Validation Loss AVG: 10.3429
Epoch [3/80], Training Loss: 41.3362, Validation Loss Current: 10.3119, Validation Loss AVG: 10.3110
Epoch [4/80], Training Loss: 41.0383, Validation Loss Current: 10.2806, Validation Loss AVG: 10.2808
Epoch [5/80], Training Loss: 41.0425, Validation Loss Current: 10.2380, Validation Loss AVG: 10.2403
Epoch [6/80], Training Loss: 40.9299, Validation Loss Current: 10.2041, Validation Loss AVG: 10.2019
Epoch [7/80], Training Loss: 40.6856, Validation Loss Current: 10.1575, Validation Loss AVG: 10.1540
Epoch [8/80], Training Loss: 40.1401, Validation Loss Current: 10.1094, Validation Loss AVG: 10.1009
Epoch [9/80], Training Loss: 40.6686, Validation Loss Current: 10.0656, Validation Loss AVG: 10.0477
Epoch [10/80], Training Loss: 40.4609, Validation Loss Current: 10.0415, Validation Loss AVG: 10.0260
Epoch [11/80], Training Loss: 40.1332, Validation Loss Current: 10.0117, Validation Loss AVG: 10.0161
Epoch [12/80], Training Loss: 40.4019, Validation Loss Current: 9.9821, Validation Loss AVG: 9.9928
Epoch [13/80], Training Loss: 40.3565, Validation Loss Current: 9.9798, Validation Loss AVG: 9.9769
Epoch [14/80], Training Loss: 40.0320, Validation Loss Current: 9.9487, Validation Loss AVG: 9.9612
Epoch [15/80], Training Loss: 39.4848, Validation Loss Current: 9.9271, Validation Loss AVG: 9.9450
Epoch [16/80], Training Loss: 39.3946, Validation Loss Current: 9.8973, Validation Loss AVG: 9.9153
Epoch [17/80], Training Loss: 39.9006, Validation Loss Current: 9.8595, Validation Loss AVG: 9.8894
Epoch [18/80], Training Loss: 39.2391, Validation Loss Current: 9.8593, Validation Loss AVG: 9.8846
Epoch [19/80], Training Loss: 39.1624, Validation Loss Current: 9.8189, Validation Loss AVG: 9.8468
Epoch [20/80], Training Loss: 39.0741, Validation Loss Current: 9.7911, Validation Loss AVG: 9.8629
Epoch [21/80], Training Loss: 38.9163, Validation Loss Current: 9.7676, Validation Loss AVG: 9.8525
Epoch [22/80], Training Loss: 39.2070, Validation Loss Current: 9.7410, Validation Loss AVG: 9.8415
Epoch [23/80], Training Loss: 38.8359, Validation Loss Current: 9.7135, Validation Loss AVG: 9.8040
Epoch [24/80], Training Loss: 39.2177, Validation Loss Current: 9.7057, Validation Loss AVG: 9.8234
Epoch [25/80], Training Loss: 38.8602, Validation Loss Current: 9.6629, Validation Loss AVG: 9.8182
Epoch [26/80], Training Loss: 38.8098, Validation Loss Current: 9.6309, Validation Loss AVG: 9.7609
Epoch [27/80], Training Loss: 38.3053, Validation Loss Current: 9.6122, Validation Loss AVG: 9.7771
Epoch [28/80], Training Loss: 38.4936, Validation Loss Current: 9.5892, Validation Loss AVG: 9.7386
Epoch [29/80], Training Loss: 38.3388, Validation Loss Current: 9.5077, Validation Loss AVG: 9.7390
Epoch [30/80], Training Loss: 37.6536, Validation Loss Current: 9.5062, Validation Loss AVG: 9.7444
Epoch [31/80], Training Loss: 38.2385, Validation Loss Current: 9.5382, Validation Loss AVG: 9.7209
Epoch [32/80], Training Loss: 37.9625, Validation Loss Current: 9.4415, Validation Loss AVG: 9.7069
Epoch [33/80], Training Loss: 37.5617, Validation Loss Current: 9.3919, Validation Loss AVG: 9.7020
Epoch [34/80], Training Loss: 37.3665, Validation Loss Current: 9.3986, Validation Loss AVG: 9.6580
Epoch [35/80], Training Loss: 37.3334, Validation Loss Current: 9.2769, Validation Loss AVG: 9.6342
Epoch [36/80], Training Loss: 36.3049, Validation Loss Current: 9.2672, Validation Loss AVG: 9.6830
Epoch [37/80], Training Loss: 37.3980, Validation Loss Current: 9.1995, Validation Loss AVG: 9.6015
Epoch [38/80], Training Loss: 36.3034, Validation Loss Current: 9.1482, Validation Loss AVG: 9.5710
Epoch [39/80], Training Loss: 36.2453, Validation Loss Current: 9.1021, Validation Loss AVG: 9.6248
Epoch [40/80], Training Loss: 36.2782, Validation Loss Current: 9.1659, Validation Loss AVG: 9.6185
Epoch [41/80], Training Loss: 35.6832, Validation Loss Current: 8.9173, Validation Loss AVG: 9.4371
Epoch [42/80], Training Loss: 35.1097, Validation Loss Current: 8.8233, Validation Loss AVG: 9.4385
Epoch [43/80], Training Loss: 34.2588, Validation Loss Current: 8.9481, Validation Loss AVG: 9.3926
Epoch [44/80], Training Loss: 33.7325, Validation Loss Current: 8.7869, Validation Loss AVG: 9.2925
Epoch [45/80], Training Loss: 34.8069, Validation Loss Current: 11.4426, Validation Loss AVG: 11.1165
Epoch [46/80], Training Loss: 39.9357, Validation Loss Current: 9.1857, Validation Loss AVG: 9.4732
Epoch [47/80], Training Loss: 35.2269, Validation Loss Current: 8.8120, Validation Loss AVG: 9.2625
Epoch [48/80], Training Loss: 33.6945, Validation Loss Current: 8.7438, Validation Loss AVG: 9.2302
Epoch [49/80], Training Loss: 33.2399, Validation Loss Current: 8.3266, Validation Loss AVG: 9.1433
Epoch [50/80], Training Loss: 32.6359, Validation Loss Current: 8.2294, Validation Loss AVG: 9.2849
Epoch [51/80], Training Loss: 32.1450, Validation Loss Current: 8.3035, Validation Loss AVG: 9.0835
Epoch [52/80], Training Loss: 31.0419, Validation Loss Current: 7.9978, Validation Loss AVG: 9.3072
Epoch [53/80], Training Loss: 31.0690, Validation Loss Current: 8.3329, Validation Loss AVG: 9.2027
Epoch [54/80], Training Loss: 31.1184, Validation Loss Current: 8.0365, Validation Loss AVG: 9.1580
Epoch [55/80], Training Loss: 30.9986, Validation Loss Current: 7.7022, Validation Loss AVG: 9.3163
Epoch [56/80], Training Loss: 29.1849, Validation Loss Current: 7.7882, Validation Loss AVG: 9.3022
Epoch [57/80], Training Loss: 29.7677, Validation Loss Current: 7.7754, Validation Loss AVG: 8.9028
Epoch [58/80], Training Loss: 29.8378, Validation Loss Current: 7.9299, Validation Loss AVG: 9.9826
Epoch [59/80], Training Loss: 28.9613, Validation Loss Current: 7.6998, Validation Loss AVG: 8.9896
Epoch [60/80], Training Loss: 30.5690, Validation Loss Current: 8.0869, Validation Loss AVG: 9.0914
Epoch [61/80], Training Loss: 30.6755, Validation Loss Current: 9.6470, Validation Loss AVG: 10.0206
Epoch [62/80], Training Loss: 31.8687, Validation Loss Current: 7.6309, Validation Loss AVG: 8.9966
Epoch [63/80], Training Loss: 28.9182, Validation Loss Current: 7.5406, Validation Loss AVG: 8.9251
Epoch [64/80], Training Loss: 28.5004, Validation Loss Current: 7.5837, Validation Loss AVG: 8.9679
Epoch [65/80], Training Loss: 28.1128, Validation Loss Current: 7.6259, Validation Loss AVG: 8.6947
Epoch [66/80], Training Loss: 28.4326, Validation Loss Current: 7.4574, Validation Loss AVG: 9.3568
Epoch [67/80], Training Loss: 26.5505, Validation Loss Current: 7.3613, Validation Loss AVG: 9.3614
Epoch [68/80], Training Loss: 27.1393, Validation Loss Current: 7.1639, Validation Loss AVG: 9.4477
Epoch [69/80], Training Loss: 27.1003, Validation Loss Current: 7.6652, Validation Loss AVG: 9.6128
Epoch [70/80], Training Loss: 28.8707, Validation Loss Current: 8.5178, Validation Loss AVG: 9.1275
Epoch [71/80], Training Loss: 29.8380, Validation Loss Current: 7.6622, Validation Loss AVG: 8.8103
Epoch [72/80], Training Loss: 29.8629, Validation Loss Current: 7.2388, Validation Loss AVG: 8.6970
Epoch [73/80], Training Loss: 27.2833, Validation Loss Current: 7.0271, Validation Loss AVG: 8.8657
Epoch [74/80], Training Loss: 26.6971, Validation Loss Current: 7.5739, Validation Loss AVG: 9.6417
Epoch [75/80], Training Loss: 27.3375, Validation Loss Current: 7.0405, Validation Loss AVG: 8.3568
Epoch [76/80], Training Loss: 25.4548, Validation Loss Current: 7.0282, Validation Loss AVG: 8.8256
Epoch [77/80], Training Loss: 25.6551, Validation Loss Current: 8.2917, Validation Loss AVG: 9.2911
Epoch [78/80], Training Loss: 27.7058, Validation Loss Current: 7.5755, Validation Loss AVG: 9.8686
Epoch [79/80], Training Loss: 26.2245, Validation Loss Current: 6.9446, Validation Loss AVG: 8.5306
Epoch [80/80], Training Loss: 24.8505, Validation Loss Current: 6.8178, Validation Loss AVG: 8.9494
Patch distance: 1 finished training. Best epoch: 80 Best val accuracy: 0.3970394736842105 Best val loss: 6.8178324699401855


Current group: 0.4
Epoch [1/80], Training Loss: 34.9692, Validation Loss Current: 9.0988, Validation Loss AVG: 8.9119
Epoch [2/80], Training Loss: 33.7736, Validation Loss Current: 8.2764, Validation Loss AVG: 8.0607
Epoch [3/80], Training Loss: 32.5407, Validation Loss Current: 8.3519, Validation Loss AVG: 8.1226
Epoch [4/80], Training Loss: 32.1131, Validation Loss Current: 8.0025, Validation Loss AVG: 7.9227
Epoch [5/80], Training Loss: 31.7759, Validation Loss Current: 7.9636, Validation Loss AVG: 7.9395
Epoch [6/80], Training Loss: 31.8740, Validation Loss Current: 8.5348, Validation Loss AVG: 8.7278
Epoch [7/80], Training Loss: 31.5136, Validation Loss Current: 8.0072, Validation Loss AVG: 7.8787
Epoch [8/80], Training Loss: 30.4920, Validation Loss Current: 8.4840, Validation Loss AVG: 8.6757
Epoch [9/80], Training Loss: 31.5893, Validation Loss Current: 8.7039, Validation Loss AVG: 8.6726
Epoch [10/80], Training Loss: 31.3553, Validation Loss Current: 8.0186, Validation Loss AVG: 8.1129
Epoch [11/80], Training Loss: 30.6884, Validation Loss Current: 8.0006, Validation Loss AVG: 8.0039
Epoch [12/80], Training Loss: 32.4296, Validation Loss Current: 8.4842, Validation Loss AVG: 8.7305
Epoch [13/80], Training Loss: 33.3824, Validation Loss Current: 7.9337, Validation Loss AVG: 7.9608
Epoch [14/80], Training Loss: 29.8514, Validation Loss Current: 7.8353, Validation Loss AVG: 8.1820
Epoch [15/80], Training Loss: 30.4545, Validation Loss Current: 9.1774, Validation Loss AVG: 8.7663
Epoch [16/80], Training Loss: 32.1826, Validation Loss Current: 7.9833, Validation Loss AVG: 8.0366
Epoch [17/80], Training Loss: 30.9564, Validation Loss Current: 7.6910, Validation Loss AVG: 7.8976
Epoch [18/80], Training Loss: 29.4013, Validation Loss Current: 7.8393, Validation Loss AVG: 7.9433
Epoch [19/80], Training Loss: 28.7942, Validation Loss Current: 7.6478, Validation Loss AVG: 7.8061
Epoch [20/80], Training Loss: 30.4018, Validation Loss Current: 7.9643, Validation Loss AVG: 7.9633
Epoch [21/80], Training Loss: 30.0543, Validation Loss Current: 7.6238, Validation Loss AVG: 7.8188
Epoch [22/80], Training Loss: 27.6153, Validation Loss Current: 7.4881, Validation Loss AVG: 7.6953
Epoch [23/80], Training Loss: 27.6082, Validation Loss Current: 7.6771, Validation Loss AVG: 7.9083
Epoch [24/80], Training Loss: 28.3079, Validation Loss Current: 8.1387, Validation Loss AVG: 8.5990
Epoch [25/80], Training Loss: 30.5680, Validation Loss Current: 7.5538, Validation Loss AVG: 7.8037
Epoch [26/80], Training Loss: 29.1699, Validation Loss Current: 7.6759, Validation Loss AVG: 7.9074
Epoch [27/80], Training Loss: 28.0735, Validation Loss Current: 8.0628, Validation Loss AVG: 8.2557
Epoch [28/80], Training Loss: 28.8978, Validation Loss Current: 7.3854, Validation Loss AVG: 7.6449
Epoch [29/80], Training Loss: 28.7098, Validation Loss Current: 7.9762, Validation Loss AVG: 8.2618
Epoch [30/80], Training Loss: 30.2730, Validation Loss Current: 7.5881, Validation Loss AVG: 7.8881
Epoch [31/80], Training Loss: 27.9575, Validation Loss Current: 7.6049, Validation Loss AVG: 8.0710
Epoch [32/80], Training Loss: 27.6052, Validation Loss Current: 7.8084, Validation Loss AVG: 8.1348
Epoch [33/80], Training Loss: 27.5831, Validation Loss Current: 7.6786, Validation Loss AVG: 8.0843
Epoch [34/80], Training Loss: 27.4274, Validation Loss Current: 8.1967, Validation Loss AVG: 8.5246
Epoch [35/80], Training Loss: 29.7064, Validation Loss Current: 7.7565, Validation Loss AVG: 8.0813
Epoch [36/80], Training Loss: 26.6746, Validation Loss Current: 7.4457, Validation Loss AVG: 7.8617
Epoch [37/80], Training Loss: 25.7235, Validation Loss Current: 7.2372, Validation Loss AVG: 7.7373
Epoch [38/80], Training Loss: 26.0876, Validation Loss Current: 7.4039, Validation Loss AVG: 8.1365
Epoch [39/80], Training Loss: 26.7502, Validation Loss Current: 7.7576, Validation Loss AVG: 8.3028
Epoch [40/80], Training Loss: 27.7532, Validation Loss Current: 7.1427, Validation Loss AVG: 7.5639
Epoch [41/80], Training Loss: 25.0890, Validation Loss Current: 7.6166, Validation Loss AVG: 8.2781
Epoch [42/80], Training Loss: 25.0974, Validation Loss Current: 7.2725, Validation Loss AVG: 7.6924
Epoch [43/80], Training Loss: 24.4056, Validation Loss Current: 7.4288, Validation Loss AVG: 8.1224
Epoch [44/80], Training Loss: 23.2717, Validation Loss Current: 7.2976, Validation Loss AVG: 8.1652
Epoch [45/80], Training Loss: 25.4124, Validation Loss Current: 7.1237, Validation Loss AVG: 7.9179
Epoch [46/80], Training Loss: 26.5652, Validation Loss Current: 7.1247, Validation Loss AVG: 7.8154
Epoch [47/80], Training Loss: 24.5304, Validation Loss Current: 8.5603, Validation Loss AVG: 9.2418
Epoch [48/80], Training Loss: 30.0935, Validation Loss Current: 8.6370, Validation Loss AVG: 9.3528
Epoch [49/80], Training Loss: 30.2867, Validation Loss Current: 7.4783, Validation Loss AVG: 8.3199
Epoch [50/80], Training Loss: 25.9426, Validation Loss Current: 7.4639, Validation Loss AVG: 8.4255
Epoch [51/80], Training Loss: 27.5129, Validation Loss Current: 7.0421, Validation Loss AVG: 7.9449
Epoch [52/80], Training Loss: 25.1965, Validation Loss Current: 7.3341, Validation Loss AVG: 8.3607
Epoch [53/80], Training Loss: 23.6123, Validation Loss Current: 7.4535, Validation Loss AVG: 8.5022
Epoch [54/80], Training Loss: 24.9519, Validation Loss Current: 7.3270, Validation Loss AVG: 8.5114
Epoch [55/80], Training Loss: 23.5489, Validation Loss Current: 7.2054, Validation Loss AVG: 8.1461
Epoch [56/80], Training Loss: 22.6472, Validation Loss Current: 7.3004, Validation Loss AVG: 8.3847
Epoch [57/80], Training Loss: 23.8259, Validation Loss Current: 7.2028, Validation Loss AVG: 8.5138
Epoch [58/80], Training Loss: 22.0607, Validation Loss Current: 7.2479, Validation Loss AVG: 8.2671
Epoch [59/80], Training Loss: 20.3140, Validation Loss Current: 7.4537, Validation Loss AVG: 8.9296
Epoch [60/80], Training Loss: 20.0457, Validation Loss Current: 7.1086, Validation Loss AVG: 8.5172
Epoch [61/80], Training Loss: 19.7315, Validation Loss Current: 7.4931, Validation Loss AVG: 8.6922
Epoch [62/80], Training Loss: 20.8851, Validation Loss Current: 7.9540, Validation Loss AVG: 8.8915
Epoch [63/80], Training Loss: 22.1723, Validation Loss Current: 7.8582, Validation Loss AVG: 9.3486
Epoch [64/80], Training Loss: 23.7522, Validation Loss Current: 7.6373, Validation Loss AVG: 8.3461
Epoch [65/80], Training Loss: 20.2124, Validation Loss Current: 7.3285, Validation Loss AVG: 8.6862
Epoch [66/80], Training Loss: 20.4423, Validation Loss Current: 10.0302, Validation Loss AVG: 11.4026
Epoch [67/80], Training Loss: 26.7562, Validation Loss Current: 7.7390, Validation Loss AVG: 8.6894
Epoch [68/80], Training Loss: 20.8929, Validation Loss Current: 7.1646, Validation Loss AVG: 8.4300
Epoch [69/80], Training Loss: 18.8296, Validation Loss Current: 7.2711, Validation Loss AVG: 8.7168
Epoch [70/80], Training Loss: 18.1292, Validation Loss Current: 7.3405, Validation Loss AVG: 8.7445
Epoch [71/80], Training Loss: 18.0362, Validation Loss Current: 7.6289, Validation Loss AVG: 8.9278
 --- Early Stopped ---
Patch distance: 0.4 finished training. Best epoch: 51 Best val accuracy: 0.44539473684210523 Best val loss: 7.042072534561157


Current group: 0.6
Epoch [1/80], Training Loss: 24.8453, Validation Loss Current: 6.7790, Validation Loss AVG: 7.7941
Epoch [2/80], Training Loss: 25.6602, Validation Loss Current: 6.6827, Validation Loss AVG: 7.7675
Epoch [3/80], Training Loss: 26.1716, Validation Loss Current: 6.8837, Validation Loss AVG: 7.8461
Epoch [4/80], Training Loss: 25.5262, Validation Loss Current: 6.6825, Validation Loss AVG: 7.7485
Epoch [5/80], Training Loss: 22.8313, Validation Loss Current: 6.5623, Validation Loss AVG: 7.6179
Epoch [6/80], Training Loss: 22.4935, Validation Loss Current: 6.9885, Validation Loss AVG: 8.3970
Epoch [7/80], Training Loss: 23.9978, Validation Loss Current: 6.8834, Validation Loss AVG: 8.1035
Epoch [8/80], Training Loss: 24.0771, Validation Loss Current: 6.7461, Validation Loss AVG: 7.7913
Epoch [9/80], Training Loss: 22.0615, Validation Loss Current: 6.3221, Validation Loss AVG: 7.3928
Epoch [10/80], Training Loss: 19.6040, Validation Loss Current: 6.3413, Validation Loss AVG: 7.7064
Epoch [11/80], Training Loss: 18.1984, Validation Loss Current: 6.2636, Validation Loss AVG: 7.4897
Epoch [12/80], Training Loss: 18.8977, Validation Loss Current: 6.9282, Validation Loss AVG: 8.3421
Epoch [13/80], Training Loss: 19.9610, Validation Loss Current: 6.9675, Validation Loss AVG: 8.2811
Epoch [14/80], Training Loss: 19.1363, Validation Loss Current: 6.9626, Validation Loss AVG: 8.8564
Epoch [15/80], Training Loss: 18.4236, Validation Loss Current: 6.5588, Validation Loss AVG: 7.7764
Epoch [16/80], Training Loss: 19.0770, Validation Loss Current: 6.3376, Validation Loss AVG: 7.6986
Epoch [17/80], Training Loss: 17.8492, Validation Loss Current: 6.6102, Validation Loss AVG: 8.1339
Epoch [18/80], Training Loss: 17.1806, Validation Loss Current: 6.8360, Validation Loss AVG: 8.1076
Epoch [19/80], Training Loss: 20.4970, Validation Loss Current: 6.9956, Validation Loss AVG: 8.2184
Epoch [20/80], Training Loss: 20.0019, Validation Loss Current: 7.0448, Validation Loss AVG: 8.3282
Epoch [21/80], Training Loss: 18.7627, Validation Loss Current: 6.5842, Validation Loss AVG: 8.0588
Epoch [22/80], Training Loss: 16.2866, Validation Loss Current: 6.7315, Validation Loss AVG: 8.0852
Epoch [23/80], Training Loss: 16.5908, Validation Loss Current: 6.6388, Validation Loss AVG: 8.2295
Epoch [24/80], Training Loss: 15.7336, Validation Loss Current: 6.4074, Validation Loss AVG: 7.7767
Epoch [25/80], Training Loss: 16.0775, Validation Loss Current: 6.6702, Validation Loss AVG: 8.0032
Epoch [26/80], Training Loss: 15.4130, Validation Loss Current: 6.6290, Validation Loss AVG: 8.3165
Epoch [27/80], Training Loss: 14.8393, Validation Loss Current: 7.0237, Validation Loss AVG: 8.6173
Epoch [28/80], Training Loss: 12.9675, Validation Loss Current: 6.6307, Validation Loss AVG: 8.2991
Epoch [29/80], Training Loss: 12.1160, Validation Loss Current: 7.2125, Validation Loss AVG: 9.0968
Epoch [30/80], Training Loss: 12.4971, Validation Loss Current: 7.2794, Validation Loss AVG: 8.8477
Epoch [31/80], Training Loss: 14.2671, Validation Loss Current: 9.7496, Validation Loss AVG: 11.7144
 --- Early Stopped ---
Patch distance: 0.6 finished training. Best epoch: 11 Best val accuracy: 0.4963815789473684 Best val loss: 6.263565421104431


Current group: 0.8
Epoch [1/80], Training Loss: 26.2570, Validation Loss Current: 6.7256, Validation Loss AVG: 8.0693
Epoch [2/80], Training Loss: 23.5124, Validation Loss Current: 7.1703, Validation Loss AVG: 8.4069
Epoch [3/80], Training Loss: 20.8495, Validation Loss Current: 6.3875, Validation Loss AVG: 7.9792
Epoch [4/80], Training Loss: 17.7726, Validation Loss Current: 6.2045, Validation Loss AVG: 8.0386
Epoch [5/80], Training Loss: 18.1250, Validation Loss Current: 7.1981, Validation Loss AVG: 8.3322
Epoch [6/80], Training Loss: 20.9438, Validation Loss Current: 6.9087, Validation Loss AVG: 8.7535
Epoch [7/80], Training Loss: 18.4761, Validation Loss Current: 6.3324, Validation Loss AVG: 8.4065
Epoch [8/80], Training Loss: 17.3943, Validation Loss Current: 5.9823, Validation Loss AVG: 7.7295
Epoch [9/80], Training Loss: 15.9468, Validation Loss Current: 6.2044, Validation Loss AVG: 8.3719
Epoch [10/80], Training Loss: 15.5383, Validation Loss Current: 6.4698, Validation Loss AVG: 9.0308
Epoch [11/80], Training Loss: 15.0187, Validation Loss Current: 6.4284, Validation Loss AVG: 8.2748
Epoch [12/80], Training Loss: 14.2536, Validation Loss Current: 7.1709, Validation Loss AVG: 8.6492
Epoch [13/80], Training Loss: 13.8508, Validation Loss Current: 6.4702, Validation Loss AVG: 8.3724
Epoch [14/80], Training Loss: 13.9639, Validation Loss Current: 7.2693, Validation Loss AVG: 8.8970
Epoch [15/80], Training Loss: 17.1333, Validation Loss Current: 6.2639, Validation Loss AVG: 8.5691
Epoch [16/80], Training Loss: 12.6431, Validation Loss Current: 6.7716, Validation Loss AVG: 9.1978
Epoch [17/80], Training Loss: 11.5642, Validation Loss Current: 6.9107, Validation Loss AVG: 9.6479
Epoch [18/80], Training Loss: 11.2319, Validation Loss Current: 7.4140, Validation Loss AVG: 10.2496
Epoch [19/80], Training Loss: 13.2885, Validation Loss Current: 6.4053, Validation Loss AVG: 8.7404
Epoch [20/80], Training Loss: 10.9995, Validation Loss Current: 7.1215, Validation Loss AVG: 9.8181
Epoch [21/80], Training Loss: 12.4090, Validation Loss Current: 9.8548, Validation Loss AVG: 13.8387
Epoch [22/80], Training Loss: 20.0766, Validation Loss Current: 7.5641, Validation Loss AVG: 10.3218
Epoch [23/80], Training Loss: 14.3493, Validation Loss Current: 6.6610, Validation Loss AVG: 9.0961
Epoch [24/80], Training Loss: 12.5639, Validation Loss Current: 8.0025, Validation Loss AVG: 10.7213
Epoch [25/80], Training Loss: 18.5891, Validation Loss Current: 6.5611, Validation Loss AVG: 9.1105
Epoch [26/80], Training Loss: 12.6298, Validation Loss Current: 6.3749, Validation Loss AVG: 9.5586
Epoch [27/80], Training Loss: 10.6568, Validation Loss Current: 6.7332, Validation Loss AVG: 9.4723
Epoch [28/80], Training Loss: 9.7664, Validation Loss Current: 6.6187, Validation Loss AVG: 9.8219
 --- Early Stopped ---
Patch distance: 0.8 finished training. Best epoch: 8 Best val accuracy: 0.49407894736842106 Best val loss: 5.982253313064575


Current group: 0.2
Epoch [1/80], Training Loss: 40.7865, Validation Loss Current: 9.5195, Validation Loss AVG: 8.7257
Epoch [2/80], Training Loss: 36.7961, Validation Loss Current: 9.1253, Validation Loss AVG: 7.5536
Epoch [3/80], Training Loss: 33.4238, Validation Loss Current: 8.7136, Validation Loss AVG: 7.6565
Epoch [4/80], Training Loss: 31.2701, Validation Loss Current: 8.5775, Validation Loss AVG: 7.6274
Epoch [5/80], Training Loss: 29.4063, Validation Loss Current: 8.4281, Validation Loss AVG: 7.1754
Epoch [6/80], Training Loss: 28.3902, Validation Loss Current: 8.4302, Validation Loss AVG: 7.8256
Epoch [7/80], Training Loss: 28.1132, Validation Loss Current: 8.1556, Validation Loss AVG: 7.4421
Epoch [8/80], Training Loss: 27.9735, Validation Loss Current: 9.2699, Validation Loss AVG: 7.6753
Epoch [9/80], Training Loss: 31.1517, Validation Loss Current: 8.1628, Validation Loss AVG: 8.0581
Epoch [10/80], Training Loss: 27.0737, Validation Loss Current: 8.1150, Validation Loss AVG: 8.0464
Epoch [11/80], Training Loss: 26.5523, Validation Loss Current: 7.9604, Validation Loss AVG: 8.4568
Epoch [12/80], Training Loss: 25.8097, Validation Loss Current: 8.2415, Validation Loss AVG: 8.5964
Epoch [13/80], Training Loss: 28.3302, Validation Loss Current: 8.2652, Validation Loss AVG: 7.8312
Epoch [14/80], Training Loss: 25.1099, Validation Loss Current: 8.5187, Validation Loss AVG: 8.9196
Epoch [15/80], Training Loss: 25.8611, Validation Loss Current: 8.0291, Validation Loss AVG: 8.0677
Epoch [16/80], Training Loss: 25.3691, Validation Loss Current: 8.7878, Validation Loss AVG: 8.4881
Epoch [17/80], Training Loss: 27.0250, Validation Loss Current: 8.9329, Validation Loss AVG: 7.9545
Epoch [18/80], Training Loss: 24.9638, Validation Loss Current: 8.4572, Validation Loss AVG: 8.6641
Epoch [19/80], Training Loss: 23.2971, Validation Loss Current: 8.6402, Validation Loss AVG: 8.7793
Epoch [20/80], Training Loss: 25.6410, Validation Loss Current: 8.2777, Validation Loss AVG: 8.6021
Epoch [21/80], Training Loss: 25.3378, Validation Loss Current: 7.8546, Validation Loss AVG: 8.3613
Epoch [22/80], Training Loss: 25.0666, Validation Loss Current: 8.9281, Validation Loss AVG: 9.4945
Epoch [23/80], Training Loss: 25.0723, Validation Loss Current: 8.6487, Validation Loss AVG: 8.4119
Epoch [24/80], Training Loss: 24.3707, Validation Loss Current: 8.4394, Validation Loss AVG: 8.7928
Epoch [25/80], Training Loss: 25.6068, Validation Loss Current: 8.7450, Validation Loss AVG: 8.7215
Epoch [26/80], Training Loss: 25.7202, Validation Loss Current: 8.4618, Validation Loss AVG: 9.1596
Epoch [27/80], Training Loss: 28.8604, Validation Loss Current: 8.3469, Validation Loss AVG: 8.0769
Epoch [28/80], Training Loss: 24.7261, Validation Loss Current: 9.8115, Validation Loss AVG: 10.0653
Epoch [29/80], Training Loss: 26.3821, Validation Loss Current: 8.0272, Validation Loss AVG: 8.8600
Epoch [30/80], Training Loss: 21.7615, Validation Loss Current: 8.2787, Validation Loss AVG: 8.8797
Epoch [31/80], Training Loss: 20.3409, Validation Loss Current: 8.8653, Validation Loss AVG: 9.5960
Epoch [32/80], Training Loss: 21.7882, Validation Loss Current: 8.5262, Validation Loss AVG: 8.4826
Epoch [33/80], Training Loss: 19.9165, Validation Loss Current: 9.0278, Validation Loss AVG: 10.3124
Epoch [34/80], Training Loss: 19.4019, Validation Loss Current: 9.8407, Validation Loss AVG: 9.9632
Epoch [35/80], Training Loss: 19.3583, Validation Loss Current: 9.1807, Validation Loss AVG: 10.0076
Epoch [36/80], Training Loss: 17.7396, Validation Loss Current: 9.2325, Validation Loss AVG: 9.8729
Epoch [37/80], Training Loss: 16.9776, Validation Loss Current: 8.8622, Validation Loss AVG: 9.9716
Epoch [38/80], Training Loss: 17.5037, Validation Loss Current: 12.2585, Validation Loss AVG: 11.4298
Epoch [39/80], Training Loss: 28.5966, Validation Loss Current: 8.2213, Validation Loss AVG: 8.3331
Epoch [40/80], Training Loss: 19.4810, Validation Loss Current: 9.3996, Validation Loss AVG: 9.7014
Epoch [41/80], Training Loss: 15.7387, Validation Loss Current: 9.4214, Validation Loss AVG: 10.7554
 --- Early Stopped ---
Patch distance: 0.2 finished training. Best epoch: 21 Best val accuracy: 0.4046052631578948 Best val loss: 7.8546329736709595


Fold: 1
----- Training alexnet with sequence: [1, 0.4, 0.6, 0.8, 0.2] -----
Current group: 1
Epoch [1/80], Training Loss: 41.5234, Validation Loss Current: 10.3656, Validation Loss AVG: 10.3655
Epoch [2/80], Training Loss: 41.4197, Validation Loss Current: 10.3336, Validation Loss AVG: 10.3331
Epoch [3/80], Training Loss: 41.2764, Validation Loss Current: 10.2950, Validation Loss AVG: 10.2953
Epoch [4/80], Training Loss: 41.1084, Validation Loss Current: 10.2576, Validation Loss AVG: 10.2568
Epoch [5/80], Training Loss: 41.0364, Validation Loss Current: 10.2049, Validation Loss AVG: 10.2078
Epoch [6/80], Training Loss: 40.8047, Validation Loss Current: 10.1582, Validation Loss AVG: 10.1586
Epoch [7/80], Training Loss: 40.2922, Validation Loss Current: 10.0945, Validation Loss AVG: 10.0840
Epoch [8/80], Training Loss: 40.1912, Validation Loss Current: 10.0188, Validation Loss AVG: 10.0001
Epoch [9/80], Training Loss: 39.7437, Validation Loss Current: 9.9626, Validation Loss AVG: 9.9688
Epoch [10/80], Training Loss: 40.1917, Validation Loss Current: 9.9613, Validation Loss AVG: 9.9632
Epoch [11/80], Training Loss: 40.0536, Validation Loss Current: 9.9747, Validation Loss AVG: 9.9833
Epoch [12/80], Training Loss: 39.4768, Validation Loss Current: 9.9426, Validation Loss AVG: 9.9613
Epoch [13/80], Training Loss: 39.8851, Validation Loss Current: 9.8925, Validation Loss AVG: 9.9411
Epoch [14/80], Training Loss: 39.9925, Validation Loss Current: 9.8952, Validation Loss AVG: 9.9348
Epoch [15/80], Training Loss: 39.9674, Validation Loss Current: 9.8600, Validation Loss AVG: 9.8956
Epoch [16/80], Training Loss: 39.9197, Validation Loss Current: 9.8401, Validation Loss AVG: 9.8948
Epoch [17/80], Training Loss: 39.0350, Validation Loss Current: 9.7984, Validation Loss AVG: 9.8738
Epoch [18/80], Training Loss: 39.6020, Validation Loss Current: 9.7798, Validation Loss AVG: 9.8634
Epoch [19/80], Training Loss: 38.9464, Validation Loss Current: 9.7836, Validation Loss AVG: 9.8751
Epoch [20/80], Training Loss: 38.6237, Validation Loss Current: 9.7220, Validation Loss AVG: 9.8455
Epoch [21/80], Training Loss: 38.8880, Validation Loss Current: 9.6805, Validation Loss AVG: 9.8173
Epoch [22/80], Training Loss: 39.2429, Validation Loss Current: 9.6425, Validation Loss AVG: 9.8070
Epoch [23/80], Training Loss: 39.2022, Validation Loss Current: 9.6156, Validation Loss AVG: 9.7987
Epoch [24/80], Training Loss: 39.1389, Validation Loss Current: 9.6232, Validation Loss AVG: 9.7884
Epoch [25/80], Training Loss: 38.5235, Validation Loss Current: 9.5460, Validation Loss AVG: 9.7804
Epoch [26/80], Training Loss: 38.8260, Validation Loss Current: 9.5572, Validation Loss AVG: 9.7719
Epoch [27/80], Training Loss: 37.9420, Validation Loss Current: 9.4748, Validation Loss AVG: 9.7570
Epoch [28/80], Training Loss: 38.3651, Validation Loss Current: 9.4622, Validation Loss AVG: 9.7410
Epoch [29/80], Training Loss: 38.1196, Validation Loss Current: 9.3593, Validation Loss AVG: 9.6992
Epoch [30/80], Training Loss: 37.8711, Validation Loss Current: 9.3155, Validation Loss AVG: 9.6733
Epoch [31/80], Training Loss: 37.7643, Validation Loss Current: 9.2513, Validation Loss AVG: 9.6557
Epoch [32/80], Training Loss: 37.7366, Validation Loss Current: 9.2094, Validation Loss AVG: 9.6592
Epoch [33/80], Training Loss: 37.4581, Validation Loss Current: 9.1588, Validation Loss AVG: 9.5897
Epoch [34/80], Training Loss: 36.7350, Validation Loss Current: 9.0871, Validation Loss AVG: 9.5647
Epoch [35/80], Training Loss: 36.7145, Validation Loss Current: 8.9995, Validation Loss AVG: 9.5560
Epoch [36/80], Training Loss: 36.0716, Validation Loss Current: 8.8924, Validation Loss AVG: 9.4618
Epoch [37/80], Training Loss: 36.4217, Validation Loss Current: 8.7778, Validation Loss AVG: 9.4127
Epoch [38/80], Training Loss: 36.0529, Validation Loss Current: 8.7250, Validation Loss AVG: 9.4438
Epoch [39/80], Training Loss: 35.4648, Validation Loss Current: 8.5207, Validation Loss AVG: 9.4323
Epoch [40/80], Training Loss: 33.7432, Validation Loss Current: 8.4163, Validation Loss AVG: 9.2874
Epoch [41/80], Training Loss: 34.7983, Validation Loss Current: 8.2917, Validation Loss AVG: 9.1009
Epoch [42/80], Training Loss: 33.7238, Validation Loss Current: 8.7511, Validation Loss AVG: 10.1138
Epoch [43/80], Training Loss: 33.5579, Validation Loss Current: 8.2593, Validation Loss AVG: 9.5684
Epoch [44/80], Training Loss: 33.7667, Validation Loss Current: 8.1472, Validation Loss AVG: 9.0135
Epoch [45/80], Training Loss: 33.1610, Validation Loss Current: 8.9379, Validation Loss AVG: 9.3085
Epoch [46/80], Training Loss: 34.0853, Validation Loss Current: 8.5911, Validation Loss AVG: 10.0793
Epoch [47/80], Training Loss: 33.8105, Validation Loss Current: 7.9082, Validation Loss AVG: 9.1143
Epoch [48/80], Training Loss: 33.2982, Validation Loss Current: 7.8876, Validation Loss AVG: 9.0000
Epoch [49/80], Training Loss: 31.9318, Validation Loss Current: 7.9497, Validation Loss AVG: 9.1299
Epoch [50/80], Training Loss: 31.9564, Validation Loss Current: 8.2240, Validation Loss AVG: 9.0267
Epoch [51/80], Training Loss: 31.9355, Validation Loss Current: 8.0306, Validation Loss AVG: 8.9520
Epoch [52/80], Training Loss: 32.4920, Validation Loss Current: 8.0209, Validation Loss AVG: 10.0881
Epoch [53/80], Training Loss: 31.8424, Validation Loss Current: 7.7807, Validation Loss AVG: 9.6727
Epoch [54/80], Training Loss: 31.1656, Validation Loss Current: 7.6009, Validation Loss AVG: 9.2329
Epoch [55/80], Training Loss: 29.3262, Validation Loss Current: 7.5386, Validation Loss AVG: 9.2467
Epoch [56/80], Training Loss: 29.6938, Validation Loss Current: 7.8018, Validation Loss AVG: 9.7752
Epoch [57/80], Training Loss: 29.2560, Validation Loss Current: 8.2070, Validation Loss AVG: 10.7354
Epoch [58/80], Training Loss: 30.8986, Validation Loss Current: 7.4639, Validation Loss AVG: 8.8469
Epoch [59/80], Training Loss: 30.3766, Validation Loss Current: 7.2912, Validation Loss AVG: 8.9863
Epoch [60/80], Training Loss: 28.5676, Validation Loss Current: 7.4677, Validation Loss AVG: 8.8377
Epoch [61/80], Training Loss: 27.6760, Validation Loss Current: 7.7462, Validation Loss AVG: 8.9170
Epoch [62/80], Training Loss: 27.8831, Validation Loss Current: 7.2332, Validation Loss AVG: 9.5418
Epoch [63/80], Training Loss: 28.2009, Validation Loss Current: 7.3915, Validation Loss AVG: 8.7318
Epoch [64/80], Training Loss: 30.9727, Validation Loss Current: 8.3642, Validation Loss AVG: 10.6762
Epoch [65/80], Training Loss: 33.2114, Validation Loss Current: 7.8489, Validation Loss AVG: 8.5231
Epoch [66/80], Training Loss: 29.2535, Validation Loss Current: 7.3059, Validation Loss AVG: 8.5609
Epoch [67/80], Training Loss: 27.3184, Validation Loss Current: 7.1150, Validation Loss AVG: 9.0533
Epoch [68/80], Training Loss: 27.6071, Validation Loss Current: 7.4376, Validation Loss AVG: 9.8251
Epoch [69/80], Training Loss: 27.7048, Validation Loss Current: 7.1440, Validation Loss AVG: 8.6242
Epoch [70/80], Training Loss: 26.5202, Validation Loss Current: 6.7630, Validation Loss AVG: 8.9700
Epoch [71/80], Training Loss: 26.6184, Validation Loss Current: 13.8669, Validation Loss AVG: 12.8788
Epoch [72/80], Training Loss: 39.8272, Validation Loss Current: 8.8068, Validation Loss AVG: 9.3874
Epoch [73/80], Training Loss: 33.8274, Validation Loss Current: 8.2548, Validation Loss AVG: 9.1152
Epoch [74/80], Training Loss: 30.6607, Validation Loss Current: 7.6730, Validation Loss AVG: 8.7783
Epoch [75/80], Training Loss: 29.6434, Validation Loss Current: 7.5858, Validation Loss AVG: 8.7689
Epoch [76/80], Training Loss: 28.4619, Validation Loss Current: 7.2080, Validation Loss AVG: 8.4909
Epoch [77/80], Training Loss: 27.4017, Validation Loss Current: 8.1023, Validation Loss AVG: 10.2345
Epoch [78/80], Training Loss: 29.0253, Validation Loss Current: 6.7373, Validation Loss AVG: 8.1863
Epoch [79/80], Training Loss: 28.4776, Validation Loss Current: 6.9929, Validation Loss AVG: 8.2938
Epoch [80/80], Training Loss: 26.9997, Validation Loss Current: 7.0724, Validation Loss AVG: 8.7744
Patch distance: 1 finished training. Best epoch: 78 Best val accuracy: 0.41414473684210523 Best val loss: 6.737334251403809


Current group: 0.4
Epoch [1/80], Training Loss: 37.2899, Validation Loss Current: 8.7081, Validation Loss AVG: 8.0865
Epoch [2/80], Training Loss: 34.0150, Validation Loss Current: 8.9000, Validation Loss AVG: 8.1601
Epoch [3/80], Training Loss: 34.9128, Validation Loss Current: 8.2797, Validation Loss AVG: 7.9643
Epoch [4/80], Training Loss: 32.7140, Validation Loss Current: 8.0939, Validation Loss AVG: 7.9520
Epoch [5/80], Training Loss: 32.2414, Validation Loss Current: 7.9224, Validation Loss AVG: 7.7373
Epoch [6/80], Training Loss: 31.2508, Validation Loss Current: 7.9105, Validation Loss AVG: 7.7523
Epoch [7/80], Training Loss: 30.9448, Validation Loss Current: 7.9591, Validation Loss AVG: 8.0131
Epoch [8/80], Training Loss: 32.3426, Validation Loss Current: 8.0920, Validation Loss AVG: 7.7616
Epoch [9/80], Training Loss: 31.9514, Validation Loss Current: 7.7243, Validation Loss AVG: 7.7313
Epoch [10/80], Training Loss: 30.4272, Validation Loss Current: 8.2302, Validation Loss AVG: 7.9988
Epoch [11/80], Training Loss: 33.1134, Validation Loss Current: 7.7530, Validation Loss AVG: 7.6088
Epoch [12/80], Training Loss: 31.9426, Validation Loss Current: 8.0442, Validation Loss AVG: 8.2323
Epoch [13/80], Training Loss: 32.5620, Validation Loss Current: 7.7386, Validation Loss AVG: 7.5729
Epoch [14/80], Training Loss: 30.1466, Validation Loss Current: 8.0591, Validation Loss AVG: 8.2993
Epoch [15/80], Training Loss: 30.6626, Validation Loss Current: 7.5531, Validation Loss AVG: 7.5316
Epoch [16/80], Training Loss: 29.7825, Validation Loss Current: 7.6321, Validation Loss AVG: 7.7400
Epoch [17/80], Training Loss: 30.0935, Validation Loss Current: 7.6990, Validation Loss AVG: 7.6301
Epoch [18/80], Training Loss: 29.4136, Validation Loss Current: 7.6951, Validation Loss AVG: 7.8220
Epoch [19/80], Training Loss: 28.4383, Validation Loss Current: 8.1758, Validation Loss AVG: 8.5104
Epoch [20/80], Training Loss: 29.3891, Validation Loss Current: 7.3536, Validation Loss AVG: 7.5158
Epoch [21/80], Training Loss: 28.1839, Validation Loss Current: 7.7618, Validation Loss AVG: 8.1336
Epoch [22/80], Training Loss: 28.4792, Validation Loss Current: 7.4394, Validation Loss AVG: 7.4420
Epoch [23/80], Training Loss: 28.5141, Validation Loss Current: 7.2648, Validation Loss AVG: 7.4995
Epoch [24/80], Training Loss: 28.0948, Validation Loss Current: 9.2458, Validation Loss AVG: 9.6691
Epoch [25/80], Training Loss: 31.5235, Validation Loss Current: 7.9133, Validation Loss AVG: 7.8079
Epoch [26/80], Training Loss: 29.1799, Validation Loss Current: 7.3078, Validation Loss AVG: 7.5613
Epoch [27/80], Training Loss: 28.0127, Validation Loss Current: 7.4041, Validation Loss AVG: 7.6508
Epoch [28/80], Training Loss: 27.7123, Validation Loss Current: 7.4562, Validation Loss AVG: 7.9495
Epoch [29/80], Training Loss: 27.4963, Validation Loss Current: 7.1098, Validation Loss AVG: 7.5107
Epoch [30/80], Training Loss: 28.2675, Validation Loss Current: 7.6632, Validation Loss AVG: 8.0697
Epoch [31/80], Training Loss: 29.0056, Validation Loss Current: 7.2798, Validation Loss AVG: 7.4984
Epoch [32/80], Training Loss: 26.7830, Validation Loss Current: 7.3754, Validation Loss AVG: 7.6575
Epoch [33/80], Training Loss: 28.6761, Validation Loss Current: 7.7691, Validation Loss AVG: 8.3524
Epoch [34/80], Training Loss: 27.6098, Validation Loss Current: 7.3299, Validation Loss AVG: 7.7605
Epoch [35/80], Training Loss: 26.7881, Validation Loss Current: 7.1390, Validation Loss AVG: 7.4958
Epoch [36/80], Training Loss: 26.1924, Validation Loss Current: 7.1816, Validation Loss AVG: 7.6578
Epoch [37/80], Training Loss: 25.7166, Validation Loss Current: 7.5244, Validation Loss AVG: 7.8423
Epoch [38/80], Training Loss: 26.0305, Validation Loss Current: 7.0808, Validation Loss AVG: 7.5672
Epoch [39/80], Training Loss: 25.2843, Validation Loss Current: 6.9476, Validation Loss AVG: 7.4855
Epoch [40/80], Training Loss: 25.7199, Validation Loss Current: 10.0620, Validation Loss AVG: 10.7278
Epoch [41/80], Training Loss: 33.1331, Validation Loss Current: 7.7316, Validation Loss AVG: 7.9894
Epoch [42/80], Training Loss: 28.3240, Validation Loss Current: 7.0736, Validation Loss AVG: 7.5202
Epoch [43/80], Training Loss: 26.0113, Validation Loss Current: 7.8954, Validation Loss AVG: 8.3373
Epoch [44/80], Training Loss: 27.3893, Validation Loss Current: 7.4116, Validation Loss AVG: 7.8439
Epoch [45/80], Training Loss: 25.3082, Validation Loss Current: 6.9196, Validation Loss AVG: 7.5917
Epoch [46/80], Training Loss: 24.5662, Validation Loss Current: 7.5251, Validation Loss AVG: 8.2963
Epoch [47/80], Training Loss: 27.1247, Validation Loss Current: 7.1740, Validation Loss AVG: 7.7871
Epoch [48/80], Training Loss: 26.5284, Validation Loss Current: 7.1366, Validation Loss AVG: 7.8308
Epoch [49/80], Training Loss: 25.8592, Validation Loss Current: 7.3143, Validation Loss AVG: 7.9869
Epoch [50/80], Training Loss: 25.1895, Validation Loss Current: 6.8317, Validation Loss AVG: 7.8428
Epoch [51/80], Training Loss: 25.1032, Validation Loss Current: 7.1047, Validation Loss AVG: 7.9499
Epoch [52/80], Training Loss: 23.3804, Validation Loss Current: 6.8850, Validation Loss AVG: 7.7442
Epoch [53/80], Training Loss: 23.4776, Validation Loss Current: 7.0417, Validation Loss AVG: 7.6991
Epoch [54/80], Training Loss: 22.4609, Validation Loss Current: 6.9076, Validation Loss AVG: 7.9924
Epoch [55/80], Training Loss: 22.8395, Validation Loss Current: 6.6992, Validation Loss AVG: 7.5756
Epoch [56/80], Training Loss: 25.4835, Validation Loss Current: 7.4424, Validation Loss AVG: 8.0767
Epoch [57/80], Training Loss: 26.6001, Validation Loss Current: 7.5080, Validation Loss AVG: 8.2681
Epoch [58/80], Training Loss: 25.0952, Validation Loss Current: 7.5254, Validation Loss AVG: 8.1086
Epoch [59/80], Training Loss: 24.5149, Validation Loss Current: 6.8736, Validation Loss AVG: 8.0000
Epoch [60/80], Training Loss: 22.3994, Validation Loss Current: 6.9729, Validation Loss AVG: 8.0300
Epoch [61/80], Training Loss: 22.3569, Validation Loss Current: 7.1400, Validation Loss AVG: 8.3325
Epoch [62/80], Training Loss: 22.3247, Validation Loss Current: 7.0678, Validation Loss AVG: 7.8425
Epoch [63/80], Training Loss: 24.4435, Validation Loss Current: 6.7734, Validation Loss AVG: 7.6302
Epoch [64/80], Training Loss: 23.5394, Validation Loss Current: 6.6276, Validation Loss AVG: 7.5663
Epoch [65/80], Training Loss: 21.9597, Validation Loss Current: 7.4686, Validation Loss AVG: 8.4052
Epoch [66/80], Training Loss: 27.9095, Validation Loss Current: 6.8591, Validation Loss AVG: 7.5087
Epoch [67/80], Training Loss: 24.3478, Validation Loss Current: 7.0663, Validation Loss AVG: 8.2858
Epoch [68/80], Training Loss: 23.0205, Validation Loss Current: 7.7109, Validation Loss AVG: 8.4520
Epoch [69/80], Training Loss: 23.1387, Validation Loss Current: 7.3493, Validation Loss AVG: 8.8735
Epoch [70/80], Training Loss: 21.6171, Validation Loss Current: 7.2529, Validation Loss AVG: 8.2275
Epoch [71/80], Training Loss: 21.9474, Validation Loss Current: 6.5650, Validation Loss AVG: 7.7287
Epoch [72/80], Training Loss: 22.0834, Validation Loss Current: 6.7727, Validation Loss AVG: 8.0557
Epoch [73/80], Training Loss: 24.6710, Validation Loss Current: 6.9276, Validation Loss AVG: 8.1993
Epoch [74/80], Training Loss: 23.5108, Validation Loss Current: 6.7640, Validation Loss AVG: 7.7796
Epoch [75/80], Training Loss: 21.8653, Validation Loss Current: 7.5135, Validation Loss AVG: 8.3654
Epoch [76/80], Training Loss: 22.6830, Validation Loss Current: 7.1114, Validation Loss AVG: 8.1602
Epoch [77/80], Training Loss: 20.3637, Validation Loss Current: 6.8674, Validation Loss AVG: 8.2741
Epoch [78/80], Training Loss: 19.3754, Validation Loss Current: 6.5126, Validation Loss AVG: 7.8725
Epoch [79/80], Training Loss: 17.7413, Validation Loss Current: 6.9033, Validation Loss AVG: 8.2846
Epoch [80/80], Training Loss: 17.6068, Validation Loss Current: 6.9852, Validation Loss AVG: 8.5954
Patch distance: 0.4 finished training. Best epoch: 78 Best val accuracy: 0.49309210526315794 Best val loss: 6.512557625770569


Current group: 0.6
Epoch [1/80], Training Loss: 23.3397, Validation Loss Current: 6.2190, Validation Loss AVG: 7.2560
Epoch [2/80], Training Loss: 21.3542, Validation Loss Current: 6.2769, Validation Loss AVG: 7.3369
Epoch [3/80], Training Loss: 20.3248, Validation Loss Current: 6.1564, Validation Loss AVG: 7.3650
Epoch [4/80], Training Loss: 19.8211, Validation Loss Current: 6.0304, Validation Loss AVG: 7.1831
Epoch [5/80], Training Loss: 20.1296, Validation Loss Current: 6.4151, Validation Loss AVG: 7.6620
Epoch [6/80], Training Loss: 20.2566, Validation Loss Current: 6.0189, Validation Loss AVG: 7.2435
Epoch [7/80], Training Loss: 19.0302, Validation Loss Current: 6.5153, Validation Loss AVG: 7.6364
Epoch [8/80], Training Loss: 20.2079, Validation Loss Current: 6.1151, Validation Loss AVG: 7.3882
Epoch [9/80], Training Loss: 19.1986, Validation Loss Current: 6.1658, Validation Loss AVG: 7.5898
Epoch [10/80], Training Loss: 17.6765, Validation Loss Current: 5.9382, Validation Loss AVG: 7.2680
Epoch [11/80], Training Loss: 16.7220, Validation Loss Current: 6.0877, Validation Loss AVG: 7.5853
Epoch [12/80], Training Loss: 16.7219, Validation Loss Current: 6.3691, Validation Loss AVG: 7.7807
Epoch [13/80], Training Loss: 16.6276, Validation Loss Current: 6.7065, Validation Loss AVG: 7.9372
Epoch [14/80], Training Loss: 17.6869, Validation Loss Current: 6.5256, Validation Loss AVG: 8.0339
Epoch [15/80], Training Loss: 16.5615, Validation Loss Current: 6.4640, Validation Loss AVG: 7.8756
Epoch [16/80], Training Loss: 16.4286, Validation Loss Current: 6.2140, Validation Loss AVG: 7.6156
Epoch [17/80], Training Loss: 16.8234, Validation Loss Current: 8.5506, Validation Loss AVG: 9.8371
Epoch [18/80], Training Loss: 24.2954, Validation Loss Current: 8.0289, Validation Loss AVG: 9.0204
Epoch [19/80], Training Loss: 21.1579, Validation Loss Current: 6.3191, Validation Loss AVG: 7.6013
Epoch [20/80], Training Loss: 17.9816, Validation Loss Current: 6.9422, Validation Loss AVG: 8.0848
Epoch [21/80], Training Loss: 15.9823, Validation Loss Current: 6.8619, Validation Loss AVG: 8.3972
Epoch [22/80], Training Loss: 16.1236, Validation Loss Current: 6.5314, Validation Loss AVG: 7.8718
Epoch [23/80], Training Loss: 17.7203, Validation Loss Current: 6.7388, Validation Loss AVG: 8.5344
Epoch [24/80], Training Loss: 17.4170, Validation Loss Current: 7.5804, Validation Loss AVG: 9.1351
Epoch [25/80], Training Loss: 21.0952, Validation Loss Current: 6.3722, Validation Loss AVG: 7.8889
Epoch [26/80], Training Loss: 17.3795, Validation Loss Current: 6.0041, Validation Loss AVG: 7.5630
Epoch [27/80], Training Loss: 15.2806, Validation Loss Current: 6.0498, Validation Loss AVG: 7.7047
Epoch [28/80], Training Loss: 14.3291, Validation Loss Current: 6.4693, Validation Loss AVG: 8.1533
Epoch [29/80], Training Loss: 14.5779, Validation Loss Current: 8.3913, Validation Loss AVG: 9.8597
Epoch [30/80], Training Loss: 21.2686, Validation Loss Current: 8.1779, Validation Loss AVG: 9.3948
 --- Early Stopped ---
Patch distance: 0.6 finished training. Best epoch: 10 Best val accuracy: 0.5335526315789474 Best val loss: 5.938152313232422


Current group: 0.8
Epoch [1/80], Training Loss: 25.2910, Validation Loss Current: 6.1629, Validation Loss AVG: 7.3071
Epoch [2/80], Training Loss: 22.1045, Validation Loss Current: 6.0690, Validation Loss AVG: 7.6128
Epoch [3/80], Training Loss: 19.1003, Validation Loss Current: 6.2652, Validation Loss AVG: 8.0496
Epoch [4/80], Training Loss: 19.4937, Validation Loss Current: 5.7425, Validation Loss AVG: 7.3425
Epoch [5/80], Training Loss: 18.7360, Validation Loss Current: 5.7799, Validation Loss AVG: 7.5441
Epoch [6/80], Training Loss: 17.3270, Validation Loss Current: 5.6694, Validation Loss AVG: 7.5927
Epoch [7/80], Training Loss: 16.8043, Validation Loss Current: 6.1008, Validation Loss AVG: 7.5752
Epoch [8/80], Training Loss: 16.2655, Validation Loss Current: 5.8361, Validation Loss AVG: 7.9151
Epoch [9/80], Training Loss: 15.9589, Validation Loss Current: 5.9600, Validation Loss AVG: 7.7007
Epoch [10/80], Training Loss: 15.0111, Validation Loss Current: 5.9226, Validation Loss AVG: 7.8194
Epoch [11/80], Training Loss: 14.2223, Validation Loss Current: 6.0876, Validation Loss AVG: 8.2012
Epoch [12/80], Training Loss: 15.0259, Validation Loss Current: 5.9352, Validation Loss AVG: 7.8467
Epoch [13/80], Training Loss: 14.5002, Validation Loss Current: 7.8222, Validation Loss AVG: 9.5901
Epoch [14/80], Training Loss: 18.7896, Validation Loss Current: 6.7042, Validation Loss AVG: 8.8023
Epoch [15/80], Training Loss: 19.9103, Validation Loss Current: 6.5518, Validation Loss AVG: 8.9273
Epoch [16/80], Training Loss: 17.0624, Validation Loss Current: 6.0758, Validation Loss AVG: 8.0857
Epoch [17/80], Training Loss: 14.9229, Validation Loss Current: 6.8023, Validation Loss AVG: 9.2434
Epoch [18/80], Training Loss: 15.1460, Validation Loss Current: 6.0356, Validation Loss AVG: 8.0712
Epoch [19/80], Training Loss: 12.6454, Validation Loss Current: 7.3297, Validation Loss AVG: 9.1659
Epoch [20/80], Training Loss: 14.1449, Validation Loss Current: 6.2724, Validation Loss AVG: 8.4008
Epoch [21/80], Training Loss: 12.0752, Validation Loss Current: 5.8547, Validation Loss AVG: 8.3137
Epoch [22/80], Training Loss: 11.6738, Validation Loss Current: 6.6905, Validation Loss AVG: 9.8411
Epoch [23/80], Training Loss: 11.8484, Validation Loss Current: 6.3041, Validation Loss AVG: 8.7452
Epoch [24/80], Training Loss: 11.1524, Validation Loss Current: 6.7704, Validation Loss AVG: 9.1168
Epoch [25/80], Training Loss: 9.1819, Validation Loss Current: 6.6053, Validation Loss AVG: 9.2853
Epoch [26/80], Training Loss: 9.6899, Validation Loss Current: 18.6072, Validation Loss AVG: 20.0074
 --- Early Stopped ---
Patch distance: 0.8 finished training. Best epoch: 6 Best val accuracy: 0.5138157894736841 Best val loss: 5.669435143470764


Current group: 0.2
Epoch [1/80], Training Loss: 50.1932, Validation Loss Current: 9.8123, Validation Loss AVG: 9.7505
Epoch [2/80], Training Loss: 39.6101, Validation Loss Current: 9.7800, Validation Loss AVG: 9.7332
Epoch [3/80], Training Loss: 39.0114, Validation Loss Current: 9.5029, Validation Loss AVG: 9.5362
Epoch [4/80], Training Loss: 38.1187, Validation Loss Current: 9.4820, Validation Loss AVG: 9.8821
Epoch [5/80], Training Loss: 38.1935, Validation Loss Current: 9.2762, Validation Loss AVG: 9.6929
Epoch [6/80], Training Loss: 37.4521, Validation Loss Current: 9.0278, Validation Loss AVG: 9.3522
Epoch [7/80], Training Loss: 37.8120, Validation Loss Current: 9.2586, Validation Loss AVG: 9.3456
Epoch [8/80], Training Loss: 36.4335, Validation Loss Current: 9.0619, Validation Loss AVG: 9.2198
Epoch [9/80], Training Loss: 35.6104, Validation Loss Current: 8.8183, Validation Loss AVG: 8.9860
Epoch [10/80], Training Loss: 34.5751, Validation Loss Current: 8.6622, Validation Loss AVG: 8.9702
Epoch [11/80], Training Loss: 33.5270, Validation Loss Current: 8.3537, Validation Loss AVG: 8.5914
Epoch [12/80], Training Loss: 32.6651, Validation Loss Current: 10.3988, Validation Loss AVG: 10.9758
Epoch [13/80], Training Loss: 35.0229, Validation Loss Current: 8.9245, Validation Loss AVG: 9.4331
Epoch [14/80], Training Loss: 33.7824, Validation Loss Current: 8.1139, Validation Loss AVG: 8.5674
Epoch [15/80], Training Loss: 32.0099, Validation Loss Current: 8.0437, Validation Loss AVG: 8.6499
Epoch [16/80], Training Loss: 31.6727, Validation Loss Current: 9.8811, Validation Loss AVG: 8.8556
Epoch [17/80], Training Loss: 34.2739, Validation Loss Current: 8.8366, Validation Loss AVG: 8.5218
Epoch [18/80], Training Loss: 33.6899, Validation Loss Current: 8.2587, Validation Loss AVG: 8.7702
Epoch [19/80], Training Loss: 31.7179, Validation Loss Current: 8.5358, Validation Loss AVG: 8.9486
Epoch [20/80], Training Loss: 30.9585, Validation Loss Current: 7.8465, Validation Loss AVG: 8.7434
Epoch [21/80], Training Loss: 30.5582, Validation Loss Current: 7.8674, Validation Loss AVG: 8.4932
Epoch [22/80], Training Loss: 32.3831, Validation Loss Current: 9.2000, Validation Loss AVG: 9.8549
Epoch [23/80], Training Loss: 37.3715, Validation Loss Current: 9.0030, Validation Loss AVG: 9.6011
Epoch [24/80], Training Loss: 35.8151, Validation Loss Current: 8.5615, Validation Loss AVG: 8.8814
Epoch [25/80], Training Loss: 32.9110, Validation Loss Current: 8.2341, Validation Loss AVG: 9.1577
Epoch [26/80], Training Loss: 30.6277, Validation Loss Current: 8.1185, Validation Loss AVG: 9.1502
Epoch [27/80], Training Loss: 29.9361, Validation Loss Current: 8.3176, Validation Loss AVG: 8.8031
Epoch [28/80], Training Loss: 29.7601, Validation Loss Current: 8.0565, Validation Loss AVG: 9.3878
Epoch [29/80], Training Loss: 31.0201, Validation Loss Current: 8.0712, Validation Loss AVG: 8.5499
Epoch [30/80], Training Loss: 30.2392, Validation Loss Current: 8.2243, Validation Loss AVG: 8.8162
Epoch [31/80], Training Loss: 32.6661, Validation Loss Current: 8.1651, Validation Loss AVG: 9.1303
Epoch [32/80], Training Loss: 30.1330, Validation Loss Current: 7.8068, Validation Loss AVG: 9.2765
Epoch [33/80], Training Loss: 30.6105, Validation Loss Current: 7.6141, Validation Loss AVG: 8.7383
Epoch [34/80], Training Loss: 28.9023, Validation Loss Current: 7.6885, Validation Loss AVG: 9.0257
Epoch [35/80], Training Loss: 29.6699, Validation Loss Current: 7.8280, Validation Loss AVG: 9.4028
Epoch [36/80], Training Loss: 30.2955, Validation Loss Current: 8.0053, Validation Loss AVG: 9.2636
Epoch [37/80], Training Loss: 27.9138, Validation Loss Current: 7.4898, Validation Loss AVG: 9.0139
Epoch [38/80], Training Loss: 27.0752, Validation Loss Current: 7.6290, Validation Loss AVG: 9.1717
Epoch [39/80], Training Loss: 25.7098, Validation Loss Current: 7.5388, Validation Loss AVG: 8.9887
Epoch [40/80], Training Loss: 26.5671, Validation Loss Current: 8.1095, Validation Loss AVG: 9.0969
Epoch [41/80], Training Loss: 27.8379, Validation Loss Current: 7.3214, Validation Loss AVG: 9.0623
Epoch [42/80], Training Loss: 25.5280, Validation Loss Current: 7.7027, Validation Loss AVG: 9.6564
Epoch [43/80], Training Loss: 27.1922, Validation Loss Current: 8.5142, Validation Loss AVG: 10.1668
Epoch [44/80], Training Loss: 28.2544, Validation Loss Current: 7.8390, Validation Loss AVG: 10.3064
Epoch [45/80], Training Loss: 28.4551, Validation Loss Current: 7.7732, Validation Loss AVG: 9.3620
Epoch [46/80], Training Loss: 26.9025, Validation Loss Current: 7.4607, Validation Loss AVG: 9.2051
Epoch [47/80], Training Loss: 24.8687, Validation Loss Current: 7.5716, Validation Loss AVG: 10.1543
Epoch [48/80], Training Loss: 24.1709, Validation Loss Current: 8.0732, Validation Loss AVG: 10.7046
Epoch [49/80], Training Loss: 26.9992, Validation Loss Current: 7.4053, Validation Loss AVG: 9.7735
Epoch [50/80], Training Loss: 24.3456, Validation Loss Current: 7.5872, Validation Loss AVG: 9.8008
Epoch [51/80], Training Loss: 24.4750, Validation Loss Current: 7.7874, Validation Loss AVG: 9.3801
Epoch [52/80], Training Loss: 22.8274, Validation Loss Current: 7.7409, Validation Loss AVG: 10.1475
Epoch [53/80], Training Loss: 22.8621, Validation Loss Current: 8.8383, Validation Loss AVG: 11.9260
Epoch [54/80], Training Loss: 25.9890, Validation Loss Current: 7.8190, Validation Loss AVG: 9.7544
Epoch [55/80], Training Loss: 21.8458, Validation Loss Current: 7.7386, Validation Loss AVG: 10.4115
Epoch [56/80], Training Loss: 21.8186, Validation Loss Current: 7.5682, Validation Loss AVG: 10.3699
Epoch [57/80], Training Loss: 21.7820, Validation Loss Current: 7.6929, Validation Loss AVG: 10.3135
Epoch [58/80], Training Loss: 20.3186, Validation Loss Current: 7.6125, Validation Loss AVG: 10.2683
Epoch [59/80], Training Loss: 20.4771, Validation Loss Current: 8.4097, Validation Loss AVG: 10.7239
Epoch [60/80], Training Loss: 22.6659, Validation Loss Current: 7.5591, Validation Loss AVG: 10.0460
Epoch [61/80], Training Loss: 20.5522, Validation Loss Current: 8.4334, Validation Loss AVG: 11.0936
 --- Early Stopped ---
Patch distance: 0.2 finished training. Best epoch: 41 Best val accuracy: 0.39046052631578954 Best val loss: 7.321420431137085


Fold: 2
----- Training alexnet with sequence: [1, 0.4, 0.6, 0.8, 0.2] -----
Current group: 1
Epoch [1/80], Training Loss: 41.5702, Validation Loss Current: 10.3818, Validation Loss AVG: 10.3818
Epoch [2/80], Training Loss: 41.4253, Validation Loss Current: 10.3480, Validation Loss AVG: 10.3489
Epoch [3/80], Training Loss: 41.3433, Validation Loss Current: 10.3129, Validation Loss AVG: 10.3143
Epoch [4/80], Training Loss: 41.1149, Validation Loss Current: 10.2789, Validation Loss AVG: 10.2795
Epoch [5/80], Training Loss: 40.9606, Validation Loss Current: 10.2351, Validation Loss AVG: 10.2353
Epoch [6/80], Training Loss: 40.6587, Validation Loss Current: 10.1775, Validation Loss AVG: 10.1701
Epoch [7/80], Training Loss: 40.1027, Validation Loss Current: 10.1059, Validation Loss AVG: 10.0968
Epoch [8/80], Training Loss: 40.4028, Validation Loss Current: 10.0883, Validation Loss AVG: 10.0782
Epoch [9/80], Training Loss: 39.8744, Validation Loss Current: 10.0805, Validation Loss AVG: 10.0742
Epoch [10/80], Training Loss: 39.6624, Validation Loss Current: 10.1007, Validation Loss AVG: 10.0992
Epoch [11/80], Training Loss: 40.1362, Validation Loss Current: 10.0605, Validation Loss AVG: 10.0855
Epoch [12/80], Training Loss: 39.9523, Validation Loss Current: 10.0388, Validation Loss AVG: 10.0537
Epoch [13/80], Training Loss: 40.1319, Validation Loss Current: 9.9998, Validation Loss AVG: 10.0347
Epoch [14/80], Training Loss: 40.1706, Validation Loss Current: 9.9871, Validation Loss AVG: 10.0275
Epoch [15/80], Training Loss: 39.8677, Validation Loss Current: 9.9485, Validation Loss AVG: 10.0070
Epoch [16/80], Training Loss: 39.6463, Validation Loss Current: 9.9385, Validation Loss AVG: 10.0013
Epoch [17/80], Training Loss: 39.5043, Validation Loss Current: 9.9173, Validation Loss AVG: 10.0034
Epoch [18/80], Training Loss: 39.3854, Validation Loss Current: 9.9033, Validation Loss AVG: 10.0054
Epoch [19/80], Training Loss: 39.3674, Validation Loss Current: 9.8766, Validation Loss AVG: 10.0000
Epoch [20/80], Training Loss: 38.9937, Validation Loss Current: 9.8840, Validation Loss AVG: 10.0191
Epoch [21/80], Training Loss: 38.5484, Validation Loss Current: 9.8350, Validation Loss AVG: 9.9971
Epoch [22/80], Training Loss: 38.4401, Validation Loss Current: 9.8027, Validation Loss AVG: 9.9854
Epoch [23/80], Training Loss: 38.7029, Validation Loss Current: 9.8033, Validation Loss AVG: 9.9701
Epoch [24/80], Training Loss: 38.8242, Validation Loss Current: 9.7904, Validation Loss AVG: 9.9990
Epoch [25/80], Training Loss: 38.2410, Validation Loss Current: 9.7366, Validation Loss AVG: 9.9747
Epoch [26/80], Training Loss: 37.7889, Validation Loss Current: 9.6610, Validation Loss AVG: 9.9395
Epoch [27/80], Training Loss: 37.9647, Validation Loss Current: 9.6330, Validation Loss AVG: 9.9055
Epoch [28/80], Training Loss: 38.1714, Validation Loss Current: 9.5879, Validation Loss AVG: 9.8708
Epoch [29/80], Training Loss: 37.6299, Validation Loss Current: 9.5000, Validation Loss AVG: 9.9194
Epoch [30/80], Training Loss: 37.7558, Validation Loss Current: 9.4574, Validation Loss AVG: 9.8742
Epoch [31/80], Training Loss: 37.2844, Validation Loss Current: 9.3943, Validation Loss AVG: 9.8915
Epoch [32/80], Training Loss: 37.2510, Validation Loss Current: 9.2749, Validation Loss AVG: 9.8252
Epoch [33/80], Training Loss: 37.0155, Validation Loss Current: 9.1957, Validation Loss AVG: 9.7565
Epoch [34/80], Training Loss: 37.0832, Validation Loss Current: 9.0831, Validation Loss AVG: 9.6922
Epoch [35/80], Training Loss: 36.0924, Validation Loss Current: 9.0015, Validation Loss AVG: 9.6637
Epoch [36/80], Training Loss: 35.8534, Validation Loss Current: 9.2587, Validation Loss AVG: 10.0085
Epoch [37/80], Training Loss: 36.9139, Validation Loss Current: 8.8915, Validation Loss AVG: 9.5519
Epoch [38/80], Training Loss: 36.1129, Validation Loss Current: 8.7895, Validation Loss AVG: 9.4968
Epoch [39/80], Training Loss: 34.5408, Validation Loss Current: 8.7396, Validation Loss AVG: 9.4837
Epoch [40/80], Training Loss: 35.0719, Validation Loss Current: 8.6129, Validation Loss AVG: 9.5466
Epoch [41/80], Training Loss: 34.8704, Validation Loss Current: 8.6210, Validation Loss AVG: 9.5747
Epoch [42/80], Training Loss: 33.7910, Validation Loss Current: 10.5466, Validation Loss AVG: 10.8303
Epoch [43/80], Training Loss: 36.1337, Validation Loss Current: 9.0169, Validation Loss AVG: 9.9231
Epoch [44/80], Training Loss: 35.4696, Validation Loss Current: 8.6483, Validation Loss AVG: 9.4866
Epoch [45/80], Training Loss: 32.8810, Validation Loss Current: 8.2730, Validation Loss AVG: 9.1159
Epoch [46/80], Training Loss: 33.2173, Validation Loss Current: 8.5613, Validation Loss AVG: 10.0496
Epoch [47/80], Training Loss: 33.8279, Validation Loss Current: 8.3295, Validation Loss AVG: 9.0054
Epoch [48/80], Training Loss: 32.5523, Validation Loss Current: 7.9734, Validation Loss AVG: 9.0996
Epoch [49/80], Training Loss: 32.3426, Validation Loss Current: 8.2492, Validation Loss AVG: 9.6238
Epoch [50/80], Training Loss: 32.2141, Validation Loss Current: 8.3910, Validation Loss AVG: 9.1494
Epoch [51/80], Training Loss: 31.3002, Validation Loss Current: 7.9293, Validation Loss AVG: 8.9311
Epoch [52/80], Training Loss: 31.9500, Validation Loss Current: 7.6878, Validation Loss AVG: 8.9615
Epoch [53/80], Training Loss: 32.8622, Validation Loss Current: 9.8548, Validation Loss AVG: 10.2361
Epoch [54/80], Training Loss: 34.1680, Validation Loss Current: 8.0780, Validation Loss AVG: 8.9621
Epoch [55/80], Training Loss: 30.1395, Validation Loss Current: 7.9992, Validation Loss AVG: 8.9254
Epoch [56/80], Training Loss: 30.1068, Validation Loss Current: 8.1096, Validation Loss AVG: 8.8952
Epoch [57/80], Training Loss: 30.1912, Validation Loss Current: 7.6369, Validation Loss AVG: 8.9597
Epoch [58/80], Training Loss: 30.4353, Validation Loss Current: 7.5982, Validation Loss AVG: 8.9484
Epoch [59/80], Training Loss: 28.6943, Validation Loss Current: 7.7736, Validation Loss AVG: 9.0031
Epoch [60/80], Training Loss: 28.1674, Validation Loss Current: 9.0550, Validation Loss AVG: 9.7038
Epoch [61/80], Training Loss: 31.1486, Validation Loss Current: 7.7940, Validation Loss AVG: 9.3318
Epoch [62/80], Training Loss: 28.7371, Validation Loss Current: 7.6624, Validation Loss AVG: 8.9551
Epoch [63/80], Training Loss: 29.5753, Validation Loss Current: 7.5877, Validation Loss AVG: 9.1901
Epoch [64/80], Training Loss: 30.2077, Validation Loss Current: 7.4531, Validation Loss AVG: 9.1454
Epoch [65/80], Training Loss: 30.8022, Validation Loss Current: 7.6130, Validation Loss AVG: 9.1523
Epoch [66/80], Training Loss: 29.0815, Validation Loss Current: 7.4190, Validation Loss AVG: 8.7479
Epoch [67/80], Training Loss: 26.8530, Validation Loss Current: 7.6305, Validation Loss AVG: 9.3621
Epoch [68/80], Training Loss: 26.5062, Validation Loss Current: 7.6003, Validation Loss AVG: 9.4712
Epoch [69/80], Training Loss: 27.0037, Validation Loss Current: 7.5871, Validation Loss AVG: 9.5776
Epoch [70/80], Training Loss: 26.9597, Validation Loss Current: 7.3383, Validation Loss AVG: 8.7090
Epoch [71/80], Training Loss: 25.9299, Validation Loss Current: 7.2022, Validation Loss AVG: 8.8171
Epoch [72/80], Training Loss: 25.9152, Validation Loss Current: 7.3277, Validation Loss AVG: 9.3616
Epoch [73/80], Training Loss: 26.8723, Validation Loss Current: 7.3386, Validation Loss AVG: 9.1734
Epoch [74/80], Training Loss: 27.6058, Validation Loss Current: 8.0136, Validation Loss AVG: 8.8424
Epoch [75/80], Training Loss: 27.4291, Validation Loss Current: 7.4397, Validation Loss AVG: 9.3461
Epoch [76/80], Training Loss: 27.0421, Validation Loss Current: 6.9852, Validation Loss AVG: 8.7831
Epoch [77/80], Training Loss: 25.4865, Validation Loss Current: 7.0804, Validation Loss AVG: 9.0195
Epoch [78/80], Training Loss: 25.2204, Validation Loss Current: 9.1965, Validation Loss AVG: 9.9651
Epoch [79/80], Training Loss: 30.2820, Validation Loss Current: 7.1741, Validation Loss AVG: 8.6507
Epoch [80/80], Training Loss: 27.0511, Validation Loss Current: 7.0477, Validation Loss AVG: 8.7329
Patch distance: 1 finished training. Best epoch: 76 Best val accuracy: 0.3930921052631579 Best val loss: 6.9852365255355835


Current group: 0.4
Epoch [1/80], Training Loss: 34.6646, Validation Loss Current: 8.5385, Validation Loss AVG: 8.0232
Epoch [2/80], Training Loss: 32.2755, Validation Loss Current: 8.4709, Validation Loss AVG: 7.9920
Epoch [3/80], Training Loss: 32.2884, Validation Loss Current: 8.3690, Validation Loss AVG: 7.9767
Epoch [4/80], Training Loss: 31.7825, Validation Loss Current: 9.7426, Validation Loss AVG: 9.8504
Epoch [5/80], Training Loss: 34.4172, Validation Loss Current: 8.7085, Validation Loss AVG: 8.3701
Epoch [6/80], Training Loss: 32.0539, Validation Loss Current: 8.1083, Validation Loss AVG: 8.1008
Epoch [7/80], Training Loss: 31.8724, Validation Loss Current: 7.8729, Validation Loss AVG: 7.7915
Epoch [8/80], Training Loss: 31.1378, Validation Loss Current: 8.4627, Validation Loss AVG: 8.4060
Epoch [9/80], Training Loss: 33.4356, Validation Loss Current: 7.9596, Validation Loss AVG: 7.8790
Epoch [10/80], Training Loss: 32.5350, Validation Loss Current: 8.3950, Validation Loss AVG: 8.4875
Epoch [11/80], Training Loss: 31.4745, Validation Loss Current: 8.0773, Validation Loss AVG: 8.1058
Epoch [12/80], Training Loss: 32.2131, Validation Loss Current: 7.8908, Validation Loss AVG: 7.7567
Epoch [13/80], Training Loss: 31.1377, Validation Loss Current: 7.8987, Validation Loss AVG: 7.9683
Epoch [14/80], Training Loss: 30.4831, Validation Loss Current: 7.7221, Validation Loss AVG: 7.8714
Epoch [15/80], Training Loss: 29.9489, Validation Loss Current: 7.8663, Validation Loss AVG: 8.0948
Epoch [16/80], Training Loss: 31.0332, Validation Loss Current: 7.8483, Validation Loss AVG: 7.8923
Epoch [17/80], Training Loss: 30.2752, Validation Loss Current: 7.7169, Validation Loss AVG: 7.7152
Epoch [18/80], Training Loss: 28.9577, Validation Loss Current: 7.5936, Validation Loss AVG: 7.9366
Epoch [19/80], Training Loss: 28.7966, Validation Loss Current: 7.7669, Validation Loss AVG: 7.9560
Epoch [20/80], Training Loss: 30.2564, Validation Loss Current: 8.1915, Validation Loss AVG: 8.1511
Epoch [21/80], Training Loss: 33.5869, Validation Loss Current: 8.2113, Validation Loss AVG: 8.2605
Epoch [22/80], Training Loss: 30.9778, Validation Loss Current: 7.5662, Validation Loss AVG: 7.8338
Epoch [23/80], Training Loss: 28.2672, Validation Loss Current: 7.8076, Validation Loss AVG: 7.9746
Epoch [24/80], Training Loss: 28.5787, Validation Loss Current: 7.9222, Validation Loss AVG: 8.1896
Epoch [25/80], Training Loss: 29.8089, Validation Loss Current: 7.7703, Validation Loss AVG: 7.9607
Epoch [26/80], Training Loss: 28.2002, Validation Loss Current: 7.5870, Validation Loss AVG: 7.9808
Epoch [27/80], Training Loss: 28.9879, Validation Loss Current: 7.3746, Validation Loss AVG: 7.8441
Epoch [28/80], Training Loss: 28.0011, Validation Loss Current: 7.5545, Validation Loss AVG: 7.8018
Epoch [29/80], Training Loss: 27.3815, Validation Loss Current: 7.6102, Validation Loss AVG: 8.0978
Epoch [30/80], Training Loss: 27.6370, Validation Loss Current: 7.8876, Validation Loss AVG: 8.1762
Epoch [31/80], Training Loss: 30.4250, Validation Loss Current: 7.9356, Validation Loss AVG: 8.1631
Epoch [32/80], Training Loss: 29.2249, Validation Loss Current: 7.5879, Validation Loss AVG: 7.8760
Epoch [33/80], Training Loss: 28.5485, Validation Loss Current: 8.0186, Validation Loss AVG: 8.6700
Epoch [34/80], Training Loss: 28.6892, Validation Loss Current: 7.5193, Validation Loss AVG: 7.7821
Epoch [35/80], Training Loss: 27.3762, Validation Loss Current: 7.8367, Validation Loss AVG: 8.3027
Epoch [36/80], Training Loss: 28.1593, Validation Loss Current: 7.2170, Validation Loss AVG: 7.7120
Epoch [37/80], Training Loss: 25.5813, Validation Loss Current: 7.4375, Validation Loss AVG: 7.9902
Epoch [38/80], Training Loss: 24.8343, Validation Loss Current: 7.3225, Validation Loss AVG: 7.9622
Epoch [39/80], Training Loss: 24.4422, Validation Loss Current: 7.3171, Validation Loss AVG: 7.9166
Epoch [40/80], Training Loss: 26.2094, Validation Loss Current: 8.3066, Validation Loss AVG: 8.8388
Epoch [41/80], Training Loss: 31.6154, Validation Loss Current: 8.0430, Validation Loss AVG: 8.3819
Epoch [42/80], Training Loss: 28.9625, Validation Loss Current: 7.3004, Validation Loss AVG: 8.0737
Epoch [43/80], Training Loss: 25.3870, Validation Loss Current: 7.9805, Validation Loss AVG: 8.7243
Epoch [44/80], Training Loss: 25.7911, Validation Loss Current: 6.9429, Validation Loss AVG: 7.7827
Epoch [45/80], Training Loss: 24.2829, Validation Loss Current: 7.7717, Validation Loss AVG: 8.4937
Epoch [46/80], Training Loss: 26.6215, Validation Loss Current: 7.1545, Validation Loss AVG: 7.7857
Epoch [47/80], Training Loss: 25.7902, Validation Loss Current: 7.0795, Validation Loss AVG: 8.0321
Epoch [48/80], Training Loss: 24.5717, Validation Loss Current: 7.0953, Validation Loss AVG: 7.9602
Epoch [49/80], Training Loss: 22.9453, Validation Loss Current: 7.1234, Validation Loss AVG: 8.1086
Epoch [50/80], Training Loss: 23.5618, Validation Loss Current: 7.2344, Validation Loss AVG: 7.9540
Epoch [51/80], Training Loss: 24.0928, Validation Loss Current: 7.3546, Validation Loss AVG: 7.9489
Epoch [52/80], Training Loss: 23.2613, Validation Loss Current: 6.9933, Validation Loss AVG: 7.8939
Epoch [53/80], Training Loss: 24.0294, Validation Loss Current: 7.0748, Validation Loss AVG: 7.8031
Epoch [54/80], Training Loss: 24.9788, Validation Loss Current: 7.1400, Validation Loss AVG: 7.8756
Epoch [55/80], Training Loss: 24.7718, Validation Loss Current: 7.1139, Validation Loss AVG: 8.0010
Epoch [56/80], Training Loss: 23.6364, Validation Loss Current: 7.1373, Validation Loss AVG: 7.6683
Epoch [57/80], Training Loss: 23.2840, Validation Loss Current: 6.9619, Validation Loss AVG: 7.9908
Epoch [58/80], Training Loss: 21.4282, Validation Loss Current: 7.5775, Validation Loss AVG: 8.6248
Epoch [59/80], Training Loss: 21.4107, Validation Loss Current: 7.0518, Validation Loss AVG: 8.0762
Epoch [60/80], Training Loss: 21.3310, Validation Loss Current: 7.2981, Validation Loss AVG: 8.9604
Epoch [61/80], Training Loss: 25.8779, Validation Loss Current: 7.0312, Validation Loss AVG: 7.9917
Epoch [62/80], Training Loss: 23.8436, Validation Loss Current: 7.1479, Validation Loss AVG: 8.0288
Epoch [63/80], Training Loss: 20.1995, Validation Loss Current: 7.0890, Validation Loss AVG: 8.1985
Epoch [64/80], Training Loss: 20.2550, Validation Loss Current: 7.4259, Validation Loss AVG: 8.6096
 --- Early Stopped ---
Patch distance: 0.4 finished training. Best epoch: 44 Best val accuracy: 0.47368421052631576 Best val loss: 6.942870616912842


Current group: 0.6
Epoch [1/80], Training Loss: 24.4127, Validation Loss Current: 6.8516, Validation Loss AVG: 7.7535
Epoch [2/80], Training Loss: 23.1278, Validation Loss Current: 6.4970, Validation Loss AVG: 7.5872
Epoch [3/80], Training Loss: 22.3288, Validation Loss Current: 6.3276, Validation Loss AVG: 7.3707
Epoch [4/80], Training Loss: 22.4210, Validation Loss Current: 6.9605, Validation Loss AVG: 7.9828
Epoch [5/80], Training Loss: 21.9313, Validation Loss Current: 6.5329, Validation Loss AVG: 7.6255
Epoch [6/80], Training Loss: 21.3501, Validation Loss Current: 6.0166, Validation Loss AVG: 7.1946
Epoch [7/80], Training Loss: 21.3295, Validation Loss Current: 6.4020, Validation Loss AVG: 7.5600
Epoch [8/80], Training Loss: 20.2883, Validation Loss Current: 6.1203, Validation Loss AVG: 7.4532
Epoch [9/80], Training Loss: 20.2394, Validation Loss Current: 6.4149, Validation Loss AVG: 7.5775
Epoch [10/80], Training Loss: 19.8074, Validation Loss Current: 6.0967, Validation Loss AVG: 7.4755
Epoch [11/80], Training Loss: 19.4827, Validation Loss Current: 7.1825, Validation Loss AVG: 8.5997
Epoch [12/80], Training Loss: 21.0151, Validation Loss Current: 5.8986, Validation Loss AVG: 7.1609
Epoch [13/80], Training Loss: 18.5534, Validation Loss Current: 6.0668, Validation Loss AVG: 7.3876
Epoch [14/80], Training Loss: 17.9121, Validation Loss Current: 6.0166, Validation Loss AVG: 7.3917
Epoch [15/80], Training Loss: 17.8090, Validation Loss Current: 6.2394, Validation Loss AVG: 7.5506
Epoch [16/80], Training Loss: 18.5325, Validation Loss Current: 5.9782, Validation Loss AVG: 7.3043
Epoch [17/80], Training Loss: 19.9955, Validation Loss Current: 6.5425, Validation Loss AVG: 7.7666
Epoch [18/80], Training Loss: 18.5611, Validation Loss Current: 6.5053, Validation Loss AVG: 7.7093
Epoch [19/80], Training Loss: 17.0816, Validation Loss Current: 6.0650, Validation Loss AVG: 7.4340
Epoch [20/80], Training Loss: 16.2595, Validation Loss Current: 6.5365, Validation Loss AVG: 8.2236
Epoch [21/80], Training Loss: 17.0660, Validation Loss Current: 7.5699, Validation Loss AVG: 8.9892
Epoch [22/80], Training Loss: 18.5580, Validation Loss Current: 6.3645, Validation Loss AVG: 7.7080
Epoch [23/80], Training Loss: 16.9087, Validation Loss Current: 6.7826, Validation Loss AVG: 8.4308
Epoch [24/80], Training Loss: 16.9086, Validation Loss Current: 6.2162, Validation Loss AVG: 7.6939
Epoch [25/80], Training Loss: 17.4528, Validation Loss Current: 7.8605, Validation Loss AVG: 9.1394
Epoch [26/80], Training Loss: 22.5871, Validation Loss Current: 6.4282, Validation Loss AVG: 7.5882
Epoch [27/80], Training Loss: 17.7343, Validation Loss Current: 6.4507, Validation Loss AVG: 7.7469
Epoch [28/80], Training Loss: 15.5450, Validation Loss Current: 6.6742, Validation Loss AVG: 8.1640
Epoch [29/80], Training Loss: 15.8492, Validation Loss Current: 7.8322, Validation Loss AVG: 9.6935
Epoch [30/80], Training Loss: 18.1885, Validation Loss Current: 6.5463, Validation Loss AVG: 8.0890
Epoch [31/80], Training Loss: 14.9350, Validation Loss Current: 6.1056, Validation Loss AVG: 7.7640
Epoch [32/80], Training Loss: 14.5006, Validation Loss Current: 6.8405, Validation Loss AVG: 8.9669
 --- Early Stopped ---
Patch distance: 0.6 finished training. Best epoch: 12 Best val accuracy: 0.5174342105263158 Best val loss: 5.898589134216309


Current group: 0.8
Epoch [1/80], Training Loss: 21.8363, Validation Loss Current: 5.9683, Validation Loss AVG: 7.8768
Epoch [2/80], Training Loss: 19.4185, Validation Loss Current: 7.5285, Validation Loss AVG: 10.0261
Epoch [3/80], Training Loss: 21.8002, Validation Loss Current: 5.9702, Validation Loss AVG: 7.5583
Epoch [4/80], Training Loss: 19.0359, Validation Loss Current: 5.8524, Validation Loss AVG: 7.4806
Epoch [5/80], Training Loss: 18.8537, Validation Loss Current: 6.3019, Validation Loss AVG: 8.1547
Epoch [6/80], Training Loss: 22.1129, Validation Loss Current: 5.8223, Validation Loss AVG: 7.5420
Epoch [7/80], Training Loss: 20.8791, Validation Loss Current: 6.0961, Validation Loss AVG: 7.9512
Epoch [8/80], Training Loss: 18.4949, Validation Loss Current: 6.0468, Validation Loss AVG: 7.9136
Epoch [9/80], Training Loss: 17.2150, Validation Loss Current: 7.1567, Validation Loss AVG: 8.9286
Epoch [10/80], Training Loss: 19.0223, Validation Loss Current: 6.3437, Validation Loss AVG: 7.9397
Epoch [11/80], Training Loss: 18.5213, Validation Loss Current: 6.4049, Validation Loss AVG: 8.3166
Epoch [12/80], Training Loss: 16.6874, Validation Loss Current: 5.9158, Validation Loss AVG: 7.9893
Epoch [13/80], Training Loss: 15.2722, Validation Loss Current: 6.2574, Validation Loss AVG: 8.4127
Epoch [14/80], Training Loss: 14.0512, Validation Loss Current: 5.8542, Validation Loss AVG: 7.9264
Epoch [15/80], Training Loss: 15.4864, Validation Loss Current: 5.5548, Validation Loss AVG: 7.9402
Epoch [16/80], Training Loss: 17.1282, Validation Loss Current: 6.2254, Validation Loss AVG: 9.2216
Epoch [17/80], Training Loss: 16.9143, Validation Loss Current: 6.8800, Validation Loss AVG: 8.3956
Epoch [18/80], Training Loss: 15.0018, Validation Loss Current: 6.1691, Validation Loss AVG: 8.2234
Epoch [19/80], Training Loss: 13.2678, Validation Loss Current: 6.5904, Validation Loss AVG: 9.5026
Epoch [20/80], Training Loss: 12.5481, Validation Loss Current: 6.5769, Validation Loss AVG: 8.5709
Epoch [21/80], Training Loss: 15.0981, Validation Loss Current: 7.6132, Validation Loss AVG: 10.8806
Epoch [22/80], Training Loss: 21.9729, Validation Loss Current: 5.7704, Validation Loss AVG: 7.6342
Epoch [23/80], Training Loss: 15.4378, Validation Loss Current: 6.3811, Validation Loss AVG: 8.9232
Epoch [24/80], Training Loss: 13.5674, Validation Loss Current: 5.8960, Validation Loss AVG: 7.7880
Epoch [25/80], Training Loss: 12.6518, Validation Loss Current: 7.0625, Validation Loss AVG: 9.1428
Epoch [26/80], Training Loss: 12.4464, Validation Loss Current: 6.1452, Validation Loss AVG: 8.7812
Epoch [27/80], Training Loss: 10.9902, Validation Loss Current: 6.9881, Validation Loss AVG: 10.1814
Epoch [28/80], Training Loss: 10.8768, Validation Loss Current: 6.5369, Validation Loss AVG: 8.5527
Epoch [29/80], Training Loss: 10.5239, Validation Loss Current: 6.9865, Validation Loss AVG: 9.3269
Epoch [30/80], Training Loss: 12.0383, Validation Loss Current: 6.8333, Validation Loss AVG: 9.6953
Epoch [31/80], Training Loss: 13.7366, Validation Loss Current: 6.0838, Validation Loss AVG: 8.2071
Epoch [32/80], Training Loss: 12.0617, Validation Loss Current: 6.6443, Validation Loss AVG: 9.0297
Epoch [33/80], Training Loss: 11.3003, Validation Loss Current: 7.4373, Validation Loss AVG: 10.0262
Epoch [34/80], Training Loss: 10.8872, Validation Loss Current: 16.2334, Validation Loss AVG: 18.3226
Epoch [35/80], Training Loss: 23.4413, Validation Loss Current: 6.2622, Validation Loss AVG: 8.0741
 --- Early Stopped ---
Patch distance: 0.8 finished training. Best epoch: 15 Best val accuracy: 0.5157894736842106 Best val loss: 5.554811120033264


Current group: 0.2
Epoch [1/80], Training Loss: 39.3343, Validation Loss Current: 9.2464, Validation Loss AVG: 7.8185
Epoch [2/80], Training Loss: 35.5690, Validation Loss Current: 8.4909, Validation Loss AVG: 7.2483
Epoch [3/80], Training Loss: 33.2877, Validation Loss Current: 8.2629, Validation Loss AVG: 7.5890
Epoch [4/80], Training Loss: 30.9480, Validation Loss Current: 8.4131, Validation Loss AVG: 7.6764
Epoch [5/80], Training Loss: 30.9491, Validation Loss Current: 7.8528, Validation Loss AVG: 7.2480
Epoch [6/80], Training Loss: 30.1850, Validation Loss Current: 7.8962, Validation Loss AVG: 7.8894
Epoch [7/80], Training Loss: 29.2239, Validation Loss Current: 7.9480, Validation Loss AVG: 8.1481
Epoch [8/80], Training Loss: 29.6258, Validation Loss Current: 7.8471, Validation Loss AVG: 7.6882
Epoch [9/80], Training Loss: 29.1577, Validation Loss Current: 7.7494, Validation Loss AVG: 7.6295
Epoch [10/80], Training Loss: 27.7457, Validation Loss Current: 8.4601, Validation Loss AVG: 7.3507
Epoch [11/80], Training Loss: 26.6571, Validation Loss Current: 9.3765, Validation Loss AVG: 7.5921
Epoch [12/80], Training Loss: 31.1043, Validation Loss Current: 7.9890, Validation Loss AVG: 7.5489
Epoch [13/80], Training Loss: 26.7158, Validation Loss Current: 7.7065, Validation Loss AVG: 7.8612
Epoch [14/80], Training Loss: 25.6785, Validation Loss Current: 8.0159, Validation Loss AVG: 8.0908
Epoch [15/80], Training Loss: 26.4559, Validation Loss Current: 8.2244, Validation Loss AVG: 8.0477
Epoch [16/80], Training Loss: 26.3058, Validation Loss Current: 7.8954, Validation Loss AVG: 7.9815
Epoch [17/80], Training Loss: 25.4006, Validation Loss Current: 7.7945, Validation Loss AVG: 8.6532
Epoch [18/80], Training Loss: 25.1884, Validation Loss Current: 7.4525, Validation Loss AVG: 8.2320
Epoch [19/80], Training Loss: 24.4553, Validation Loss Current: 8.0561, Validation Loss AVG: 9.0328
Epoch [20/80], Training Loss: 25.7687, Validation Loss Current: 7.5463, Validation Loss AVG: 8.5334
Epoch [21/80], Training Loss: 21.6942, Validation Loss Current: 7.8281, Validation Loss AVG: 8.5172
Epoch [22/80], Training Loss: 21.8181, Validation Loss Current: 10.1175, Validation Loss AVG: 11.7470
Epoch [23/80], Training Loss: 25.7706, Validation Loss Current: 7.6126, Validation Loss AVG: 8.9948
Epoch [24/80], Training Loss: 24.5034, Validation Loss Current: 8.2104, Validation Loss AVG: 9.3334
Epoch [25/80], Training Loss: 21.6505, Validation Loss Current: 7.7796, Validation Loss AVG: 10.7148
Epoch [26/80], Training Loss: 21.3508, Validation Loss Current: 8.1051, Validation Loss AVG: 9.8334
Epoch [27/80], Training Loss: 20.3587, Validation Loss Current: 8.2770, Validation Loss AVG: 9.7160
Epoch [28/80], Training Loss: 19.1578, Validation Loss Current: 8.5458, Validation Loss AVG: 10.6042
Epoch [29/80], Training Loss: 19.3378, Validation Loss Current: 8.2693, Validation Loss AVG: 9.9707
Epoch [30/80], Training Loss: 17.3626, Validation Loss Current: 9.0498, Validation Loss AVG: 10.5944
Epoch [31/80], Training Loss: 18.8689, Validation Loss Current: 9.0515, Validation Loss AVG: 9.8411
Epoch [32/80], Training Loss: 17.6190, Validation Loss Current: 8.9904, Validation Loss AVG: 10.5207
Epoch [33/80], Training Loss: 17.7649, Validation Loss Current: 8.9184, Validation Loss AVG: 10.0659
Epoch [34/80], Training Loss: 18.8460, Validation Loss Current: 10.1708, Validation Loss AVG: 10.3011
Epoch [35/80], Training Loss: 29.3075, Validation Loss Current: 9.1637, Validation Loss AVG: 10.7869
Epoch [36/80], Training Loss: 32.9705, Validation Loss Current: 8.1401, Validation Loss AVG: 8.2502
Epoch [37/80], Training Loss: 26.4675, Validation Loss Current: 7.9192, Validation Loss AVG: 9.0070
Epoch [38/80], Training Loss: 23.0107, Validation Loss Current: 8.3233, Validation Loss AVG: 9.3555
 --- Early Stopped ---
Patch distance: 0.2 finished training. Best epoch: 18 Best val accuracy: 0.41907894736842105 Best val loss: 7.452544927597046


Fold: 3
----- Training alexnet with sequence: [1, 0.4, 0.6, 0.8, 0.2] -----
Current group: 1
Epoch [1/80], Training Loss: 41.4836, Validation Loss Current: 10.3640, Validation Loss AVG: 10.3633
Epoch [2/80], Training Loss: 41.4027, Validation Loss Current: 10.3254, Validation Loss AVG: 10.3267
Epoch [3/80], Training Loss: 41.2685, Validation Loss Current: 10.2980, Validation Loss AVG: 10.2985
Epoch [4/80], Training Loss: 41.0799, Validation Loss Current: 10.2720, Validation Loss AVG: 10.2689
Epoch [5/80], Training Loss: 41.0301, Validation Loss Current: 10.2286, Validation Loss AVG: 10.2304
Epoch [6/80], Training Loss: 40.8443, Validation Loss Current: 10.1910, Validation Loss AVG: 10.1904
Epoch [7/80], Training Loss: 40.7310, Validation Loss Current: 10.1479, Validation Loss AVG: 10.1381
Epoch [8/80], Training Loss: 40.3627, Validation Loss Current: 10.0908, Validation Loss AVG: 10.0870
Epoch [9/80], Training Loss: 40.4377, Validation Loss Current: 10.0678, Validation Loss AVG: 10.0571
Epoch [10/80], Training Loss: 40.2304, Validation Loss Current: 10.0517, Validation Loss AVG: 10.0528
Epoch [11/80], Training Loss: 40.3918, Validation Loss Current: 10.0062, Validation Loss AVG: 10.0314
Epoch [12/80], Training Loss: 40.2569, Validation Loss Current: 10.0085, Validation Loss AVG: 10.0284
Epoch [13/80], Training Loss: 39.9168, Validation Loss Current: 9.9924, Validation Loss AVG: 10.0113
Epoch [14/80], Training Loss: 39.7677, Validation Loss Current: 9.9727, Validation Loss AVG: 9.9999
Epoch [15/80], Training Loss: 39.5312, Validation Loss Current: 9.9422, Validation Loss AVG: 10.0028
Epoch [16/80], Training Loss: 39.6099, Validation Loss Current: 9.9369, Validation Loss AVG: 10.0076
Epoch [17/80], Training Loss: 39.1311, Validation Loss Current: 9.9086, Validation Loss AVG: 9.9845
Epoch [18/80], Training Loss: 39.8998, Validation Loss Current: 9.8706, Validation Loss AVG: 9.9737
Epoch [19/80], Training Loss: 39.0748, Validation Loss Current: 9.8693, Validation Loss AVG: 9.9658
Epoch [20/80], Training Loss: 39.5916, Validation Loss Current: 9.8452, Validation Loss AVG: 9.9442
Epoch [21/80], Training Loss: 38.7533, Validation Loss Current: 9.8005, Validation Loss AVG: 9.9355
Epoch [22/80], Training Loss: 39.2032, Validation Loss Current: 9.7860, Validation Loss AVG: 9.9295
Epoch [23/80], Training Loss: 39.1423, Validation Loss Current: 9.7345, Validation Loss AVG: 9.9292
Epoch [24/80], Training Loss: 38.4131, Validation Loss Current: 9.7228, Validation Loss AVG: 9.9559
Epoch [25/80], Training Loss: 38.3241, Validation Loss Current: 9.6989, Validation Loss AVG: 9.9181
Epoch [26/80], Training Loss: 38.8140, Validation Loss Current: 9.6708, Validation Loss AVG: 9.9104
Epoch [27/80], Training Loss: 38.3448, Validation Loss Current: 9.6371, Validation Loss AVG: 9.9967
Epoch [28/80], Training Loss: 38.2929, Validation Loss Current: 9.5782, Validation Loss AVG: 9.8901
Epoch [29/80], Training Loss: 37.8705, Validation Loss Current: 9.5274, Validation Loss AVG: 9.9014
Epoch [30/80], Training Loss: 36.9141, Validation Loss Current: 9.5609, Validation Loss AVG: 10.0208
Epoch [31/80], Training Loss: 38.2045, Validation Loss Current: 9.4343, Validation Loss AVG: 9.8128
Epoch [32/80], Training Loss: 37.8454, Validation Loss Current: 9.4305, Validation Loss AVG: 9.8065
Epoch [33/80], Training Loss: 36.2577, Validation Loss Current: 9.3984, Validation Loss AVG: 9.8674
Epoch [34/80], Training Loss: 37.3337, Validation Loss Current: 9.2714, Validation Loss AVG: 9.8286
Epoch [35/80], Training Loss: 37.0083, Validation Loss Current: 9.2343, Validation Loss AVG: 9.7583
Epoch [36/80], Training Loss: 36.2873, Validation Loss Current: 9.2840, Validation Loss AVG: 9.8707
Epoch [37/80], Training Loss: 35.1532, Validation Loss Current: 9.2793, Validation Loss AVG: 9.9267
Epoch [38/80], Training Loss: 36.4718, Validation Loss Current: 8.9162, Validation Loss AVG: 9.5812
Epoch [39/80], Training Loss: 35.9231, Validation Loss Current: 8.8928, Validation Loss AVG: 9.6186
Epoch [40/80], Training Loss: 34.4523, Validation Loss Current: 8.6477, Validation Loss AVG: 9.6046
Epoch [41/80], Training Loss: 34.0233, Validation Loss Current: 8.8853, Validation Loss AVG: 9.5886
Epoch [42/80], Training Loss: 34.1480, Validation Loss Current: 10.2255, Validation Loss AVG: 10.3745
Epoch [43/80], Training Loss: 36.0869, Validation Loss Current: 8.4908, Validation Loss AVG: 9.4110
Epoch [44/80], Training Loss: 33.8785, Validation Loss Current: 9.0566, Validation Loss AVG: 9.7417
Epoch [45/80], Training Loss: 33.7372, Validation Loss Current: 8.3779, Validation Loss AVG: 9.5892
Epoch [46/80], Training Loss: 31.6549, Validation Loss Current: 8.2310, Validation Loss AVG: 9.4911
Epoch [47/80], Training Loss: 31.5896, Validation Loss Current: 8.5166, Validation Loss AVG: 10.3458
Epoch [48/80], Training Loss: 33.1758, Validation Loss Current: 8.2568, Validation Loss AVG: 9.2792
Epoch [49/80], Training Loss: 31.3609, Validation Loss Current: 8.4831, Validation Loss AVG: 10.4000
Epoch [50/80], Training Loss: 31.2656, Validation Loss Current: 8.6065, Validation Loss AVG: 10.3123
Epoch [51/80], Training Loss: 30.6011, Validation Loss Current: 7.7904, Validation Loss AVG: 9.1985
Epoch [52/80], Training Loss: 31.0832, Validation Loss Current: 9.0783, Validation Loss AVG: 11.1518
Epoch [53/80], Training Loss: 37.1081, Validation Loss Current: 8.6350, Validation Loss AVG: 9.3055
Epoch [54/80], Training Loss: 34.5560, Validation Loss Current: 8.1422, Validation Loss AVG: 9.1065
Epoch [55/80], Training Loss: 30.5387, Validation Loss Current: 7.8859, Validation Loss AVG: 9.0569
Epoch [56/80], Training Loss: 30.9274, Validation Loss Current: 7.7145, Validation Loss AVG: 9.3991
Epoch [57/80], Training Loss: 31.2726, Validation Loss Current: 7.6940, Validation Loss AVG: 9.1069
Epoch [58/80], Training Loss: 29.7377, Validation Loss Current: 7.6779, Validation Loss AVG: 9.3684
Epoch [59/80], Training Loss: 29.2740, Validation Loss Current: 7.5042, Validation Loss AVG: 9.2610
Epoch [60/80], Training Loss: 29.1447, Validation Loss Current: 7.7198, Validation Loss AVG: 9.7159
Epoch [61/80], Training Loss: 31.8676, Validation Loss Current: 7.6115, Validation Loss AVG: 8.9207
Epoch [62/80], Training Loss: 31.2357, Validation Loss Current: 7.6807, Validation Loss AVG: 8.8974
Epoch [63/80], Training Loss: 29.9466, Validation Loss Current: 7.4374, Validation Loss AVG: 9.0053
Epoch [64/80], Training Loss: 28.8559, Validation Loss Current: 7.2404, Validation Loss AVG: 9.3447
Epoch [65/80], Training Loss: 28.0009, Validation Loss Current: 7.1998, Validation Loss AVG: 9.0824
Epoch [66/80], Training Loss: 29.1287, Validation Loss Current: 7.0804, Validation Loss AVG: 9.2631
Epoch [67/80], Training Loss: 29.6970, Validation Loss Current: 7.6382, Validation Loss AVG: 8.7320
Epoch [68/80], Training Loss: 28.9487, Validation Loss Current: 8.6996, Validation Loss AVG: 9.2188
Epoch [69/80], Training Loss: 29.5135, Validation Loss Current: 7.0097, Validation Loss AVG: 8.6586
Epoch [70/80], Training Loss: 26.7314, Validation Loss Current: 7.0747, Validation Loss AVG: 9.2073
Epoch [71/80], Training Loss: 26.0002, Validation Loss Current: 6.9477, Validation Loss AVG: 9.0287
Epoch [72/80], Training Loss: 27.5832, Validation Loss Current: 6.6850, Validation Loss AVG: 8.6355
Epoch [73/80], Training Loss: 27.7257, Validation Loss Current: 6.9111, Validation Loss AVG: 8.8857
Epoch [74/80], Training Loss: 27.4401, Validation Loss Current: 7.3879, Validation Loss AVG: 9.7131
Epoch [75/80], Training Loss: 27.2540, Validation Loss Current: 7.1886, Validation Loss AVG: 8.9218
Epoch [76/80], Training Loss: 27.5649, Validation Loss Current: 8.0458, Validation Loss AVG: 10.7233
Epoch [77/80], Training Loss: 28.7095, Validation Loss Current: 7.0452, Validation Loss AVG: 8.2724
Epoch [78/80], Training Loss: 25.9773, Validation Loss Current: 6.8565, Validation Loss AVG: 8.9611
Epoch [79/80], Training Loss: 25.5010, Validation Loss Current: 6.6640, Validation Loss AVG: 8.7348
Epoch [80/80], Training Loss: 25.1747, Validation Loss Current: 6.7782, Validation Loss AVG: 9.1643
Patch distance: 1 finished training. Best epoch: 79 Best val accuracy: 0.37105263157894736 Best val loss: 6.663990378379822


Current group: 0.4
Epoch [1/80], Training Loss: 35.0427, Validation Loss Current: 8.2879, Validation Loss AVG: 7.9279
Epoch [2/80], Training Loss: 32.5830, Validation Loss Current: 8.1966, Validation Loss AVG: 7.9917
Epoch [3/80], Training Loss: 32.0023, Validation Loss Current: 8.1065, Validation Loss AVG: 7.8570
Epoch [4/80], Training Loss: 30.8752, Validation Loss Current: 8.1707, Validation Loss AVG: 7.7476
Epoch [5/80], Training Loss: 32.1118, Validation Loss Current: 8.0301, Validation Loss AVG: 7.6583
Epoch [6/80], Training Loss: 31.0609, Validation Loss Current: 8.0537, Validation Loss AVG: 7.8447
Epoch [7/80], Training Loss: 32.0429, Validation Loss Current: 8.2326, Validation Loss AVG: 7.7762
Epoch [8/80], Training Loss: 30.9714, Validation Loss Current: 7.7984, Validation Loss AVG: 7.7118
Epoch [9/80], Training Loss: 29.1912, Validation Loss Current: 8.4796, Validation Loss AVG: 8.6735
Epoch [10/80], Training Loss: 31.1234, Validation Loss Current: 8.4500, Validation Loss AVG: 8.5610
Epoch [11/80], Training Loss: 31.1741, Validation Loss Current: 8.3221, Validation Loss AVG: 8.1652
Epoch [12/80], Training Loss: 30.4443, Validation Loss Current: 7.8767, Validation Loss AVG: 7.8462
Epoch [13/80], Training Loss: 28.9757, Validation Loss Current: 8.1789, Validation Loss AVG: 8.0176
Epoch [14/80], Training Loss: 29.4012, Validation Loss Current: 7.8228, Validation Loss AVG: 7.7011
Epoch [15/80], Training Loss: 30.8750, Validation Loss Current: 7.8234, Validation Loss AVG: 7.9084
Epoch [16/80], Training Loss: 28.7779, Validation Loss Current: 7.7683, Validation Loss AVG: 7.9275
Epoch [17/80], Training Loss: 30.2028, Validation Loss Current: 7.8515, Validation Loss AVG: 7.7073
Epoch [18/80], Training Loss: 31.4069, Validation Loss Current: 7.8430, Validation Loss AVG: 7.6425
Epoch [19/80], Training Loss: 30.2142, Validation Loss Current: 7.8036, Validation Loss AVG: 7.7232
Epoch [20/80], Training Loss: 28.5161, Validation Loss Current: 7.5156, Validation Loss AVG: 7.4970
Epoch [21/80], Training Loss: 28.2075, Validation Loss Current: 7.9400, Validation Loss AVG: 7.7213
Epoch [22/80], Training Loss: 28.2973, Validation Loss Current: 7.7496, Validation Loss AVG: 7.9730
Epoch [23/80], Training Loss: 27.2191, Validation Loss Current: 7.4624, Validation Loss AVG: 7.6077
Epoch [24/80], Training Loss: 27.3741, Validation Loss Current: 8.3329, Validation Loss AVG: 8.3256
Epoch [25/80], Training Loss: 28.0611, Validation Loss Current: 7.4468, Validation Loss AVG: 7.5543
Epoch [26/80], Training Loss: 27.0425, Validation Loss Current: 7.4957, Validation Loss AVG: 7.4148
Epoch [27/80], Training Loss: 27.0986, Validation Loss Current: 9.0175, Validation Loss AVG: 9.4789
Epoch [28/80], Training Loss: 28.0333, Validation Loss Current: 7.3309, Validation Loss AVG: 7.4902
Epoch [29/80], Training Loss: 26.4194, Validation Loss Current: 7.2006, Validation Loss AVG: 7.5857
Epoch [30/80], Training Loss: 25.8661, Validation Loss Current: 7.0277, Validation Loss AVG: 7.3353
Epoch [31/80], Training Loss: 26.6091, Validation Loss Current: 7.3660, Validation Loss AVG: 7.8615
Epoch [32/80], Training Loss: 27.3994, Validation Loss Current: 7.3255, Validation Loss AVG: 7.4658
Epoch [33/80], Training Loss: 26.5760, Validation Loss Current: 7.4132, Validation Loss AVG: 7.6760
Epoch [34/80], Training Loss: 30.4996, Validation Loss Current: 7.3251, Validation Loss AVG: 7.5931
Epoch [35/80], Training Loss: 26.5984, Validation Loss Current: 7.0052, Validation Loss AVG: 7.3697
Epoch [36/80], Training Loss: 25.2713, Validation Loss Current: 7.8193, Validation Loss AVG: 7.8929
Epoch [37/80], Training Loss: 25.6125, Validation Loss Current: 7.2227, Validation Loss AVG: 7.5267
Epoch [38/80], Training Loss: 24.8630, Validation Loss Current: 7.0071, Validation Loss AVG: 7.4431
Epoch [39/80], Training Loss: 24.2401, Validation Loss Current: 7.0445, Validation Loss AVG: 7.4366
Epoch [40/80], Training Loss: 26.9155, Validation Loss Current: 9.5709, Validation Loss AVG: 9.9512
Epoch [41/80], Training Loss: 33.6285, Validation Loss Current: 7.9712, Validation Loss AVG: 7.9592
Epoch [42/80], Training Loss: 28.8646, Validation Loss Current: 7.3161, Validation Loss AVG: 7.7018
Epoch [43/80], Training Loss: 27.2603, Validation Loss Current: 7.7288, Validation Loss AVG: 8.4492
Epoch [44/80], Training Loss: 31.4731, Validation Loss Current: 7.3962, Validation Loss AVG: 7.7592
Epoch [45/80], Training Loss: 26.3187, Validation Loss Current: 7.9775, Validation Loss AVG: 8.5511
Epoch [46/80], Training Loss: 26.3060, Validation Loss Current: 7.4075, Validation Loss AVG: 7.7248
Epoch [47/80], Training Loss: 25.0831, Validation Loss Current: 6.9819, Validation Loss AVG: 7.5510
Epoch [48/80], Training Loss: 24.5702, Validation Loss Current: 6.8318, Validation Loss AVG: 7.3038
Epoch [49/80], Training Loss: 23.3287, Validation Loss Current: 7.2068, Validation Loss AVG: 7.5746
Epoch [50/80], Training Loss: 22.7906, Validation Loss Current: 7.0488, Validation Loss AVG: 7.5774
Epoch [51/80], Training Loss: 23.3423, Validation Loss Current: 6.9998, Validation Loss AVG: 7.4141
Epoch [52/80], Training Loss: 21.9345, Validation Loss Current: 6.8904, Validation Loss AVG: 7.6617
Epoch [53/80], Training Loss: 22.2836, Validation Loss Current: 6.8175, Validation Loss AVG: 7.5606
Epoch [54/80], Training Loss: 23.8418, Validation Loss Current: 6.9524, Validation Loss AVG: 7.9679
Epoch [55/80], Training Loss: 21.8556, Validation Loss Current: 7.8476, Validation Loss AVG: 8.8166
Epoch [56/80], Training Loss: 23.7561, Validation Loss Current: 6.6732, Validation Loss AVG: 7.6952
Epoch [57/80], Training Loss: 24.1421, Validation Loss Current: 7.1227, Validation Loss AVG: 7.7136
Epoch [58/80], Training Loss: 22.8204, Validation Loss Current: 6.9525, Validation Loss AVG: 7.7021
Epoch [59/80], Training Loss: 21.5610, Validation Loss Current: 6.7837, Validation Loss AVG: 7.5641
Epoch [60/80], Training Loss: 20.6322, Validation Loss Current: 7.0010, Validation Loss AVG: 7.6269
Epoch [61/80], Training Loss: 21.0044, Validation Loss Current: 6.7676, Validation Loss AVG: 7.4944
Epoch [62/80], Training Loss: 19.7376, Validation Loss Current: 7.3962, Validation Loss AVG: 8.7417
Epoch [63/80], Training Loss: 20.2891, Validation Loss Current: 6.8865, Validation Loss AVG: 7.8294
Epoch [64/80], Training Loss: 20.3953, Validation Loss Current: 6.6941, Validation Loss AVG: 7.6233
Epoch [65/80], Training Loss: 20.6249, Validation Loss Current: 8.1228, Validation Loss AVG: 8.6142
Epoch [66/80], Training Loss: 22.9290, Validation Loss Current: 7.3539, Validation Loss AVG: 8.3116
Epoch [67/80], Training Loss: 21.2100, Validation Loss Current: 6.9198, Validation Loss AVG: 7.9471
Epoch [68/80], Training Loss: 19.8902, Validation Loss Current: 7.5498, Validation Loss AVG: 8.1996
Epoch [69/80], Training Loss: 20.7462, Validation Loss Current: 7.5793, Validation Loss AVG: 8.4116
Epoch [70/80], Training Loss: 21.5466, Validation Loss Current: 7.0295, Validation Loss AVG: 7.7468
Epoch [71/80], Training Loss: 19.8569, Validation Loss Current: 7.1907, Validation Loss AVG: 8.2094
Epoch [72/80], Training Loss: 18.6952, Validation Loss Current: 7.0743, Validation Loss AVG: 8.1218
Epoch [73/80], Training Loss: 16.8781, Validation Loss Current: 9.9882, Validation Loss AVG: 11.2386
Epoch [74/80], Training Loss: 22.0090, Validation Loss Current: 7.9129, Validation Loss AVG: 8.7461
Epoch [75/80], Training Loss: 22.0046, Validation Loss Current: 7.3204, Validation Loss AVG: 8.5061
Epoch [76/80], Training Loss: 23.5845, Validation Loss Current: 7.1916, Validation Loss AVG: 7.8226
 --- Early Stopped ---
Patch distance: 0.4 finished training. Best epoch: 56 Best val accuracy: 0.4743421052631579 Best val loss: 6.673184752464294


Current group: 0.6
Epoch [1/80], Training Loss: 24.0063, Validation Loss Current: 6.2848, Validation Loss AVG: 7.3370
Epoch [2/80], Training Loss: 23.1458, Validation Loss Current: 6.5603, Validation Loss AVG: 7.5889
Epoch [3/80], Training Loss: 21.7004, Validation Loss Current: 6.5323, Validation Loss AVG: 7.9715
Epoch [4/80], Training Loss: 21.0204, Validation Loss Current: 6.2614, Validation Loss AVG: 7.5481
Epoch [5/80], Training Loss: 20.3512, Validation Loss Current: 8.6866, Validation Loss AVG: 9.5528
Epoch [6/80], Training Loss: 26.0845, Validation Loss Current: 6.1385, Validation Loss AVG: 7.3490
Epoch [7/80], Training Loss: 21.5353, Validation Loss Current: 6.1649, Validation Loss AVG: 7.6054
Epoch [8/80], Training Loss: 20.6011, Validation Loss Current: 6.2999, Validation Loss AVG: 7.9343
Epoch [9/80], Training Loss: 20.8675, Validation Loss Current: 6.3843, Validation Loss AVG: 8.1605
Epoch [10/80], Training Loss: 20.4366, Validation Loss Current: 6.4335, Validation Loss AVG: 7.6773
Epoch [11/80], Training Loss: 19.3759, Validation Loss Current: 6.0927, Validation Loss AVG: 7.5640
Epoch [12/80], Training Loss: 19.4175, Validation Loss Current: 6.0714, Validation Loss AVG: 7.4466
Epoch [13/80], Training Loss: 17.7853, Validation Loss Current: 6.4730, Validation Loss AVG: 7.7174
Epoch [14/80], Training Loss: 17.3816, Validation Loss Current: 5.8282, Validation Loss AVG: 7.5343
Epoch [15/80], Training Loss: 16.2635, Validation Loss Current: 6.1586, Validation Loss AVG: 7.7739
Epoch [16/80], Training Loss: 15.8901, Validation Loss Current: 6.0489, Validation Loss AVG: 7.7633
Epoch [17/80], Training Loss: 16.0861, Validation Loss Current: 6.7391, Validation Loss AVG: 8.1198
Epoch [18/80], Training Loss: 16.9580, Validation Loss Current: 6.1033, Validation Loss AVG: 7.7806
Epoch [19/80], Training Loss: 15.7363, Validation Loss Current: 6.5286, Validation Loss AVG: 8.0985
Epoch [20/80], Training Loss: 15.6288, Validation Loss Current: 6.9757, Validation Loss AVG: 8.8504
Epoch [21/80], Training Loss: 15.6210, Validation Loss Current: 6.1365, Validation Loss AVG: 7.7773
Epoch [22/80], Training Loss: 14.2062, Validation Loss Current: 6.5230, Validation Loss AVG: 8.5113
Epoch [23/80], Training Loss: 13.8758, Validation Loss Current: 8.4653, Validation Loss AVG: 9.6019
Epoch [24/80], Training Loss: 15.7068, Validation Loss Current: 6.1588, Validation Loss AVG: 7.9338
Epoch [25/80], Training Loss: 14.8274, Validation Loss Current: 6.5303, Validation Loss AVG: 8.2379
Epoch [26/80], Training Loss: 12.6553, Validation Loss Current: 6.5608, Validation Loss AVG: 8.9026
Epoch [27/80], Training Loss: 12.0683, Validation Loss Current: 6.2267, Validation Loss AVG: 8.2706
Epoch [28/80], Training Loss: 11.0489, Validation Loss Current: 7.1657, Validation Loss AVG: 9.0537
Epoch [29/80], Training Loss: 11.9384, Validation Loss Current: 8.9498, Validation Loss AVG: 10.9638
Epoch [30/80], Training Loss: 22.8905, Validation Loss Current: 7.3621, Validation Loss AVG: 8.9155
Epoch [31/80], Training Loss: 16.9175, Validation Loss Current: 6.5771, Validation Loss AVG: 8.4279
Epoch [32/80], Training Loss: 13.9990, Validation Loss Current: 6.9689, Validation Loss AVG: 8.6143
Epoch [33/80], Training Loss: 12.1987, Validation Loss Current: 9.2685, Validation Loss AVG: 10.4450
Epoch [34/80], Training Loss: 18.1361, Validation Loss Current: 6.6818, Validation Loss AVG: 8.3135
 --- Early Stopped ---
Patch distance: 0.6 finished training. Best epoch: 14 Best val accuracy: 0.5023026315789474 Best val loss: 5.82822048664093


Current group: 0.8
Epoch [1/80], Training Loss: 20.6142, Validation Loss Current: 5.6175, Validation Loss AVG: 7.6045
Epoch [2/80], Training Loss: 20.1480, Validation Loss Current: 6.1185, Validation Loss AVG: 8.3542
Epoch [3/80], Training Loss: 19.4218, Validation Loss Current: 8.3713, Validation Loss AVG: 11.1544
Epoch [4/80], Training Loss: 25.3768, Validation Loss Current: 5.8568, Validation Loss AVG: 7.6487
Epoch [5/80], Training Loss: 21.5316, Validation Loss Current: 6.4161, Validation Loss AVG: 8.1304
Epoch [6/80], Training Loss: 18.1810, Validation Loss Current: 5.7490, Validation Loss AVG: 7.6332
Epoch [7/80], Training Loss: 16.6009, Validation Loss Current: 5.6063, Validation Loss AVG: 7.5138
Epoch [8/80], Training Loss: 15.7648, Validation Loss Current: 5.4430, Validation Loss AVG: 8.0848
Epoch [9/80], Training Loss: 14.4321, Validation Loss Current: 5.6793, Validation Loss AVG: 7.8573
Epoch [10/80], Training Loss: 13.4341, Validation Loss Current: 6.0493, Validation Loss AVG: 8.4291
Epoch [11/80], Training Loss: 14.0366, Validation Loss Current: 6.7301, Validation Loss AVG: 8.6810
Epoch [12/80], Training Loss: 16.7391, Validation Loss Current: 5.6471, Validation Loss AVG: 8.3403
Epoch [13/80], Training Loss: 13.0999, Validation Loss Current: 7.0816, Validation Loss AVG: 10.2894
Epoch [14/80], Training Loss: 16.7558, Validation Loss Current: 5.8402, Validation Loss AVG: 8.7283
Epoch [15/80], Training Loss: 13.7010, Validation Loss Current: 5.7637, Validation Loss AVG: 8.5354
Epoch [16/80], Training Loss: 11.6666, Validation Loss Current: 6.2946, Validation Loss AVG: 8.7349
Epoch [17/80], Training Loss: 13.3040, Validation Loss Current: 5.9643, Validation Loss AVG: 8.9676
Epoch [18/80], Training Loss: 10.5612, Validation Loss Current: 5.9224, Validation Loss AVG: 9.2507
Epoch [19/80], Training Loss: 10.4063, Validation Loss Current: 13.2216, Validation Loss AVG: 17.0432
Epoch [20/80], Training Loss: 31.4871, Validation Loss Current: 7.0289, Validation Loss AVG: 8.8381
Epoch [21/80], Training Loss: 20.2877, Validation Loss Current: 6.4968, Validation Loss AVG: 8.4709
Epoch [22/80], Training Loss: 16.8350, Validation Loss Current: 6.9812, Validation Loss AVG: 9.4961
Epoch [23/80], Training Loss: 15.9527, Validation Loss Current: 6.0407, Validation Loss AVG: 8.7346
Epoch [24/80], Training Loss: 11.9022, Validation Loss Current: 6.2535, Validation Loss AVG: 8.8637
Epoch [25/80], Training Loss: 12.3691, Validation Loss Current: 7.7709, Validation Loss AVG: 11.3041
Epoch [26/80], Training Loss: 18.4600, Validation Loss Current: 6.6151, Validation Loss AVG: 9.7302
Epoch [27/80], Training Loss: 13.8887, Validation Loss Current: 6.7071, Validation Loss AVG: 9.8045
Epoch [28/80], Training Loss: 13.8192, Validation Loss Current: 6.2821, Validation Loss AVG: 8.9595
 --- Early Stopped ---
Patch distance: 0.8 finished training. Best epoch: 8 Best val accuracy: 0.5016447368421053 Best val loss: 5.4429603815078735


Current group: 0.2
Epoch [1/80], Training Loss: 39.8266, Validation Loss Current: 9.1128, Validation Loss AVG: 7.9994
Epoch [2/80], Training Loss: 34.1580, Validation Loss Current: 9.3610, Validation Loss AVG: 9.0876
Epoch [3/80], Training Loss: 35.3203, Validation Loss Current: 8.5792, Validation Loss AVG: 8.3130
Epoch [4/80], Training Loss: 33.8063, Validation Loss Current: 8.4751, Validation Loss AVG: 7.6849
Epoch [5/80], Training Loss: 32.4099, Validation Loss Current: 8.2383, Validation Loss AVG: 7.6509
Epoch [6/80], Training Loss: 30.9020, Validation Loss Current: 8.1699, Validation Loss AVG: 7.7982
Epoch [7/80], Training Loss: 30.4219, Validation Loss Current: 8.3754, Validation Loss AVG: 7.5760
Epoch [8/80], Training Loss: 32.0418, Validation Loss Current: 8.1146, Validation Loss AVG: 7.5139
Epoch [9/80], Training Loss: 30.7811, Validation Loss Current: 8.4287, Validation Loss AVG: 7.9100
Epoch [10/80], Training Loss: 29.7124, Validation Loss Current: 8.7330, Validation Loss AVG: 8.1888
Epoch [11/80], Training Loss: 27.9786, Validation Loss Current: 8.1367, Validation Loss AVG: 7.9415
Epoch [12/80], Training Loss: 27.9121, Validation Loss Current: 7.9005, Validation Loss AVG: 8.2731
Epoch [13/80], Training Loss: 28.7105, Validation Loss Current: 8.3792, Validation Loss AVG: 8.3322
Epoch [14/80], Training Loss: 28.6400, Validation Loss Current: 7.9694, Validation Loss AVG: 7.9339
Epoch [15/80], Training Loss: 28.1384, Validation Loss Current: 7.8972, Validation Loss AVG: 8.7546
Epoch [16/80], Training Loss: 28.8935, Validation Loss Current: 7.9149, Validation Loss AVG: 8.2723
Epoch [17/80], Training Loss: 26.5940, Validation Loss Current: 7.7205, Validation Loss AVG: 8.3549
Epoch [18/80], Training Loss: 25.1515, Validation Loss Current: 7.8954, Validation Loss AVG: 8.1378
Epoch [19/80], Training Loss: 25.5286, Validation Loss Current: 7.9292, Validation Loss AVG: 8.7507
Epoch [20/80], Training Loss: 24.9940, Validation Loss Current: 7.8459, Validation Loss AVG: 7.9836
Epoch [21/80], Training Loss: 24.8498, Validation Loss Current: 7.7316, Validation Loss AVG: 8.4876
Epoch [22/80], Training Loss: 23.4775, Validation Loss Current: 8.2124, Validation Loss AVG: 7.7375
Epoch [23/80], Training Loss: 24.2388, Validation Loss Current: 7.8493, Validation Loss AVG: 8.0964
Epoch [24/80], Training Loss: 22.5294, Validation Loss Current: 8.1667, Validation Loss AVG: 8.1624
Epoch [25/80], Training Loss: 22.4343, Validation Loss Current: 8.2991, Validation Loss AVG: 8.0963
Epoch [26/80], Training Loss: 24.2689, Validation Loss Current: 8.1952, Validation Loss AVG: 8.5097
Epoch [27/80], Training Loss: 20.5766, Validation Loss Current: 8.0006, Validation Loss AVG: 8.5723
Epoch [28/80], Training Loss: 20.7724, Validation Loss Current: 8.7238, Validation Loss AVG: 8.7101
Epoch [29/80], Training Loss: 24.5636, Validation Loss Current: 8.8353, Validation Loss AVG: 9.6001
Epoch [30/80], Training Loss: 24.1729, Validation Loss Current: 7.8078, Validation Loss AVG: 8.6113
Epoch [31/80], Training Loss: 22.3029, Validation Loss Current: 7.8694, Validation Loss AVG: 8.0897
Epoch [32/80], Training Loss: 20.6410, Validation Loss Current: 8.1008, Validation Loss AVG: 8.3303
Epoch [33/80], Training Loss: 18.2662, Validation Loss Current: 7.9763, Validation Loss AVG: 9.3351
Epoch [34/80], Training Loss: 17.9379, Validation Loss Current: 8.5764, Validation Loss AVG: 9.5249
Epoch [35/80], Training Loss: 18.9190, Validation Loss Current: 8.2309, Validation Loss AVG: 9.4380
Epoch [36/80], Training Loss: 16.2692, Validation Loss Current: 8.5927, Validation Loss AVG: 9.5430
Epoch [37/80], Training Loss: 15.8901, Validation Loss Current: 9.5932, Validation Loss AVG: 9.8770
 --- Early Stopped ---
Patch distance: 0.2 finished training. Best epoch: 17 Best val accuracy: 0.44835526315789476 Best val loss: 7.7204670906066895


Fold: 4
----- Training alexnet with sequence: [1, 0.4, 0.6, 0.8, 0.2] -----
Current group: 1
Epoch [1/80], Training Loss: 41.5854, Validation Loss Current: 10.3847, Validation Loss AVG: 10.3845
Epoch [2/80], Training Loss: 41.4727, Validation Loss Current: 10.3535, Validation Loss AVG: 10.3524
Epoch [3/80], Training Loss: 41.3315, Validation Loss Current: 10.3179, Validation Loss AVG: 10.3176
Epoch [4/80], Training Loss: 41.1789, Validation Loss Current: 10.2802, Validation Loss AVG: 10.2773
Epoch [5/80], Training Loss: 40.9582, Validation Loss Current: 10.2318, Validation Loss AVG: 10.2337
Epoch [6/80], Training Loss: 40.9372, Validation Loss Current: 10.1742, Validation Loss AVG: 10.1737
Epoch [7/80], Training Loss: 40.7308, Validation Loss Current: 10.1256, Validation Loss AVG: 10.1177
Epoch [8/80], Training Loss: 40.5664, Validation Loss Current: 10.0456, Validation Loss AVG: 10.0373
Epoch [9/80], Training Loss: 39.8510, Validation Loss Current: 9.9861, Validation Loss AVG: 9.9813
Epoch [10/80], Training Loss: 40.2145, Validation Loss Current: 9.9704, Validation Loss AVG: 9.9639
Epoch [11/80], Training Loss: 40.1749, Validation Loss Current: 9.9919, Validation Loss AVG: 9.9771
Epoch [12/80], Training Loss: 39.7164, Validation Loss Current: 9.9683, Validation Loss AVG: 9.9681
Epoch [13/80], Training Loss: 39.5483, Validation Loss Current: 9.9638, Validation Loss AVG: 9.9870
Epoch [14/80], Training Loss: 39.8589, Validation Loss Current: 9.9519, Validation Loss AVG: 9.9671
Epoch [15/80], Training Loss: 39.6831, Validation Loss Current: 9.9110, Validation Loss AVG: 9.9129
Epoch [16/80], Training Loss: 40.0228, Validation Loss Current: 9.8867, Validation Loss AVG: 9.8991
Epoch [17/80], Training Loss: 39.3937, Validation Loss Current: 9.8769, Validation Loss AVG: 9.9103
Epoch [18/80], Training Loss: 39.9003, Validation Loss Current: 9.8669, Validation Loss AVG: 9.9097
Epoch [19/80], Training Loss: 39.5378, Validation Loss Current: 9.8122, Validation Loss AVG: 9.8669
Epoch [20/80], Training Loss: 39.6049, Validation Loss Current: 9.7759, Validation Loss AVG: 9.8352
Epoch [21/80], Training Loss: 39.2117, Validation Loss Current: 9.7208, Validation Loss AVG: 9.7982
Epoch [22/80], Training Loss: 39.4478, Validation Loss Current: 9.7014, Validation Loss AVG: 9.7803
Epoch [23/80], Training Loss: 39.4385, Validation Loss Current: 9.6695, Validation Loss AVG: 9.7642
Epoch [24/80], Training Loss: 38.5358, Validation Loss Current: 9.6184, Validation Loss AVG: 9.7446
Epoch [25/80], Training Loss: 39.0854, Validation Loss Current: 9.6409, Validation Loss AVG: 9.7551
Epoch [26/80], Training Loss: 38.4928, Validation Loss Current: 9.5540, Validation Loss AVG: 9.7125
Epoch [27/80], Training Loss: 39.6665, Validation Loss Current: 9.6235, Validation Loss AVG: 9.7543
Epoch [28/80], Training Loss: 39.3574, Validation Loss Current: 9.5257, Validation Loss AVG: 9.6918
Epoch [29/80], Training Loss: 37.9281, Validation Loss Current: 9.4717, Validation Loss AVG: 9.6729
Epoch [30/80], Training Loss: 38.3241, Validation Loss Current: 9.4666, Validation Loss AVG: 9.6851
Epoch [31/80], Training Loss: 38.6261, Validation Loss Current: 9.3860, Validation Loss AVG: 9.6655
Epoch [32/80], Training Loss: 38.2682, Validation Loss Current: 9.3213, Validation Loss AVG: 9.6374
Epoch [33/80], Training Loss: 37.6420, Validation Loss Current: 9.2950, Validation Loss AVG: 9.6318
Epoch [34/80], Training Loss: 37.2702, Validation Loss Current: 9.1866, Validation Loss AVG: 9.5701
Epoch [35/80], Training Loss: 36.9816, Validation Loss Current: 9.1142, Validation Loss AVG: 9.5900
Epoch [36/80], Training Loss: 37.3741, Validation Loss Current: 9.0532, Validation Loss AVG: 9.5171
Epoch [37/80], Training Loss: 36.6791, Validation Loss Current: 8.8996, Validation Loss AVG: 9.4848
Epoch [38/80], Training Loss: 36.9482, Validation Loss Current: 8.8450, Validation Loss AVG: 9.5011
Epoch [39/80], Training Loss: 36.4759, Validation Loss Current: 8.7829, Validation Loss AVG: 9.4375
Epoch [40/80], Training Loss: 36.0271, Validation Loss Current: 8.6572, Validation Loss AVG: 9.3835
Epoch [41/80], Training Loss: 36.4850, Validation Loss Current: 8.7827, Validation Loss AVG: 9.6115
Epoch [42/80], Training Loss: 37.2282, Validation Loss Current: 8.7730, Validation Loss AVG: 9.4327
Epoch [43/80], Training Loss: 35.6193, Validation Loss Current: 8.5252, Validation Loss AVG: 9.2807
Epoch [44/80], Training Loss: 34.5358, Validation Loss Current: 8.3554, Validation Loss AVG: 9.2123
Epoch [45/80], Training Loss: 34.0381, Validation Loss Current: 8.5711, Validation Loss AVG: 9.7216
Epoch [46/80], Training Loss: 34.0427, Validation Loss Current: 8.1751, Validation Loss AVG: 9.2964
Epoch [47/80], Training Loss: 34.6365, Validation Loss Current: 8.0178, Validation Loss AVG: 9.1148
Epoch [48/80], Training Loss: 33.2691, Validation Loss Current: 7.9951, Validation Loss AVG: 8.9677
Epoch [49/80], Training Loss: 33.3036, Validation Loss Current: 7.9860, Validation Loss AVG: 9.3020
Epoch [50/80], Training Loss: 33.4882, Validation Loss Current: 9.3058, Validation Loss AVG: 10.9098
Epoch [51/80], Training Loss: 35.6810, Validation Loss Current: 8.0621, Validation Loss AVG: 8.8149
Epoch [52/80], Training Loss: 32.9212, Validation Loss Current: 8.1236, Validation Loss AVG: 8.8725
Epoch [53/80], Training Loss: 32.4606, Validation Loss Current: 7.7926, Validation Loss AVG: 9.1367
Epoch [54/80], Training Loss: 30.9049, Validation Loss Current: 7.7463, Validation Loss AVG: 8.8756
Epoch [55/80], Training Loss: 31.7544, Validation Loss Current: 7.6867, Validation Loss AVG: 9.4790
Epoch [56/80], Training Loss: 31.3872, Validation Loss Current: 7.9741, Validation Loss AVG: 8.8736
Epoch [57/80], Training Loss: 31.9476, Validation Loss Current: 7.6374, Validation Loss AVG: 9.2878
Epoch [58/80], Training Loss: 30.6841, Validation Loss Current: 7.7925, Validation Loss AVG: 9.7939
Epoch [59/80], Training Loss: 30.0228, Validation Loss Current: 7.4842, Validation Loss AVG: 9.4135
Epoch [60/80], Training Loss: 30.8131, Validation Loss Current: 8.0889, Validation Loss AVG: 10.3871
Epoch [61/80], Training Loss: 33.9398, Validation Loss Current: 7.8505, Validation Loss AVG: 9.2215
Epoch [62/80], Training Loss: 30.4783, Validation Loss Current: 7.3045, Validation Loss AVG: 9.0678
Epoch [63/80], Training Loss: 29.9227, Validation Loss Current: 7.5002, Validation Loss AVG: 9.3175
Epoch [64/80], Training Loss: 29.1513, Validation Loss Current: 7.1852, Validation Loss AVG: 8.8330
Epoch [65/80], Training Loss: 28.1811, Validation Loss Current: 8.0919, Validation Loss AVG: 8.8614
Epoch [66/80], Training Loss: 29.3698, Validation Loss Current: 7.3977, Validation Loss AVG: 8.5289
Epoch [67/80], Training Loss: 29.1263, Validation Loss Current: 7.0875, Validation Loss AVG: 8.4653
Epoch [68/80], Training Loss: 28.6756, Validation Loss Current: 8.0788, Validation Loss AVG: 10.8915
Epoch [69/80], Training Loss: 30.7460, Validation Loss Current: 6.8633, Validation Loss AVG: 8.3411
Epoch [70/80], Training Loss: 28.2988, Validation Loss Current: 6.6820, Validation Loss AVG: 8.7218
Epoch [71/80], Training Loss: 28.5632, Validation Loss Current: 7.5587, Validation Loss AVG: 8.5954
Epoch [72/80], Training Loss: 30.4085, Validation Loss Current: 6.7597, Validation Loss AVG: 8.5287
Epoch [73/80], Training Loss: 28.6023, Validation Loss Current: 6.7356, Validation Loss AVG: 8.9346
Epoch [74/80], Training Loss: 27.7967, Validation Loss Current: 6.6833, Validation Loss AVG: 8.8051
Epoch [75/80], Training Loss: 27.9681, Validation Loss Current: 6.5097, Validation Loss AVG: 8.5721
Epoch [76/80], Training Loss: 27.4458, Validation Loss Current: 7.4820, Validation Loss AVG: 10.0481
Epoch [77/80], Training Loss: 27.8989, Validation Loss Current: 6.6178, Validation Loss AVG: 8.6715
Epoch [78/80], Training Loss: 25.9083, Validation Loss Current: 6.3576, Validation Loss AVG: 8.5883
Epoch [79/80], Training Loss: 26.4961, Validation Loss Current: 7.7598, Validation Loss AVG: 9.2447
Epoch [80/80], Training Loss: 28.7700, Validation Loss Current: 7.4100, Validation Loss AVG: 8.5715
Patch distance: 1 finished training. Best epoch: 78 Best val accuracy: 0.39276315789473687 Best val loss: 6.357571125030518


Current group: 0.4
Epoch [1/80], Training Loss: 35.8659, Validation Loss Current: 8.6247, Validation Loss AVG: 8.0918
Epoch [2/80], Training Loss: 33.2256, Validation Loss Current: 8.2790, Validation Loss AVG: 7.8331
Epoch [3/80], Training Loss: 33.1005, Validation Loss Current: 8.2146, Validation Loss AVG: 8.2599
Epoch [4/80], Training Loss: 32.3636, Validation Loss Current: 7.7887, Validation Loss AVG: 7.5513
Epoch [5/80], Training Loss: 32.2235, Validation Loss Current: 7.9655, Validation Loss AVG: 8.1881
Epoch [6/80], Training Loss: 32.5525, Validation Loss Current: 7.7915, Validation Loss AVG: 7.6114
Epoch [7/80], Training Loss: 31.5501, Validation Loss Current: 8.3523, Validation Loss AVG: 8.7034
Epoch [8/80], Training Loss: 33.0929, Validation Loss Current: 8.2175, Validation Loss AVG: 7.7661
Epoch [9/80], Training Loss: 32.4453, Validation Loss Current: 7.7244, Validation Loss AVG: 7.7493
Epoch [10/80], Training Loss: 31.1942, Validation Loss Current: 7.7451, Validation Loss AVG: 7.4193
Epoch [11/80], Training Loss: 33.6669, Validation Loss Current: 7.9387, Validation Loss AVG: 7.7579
Epoch [12/80], Training Loss: 34.1036, Validation Loss Current: 7.8533, Validation Loss AVG: 7.7938
Epoch [13/80], Training Loss: 31.7938, Validation Loss Current: 8.1111, Validation Loss AVG: 8.2185
Epoch [14/80], Training Loss: 32.9800, Validation Loss Current: 7.9572, Validation Loss AVG: 7.6726
Epoch [15/80], Training Loss: 30.9905, Validation Loss Current: 7.6148, Validation Loss AVG: 7.6030
Epoch [16/80], Training Loss: 29.6095, Validation Loss Current: 7.5392, Validation Loss AVG: 7.4329
Epoch [17/80], Training Loss: 30.9421, Validation Loss Current: 7.2304, Validation Loss AVG: 7.1898
Epoch [18/80], Training Loss: 28.8610, Validation Loss Current: 7.4113, Validation Loss AVG: 7.6058
Epoch [19/80], Training Loss: 30.2974, Validation Loss Current: 7.3771, Validation Loss AVG: 7.2091
Epoch [20/80], Training Loss: 29.2704, Validation Loss Current: 7.3381, Validation Loss AVG: 7.4784
Epoch [21/80], Training Loss: 29.5136, Validation Loss Current: 7.3636, Validation Loss AVG: 7.4083
Epoch [22/80], Training Loss: 28.0377, Validation Loss Current: 7.1933, Validation Loss AVG: 7.4233
Epoch [23/80], Training Loss: 28.3026, Validation Loss Current: 7.1204, Validation Loss AVG: 7.0735
Epoch [24/80], Training Loss: 28.8420, Validation Loss Current: 7.0489, Validation Loss AVG: 7.2396
Epoch [25/80], Training Loss: 27.6899, Validation Loss Current: 8.2429, Validation Loss AVG: 8.6342
Epoch [26/80], Training Loss: 30.2412, Validation Loss Current: 7.4591, Validation Loss AVG: 7.4570
Epoch [27/80], Training Loss: 27.7538, Validation Loss Current: 8.4033, Validation Loss AVG: 8.3478
Epoch [28/80], Training Loss: 30.9451, Validation Loss Current: 7.3267, Validation Loss AVG: 7.6461
Epoch [29/80], Training Loss: 28.7338, Validation Loss Current: 6.8783, Validation Loss AVG: 7.0853
Epoch [30/80], Training Loss: 26.9444, Validation Loss Current: 6.9449, Validation Loss AVG: 7.2400
Epoch [31/80], Training Loss: 26.8190, Validation Loss Current: 7.0756, Validation Loss AVG: 7.3510
Epoch [32/80], Training Loss: 28.8379, Validation Loss Current: 7.0838, Validation Loss AVG: 7.3736
Epoch [33/80], Training Loss: 28.0279, Validation Loss Current: 7.0420, Validation Loss AVG: 7.3550
Epoch [34/80], Training Loss: 26.7082, Validation Loss Current: 6.9321, Validation Loss AVG: 7.3560
Epoch [35/80], Training Loss: 26.3938, Validation Loss Current: 7.5052, Validation Loss AVG: 7.9357
Epoch [36/80], Training Loss: 28.3756, Validation Loss Current: 6.8648, Validation Loss AVG: 7.2079
Epoch [37/80], Training Loss: 27.6545, Validation Loss Current: 7.0834, Validation Loss AVG: 7.6328
Epoch [38/80], Training Loss: 25.3733, Validation Loss Current: 6.9410, Validation Loss AVG: 7.3431
Epoch [39/80], Training Loss: 24.7421, Validation Loss Current: 6.7179, Validation Loss AVG: 7.1957
Epoch [40/80], Training Loss: 25.7552, Validation Loss Current: 7.3229, Validation Loss AVG: 7.8966
Epoch [41/80], Training Loss: 24.6223, Validation Loss Current: 6.7176, Validation Loss AVG: 7.0155
Epoch [42/80], Training Loss: 25.8719, Validation Loss Current: 7.6594, Validation Loss AVG: 8.1249
Epoch [43/80], Training Loss: 28.1120, Validation Loss Current: 7.1094, Validation Loss AVG: 7.3984
Epoch [44/80], Training Loss: 24.9440, Validation Loss Current: 7.1051, Validation Loss AVG: 7.9043
Epoch [45/80], Training Loss: 25.8298, Validation Loss Current: 6.8791, Validation Loss AVG: 7.5379
Epoch [46/80], Training Loss: 26.8985, Validation Loss Current: 7.0258, Validation Loss AVG: 7.3611
Epoch [47/80], Training Loss: 24.5428, Validation Loss Current: 6.7342, Validation Loss AVG: 7.3334
Epoch [48/80], Training Loss: 23.3858, Validation Loss Current: 6.6005, Validation Loss AVG: 7.3267
Epoch [49/80], Training Loss: 22.4604, Validation Loss Current: 7.2807, Validation Loss AVG: 7.8064
Epoch [50/80], Training Loss: 23.0237, Validation Loss Current: 6.5389, Validation Loss AVG: 7.2514
Epoch [51/80], Training Loss: 22.6060, Validation Loss Current: 6.7872, Validation Loss AVG: 7.5374
Epoch [52/80], Training Loss: 22.1477, Validation Loss Current: 7.2833, Validation Loss AVG: 8.1869
Epoch [53/80], Training Loss: 22.7293, Validation Loss Current: 6.4838, Validation Loss AVG: 7.4292
Epoch [54/80], Training Loss: 22.3138, Validation Loss Current: 6.6761, Validation Loss AVG: 7.4733
Epoch [55/80], Training Loss: 22.4292, Validation Loss Current: 7.2212, Validation Loss AVG: 8.2353
Epoch [56/80], Training Loss: 22.8872, Validation Loss Current: 8.3398, Validation Loss AVG: 8.7692
Epoch [57/80], Training Loss: 25.7868, Validation Loss Current: 6.6910, Validation Loss AVG: 7.3791
Epoch [58/80], Training Loss: 23.6816, Validation Loss Current: 6.7575, Validation Loss AVG: 7.5107
Epoch [59/80], Training Loss: 21.9905, Validation Loss Current: 6.4224, Validation Loss AVG: 7.3682
Epoch [60/80], Training Loss: 20.6193, Validation Loss Current: 6.5190, Validation Loss AVG: 7.6401
Epoch [61/80], Training Loss: 19.5671, Validation Loss Current: 6.7888, Validation Loss AVG: 7.8701
Epoch [62/80], Training Loss: 19.1044, Validation Loss Current: 6.4630, Validation Loss AVG: 7.4838
Epoch [63/80], Training Loss: 18.8862, Validation Loss Current: 7.5489, Validation Loss AVG: 8.2136
Epoch [64/80], Training Loss: 19.4330, Validation Loss Current: 6.5671, Validation Loss AVG: 7.6344
Epoch [65/80], Training Loss: 22.2377, Validation Loss Current: 7.8150, Validation Loss AVG: 8.8987
Epoch [66/80], Training Loss: 24.6138, Validation Loss Current: 7.0728, Validation Loss AVG: 7.7108
Epoch [67/80], Training Loss: 21.0028, Validation Loss Current: 7.1292, Validation Loss AVG: 8.0270
Epoch [68/80], Training Loss: 18.4999, Validation Loss Current: 7.0614, Validation Loss AVG: 8.0059
Epoch [69/80], Training Loss: 18.8660, Validation Loss Current: 6.9820, Validation Loss AVG: 8.0870
Epoch [70/80], Training Loss: 17.1514, Validation Loss Current: 6.9855, Validation Loss AVG: 8.0663
Epoch [71/80], Training Loss: 16.1962, Validation Loss Current: 6.8727, Validation Loss AVG: 7.8616
Epoch [72/80], Training Loss: 15.8155, Validation Loss Current: 9.8105, Validation Loss AVG: 10.6716
Epoch [73/80], Training Loss: 18.9305, Validation Loss Current: 10.2687, Validation Loss AVG: 10.8231
Epoch [74/80], Training Loss: 18.9050, Validation Loss Current: 6.8930, Validation Loss AVG: 8.1209
Epoch [75/80], Training Loss: 16.8699, Validation Loss Current: 8.3427, Validation Loss AVG: 9.3817
Epoch [76/80], Training Loss: 17.6368, Validation Loss Current: 6.7671, Validation Loss AVG: 7.8696
Epoch [77/80], Training Loss: 15.2289, Validation Loss Current: 8.7761, Validation Loss AVG: 9.8432
Epoch [78/80], Training Loss: 25.4838, Validation Loss Current: 7.2170, Validation Loss AVG: 7.7067
Epoch [79/80], Training Loss: 19.6730, Validation Loss Current: 8.5106, Validation Loss AVG: 9.3392
 --- Early Stopped ---
Patch distance: 0.4 finished training. Best epoch: 59 Best val accuracy: 0.4677631578947368 Best val loss: 6.42244827747345


Current group: 0.6
Epoch [1/80], Training Loss: 25.7569, Validation Loss Current: 6.7942, Validation Loss AVG: 7.9626
Epoch [2/80], Training Loss: 22.5531, Validation Loss Current: 6.1772, Validation Loss AVG: 7.4680
Epoch [3/80], Training Loss: 21.3415, Validation Loss Current: 6.0269, Validation Loss AVG: 7.0639
Epoch [4/80], Training Loss: 20.9687, Validation Loss Current: 6.0715, Validation Loss AVG: 7.2414
Epoch [5/80], Training Loss: 20.3568, Validation Loss Current: 6.1369, Validation Loss AVG: 7.5768
Epoch [6/80], Training Loss: 20.4576, Validation Loss Current: 6.4542, Validation Loss AVG: 7.8534
Epoch [7/80], Training Loss: 20.6562, Validation Loss Current: 6.2810, Validation Loss AVG: 7.7996
Epoch [8/80], Training Loss: 22.3468, Validation Loss Current: 6.0804, Validation Loss AVG: 7.6352
Epoch [9/80], Training Loss: 20.7833, Validation Loss Current: 6.0398, Validation Loss AVG: 7.3266
Epoch [10/80], Training Loss: 19.5447, Validation Loss Current: 6.1064, Validation Loss AVG: 7.5235
Epoch [11/80], Training Loss: 17.6978, Validation Loss Current: 6.1218, Validation Loss AVG: 7.3954
Epoch [12/80], Training Loss: 18.6795, Validation Loss Current: 7.0767, Validation Loss AVG: 8.1255
Epoch [13/80], Training Loss: 18.6397, Validation Loss Current: 5.8218, Validation Loss AVG: 7.2359
Epoch [14/80], Training Loss: 16.8200, Validation Loss Current: 5.5523, Validation Loss AVG: 7.1278
Epoch [15/80], Training Loss: 16.4012, Validation Loss Current: 6.1454, Validation Loss AVG: 7.4583
Epoch [16/80], Training Loss: 16.4150, Validation Loss Current: 6.6442, Validation Loss AVG: 8.0515
Epoch [17/80], Training Loss: 15.8792, Validation Loss Current: 5.9642, Validation Loss AVG: 7.7270
Epoch [18/80], Training Loss: 17.3022, Validation Loss Current: 5.7108, Validation Loss AVG: 7.4940
Epoch [19/80], Training Loss: 16.5402, Validation Loss Current: 6.0378, Validation Loss AVG: 7.6260
Epoch [20/80], Training Loss: 13.9564, Validation Loss Current: 6.2387, Validation Loss AVG: 7.6819
Epoch [21/80], Training Loss: 12.5743, Validation Loss Current: 5.8909, Validation Loss AVG: 7.7104
Epoch [22/80], Training Loss: 13.9225, Validation Loss Current: 7.4943, Validation Loss AVG: 9.4420
Epoch [23/80], Training Loss: 19.0700, Validation Loss Current: 6.0079, Validation Loss AVG: 7.3481
Epoch [24/80], Training Loss: 16.1183, Validation Loss Current: 5.7303, Validation Loss AVG: 7.5695
Epoch [25/80], Training Loss: 15.6334, Validation Loss Current: 14.0454, Validation Loss AVG: 14.9358
Epoch [26/80], Training Loss: 35.4385, Validation Loss Current: 7.0181, Validation Loss AVG: 7.9683
Epoch [27/80], Training Loss: 23.4400, Validation Loss Current: 6.0593, Validation Loss AVG: 7.1893
Epoch [28/80], Training Loss: 19.0119, Validation Loss Current: 5.5277, Validation Loss AVG: 6.9396
Epoch [29/80], Training Loss: 18.8286, Validation Loss Current: 5.6831, Validation Loss AVG: 7.1149
Epoch [30/80], Training Loss: 16.6241, Validation Loss Current: 6.7578, Validation Loss AVG: 8.1468
Epoch [31/80], Training Loss: 19.9896, Validation Loss Current: 6.3437, Validation Loss AVG: 8.3901
Epoch [32/80], Training Loss: 16.7095, Validation Loss Current: 6.5754, Validation Loss AVG: 8.6775
Epoch [33/80], Training Loss: 14.5605, Validation Loss Current: 5.9980, Validation Loss AVG: 7.8851
Epoch [34/80], Training Loss: 14.1397, Validation Loss Current: 5.7783, Validation Loss AVG: 7.6248
Epoch [35/80], Training Loss: 13.6199, Validation Loss Current: 5.9885, Validation Loss AVG: 7.7876
Epoch [36/80], Training Loss: 11.1716, Validation Loss Current: 6.7704, Validation Loss AVG: 8.6772
Epoch [37/80], Training Loss: 12.0701, Validation Loss Current: 13.5183, Validation Loss AVG: 14.8084
Epoch [38/80], Training Loss: 24.6218, Validation Loss Current: 6.0220, Validation Loss AVG: 7.4770
Epoch [39/80], Training Loss: 16.7383, Validation Loss Current: 5.8363, Validation Loss AVG: 7.6088
Epoch [40/80], Training Loss: 13.7479, Validation Loss Current: 5.9268, Validation Loss AVG: 7.7748
Epoch [41/80], Training Loss: 11.9063, Validation Loss Current: 6.6662, Validation Loss AVG: 9.1177
Epoch [42/80], Training Loss: 10.2693, Validation Loss Current: 6.6342, Validation Loss AVG: 8.5667
Epoch [43/80], Training Loss: 9.8679, Validation Loss Current: 7.1266, Validation Loss AVG: 9.8189
Epoch [44/80], Training Loss: 10.2194, Validation Loss Current: 7.0131, Validation Loss AVG: 9.5080
Epoch [45/80], Training Loss: 8.1551, Validation Loss Current: 6.6648, Validation Loss AVG: 9.5052
Epoch [46/80], Training Loss: 8.7361, Validation Loss Current: 9.1802, Validation Loss AVG: 12.3985
Epoch [47/80], Training Loss: 11.5707, Validation Loss Current: 7.0209, Validation Loss AVG: 8.9463
Epoch [48/80], Training Loss: 7.7219, Validation Loss Current: 7.4640, Validation Loss AVG: 9.8727
 --- Early Stopped ---
Patch distance: 0.6 finished training. Best epoch: 28 Best val accuracy: 0.5171052631578947 Best val loss: 5.527658641338348


Current group: 0.8
Epoch [1/80], Training Loss: 17.9681, Validation Loss Current: 5.4999, Validation Loss AVG: 7.4236
Epoch [2/80], Training Loss: 15.1449, Validation Loss Current: 7.2585, Validation Loss AVG: 11.1834
Epoch [3/80], Training Loss: 17.7359, Validation Loss Current: 6.3047, Validation Loss AVG: 9.5835
Epoch [4/80], Training Loss: 14.4633, Validation Loss Current: 5.8355, Validation Loss AVG: 8.9086
Epoch [5/80], Training Loss: 12.5042, Validation Loss Current: 6.1267, Validation Loss AVG: 8.8381
Epoch [6/80], Training Loss: 11.3202, Validation Loss Current: 6.9991, Validation Loss AVG: 9.3579
Epoch [7/80], Training Loss: 10.9656, Validation Loss Current: 6.1902, Validation Loss AVG: 9.1165
Epoch [8/80], Training Loss: 9.9203, Validation Loss Current: 6.5063, Validation Loss AVG: 10.7267
Epoch [9/80], Training Loss: 9.7248, Validation Loss Current: 6.2997, Validation Loss AVG: 9.8705
Epoch [10/80], Training Loss: 8.4296, Validation Loss Current: 6.9996, Validation Loss AVG: 9.7767
Epoch [11/80], Training Loss: 8.6647, Validation Loss Current: 7.7581, Validation Loss AVG: 11.6584
Epoch [12/80], Training Loss: 13.6115, Validation Loss Current: 7.5916, Validation Loss AVG: 11.1735
Epoch [13/80], Training Loss: 16.5272, Validation Loss Current: 5.5953, Validation Loss AVG: 8.2077
Epoch [14/80], Training Loss: 10.1835, Validation Loss Current: 6.2249, Validation Loss AVG: 9.9228
Epoch [15/80], Training Loss: 8.0781, Validation Loss Current: 6.8395, Validation Loss AVG: 10.7122
Epoch [16/80], Training Loss: 8.2117, Validation Loss Current: 6.4365, Validation Loss AVG: 10.4127
Epoch [17/80], Training Loss: 6.7574, Validation Loss Current: 7.9916, Validation Loss AVG: 11.2739
Epoch [18/80], Training Loss: 6.2867, Validation Loss Current: 7.8727, Validation Loss AVG: 12.5440
Epoch [19/80], Training Loss: 11.9869, Validation Loss Current: 6.7619, Validation Loss AVG: 9.4414
Epoch [20/80], Training Loss: 9.5733, Validation Loss Current: 7.3174, Validation Loss AVG: 11.3372
Epoch [21/80], Training Loss: 7.4145, Validation Loss Current: 8.7322, Validation Loss AVG: 14.1103
 --- Early Stopped ---
Patch distance: 0.8 finished training. Best epoch: 1 Best val accuracy: 0.5269736842105263 Best val loss: 5.4998931884765625


Current group: 0.2
Epoch [1/80], Training Loss: 44.4761, Validation Loss Current: 9.5219, Validation Loss AVG: 8.8198
Epoch [2/80], Training Loss: 37.9813, Validation Loss Current: 9.8378, Validation Loss AVG: 8.8728
Epoch [3/80], Training Loss: 38.5687, Validation Loss Current: 9.0048, Validation Loss AVG: 8.1162
Epoch [4/80], Training Loss: 34.9559, Validation Loss Current: 8.9847, Validation Loss AVG: 8.4374
Epoch [5/80], Training Loss: 34.8792, Validation Loss Current: 8.0902, Validation Loss AVG: 7.4680
Epoch [6/80], Training Loss: 32.9245, Validation Loss Current: 8.1124, Validation Loss AVG: 8.1097
Epoch [7/80], Training Loss: 31.6439, Validation Loss Current: 7.8860, Validation Loss AVG: 7.3967
Epoch [8/80], Training Loss: 30.8448, Validation Loss Current: 7.7068, Validation Loss AVG: 7.5417
Epoch [9/80], Training Loss: 28.5158, Validation Loss Current: 7.5444, Validation Loss AVG: 7.6103
Epoch [10/80], Training Loss: 27.3734, Validation Loss Current: 8.7579, Validation Loss AVG: 8.3630
Epoch [11/80], Training Loss: 29.5837, Validation Loss Current: 8.7921, Validation Loss AVG: 8.3888
Epoch [12/80], Training Loss: 33.0659, Validation Loss Current: 8.4156, Validation Loss AVG: 9.0538
Epoch [13/80], Training Loss: 32.3276, Validation Loss Current: 8.0814, Validation Loss AVG: 8.4420
Epoch [14/80], Training Loss: 30.1860, Validation Loss Current: 8.1569, Validation Loss AVG: 8.8822
Epoch [15/80], Training Loss: 28.9306, Validation Loss Current: 7.6187, Validation Loss AVG: 8.3033
Epoch [16/80], Training Loss: 27.1583, Validation Loss Current: 7.5725, Validation Loss AVG: 8.2383
Epoch [17/80], Training Loss: 25.4332, Validation Loss Current: 8.6553, Validation Loss AVG: 8.6863
Epoch [18/80], Training Loss: 28.8909, Validation Loss Current: 8.3997, Validation Loss AVG: 8.1344
Epoch [19/80], Training Loss: 29.0550, Validation Loss Current: 7.7769, Validation Loss AVG: 9.6546
Epoch [20/80], Training Loss: 25.9530, Validation Loss Current: 7.5720, Validation Loss AVG: 9.2740
Epoch [21/80], Training Loss: 23.8082, Validation Loss Current: 7.8533, Validation Loss AVG: 9.7723
Epoch [22/80], Training Loss: 24.0838, Validation Loss Current: 7.5094, Validation Loss AVG: 8.8696
Epoch [23/80], Training Loss: 22.8742, Validation Loss Current: 7.6084, Validation Loss AVG: 8.7415
Epoch [24/80], Training Loss: 20.9729, Validation Loss Current: 7.9206, Validation Loss AVG: 9.8951
Epoch [25/80], Training Loss: 21.3001, Validation Loss Current: 8.2695, Validation Loss AVG: 9.6727
Epoch [26/80], Training Loss: 22.2797, Validation Loss Current: 8.5977, Validation Loss AVG: 9.5176
Epoch [27/80], Training Loss: 21.4545, Validation Loss Current: 8.0853, Validation Loss AVG: 9.6937
Epoch [28/80], Training Loss: 20.5102, Validation Loss Current: 7.8236, Validation Loss AVG: 9.7071
Epoch [29/80], Training Loss: 18.4788, Validation Loss Current: 8.6070, Validation Loss AVG: 10.8422
Epoch [30/80], Training Loss: 17.8406, Validation Loss Current: 8.5231, Validation Loss AVG: 9.6884
Epoch [31/80], Training Loss: 16.5453, Validation Loss Current: 8.6115, Validation Loss AVG: 10.4076
Epoch [32/80], Training Loss: 15.1628, Validation Loss Current: 9.1932, Validation Loss AVG: 11.3505
Epoch [33/80], Training Loss: 15.2299, Validation Loss Current: 9.7280, Validation Loss AVG: 11.8685
Epoch [34/80], Training Loss: 17.6516, Validation Loss Current: 9.5575, Validation Loss AVG: 12.4008
Epoch [35/80], Training Loss: 25.8334, Validation Loss Current: 9.1112, Validation Loss AVG: 9.8925
Epoch [36/80], Training Loss: 23.1858, Validation Loss Current: 9.4817, Validation Loss AVG: 9.8157
Epoch [37/80], Training Loss: 22.7148, Validation Loss Current: 8.6950, Validation Loss AVG: 9.7878
Epoch [38/80], Training Loss: 23.2417, Validation Loss Current: 9.5409, Validation Loss AVG: 10.0889
Epoch [39/80], Training Loss: 21.0394, Validation Loss Current: 8.5705, Validation Loss AVG: 10.4443
Epoch [40/80], Training Loss: 24.2224, Validation Loss Current: 7.6975, Validation Loss AVG: 9.5692
Epoch [41/80], Training Loss: 21.0481, Validation Loss Current: 8.2700, Validation Loss AVG: 10.7001
Epoch [42/80], Training Loss: 17.5397, Validation Loss Current: 9.5170, Validation Loss AVG: 11.7229
 --- Early Stopped ---
Patch distance: 0.2 finished training. Best epoch: 22 Best val accuracy: 0.4115131578947368 Best val loss: 7.509354948997498


-------------------- All training done --------------------


 --- Evaluating ---
Fold: 0
---- Testing model trained on sequence: [1, 0.4, 0.6, 0.8, 0.2] ----
Test set distance: 1 Top 1 Accuracy: 0.32238349885408707
Test set distance: 0.4 Top 1 Accuracy: 0.4789915966386555
Test set distance: 0.6 Top 1 Accuracy: 0.4186401833460657
Test set distance: 0.8 Top 1 Accuracy: 0.37662337662337664
Test set distance: 0.2 Top 1 Accuracy: 0.4698242933537051
Fold: 1
---- Testing model trained on sequence: [1, 0.4, 0.6, 0.8, 0.2] ----
Test set distance: 1 Top 1 Accuracy: 0.22994652406417113
Test set distance: 0.4 Top 1 Accuracy: 0.37662337662337664
Test set distance: 0.6 Top 1 Accuracy: 0.30786860198624905
Test set distance: 0.8 Top 1 Accuracy: 0.24293353705118412
Test set distance: 0.2 Top 1 Accuracy: 0.4232238349885409
Fold: 2
---- Testing model trained on sequence: [1, 0.4, 0.6, 0.8, 0.2] ----
Test set distance: 1 Top 1 Accuracy: 0.27960275019098546
Test set distance: 0.4 Top 1 Accuracy: 0.4560733384262796
Test set distance: 0.6 Top 1 Accuracy: 0.38808250572956454
Test set distance: 0.8 Top 1 Accuracy: 0.33766233766233766
Test set distance: 0.2 Top 1 Accuracy: 0.4423223834988541
Fold: 3
---- Testing model trained on sequence: [1, 0.4, 0.6, 0.8, 0.2] ----
Test set distance: 1 Top 1 Accuracy: 0.28036669213139803
Test set distance: 0.4 Top 1 Accuracy: 0.4346829640947288
Test set distance: 0.6 Top 1 Accuracy: 0.40106951871657753
Test set distance: 0.8 Top 1 Accuracy: 0.34530175706646293
Test set distance: 0.2 Top 1 Accuracy: 0.4025974025974026
Fold: 4
---- Testing model trained on sequence: [1, 0.4, 0.6, 0.8, 0.2] ----
Test set distance: 1 Top 1 Accuracy: 0.25286478227654696
Test set distance: 0.4 Top 1 Accuracy: 0.45760122230710465
Test set distance: 0.6 Top 1 Accuracy: 0.3758594346829641
Test set distance: 0.8 Top 1 Accuracy: 0.3048128342245989
Test set distance: 0.2 Top 1 Accuracy: 0.4744079449961803
------------------------------ End ------------------------------








